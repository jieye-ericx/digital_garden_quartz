<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>优化 on</title><link>https://jieye-ericx.github.io/tags/%E4%BC%98%E5%8C%96/</link><description>Recent content in 优化 on</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="https://jieye-ericx.github.io/tags/%E4%BC%98%E5%8C%96/index.xml" rel="self" type="application/rss+xml"/><item><title>Mysql join优化</title><link>https://jieye-ericx.github.io/Mysql-join%E4%BC%98%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jieye-ericx.github.io/Mysql-join%E4%BC%98%E5%8C%96/</guid><description>先说结论 首先，使用[[小表]]作为驱动表
如果可以使用 Index Nested-Loop Join 算法，也就是说可以用上被驱动表上的索引，其实是没问题的； 如果使用 Block Nested-Loop Join 算法，扫描行数就会过多。尤其是在大表上的 join 操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种 join 尽量不要用。 判断要不要使用 join 语句时，就是看 explain 结果里面，Extra 字段里面有没有出现“Block Nested Loop”字样</description></item><item><title>使用LinkedBlockingQueue时线程池的坑</title><link>https://jieye-ericx.github.io/%E4%BD%BF%E7%94%A8LinkedBlockingQueue%E6%97%B6%E7%BA%BF%E7%A8%8B%E6%B1%A0%E7%9A%84%E5%9D%91/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jieye-ericx.github.io/%E4%BD%BF%E7%94%A8LinkedBlockingQueue%E6%97%B6%E7%BA%BF%E7%A8%8B%E6%B1%A0%E7%9A%84%E5%9D%91/</guid><description>最近遇到一个问题HTTP 线程快速上升，外部服务调用超时，服务本身响应时间超长
现象 应用平常运行正常没有任何问题，突然有一天应用的访问量爆增，出现外部服务调用超时，服务本身响应时间超长，HTTP线程快速上升，但CPU利用率并不高
定位思路 分析这类线程异常问题，最直接有效的方法是查看线程的 dump
根据dump极有可能是线程池设置不合理导致，查看线程池 cachedTreadPool 的定义，发现定义如下：
核心线程数为 cpu 的数量2倍，最大线程数为 cpu 的数量乘以配置的数量，任务队列为 LinkedBlockingQueue，也就是最大数量为 Integer.MAX_VALUE，对线程池了解的话，这样的设置会导致线程池的任务堆积。因为线程池默认实现是核心线程数满了之后，再填充任务队列，如果线程数量小于最大线程数再创建线程。而这儿设置的任务队列几乎是无界的，也就是说这个线程池真正工作的线程只会是 cpu 的数量的 2 倍。</description></item><item><title>系统监控异常处理</title><link>https://jieye-ericx.github.io/%E7%B3%BB%E7%BB%9F%E7%9B%91%E6%8E%A7%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jieye-ericx.github.io/%E7%B3%BB%E7%BB%9F%E7%9B%91%E6%8E%A7%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/</guid><description>写起来思路很清晰，但实际上整个过程花费了大半天的时间，其实排查的过程中，有很多关键点没有抓到，有很多现象，是可以凭经验条件反射的推断出原因的：
看到cpu使用率上涨，用jstack看使用cpu的线程，以及该线程在跑什么代码。 找到是gc线程，然后看gc曲线是否正常。 看堆内存曲线，正常的曲线是锯齿形的，如果不是，一次full GC之后内存没有明显下降，那基本可以推断发生内存泄漏了。 怀疑是内存泄漏的问题，可以跑jmap，然后拉到MAT分析。 第四步比较耗时的话，可以同时跑这个命令：jmap -histo pid。看看有没有线索。 jvm的一些工具：jstack、jmap、jstat、jhat。包括可视化工具：MAT、jvisualvm</description></item></channel></rss>