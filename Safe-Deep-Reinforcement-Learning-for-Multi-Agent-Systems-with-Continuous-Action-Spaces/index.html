<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="ç›¸å…³ Safe Exploration in Continuous Action Spaces (Dalal et al., 2018)
Multi-agent actor-critic for mixed cooperative competitive environments (Lowe et al."><meta property="og:title" content><meta property="og:description" content="ç›¸å…³ Safe Exploration in Continuous Action Spaces (Dalal et al., 2018)
Multi-agent actor-critic for mixed cooperative competitive environments (Lowe et al."><meta property="og:type" content="website"><meta property="og:image" content="https://jieye-ericx.github.io/icon.png"><meta property="og:url" content="https://jieye-ericx.github.io/Safe-Deep-Reinforcement-Learning-for-Multi-Agent-Systems-with-Continuous-Action-Spaces/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="ç›¸å…³ Safe Exploration in Continuous Action Spaces (Dalal et al., 2018)
Multi-agent actor-critic for mixed cooperative competitive environments (Lowe et al."><meta name=twitter:image content="https://jieye-ericx.github.io/icon.png"><title>ericx 's æ•°å­—èŠ±å›­</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://jieye-ericx.github.io//icon.png><link href=https://jieye-ericx.github.io/styles.80333fa2099c0bee674efa435fde378c.min.css rel=stylesheet><link href=https://jieye-ericx.github.io/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://jieye-ericx.github.io/js/darkmode.48459b7116d092b4e98d2cab704cad80.min.js></script>
<script src=https://jieye-ericx.github.io/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://jieye-ericx.github.io/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://jieye-ericx.github.io/",fetchData=Promise.all([fetch("https://jieye-ericx.github.io/indices/linkIndex.498f8802e86f71aa2af4d45d6c4096d6.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://jieye-ericx.github.io/indices/contentIndex.5368a39ad15318a851f2d088b43f3a36.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://jieye-ericx.github.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://jieye-ericx.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/jieye-ericx.github.io\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=jieye-ericx.github.io src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://jieye-ericx.github.io/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://jieye-ericx.github.io/>ericx 's æ•°å­—èŠ±å›­</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated
Unknown
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/Safe%20Deep%20Reinforcement%20Learning%20for%20Multi-Agent%20Systems%20with%20Continuous%20Action%20Spaces.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#ç›¸å…³>ç›¸å…³</a></li><li><a href=#abstract><strong>Abstract</strong></a></li><li><a href=#1-introduction-and-related-work><strong>1 Introduction and Related Work</strong></a><ol><li><a href=#ç¬¬ä¸€æ®µ><strong>ç¬¬ä¸€æ®µ</strong></a></li><li><a href=#ç¬¬äºŒæ®µ>ç¬¬äºŒæ®µ</a></li><li><a href=#ç¬¬ä¸‰æ®µ>ç¬¬ä¸‰æ®µ</a></li><li><a href=#ç¬¬å››æ®µ>ç¬¬å››æ®µ</a></li><li><a href=#ç¬¬äº”æ®µ>ç¬¬äº”æ®µ</a></li></ol></li><li><a href=#2-models-and-methods><strong>2 Models and Methods</strong></a><ol><li><a href=#21-problem-formulation>2.1 <strong>Problem Formulation</strong></a></li><li><a href=#22-safety-signal-model><strong>2.2. Safety Signal Model</strong></a></li><li><a href=#23-safety-layer-optimization><strong>2.3. Safety Layer Optimization</strong></a></li><li><a href=#24-maddpg>2.4. MADDPG</a></li><li><a href=#25-implementation-details><strong>2.5. Implementation Details</strong></a></li></ol></li><li><a href=#3-results>3 Results</a></li><li><a href=#4-conclusion>4 Conclusion</a></li></ol></nav></details></aside><a href=#ç›¸å…³><h2 id=ç›¸å…³><span class=hanchor arialabel=Anchor># </span>ç›¸å…³</h2></a><p>Safe Exploration in Continuous Action Spaces (Dalal et al., 2018)</p><p>Multi-agent actor-critic for mixed cooperative competitive environments (Lowe et al., 2017b)</p><p>å‡¸äºŒæ¬¡ä¼˜åŒ–é—®é¢˜</p><a href=#abstract><h2 id=abstract><span class=hanchor arialabel=Anchor># </span><strong>Abstract</strong></h2></a><p>å¤šä»£ç†æ§åˆ¶é—®é¢˜æ„æˆäº†å…·æœ‰è¿ç»­è¡ŒåŠ¨ç©ºé—´çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹çš„ä¸€ä¸ªæœ‰è¶£çš„åº”ç”¨é¢†åŸŸã€‚ç„¶è€Œï¼Œè¿™ç§ç°å®ä¸–ç•Œçš„åº”ç”¨é€šå¸¸å¸¦æœ‰å…³é”®çš„å®‰å…¨çº¦æŸï¼Œä¸èƒ½è¢«è¿åã€‚ä¸ºäº†ç¡®ä¿å®‰å…¨ï¼Œ</p><ol><li><strong>æˆ‘ä»¬é€šè¿‡åœ¨æ·±åº¦ç­–ç•¥ç½‘ç»œä¸­æ·»åŠ ä¸€ä¸ªå®‰å…¨å±‚æ¥å¢å¼ºè‘—åçš„å¤šä»£ç†æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼ˆMADDPGï¼‰æ¡†æ¶ã€‚</strong></li><li><strong>æˆ‘ä»¬å°†çº¿æ€§åŒ–å•æ­¥è¿‡æ¸¡åŠ¨æ€çš„æƒ³æ³•æ‰©å±•åˆ°äº†å¤šä»£ç†è®¾ç½®ä¸­ï¼Œæ­£å¦‚Safe DDPGï¼ˆDalalç­‰äººï¼Œ2018ï¼‰ä¸­é’ˆå¯¹å•ä»£ç†ç³»ç»Ÿæ‰€åšçš„é‚£æ ·ã€‚</strong></li><li>æˆ‘ä»¬è¿˜æè®®ä½¿ç”¨è½¯çº¦æŸï¼ˆKerrigan & Maciejowskiï¼Œ2000ï¼‰æ¥è§„é¿è¡ŒåŠ¨ä¿®æ­£æ­¥éª¤ä¸­çš„ä¸å¯è¡Œæ€§é—®é¢˜ã€‚åœ¨æ¸©å’Œçš„å‡è®¾æ¡ä»¶ä¸‹ï¼Œç²¾ç¡®æƒ©ç½šå‡½æ•°ç†è®ºçš„ç»“æœå¯ä»¥ç”¨æ¥ä¿è¯è½¯çº¦æŸçš„æ»¡è¶³ã€‚æˆ‘ä»¬ä»ç»éªŒä¸Šå‘ç°ï¼Œè½¯çº¦æŸçš„è¡¨è¿°å®ç°äº†çº¦æŸè¿åçš„æ€¥å‰§å‡å°‘ï¼Œç”šè‡³åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ä¹Ÿèƒ½ä¿è¯å®‰å…¨ã€‚</li></ol><a href=#1-introduction-and-related-work><h2 id=1-introduction-and-related-work><span class=hanchor arialabel=Anchor># </span><strong>1 Introduction and Related Work</strong></h2></a><a href=#ç¬¬ä¸€æ®µ><h3 id=ç¬¬ä¸€æ®µ><span class=hanchor arialabel=Anchor># </span><strong>ç¬¬ä¸€æ®µ</strong></h3></a><p>In recent years, deep reinforcement learning (Deep RL) with continuous action spaces has received increasing attention in the context of real-world applications such as autonomous driving (Sallab et al., 2017), single- (Gu et al., 2017) and multi robot systems (Hu et al., 2020), as well as data center cooling (Lazic et al., 2018). Contrary to more mature applications of RL such as video games (Mnih et al., 2015), these real-world cases naturally require a set of safety constraints to be fulfifilled (e.g. in the case of robot arms, avoiding obstacles and self-collisions, or limiting angles).</p><p>The main caveat of safety in reinforcement learning is that <strong>the dynamics of the system are a-priori unknown and hence one does not know which actions are safe ahead of time</strong>. Whenever accurate offlfline simulations or a model of the environment are available, safety can be introduced ex-post by correcting a learned policy for example via shielding (Alshiekh et al.,\2018) or via a pre-determined backup controller (Wabersich & Zeilinger, 2018). <strong>Yet, many real-world applications require safety to be enforced during both learning and deployment, and a model of the environment is not always available.</strong></p><p>å…·æœ‰è¿ç»­åŠ¨ä½œç©ºé—´çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ (DeepRL)åœ¨è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººç³»ç»Ÿç­‰åº”ç”¨ä¸­å—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ä¸ç”µå­æ¸¸æˆç­‰æ›´æˆç†Ÿçš„åº”ç”¨ç›¸åï¼Œè¿™äº›ç°å®ä¸–ç•Œçš„æƒ…å†µè‡ªç„¶éœ€è¦æ»¡è¶³ä¸€å¥—å®‰å…¨çº¦æŸ(ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººæ‰‹è‡‚çš„æƒ…å†µä¸‹ï¼Œè¦é¿å…å„ç§ç¢°æ’ç­‰ï¼‰</p><p>å¼ºåŒ–å­¦ä¹ ä¸­å®‰å…¨çš„ä¸»è¦è­¦å‘Šæ˜¯ï¼Œç³»ç»Ÿçš„åŠ¨æ€æ˜¯å…ˆéªŒæœªçŸ¥çš„ï¼Œå› æ­¤äººä»¬ä¸æå‰çŸ¥é“å“ªäº›åŠ¨ä½œæ˜¯å®‰å…¨çš„ã€‚</p><p>åªè¦æœ‰ç²¾ç¡®çš„ç¦»çº¿æ¨¡æ‹Ÿæˆ–ç¯å¢ƒæ¨¡å‹ï¼Œå°±å¯ä»¥é€šè¿‡çº æ­£å­¦ä¹ åˆ°çš„ç­–ç•¥ï¼Œä¾‹å¦‚é€šè¿‡å±è”½(Alshiekhç­‰äººï¼Œ2018)æˆ–é€šè¿‡é¢„å…ˆç¡®å®šçš„å¤‡ä»½æ§åˆ¶å™¨(Wabersich&æ³½æ—æ ¼ï¼Œ2018)æ¥é¢„å…ˆå¼•å…¥å®‰å…¨æ€§ã€‚</p><p>ç„¶è€Œï¼Œè®¸å¤šç°å®ä¸–ç•Œçš„åº”ç”¨ç¨‹åºåœ¨å­¦ä¹ å’Œéƒ¨ç½²æœŸé—´éƒ½éœ€è¦å¼ºåˆ¶æ‰§è¡Œå®‰å…¨æ€§ï¼Œè€Œä¸”ç¯å¢ƒæ¨¡å‹å¹¶ä¸æ€»æ˜¯å¯ç”¨çš„ã€‚</p><a href=#ç¬¬äºŒæ®µ><h3 id=ç¬¬äºŒæ®µ><span class=hanchor arialabel=Anchor># </span>ç¬¬äºŒæ®µ</h3></a><p>A growing line of research addresses the problem of safety of the learning process in model-free settings.</p><p><strong>A traditional approach is reward shaping, where one attempts to encode information on undesirable actions state pairs in the reward function.</strong> ç¼ºç‚¹ï¼šUnfortunately, this approach comes with the downside that unsafe behavior is discouraged only as long as the relevant trajectories remain stored in the
<a href=https://blog.csdn.net/suoyan1539/article/details/79571010 rel=noopener><strong>experience replay buffer</strong></a>.</p><p>In (Lipton et al., 2018), the authors propose Intrinsic Fear, a framework that mitigates this issue by training a neural network to identify unsafe states, which is then used in shaping the reward function. Although this approach alleviates the problem of periodically revisiting unsafe states, it is still required to visit those states to gather enough information to avoid them.</p><p>ç¬¬äºŒæ®µä½œè€…ä»‹ç»äº†è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶çº¿è§£å†³äº†åœ¨æ— æ¨¡å‹ç¯å¢ƒä¸­å­¦ä¹ è¿‡ç¨‹çš„å®‰å…¨æ€§é—®é¢˜ã€‚</p><p>**ä¸€ç§ä¼ ç»Ÿçš„æ–¹æ³•æ˜¯å¥–åŠ±å¡‘é€ ï¼Œå³äººä»¬è¯•å›¾å¯¹å¥–åŠ±å‡½æ•°ä¸­çš„ä¸è‰¯è¡Œä¸ºçŠ¶æ€å¯¹è¿›è¡Œç¼–ç ã€‚ä¸å¹¸çš„æ˜¯ï¼Œè¿™ç§æ–¹æ³•çš„ç¼ºç‚¹æ˜¯ï¼Œåªæœ‰å½“ç›¸å…³çš„è½¨è¿¹ä»ç„¶å­˜å‚¨åœ¨ç»éªŒå›æ”¾ç¼“å†²åŒº(experience replay buffer)ä¸­æ—¶ï¼Œæ‰ä¸é¼“åŠ±å‡ºç°ä¸å®‰å…¨çš„è¡Œä¸ºã€‚**åœ¨(Liptonetal.ï¼Œ2018)ä¸­ï¼Œä½œè€…æå‡ºäº†å†…åœ¨ææƒ§æ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒç¥ç»ç½‘ç»œè¯†åˆ«ä¸å®‰å…¨çŠ¶æ€æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œç„¶åè¯¥çŠ¶æ€ç”¨äºå¡‘é€ å¥–åŠ±å‡½æ•°ã€‚è™½ç„¶è¿™ç§æ–¹æ³•å‡è½»äº†å®šæœŸé‡æ–°è®¿é—®ä¸å®‰å…¨çŠ¶æ€çš„é—®é¢˜ï¼Œä½†ä»ç„¶éœ€è¦è®¿é—®è¿™äº›çŠ¶æ€ä»¥æ”¶é›†è¶³å¤Ÿçš„ä¿¡æ¯ä»¥é¿å…è®¿é—®å®ƒä»¬ã€‚</p><a href=#ç¬¬ä¸‰æ®µ><h3 id=ç¬¬ä¸‰æ®µ><span class=hanchor arialabel=Anchor># </span>ç¬¬ä¸‰æ®µ</h3></a><p><strong>Another family of approaches focuses on safety for discrete state/action spaces and thus studies the problem through the lens of finite Constrained Markov Decision Processes</strong> (CMDPs) (Altman, 1998). Along those lines, multiple approaches have been proposed. For example in (Efroni et al.,\2020) the authors propose a framework which focuses on learning the underlying CMDP <strong>based purely on logged historical data</strong>.</p><p>ç¼ºç‚¹ï¼šA common limitation to such approaches is that it is hard to generalize to continuous action spaces, although there exists some work on that direction, as in (Chow et al.,\2019) where the authors leverage Lyapunov functions to handle constraints in continuous settings.</p><p><strong>å¦ä¸€ç±»æ–¹æ³•ä¾§é‡äºç¦»æ•£çŠ¶æ€/åŠ¨ä½œç©ºé—´çš„å®‰å…¨æ€§</strong>ï¼Œ<strong>ä»è€Œé€šè¿‡æœ‰é™çº¦æŸé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(CMDPs)çš„é€é•œæ¥ç ”ç©¶è¿™ä¸ªé—®é¢˜</strong>(Altmanï¼Œ1998)ã€‚å› æ­¤ï¼Œå·²ç»æå‡ºäº†å¤šç§æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨(Efroniç­‰äººï¼Œ2020)ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œ<strong>ä¸“æ³¨äºçº¯ç²¹åŸºäºè®°å½•çš„å†å²æ•°æ®å­¦ä¹ åº•å±‚CMDP</strong>ã€‚è¿™ç§æ–¹æ³•çš„ä¸€ä¸ªå…±åŒé™åˆ¶æ˜¯ï¼Œå¾ˆéš¾æ¨å¹¿åˆ°è¿ç»­åŠ¨ä½œç©ºé—´ï¼Œå°½ç®¡åœ¨è¿™ä¸ªæ–¹å‘ä¸Šå­˜åœ¨ä¸€äº›å·¥ä½œï¼Œå¦‚(Chowetal.ï¼Œ2019)ï¼Œä½œè€…åˆ©ç”¨æé›…æ™®è¯ºå¤«å‡½æ•°æ¥å¤„ç†è¿ç»­è®¾ç½®ä¸­çš„çº¦æŸã€‚</p><a href=#ç¬¬å››æ®µ><h3 id=ç¬¬å››æ®µ><span class=hanchor arialabel=Anchor># </span>ç¬¬å››æ®µ</h3></a><p>For safe control in physical systems, where actions have relatively short-term consequences.</p><p>(Dalal et al., 2018) propose an off-policy Deep RL method that effificiently exploits single-step transition data to estimate the safety of state-action pairs, thereby successfully eliminating the need of behavior-policy knowledge of traditional off-policy approaches to safe exploration. <strong>In particular, the authors directly add a safety layer to a single agentâ€™s policy that projects unsafe actions onto the safe domain using a linear approximation of the constraint function, which allows the safety layer to be casted as a quadratic program.</strong> This approximation arises from a fifirst-order Taylor approximation of the constraints in the action space, whose sensitivity is parameterized by a neural network, which was pre-trained on logged historical data.</p><p>å¯¹äºç‰©ç†ç³»ç»Ÿä¸­çš„å®‰å…¨æ§åˆ¶ï¼ŒåŠ¨ä½œæœ‰ç›¸å¯¹çŸ­æœŸçš„ç»“æœã€‚</p><p>(Dalalaletal.ï¼Œ2018)æå‡ºäº†ä¸€ç§éç­–ç•¥çš„æ·±åº¦RLæ–¹æ³•ï¼Œæœ‰æ•ˆåœ°æ’é™¤åˆ©ç”¨å•æ­¥è¿‡æ¸¡æ•°æ®æ¥ä¼°è®¡state-actionçš„å®‰å…¨æ€§ï¼Œä»è€ŒæˆåŠŸåœ°æ¶ˆé™¤äº†ä¼ ç»Ÿoff-policyæ–¹æ³•çš„è¡Œä¸ºç­–ç•¥çŸ¥è¯†çš„å®‰å…¨æ¢ç´¢ã€‚ç‰¹åˆ«æ˜¯ï¼Œ**ä½œè€…ç›´æ¥å‘å•ä¸ªä»£ç†çš„ç­–ç•¥æ·»åŠ ä¸€ä¸ªå®‰å…¨å±‚ï¼Œè¯¥ç­–ç•¥ä½¿ç”¨çº¦æŸå‡½æ•°çš„çº¿æ€§è¿‘ä¼¼å°†ä¸å®‰å…¨æ“ä½œæŠ•å½±åˆ°å®‰å…¨åŸŸä¸Šï¼Œå®‰å…¨å±‚å¯çœ‹ä½œäºŒæ¬¡è§„åˆ’ï¼ˆ<em>Quadratic programming</em>ï¼‰ã€‚**è¿™ç§è¿‘ä¼¼æ¥è‡ªäºåŠ¨ä½œç©ºé—´ä¸­çº¦æŸçš„ä¸€é˜¶æ³°å‹’è¿‘ä¼¼ï¼Œå…¶çµæ•åº¦ç”±ä¸€ä¸ªç¥ç»ç½‘ç»œå‚æ•°åŒ–ï¼Œè¯¥ç¥ç»ç½‘ç»œå¯¹è®°å½•çš„å†å²æ•°æ®è¿›è¡Œäº†é¢„å…ˆè®­ç»ƒã€‚</p><a href=#ç¬¬äº”æ®µ><h3 id=ç¬¬äº”æ®µ><span class=hanchor arialabel=Anchor># </span>ç¬¬äº”æ®µ</h3></a><p>In this work, we propose a multi-agent extension of the approach presented in (Dalal et al., 2018).</p><p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åœ¨(Dalaletal.ï¼Œ2018)ä¸­æå‡ºçš„æ–¹æ³•çš„å¤šä»£ç†æ‰©å±•ã€‚</p><p>We base our method on the MADDPG framework (Lowe et al., 2017b) and aim at preserving safety for all agents during the whole training procedure. æˆ‘ä»¬çš„æ–¹æ³•åŸºäºMADDPGæ¡†æ¶(Loweç­‰äººï¼Œ2017b)ï¼Œç›®çš„æ˜¯åœ¨æ•´ä¸ªåŸ¹è®­è¿‡ç¨‹ä¸­ä¿æŒæ‰€æœ‰ä»£ç†çš„å®‰å…¨ã€‚</p><p>Thereby, <strong>we drop the conservative assumptions made in (Dalal et al., 2018), that the optimization problem that corrects unsafe actions only has one constraint active at a time and is thus always feasible.</strong> In real world problems,the optimization formulation proposed has no guarantees to be recursively feasible and in multi-agent coordination problems where agents impose constraints on one another,one always has more than one constraint active due to the natural symmetry. **å› æ­¤ï¼Œæˆ‘ä»¬æ”¾å¼ƒäº†(Dalaletal.ï¼Œ2018)ä¸­çš„ä¿å®ˆå‡è®¾ï¼Œå³çº æ­£ä¸å®‰å…¨è¡Œä¸ºçš„ä¼˜åŒ–é—®é¢˜ä¸€æ¬¡åªæœ‰ä¸€ä¸ªçº¦æŸï¼Œå› æ­¤æ€»æ˜¯å¯è¡Œçš„ã€‚**åœ¨ç°å®ä¸–ç•Œçš„é—®é¢˜ä¸­ï¼Œæ‰€æå‡ºçš„ä¼˜åŒ–å…¬å¼ä¸èƒ½ä¿è¯æ˜¯é€’å½’å¯è¡Œçš„ï¼Œè€Œåœ¨ä»£ç†ç›¸äº’æ–½åŠ çº¦æŸçš„å¤šä»£ç†åè°ƒé—®é¢˜ä¸­ï¼Œç”±äºè‡ªç„¶å¯¹ç§°ï¼Œä¸€ä¸ªä»£ç†æ€»æ˜¯æœ‰å¤šä¸ªçº¦æŸæ´»è·ƒã€‚</p><p>Instead, <strong>we propose to use a specifific soft constrained formulation of the problem that addresses the lack of recursive feasibility guarantees in the hard constrained formulation. This enhances safety signifificantly in practical situations and is general enough to capture the complicated dynamics of multi-agent problems.</strong> ç›¸åï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ä¸€ä¸ªç‰¹å®šçš„è½¯çº¦æŸå…¬å¼çš„é—®é¢˜ï¼Œä»¥è§£å†³åœ¨ç¡¬çº¦æŸå…¬å¼ä¸­ç¼ºä¹é€’å½’çš„å¯è¡Œæ€§ä¿è¯ã€‚è¿™æ˜¾è‘—æé«˜äº†å®é™…æƒ…å†µä¸‹çš„å®‰å…¨æ€§ï¼Œå¹¶ä¸”è¶³å¤Ÿé€šç”¨ï¼Œå¯ä»¥æ•æ‰å¤šæ™ºèƒ½ä½“é—®é¢˜çš„å¤æ‚åŠ¨æ€ã€‚</p><p>This approach supersedes the need for a backup policy (as in e.g. (Zhang et al., 2019) and (Khan et al., 2019)) because the optimizer is allowed to loosen the constraints by a penalized margin as proposed in (Kerrigan & Maciejowski, 2000). è¿™ç§æ–¹æ³•å–ä»£äº†å¤‡ä»½ç­–ç•¥çš„éœ€æ±‚(ä¾‹å¦‚(Zhangç­‰äººï¼Œ2019)å’Œ(Khanç­‰äººï¼Œ2019))ï¼Œå› ä¸ºä¼˜åŒ–å™¨å¯ä»¥æŒ‰ç…§(Kerrigan&macejoskiï¼Œ2000)ä¸­æå‡ºçš„æƒ©ç½šè¾¹é™…æ”¾æ¾çº¦æŸã€‚</p><p>Thus, our approach does not guarantee zero constraint violations in all situations examined, but by tightening the constraints by a tolerance, one could achieve almost safe behavior during training and in fact, we observe only very rare violations in an extensive set of simulations (Section 3).å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¹¶ä¸èƒ½ä¿è¯åœ¨æ‰€æœ‰æ£€æŸ¥çš„æƒ…å†µä¸‹éƒ½ä¸è¿åçº¦æŸï¼Œä½†é€šè¿‡å…¬å·®æ”¶ç´§çº¦æŸï¼Œå¯ä»¥åœ¨è®­ç»ƒä¸­å®ç°å‡ ä¹å®‰å…¨çš„è¡Œä¸ºï¼Œäº‹å®ä¸Šï¼Œæˆ‘ä»¬åœ¨å¹¿æ³›çš„æ¨¡æ‹Ÿä¸­åªè§‚å¯Ÿåˆ°éå¸¸ç½•è§çš„è¿åï¼ˆç¬¬3èŠ‚ï¼‰ã€‚</p><p>In summary, our contribution lies in extending the approach proposed in (Dalal et al., 2018) (Safe DDPG) to a multi-agent setting, while effificiently circumventing infeasibility problems by reformulating the quadratic safety program in a soft-constrained manner.</p><p>æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„è´¡çŒ®åœ¨äºå°†(Dalalç­‰äººï¼Œ2018)(Safe DDPG)ä¸­æå‡ºçš„æ–¹æ³•æ‰©å±•åˆ°å¤šæ™ºèƒ½ä½“è®¾ç½®ï¼ŒåŒæ—¶é€šè¿‡ä»¥è½¯çº¦æŸçš„æ–¹å¼é‡æ–°åˆ¶å®šäºŒæ¬¡å®‰å…¨ç¨‹åºï¼Œæœ‰æ•ˆåœ°è§„é¿äº†ä¸å¯è¡Œçš„é—®é¢˜ã€‚</p><a href=#2-models-and-methods><h2 id=2-models-and-methods><span class=hanchor arialabel=Anchor># </span><strong>2 Models and Methods</strong></h2></a><a href=#21-problem-formulation><h3 id=21-problem-formulation><span class=hanchor arialabel=Anchor># </span>2.1 <strong>Problem Formulation</strong></h3></a><p>We consider a discrete-time, fifinite dimensional, decentralized, non-cooperative multi-agent system with <em>N</em> agents,</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020004014392.png width=auto alt=image-20211020004014392></p><p><strong>x</strong> = (<em>x</em>1*, . . . , x<strong>N* )<em>,</em> <strong>a</strong> = (<em>a</em>1*, . . . , a</strong>N* ) and <strong>R</strong> = (<em>R</em>1*, . . . , R**N* )ï¼Œä¸Šæ ‡tè¡¨ç¤ºæ—¶é—´</p><p>meaning that each constraint may depend on the state of more than one agent. è¿™æ„å‘³ç€æ¯ä¸ªçº¦æŸå¯èƒ½å–å†³äºå¤šä¸ªä»£ç†çš„çŠ¶æ€ã€‚</p><p>Finally, we defifine a policy <em>Ï€i</em> to be a function mapping the state of agent <em>i</em> to its local action. In the scope of this work, we consider deterministic policies parameterized by <strong>Î¸</strong> = (<em>Î¸</em>1*, &mldr;, Î¸<strong>N* ), and thus use the notation *Ï€</strong>Î¸**i* . In this context, we examine the problem of safe exploration in a constrained Markov Game (CMG) and therefore we aim to solve the following optimization <em>problem</em> for each agent:</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020010046067.png width=auto alt=image-20211020010046067></p><p>æœ€åï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªç­–ç•¥Ï€iå®šä¹‰ä¸ºä¸€ä¸ªå°†ä»£ç†içš„çŠ¶æ€æ˜ å°„åˆ°å…¶å±€éƒ¨åŠ¨ä½œçš„å‡½æ•°ã€‚åœ¨æœ¬å·¥ä½œçš„èŒƒå›´å†…ï¼Œæˆ‘ä»¬è€ƒè™‘ç”±Î¸=(Î¸1ï¼Œ&mldr;ï¼ŒÎ¸N)å‚æ•°åŒ–çš„ç¡®å®šæ€§ç­–ç•¥ï¼Œä»è€Œä½¿ç”¨ç¬¦å·Ï€Î¸iã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬ç ”ç©¶äº†çº¦æŸé©¬å°”å¯å¤«åšå¼ˆ(CMG)ä¸­çš„å®‰å…¨æ¢ç´¢é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è§£å†³æ¯ä¸ªä»£ç†çš„ä»¥ä¸‹ä¼˜åŒ–é—®é¢˜ï¼š</p><p>The above expectation is taken with respect to all agents future action/state pairs - quantities that depend on the policy of each agent - which gives rise to a well-known problem in multi-agent settings, namely the non-stationarity of the environment from the point of view of an individual agent (Lowe et al., 2017b). ä¸Šè¿°æœŸæœ›æ˜¯é’ˆå¯¹æ‰€æœ‰agentsæœªæ¥çš„è¡ŒåŠ¨/çŠ¶æ€å¯¹â€”â€”ä¾èµ–äºæ¯ä¸ªagentç­–ç•¥çš„æ•°é‡â€”â€”è¿™åœ¨å¤šä»£ç†è®¾ç½®ä¸­å¯¼è‡´äº†ä¸€ä¸ªä¼—æ‰€å‘¨çŸ¥çš„é—®é¢˜ï¼Œå³ä»å•ä¸ªä»£ç†çš„è§’åº¦æ¥çœ‹ï¼Œç¯å¢ƒçš„éå¹³ç¨³æ€§ã€‚</p><p>Alongside the constraint dependence on multiple agents, this is what constitutes the prime diffificulty in guaranteeing safety in decentralized multi-agent environments.é™¤äº†å¯¹å¤šä¸ªä»£ç†çš„çº¦æŸä¾èµ–å¤–ï¼Œè¿™ä¹Ÿæ„æˆäº†åœ¨åˆ†æ•£çš„å¤šä»£ç†ç¯å¢ƒä¸­ä¿è¯å®‰å…¨æ€§çš„ä¸»è¦ç¼ºé™·ã€‚</p><a href=#22-safety-signal-model><h3 id=22-safety-signal-model><span class=hanchor arialabel=Anchor># </span><strong>2.2. Safety Signal Model</strong></h3></a><p>Following (Dalal et al., 2018), we make a first order approximation of the constraint function in (1) with respect to action <strong>a</strong>ï¼šè·Ÿéš(Dalalç­‰äººï¼Œ2018)ï¼Œæˆ‘ä»¬å¯¹ï¼ˆ1ï¼‰ä¸­çº¦æŸå‡½æ•°çš„åŠ¨ä½œaè¿›è¡Œäº†ä¸€é˜¶è¿‘ä¼¼</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020104954812.png width=auto alt=image-20211020104954812></p><p>xâ€˜ denotes the state that followed <strong>x</strong> after applying action <strong>a</strong></p><p>function <em>g</em> represents a neural network with input <strong>x</strong>, output of the same dimension as the action <strong>a</strong> and weights wj</p><p>This network effificiently learns the constraintsâ€™ sensitivity to the applied actions given features of the current state based on a set of single-step transition data
<img src=https://jieye-ericx.github.io//../pics/image-20211020105409590.png width=auto alt=image-20211020105409590>è¯¥ç½‘ç»œåŸºäºä¸€ç»„å•æ­¥è½¬æ¢æ•°æ®ï¼Œæœ‰æ•ˆåœ°å­¦ä¹ äº†å½“å‰çŠ¶æ€ç‰¹å¾ä¸‹çº¦æŸæ¡ä»¶å¯¹åº”ç”¨åŠ¨ä½œçš„æ•æ„Ÿæ€§</p><p>In our experiments, we generate <em>D</em> by initializing agents with a random state and choosing actions according to a sufficiently exploratory (random) policy for multiple episodes.åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä»¥éšæœºçŠ¶æ€åˆå§‹åŒ–agentå¹¶æ ¹æ®å……åˆ†çš„æ¢ç´¢æ€§ï¼ˆéšæœºï¼‰ç­–ç•¥é€‰æ‹©åŠ¨ä½œæ¥ç”Ÿæˆ<em>D</em>ã€‚</p><p>With the generated data, the sensitivity network can be trained by specifying the loss function for each constraint as
<img src=https://jieye-ericx.github.io//../pics/image-20211020105611202.png width=auto alt=image-20211020105611202>where each constraintsâ€™ sensitivity will be trained separately.</p><p>é€šè¿‡ç”Ÿæˆçš„æ•°æ®ï¼Œå¯ä»¥é€šè¿‡æŒ‡å®šæ¯ä¸ªçº¦æŸçš„æŸå¤±å‡½æ•°æ¥å°†çµæ•åº¦ç½‘ç»œè®­ç»ƒä¸º:(ä¸Šå›¾)å…¶ä¸­æ¯ä¸ªçº¦æŸæ¡ä»¶çš„æ•æ„Ÿæ€§å°†è¢«å•ç‹¬è®­ç»ƒ</p><a href=#23-safety-layer-optimization><h3 id=23-safety-layer-optimization><span class=hanchor arialabel=Anchor># </span><strong>2.3. Safety Layer Optimization</strong></h3></a><p>Given the one-step safety signals introduced in (2), we augment the policy networks by introducing an additional centralized safety layer, which enhances safety by solving</p><p>è€ƒè™‘åˆ°ï¼ˆ2ï¼‰ä¸­å¼•å…¥çš„ä¸€æ­¥å®‰å…¨ä¿¡å·ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ä¸ªé¢å¤–çš„é›†ä¸­å¼å®‰å…¨å±‚æ¥å¢å¼ºç­–ç•¥ç½‘ç»œï¼Œä»è€Œé€šè¿‡è§£å†³è¯¥å…¬å¼æ¥æé«˜å®‰å…¨æ€§ï¼š</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020143746639.png width=auto alt=image-20211020143746639></p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020144127026.png width=auto alt=image-20211020144127026></p><p>This constitutes a quadratic program which computes the (minimum distance) projection of the actions proposed by each of the policy networks Ï€Î¸i (xi) onto the linearized safety set. Figure 1 illustrates the whole pipeline for computing an action from a given state.</p><p>è¿™æ„æˆäº†ä¸€ä¸ªäºŒæ¬¡ç¨‹åºï¼Œå®ƒè®¡ç®—æ¯ä¸ªç­–ç•¥ç½‘ç»œÏ€Î¸i(xi)æå‡ºçš„åŠ¨ä½œåœ¨çº¿æ€§åŒ–å®‰å…¨é›†ä¸Šçš„ï¼ˆæœ€å°è·ç¦»ï¼‰æŠ•å½±ã€‚å›¾1è¯´æ˜äº†ä»ç»™å®šçŠ¶æ€è®¡ç®—æ“ä½œçš„æ•´ä¸ªç®¡é“ã€‚</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020145219758.png width=auto alt=image-20211020145219758></p><p><em>Figure 1.</em> <strong>An illustration of the safety layer used in combination with the MADDPG networks in order to apply the safe projection of the optimal action</strong>. The individual states of all agents <em>xi</em> are fed into their corresponding policy networks, outputting the evaluation of their current policies at those states which are then concatenated into a single vector. Finally, a <strong>convex quadratic optimization</strong> problem is solved in order to produce the optimal safe action aâˆ—.</p><p>å›¾1ã€‚ä¸MADDPGç½‘ç»œç»“åˆä½¿ç”¨çš„å®‰å…¨å±‚çš„è¯´æ˜ï¼Œä»¥åº”ç”¨æœ€ä¼˜åŠ¨ä½œçš„å®‰å…¨æŠ•å½±ã€‚æ‰€æœ‰ä»£ç†xiçš„å„ä¸ªçŠ¶æ€è¢«è¾“å…¥å®ƒä»¬ç›¸åº”çš„ç­–ç•¥ç½‘ç»œï¼Œè¾“å‡ºå®ƒä»¬å½“å‰ç­–ç•¥çš„è¯„ä¼°ï¼Œè¿™äº›çŠ¶æ€å°†è¿æ¥åˆ°å•ä¸ªå‘é‡ä¸­ã€‚æœ€åï¼Œæ±‚è§£ä¸€ä¸ª<strong>å‡¸äºŒæ¬¡ä¼˜åŒ–é—®é¢˜</strong>ï¼Œå¾—åˆ°æœ€ä¼˜å®‰å…¨ä½œç”¨ã€‚</p><p>Due to the strong convexity of the resulting optimization problem, there exists a global unique minimizer to the problem whenever the feasible set is non-empty. In contrast to (Dalal et al., 2018), where recursive feasibility was assumed and therefore a closed form solution using the Lagrangian multipliers was derived, we used a numerical QP-solver to defer from making this rather strong assumption on the existence of the solution, which is not guaranteed for dynamical systems.</p><p>ç”±äºæ‰€å¾—åˆ°çš„ä¼˜åŒ–é—®é¢˜çš„å¼ºå‡¸æ€§ï¼Œå½“å¯è¡Œé›†éç©ºæ—¶ï¼Œè¯¥é—®é¢˜å­˜åœ¨ä¸€ä¸ªå…¨å±€å”¯ä¸€æœ€å°åŒ–å™¨ã€‚ä¸(Dalaletal.ï¼Œ2018)ç›¸æ¯”ï¼Œå‡è®¾é€’æ¨å¯è¡Œæ€§ï¼Œå› æ­¤å¾—åˆ°äº†ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­çš„å°é—­å½¢å¼è§£ï¼Œæˆ‘ä»¬ä½¿ç”¨æ•°å€¼qpæ±‚è§£å™¨æ¥æ¨è¿Ÿå¯¹è§£çš„å­˜åœ¨åšå‡ºç›¸å½“å¼ºçš„å‡è®¾ï¼Œè¿™å¯¹äºåŠ¨æ€ç³»ç»Ÿæ˜¯ä¸ä¿è¯çš„ã€‚</p><p>Due to the generality of the formulation, it is possible that there exists no recoverable action that can guarantee the agents to be taken to a safe state although the previous iteration of the optimization was indeed feasible. The reason for that is that we assume a limited control authority, which further must respect the dynamics of the underlying system.ç”±äºå…¬å¼çš„é€šç”¨æ€§ï¼Œå¯èƒ½ä¸å­˜åœ¨å¯æ¢å¤çš„æ“ä½œå¯ä»¥ä¿è¯agentåˆ°è¾¾å®‰å…¨çŠ¶æ€ï¼Œå°½ç®¡ä¹‹å‰çš„ä¼˜åŒ–è¿­ä»£ç¡®å®æ˜¯å¯è¡Œçš„ã€‚åŸå› æ˜¯æˆ‘ä»¬å‡å®šæˆ‘ä»¬çš„æ§åˆ¶æ˜¯æœ‰é™åº¦çš„ï¼Œè¿™ä¹Ÿæ˜¯å°Šé‡é‡åº•å±‚ç³»ç»Ÿçš„åŠ¨æ€æ€§ã€‚</p><p>To take this into account without running into infeasibility problems where the agents would require a backup policy to exit unrecoverable states, we propose a soft constrained formulation, whose solution is equivalent to the original formulation whenever (4) is feasible. Otherwise, the optimizer is allowed to loosen the constraints by a penalized margin as proposed in (Kerrigan & Maciejowski, 2000). We thus reformulate (4) as followsä¸ºäº†è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œè€Œä¸é‡åˆ°ä»£ç†éœ€è¦å¤‡ä»½ç­–ç•¥æ¥é€€å‡ºä¸å¯æ¢å¤çŠ¶æ€çš„ä¸å¯è¡Œæ€§é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè½¯çº¦æŸå…¬å¼ï¼Œå½“ï¼ˆ4ï¼‰æˆç«‹æ—¶ï¼Œå…¶è§£ç­‰ä»·äºåŸå§‹å…¬å¼ã€‚å¦åˆ™ï¼Œå¦‚(Kerrigan&Maciejoskiï¼Œ2000)æ‰€è¿°ï¼Œä¼˜åŒ–å™¨è¢«å…è®¸ä»¥æƒ©ç½šå¹…åº¦æ”¾æ¾çº¦æŸã€‚å› æ­¤ï¼Œæˆ‘ä»¬é‡æ–°åˆ¶å®šäº†ï¼ˆ4ï¼‰å¦‚ä¸‹</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020152240878.png width=auto alt=image-20211020152240878></p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020152429291.png width=auto alt=image-20211020152429291></p><p>å…¶ä¸­=(1ï¼Œ&mldr;ï¼ŒK)ä¸ºæ¾å¼›å˜é‡ï¼ŒÏä¸ºçº¦æŸè¿åæƒ©ç½šæƒé‡ã€‚</p><p>æˆ‘ä»¬é€‰æ‹©Ï>kÎ»âˆ—kâˆï¼Œå…¶ä¸­Î»âˆ—æ˜¯ï¼ˆ4)ä¸­åŸå§‹é—®é¢˜å…¬å¼çš„æœ€ä¼˜æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼Œè¿™ä¿è¯äº†å½“(4)å¯è¡Œçš„æ—¶å€™ï¼Œè½¯çº¦æŸé—®é¢˜ä¼šäº§ç”Ÿç­‰ä»·çš„è§£(è§(å…‹é‡Œæ ¹&é©¬è¥¿è€¶å¤«æ–¯åŸºï¼Œ2000)ï¼‰ã€‚</p><p>Since exactly quantifying the optimal Lagrange multiplier is time-consuming, we assign a large value of <em>Ï</em> by inspection. It is important to mention that the reformulation in (5) still constitutes a quadratic program when extending the optimization vector into (<strong>a</strong>*,* <strong></strong>) and using an epigraph formulation (Rockafellar, 2015). ç”±äºç²¾ç¡®åœ°é‡åŒ–æœ€ä¼˜æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ˜¯è€—æ—¶çš„ï¼Œæˆ‘ä»¬é€šè¿‡æ£€æŸ¥æ¥åˆ†é…ä¸€ä¸ªå¾ˆå¤§çš„Ïå€¼ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå½“å°†ä¼˜åŒ–å‘é‡æ‰©å±•åˆ°(aï¼Œ)å¹¶ä½¿ç”¨è¡¨è§‚å…¬å¼æ—¶ï¼Œï¼ˆ5ï¼‰ä¸­çš„é‡æ–°å…¬å¼ä»ç„¶æ„æˆäº†ä¸€ä¸ªäºŒæ¬¡ç¨‹åº(Rockafellarï¼Œ2015)ã€‚</p><p>Notably, this formulation does not necessarily guarantee zero constraint violations.However, we observe empirically that violations remain very small, when setting a rather high penalty value <em>Ï</em> (see Figure 5).å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªå…¬å¼å¹¶ä¸ä¸€å®šä¿è¯æ²¡æœ‰çº¦æŸè¢«è¿åã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä»ç»éªŒä¸Šè§‚å¯Ÿåˆ°ï¼Œå½“è®¾ç½®ä¸€ä¸ªç›¸å½“é«˜çš„æƒ©ç½šå€¼Ïæ—¶ï¼Œè¿è§„è¡Œä¸ºä»ç„¶éå¸¸å°ï¼ˆè§å›¾5ï¼‰ã€‚</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020154433535.png width=auto alt=image-20211020154433535></p><p>è¯´æ˜åœ¨ä¸¤ä¸ªå®éªŒè®­ç»ƒä¸­è·å¾—çš„ç´¯ç§¯ç¢°æ’æ¬¡æ•°ã€‚æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œåœ¨æ— çº¦æŸMADDPGè®­ç»ƒä¸­ï¼Œè§‚å¯Ÿåˆ°å¤§é‡çš„ç¢°æ’ï¼Œè€Œä½¿ç”¨ç¡¬MADDPGï¼Œç¢°æ’å‡å°‘ï¼Œä½†ä¸è½¯çº¦æŸç›¸æ¯”ï¼Œå‡å°‘å¹¶æ²¡æœ‰é‚£ä¹ˆæ˜¾è‘—ã€‚</p><a href=#24-maddpg><h3 id=24-maddpg><span class=hanchor arialabel=Anchor># </span>2.4. MADDPG</h3></a><p>For training Deep RL agents in continuous action spaces,the use of policy gradient algorithms, in which the agentâ€™s policy is directly parameterized by a neural network, is particularly well suited as it avoids explicit maximization over continuous actions which is intractable. We thus opt for the Multi-Agent Deep Deterministic Policy Gradient(MADDPG) algorithm (Lowe et al., 2017a) which is a multiagent generalization of the well-known DDPG methods,originally proposed in (Lillicrap et al., 2015).å¯¹äºåœ¨è¿ç»­åŠ¨ä½œç©ºé—´ä¸­è®­ç»ƒæ·±åº¦RLçš„agentï¼Œä½¿ç”¨ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œå…¶ä¸­agentçš„ç­–ç•¥è¢«ç¥ç»ç½‘ç»œç›´æ¥å‚æ•°åŒ–ï¼Œå› ä¸ºå®ƒé¿å…äº†æ£˜æ‰‹çš„è¿ç»­åŠ¨ä½œçš„æ˜¾å¼æœ€å¤§åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€‰æ‹©äº†å¤šagentæ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦(MADDPG)ç®—æ³•(Loweç­‰äººï¼Œ2017a)ï¼Œè¿™æ˜¯å¯¹è‘—åçš„DDPGæ–¹æ³•çš„å¤šä»£ç†æ¨å¹¿ï¼Œæœ€åˆåœ¨(Lillicrapç­‰äººï¼Œ2015)ä¸­æå‡ºã€‚</p><p>The MADDPG algorithm is in essence a multi-agent variation of the Actor-Critic architecture, <strong>where the problem of the environmentâ€™s non-stationarity is addressed by utilizing a series of centralized Q-networks which approximate the agentsâ€™ respective optimal Q-value functions using full state and action information</strong>. This unavoidably enforces information exchange during training time, which is sometimes referenced as â€œcentralized trainingâ€. On the other hand, the actors employ a policy gradient scheme, where each policy network has access to agent specifific information only.MADDPGç®—æ³•æœ¬è´¨ä¸Šæ˜¯Actor-Critic architectureçš„å¤šagentå˜ä½“ï¼Œ<strong>å…¶ä¸­ç¯å¢ƒçš„éå¹³ç¨³æ€§é—®é¢˜æ˜¯é€šè¿‡åˆ©ç”¨ä¸€ç³»åˆ—é›†ä¸­çš„qç½‘ç»œï¼Œä½¿ç”¨å®Œæ•´çš„çŠ¶æ€å’ŒåŠ¨ä½œä¿¡æ¯è¿‘ä¼¼ä»£ç†å„è‡ªçš„æœ€ä¼˜qå€¼å‡½æ•°</strong>ã€‚è¿™ä¸å¯é¿å…åœ°ä¼šåœ¨è®­ç»ƒæœŸé—´åŠ å¼ºä¿¡æ¯äº¤æ¢ï¼Œè¿™æœ‰æ—¶è¢«ç§°ä¸ºâ€œé›†ä¸­åŸ¹è®­â€ã€‚å¦ä¸€æ–¹é¢ï¼Œactoré‡‡ç”¨äº†ä¸€ä¸ªç­–ç•¥æ¢¯åº¦æ–¹æ¡ˆï¼Œå…¶ä¸­æ¯ä¸ªç­–ç•¥ç½‘ç»œåªèƒ½è®¿é—®ä»£ç†ç‰¹å®šçš„ä¿¡æ¯ã€‚</p><p>Once the policy networks converge, only local observations are required to compute each agentâ€™s actions, thus allowing decentralized execution.ä¸€æ—¦ç­–ç•¥ç½‘ç»œæ”¶æ•›ï¼Œåªéœ€è¦å±€éƒ¨è§‚å¯Ÿæ¥è®¡ç®—æ¯ä¸ªagentçš„actionï¼Œä»è€Œå…è®¸åˆ†æ•£æ‰§è¡Œã€‚</p><p>For stability purposes, MADDPG incorporates ideas from Deep Q Networks, originally introduced in (Mnih et al.,2013). Specififically, a replay buffer <em>R</em> stores historical tuples(<strong>x</strong>*,* <strong>a</strong>*,* <strong>R</strong>*,* <strong>x</strong><em>0</em>), which can be used for off-policy learning and also for breaking the temporal correlation between samples.Furthermore, for each actor and critic network, additional target networks are used to enhance stability of the learning process. ä¸ºäº†ç¨³å®šæ€§çš„ç›®çš„ï¼ŒMADDPGæ•´åˆäº†æ¥è‡ªæ·±åº¦Qç½‘ç»œçš„æƒ³æ³•ï¼Œæœ€åˆå¼•å…¥äº(Mnihç­‰äººï¼Œ2013)ã€‚å…·ä½“æ¥è¯´ï¼Œå›æ”¾ç¼“å†²åŒºRå­˜å‚¨å†å²å…ƒç»„(xã€aã€Rã€x0)ï¼Œå®ƒå¯ä»¥ç”¨äºoff-policyå­¦ä¹ ï¼Œä¹Ÿå¯ä»¥ç”¨äºæ‰“ç ´æ ·æœ¬ä¹‹é—´çš„æ—¶é—´ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œå¯¹äºæ¯ä¸ªactor å’Œcritic ç½‘ç»œï¼Œä½¿ç”¨é¢å¤–çš„ç›®æ ‡ç½‘ç»œæ¥å¢å¼ºå­¦ä¹ è¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020164216371.png width=auto alt=image-20211020164216371></p><p>We denote as <em>Q**Ï€</em>i* (<strong>x</strong>*,* <strong>a</strong>; <em>Î²**i</em>) the critic network, parameterized by <em>Î²**i</em> , and as <em>Ï€**i</em>(<em>x**i</em>; <em>Î¸**i</em>) the actor network for agent <em>i</em>, parameterized by <em>Î¸**i</em> . As for the target networks we denote them as
<img src=https://jieye-ericx.github.io//../pics/image-20211020162954477.png width=auto alt=image-20211020162954477>respectively.</p><p>æˆ‘ä»¬ç”¨
<img src=https://jieye-ericx.github.io//../pics/image-20211020164310904.png width=auto alt=image-20211020164310904>è¡¨ç¤ºcriticç½‘ç»œï¼Œç”±Î²iå‚æ•°åŒ–ï¼Œç”¨
<img src=https://jieye-ericx.github.io//../pics/image-20211020164326930.png width=auto alt=image-20211020164326930>è¡¨ç¤ºagent içš„actorç½‘ç»œï¼Œç”±Î¸iå‚æ•°åŒ–ã€‚ç›®æ ‡ç½‘ç»œåˆ†åˆ«å°†å®ƒä»¬è¡¨ç¤ºä¸º
<img src=https://jieye-ericx.github.io//../pics/image-20211020164400245.png width=auto alt=image-20211020164400245></p><p>Finally, we use <em>Ï„</em> to denote the convex combination factor for updating the target networks.æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨Ï„æ¥è¡¨ç¤ºæ›´æ–°ç›®æ ‡ç½‘ç»œçš„å‡¸ç»„åˆå› å­ã€‚</p><a href=#25-implementation-details><h3 id=25-implementation-details><span class=hanchor arialabel=Anchor># </span><strong>2.5. Implementation Details</strong></h3></a><p>In order to assess the performance of our proposed method we conducted experiments using the multi-agent particle environment, which was previously studied in (Lowe et al.,2017a) and (Mordatch & Abbeel, 2018). In this environment, a fixed number of agents are moving collaboratively in a 2-D grid trying to reach specifific target positions. In our experiments, we used three agents that are constrained to avoid collisions among them. Each agentâ€™s state <em>xi</em> is composed of a vector in R10, containing its position and velocity, the relative distances to the other agents and the target landmark location. Moreover, the actions <em>ai</em> are defined as vectors in R2 containing the acceleration on the two axes.ä¸ºäº†è¯„ä¼°æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æ€§èƒ½ï¼Œæˆ‘ä»¬ä½¿ç”¨multi-agent particle ç¯å¢ƒè¿›è¡Œäº†å®éªŒï¼Œè¿™ä¹‹å‰åœ¨(Loweç­‰äººï¼Œ2017a)å’Œ(è«è¾¾å¥‡&Abbeelï¼Œ2018)ä¸­è¿›è¡Œäº†ç ”ç©¶ã€‚åœ¨è¿™ç§ç¯å¢ƒä¸‹ï¼Œå›ºå®šæ•°é‡çš„agentåœ¨äºŒç»´ç½‘æ ¼ä¸­åä½œç§»åŠ¨ï¼Œè¯•å›¾è¾¾åˆ°ç‰¹å®šçš„ç›®æ ‡ä½ç½®ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸‰ç§å—çº¦æŸé¿å…å®ƒä»¬ä¹‹é—´çš„ç¢°æ’çš„agent.æ¯ä¸ªä»£ç†çš„çŠ¶æ€xiç”±R10ä¸­çš„ä¸€ä¸ªå‘é‡ç»„æˆï¼Œå…¶ä¸­åŒ…å«å…¶ä½ç½®å’Œé€Ÿåº¦ã€ä¸å…¶ä»–agentçš„ç›¸å¯¹è·ç¦»å’Œç›®æ ‡åœ°æ ‡ä½ç½®ã€‚æ­¤å¤–ï¼ŒåŠ¨ä½œaiåœ¨r2ä¸­ä½œä¸ºåŒ…å«ä¸¤ä¸ªè½´ä¸Šçš„åŠ é€Ÿåº¦çš„å‘é‡ã€‚</p><p>The reward assigned to each agent is proportional to the negative l1 distance of the agent from its corresponding target and furthermore, collisions are being penalized. The agents receive individual rewards based on their respective performance. For the safety layer pre-training, we train a simple fully connected ReLU network <em>g</em>(<strong>x</strong>*, w**i*) with a 10-neuron hidden layer for each of the six existing constraints (two possible collisions for each agent), on the randomly produced dataset <em>D</em>. Note that, due to the pairwise symmetry of the constraints used in the experiment (A colliding with B implies B colliding with A), we could in principle simplify the network design, but for generality we decided to consider them as independent constraints. Based on our empirical results (Section 3), we found it unnecessary to increase the complexity of the network. åˆ†é…ç»™æ¯ä¸ªagent çš„å¥–åŠ±ä¸agent ä¸å…¶ç›¸åº”ç›®æ ‡çš„è´Ÿl1è·ç¦»æˆæ­£æ¯”ï¼Œæ­¤å¤–ï¼Œå†²çªä¼šå—åˆ°æƒ©ç½šã€‚agent ä¼šæ ¹æ®å„è‡ªçš„è¡¨ç°è·å¾—å¥–åŠ±ã€‚å¯¹äºåœ¨éšæœºç”Ÿæˆçš„æ•°æ®é›†Dä¸Šçš„å®‰å…¨å±‚é¢„è®­ç»ƒï¼Œæˆ‘ä»¬åœ¨éšæœºäº§ç”Ÿçš„æ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ReLUç½‘ç»œg(xï¼Œwi)ï¼Œ6ä¸ªçº¦æŸï¼ˆæ¯ä¸ªagent æœ‰ä¸¤ä¸ªå¯èƒ½çš„ç¢°æ’ï¼‰ï¼Œæ¯ä¸ªçº¦æŸéƒ½æœ‰ä¸€ä¸ª10ä¸ªç¥ç»å…ƒçš„éšè—å±‚ã€‚æ³¨æ„ï¼Œç”±äºå®éªŒä¸­ä½¿ç”¨çš„çº¦æŸçš„æˆå¯¹å¯¹ç§°(ä¸Bç¢°æ’æ„å‘³ç€Bä¸Aç¢°æ’)ï¼Œæˆ‘ä»¬åŸåˆ™ä¸Šå¯ä»¥ç®€åŒ–ç½‘ç»œè®¾è®¡ï¼Œä½†ä¸ºäº†é€šç”¨ï¼Œæˆ‘ä»¬å†³å®šå°†å®ƒä»¬è§†ä¸ºç‹¬ç«‹çš„çº¦æŸã€‚åŸºäºæˆ‘ä»¬çš„å®è¯ç»“æœï¼ˆç¬¬3èŠ‚ï¼‰ï¼Œæˆ‘ä»¬å‘ç°æ²¡æœ‰å¿…è¦å¢åŠ ç½‘ç»œçš„å¤æ‚æ€§ã€‚</p><p>We train the model using the popular Adam optimizer (Kingma & Ba, 2015) with a batch size of 256 samples. For solving the QP Problem, we adopted the qpsolvers library, which employs a dual active set algorithm originally proposed in (Goldfarb& Idnani, 1983). We further used a value of <em>Ï</em> = 1000 in the cost function of the soft formulation. For the MADDPG algorithm implementation, we used three pairs of fully connected actor-critic networks. These networks are composed of two hidden layers with 100 and 500 neurons respectively.The choice for all activation functions is ReLU except for the output layer of the actor networks, where tanh was used to compress the actions in the [-1,1] range and represent the agentsâ€™ limited control authority. The convex combination rate <em>Ï„</em> , used when updating the target networks,was set to 0*.*01.æˆ‘ä»¬ä½¿ç”¨æµè¡Œçš„Adamä¼˜åŒ–å™¨(Kingma&Baï¼Œ2015)è®­ç»ƒæ¨¡å‹ï¼Œbatch sizeä¸º256ä¸ªæ ·æœ¬ã€‚ä¸ºäº†æ±‚è§£QPé—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†qpsolvers ï¼Œå®ƒé‡‡ç”¨äº†(Goldfarb&Idnani&1983)æœ€åˆæå‡ºçš„åŒä¸»åŠ¨é›†ç®—æ³•ï¼ˆdual active set algorithmï¼‰ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åœ¨è½¯çº¦æŸçš„æˆæœ¬å‡½æ•°ä¸­ä½¿ç”¨äº†Ï=1000çš„å€¼ã€‚å¯¹äºMADDPGç®—æ³•çš„å®ç°ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸‰å¯¹å®Œå…¨è¿æ¥çš„actor-critic ç½‘ç»œã€‚è¿™äº›ç½‘ç»œç”±ä¸¤ä¸ªéšè—å±‚ç»„æˆï¼Œåˆ†åˆ«ç”±100å’Œ500ä¸ªç¥ç»å…ƒç»„æˆã€‚é™¤actor ç½‘ç»œçš„è¾“å‡ºå±‚å¤–ï¼Œæ‰€æœ‰æ¿€æ´»å‡½æ•°çš„é€‰æ‹©éƒ½æ˜¯ReLUï¼Œå‚ä¸è€…ç½‘ç»œçš„è¾“å‡ºå±‚å°†tanhç”¨äºå°†åŠ¨ä½œå‹ç¼©åˆ°[-1,1]èŒƒå›´å†…ï¼Œå¹¶è¡¨ç¤ºagentæœ‰é™çš„æ§åˆ¶æƒé™ã€‚å°†æ›´æ–°ç›®æ ‡ç½‘ç»œæ—¶ä½¿ç”¨çš„å‡¸ç»„åˆç‡Ï„è®¾ç½®ä¸º0.01ã€‚</p><p>To evaluate the algorithmâ€™s robustness and its capability of coming up with an implicit recovery policy, we conduct two case studies:ä¸ºäº†è¯„ä¼°è¯¥ç®—æ³•çš„é²æ£’æ€§åŠå…¶æå‡ºéšå¼æ¢å¤ç­–ç•¥çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸¤ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼š</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020213527611.png width=auto alt=image-20211020213527611></p><p>**(ED)**Inject an <strong>E</strong>xogenous uniform <strong>D</strong>isturbance after each step of the environment, which resembles a very common scenario in real life deployment where environment mismatch could lead to such a behaviour.åœ¨ç¯å¢ƒçš„æ¯ä¸€æ­¥ä¹‹åè¾“å…¥ä¸€ä¸ªå¤–æºæ€§å‡åŒ€å¹²æ‰°ï¼Œè¿™ç±»ä¼¼äºç°å®ç”Ÿæ´»éƒ¨ç½²ä¸­éå¸¸å¸¸è§çš„åœºæ™¯ï¼Œå…¶ä¸­ç¯å¢ƒä¸åŒ¹é…å¯èƒ½å¯¼è‡´è¿™ç§è¡Œä¸ºã€‚</p><p>**(UI)**Allow the environment to be <strong>U</strong>nsafely <strong>I</strong>nitialized,which can also occur in practice.å…è®¸ç¯å¢ƒä¸å®‰å…¨åœ°åˆå§‹åŒ–ï¼Œè¿™ä¹Ÿå¯èƒ½åœ¨å®è·µä¸­å‘ç”Ÿã€‚</p><a href=#3-results><h2 id=3-results><span class=hanchor arialabel=Anchor># </span>3 Results</h2></a><p>We assess the performance of the proposed algorithm on three metrics: average reward, number of collisions during training, and number of collisions during testing (i.e. aftertraining has converged). An infeasible occurrence appears in case the hard-constrained QP in (4) fails to determine a solution that satisfifies the imposed constraints.æˆ‘ä»¬åœ¨ä¸‰ä¸ªæŒ‡æ ‡ä¸Šè¯„ä¼°äº†è¯¥ç®—æ³•çš„æ€§èƒ½ï¼šå¹³å‡å¥–åŠ±ã€è®­ç»ƒä¸­çš„ç¢°æ’æ¬¡æ•°å’Œæµ‹è¯•ä¸­çš„ç¢°æ’æ¬¡æ•°ï¼ˆå³è®­ç»ƒæ”¶æ•›åï¼‰ã€‚å¦‚æœï¼ˆ4ï¼‰ä¸­çš„ç¡¬çº¦æŸQPä¸èƒ½ç¡®å®šæ»¡è¶³æ‰€æ–½åŠ çº¦æŸçš„è§£ï¼Œåˆ™ä¼šå‡ºç°ä¸å¯è¡Œçš„æƒ…å†µã€‚</p><p>We benchmark a total of three different Deep RL strategies:æˆ‘ä»¬æ€»å…±å¯¹ä¸‰ç§ä¸åŒçš„æ·±åº¦RLç­–ç•¥è¿›è¡Œäº†åŸºå‡†æµ‹è¯•</p><ul><li>unconstrained MADDPG ä¼˜å…ˆè€ƒè™‘æ¢ç´¢å’Œå­¦ä¹ è€Œä¸æ˜¯å®‰å…¨ï¼Œå› ä¸ºçº¦æŸä¸æ˜¯ç›´æ¥æ–½åŠ çš„</li><li>hard-constrained MADDPG (hard MADDPG) æ–½åŠ ç¡¬çŠ¶æ€çº¦æŸæ¥è€ƒè™‘åˆ°å®‰å…¨æ“ä½œ</li><li>soft-constrained MADDPG (soft MADDPG) æŒ‰ç…§æˆ‘ä»¬åœ¨ï¼ˆ5ï¼‰ä¸­çš„å…¬å¼ï¼Œæ–½åŠ äº†çŠ¶æ€çº¦æŸçš„æ”¾æ¾ç‰ˆæœ¬ï¼ŒåŒæ—¶penalizing the amount of slack</li></ul><p>The first approach prioritizes exploration and learning over safety since constraints are not directly imposed, whereas hard MADDPG takes into account safe operation by imposing hard state constraints. Finally, soft MADDPG, as presented in Algorithm 1, imposes a relaxed version of the state constraints while penalizing the amount of slack, following our formulation in (5).ç¬¬ä¸€ç§æ–¹æ³•ä¼˜å…ˆè€ƒè™‘æ¢ç´¢å’Œå­¦ä¹ è€Œä¸æ˜¯å®‰å…¨ï¼Œå› ä¸ºçº¦æŸä¸æ˜¯ç›´æ¥æ–½åŠ çš„ï¼Œè€Œç¡¬MADDPGåˆ™é€šè¿‡æ–½åŠ ç¡¬çŠ¶æ€çº¦æŸæ¥è€ƒè™‘åˆ°å®‰å…¨æ“ä½œã€‚æœ€åï¼Œå¦‚ç®—æ³•1æ‰€ç¤ºçš„è½¯MADDPGï¼ŒæŒ‰ç…§æˆ‘ä»¬åœ¨ï¼ˆ5ï¼‰ä¸­çš„å…¬å¼ï¼Œæ–½åŠ äº†çŠ¶æ€çº¦æŸçš„æ”¾æ¾ç‰ˆæœ¬ï¼ŒåŒæ—¶æƒ©ç½šæ¾å¼›é‡ã€‚</p><p>The duration of each experiment is 8000 episodes and, in order to assess uncertainty, we repeat each experiment 10 times using different initial random seeds.æ¯ä¸ªå®éªŒçš„æŒç»­æ—¶é—´ä¸º8000é›†ï¼Œä¸ºäº†è¯„ä¼°ä¸ç¡®å®šæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„åˆå§‹éšæœºç§å­é‡å¤æ¯ä¸ªå®éªŒ10æ¬¡ã€‚</p><p>Under normal operating conditions (safe initialization without disturbances), both the hard and the soft-constrained MADDPG strategies achieve 0 constraint violations in our experiments during the training and the testing phase.åœ¨æ­£å¸¸æ“ä½œæ¡ä»¶ä¸‹ï¼ˆå®‰å…¨åˆå§‹åŒ–æ— å¹²æ‰°ï¼‰ï¼Œåœ¨è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µï¼Œåœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œç¡¬çº¦æŸå’Œè½¯çº¦æŸMADDPGç­–ç•¥éƒ½å®ç°äº†0ä¸ªçº¦æŸè¿åã€‚</p><p>However,in order to examine the robustness properties of the aforementioned methods, we evaluate our models under the case studies mentioned in the end of Section 2.5. The outcome of the experiments along with the 95% confifidence intervals are summarized in Table 1.ä¸ºäº†æ£€éªŒä¸Šè¿°æ–¹æ³•çš„é²æ£’æ€§ï¼Œæˆ‘ä»¬åœ¨ç¬¬2.5èŠ‚æœ«å°¾æåˆ°çš„æ¡ˆä¾‹ç ”ç©¶ä¸‹è¯„ä¼°äº†æˆ‘ä»¬çš„æ¨¡å‹ã€‚å®éªŒç»“æœå’Œ95%äº”åº¦åŒºé—´æ±‡æ€»è§è¡¨1.</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020170221784.png width=auto alt=image-20211020170221784></p><p>è¡¨ä¸­è¯´æ˜äº†æ‰€æå‡ºçš„ç®—æ³•çš„å®éªŒç»“æœã€‚å¯¹äºæ¯ä¸ªsettingï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†å¤šæ¬¡è¿è¡Œä¸­çš„å¹³å‡å€¼å’Œç›¸åº”çš„95%ç½®ä¿¡åŒºé—´ã€‚åœ¨ç¡¬MADDPGä¸­è§‚å¯Ÿåˆ°æ›´é«˜çš„å¥–åŠ±ï¼Œç”±äºä¸å¯è¡Œæ€§é—®é¢˜ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¾ç¤ºçš„ç¢°æ’æ¬¡æ•°é«˜äºè½¯MADDPGç¢°æ’æ¬¡æ•°æœ€ä½çš„MADDPGã€‚æœ€åï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æµ‹è¯•è¿‡ç¨‹ä¸­ä¹Ÿå­˜åœ¨ç›¸åŒçš„patternã€‚</p><p>In Figure 3, the evolution over episodes of the average reward is depicted for the <strong>(ED)</strong> case. The average reward is computed as the mean over the 3 agents in a single episode.Interestingly, we observe that all three presented algorithms have a similar trend, suggesting that introducing the safetyframework (in both hard- and soft- variants) does not negatively affect the ability of the agents to reach their targets. A similar result holds for the case of unsafe itialization <strong>(UI)</strong>,so the respective plot is omitted for brevity.</p><p>åœ¨å›¾3ä¸­ï¼Œæè¿°äº†**(ED)æ¡ˆä¾‹çš„å¹³å‡å¥–åŠ±çš„æ¼”å˜ã€‚å¹³å‡å¥–åŠ±è¢«è®¡ç®—ä¸ºä¸€ä¸ªäº‹ä»¶ä¸­3ä¸ªä»£ç†çš„å¹³å‡å€¼ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ‰€æœ‰ä¸‰ç§ç®—æ³•éƒ½æœ‰ç›¸ä¼¼çš„è¶‹åŠ¿ï¼Œè¿™è¡¨æ˜å¼•å…¥å®‰å…¨æ¡†æ¶ï¼ˆåŒ…æ‹¬ç¡¬å˜ä½“å’Œè½¯å˜ä½“ï¼‰ä¸ä¼šå¯¹ä»£ç†è¾¾åˆ°ç›®æ ‡çš„èƒ½åŠ›äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ç±»ä¼¼çš„ç»“æœä¸ä¸å®‰å…¨åˆå§‹åŒ–(UI)ä¹Ÿç±»ä¼¼ï¼Œå› æ­¤ä¸ºäº†ç®€æ´èµ·è§ï¼Œçœç•¥äº†å„è‡ªçš„å›¾ã€‚</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020170649882.png width=auto alt=image-20211020170649882></p><p>Figure 4 shows for each setting, the average number of collisions per episode <em>during training</em>, while Figure 5 presents the evolution of the cumulative number of collisions over the training episodes. In particular, soft MADDPG exhibits 97*.<em>71% <strong>(UI)</strong>, 97</em>.<em>99% <strong>(ED)</strong> fewer collisions compared to the unconstrained MADDPG. On the other hand, hard MADDPG only achieves a 84</em>.<em>38% <strong>(UI)</strong>, 42</em>.*42% <strong>(ED)</strong> reduction in collisions compared to the unconstrained MADDPG,since the infeasibility of the optimization problem does at times not allow the safety fifilter to intervene and correct the proposed actions.å›¾4æ˜¾ç¤ºäº†æ¯ä¸ªè®¾ç½®ï¼Œè®­ç»ƒæœŸé—´æ¯é›†çš„å¹³å‡ç¢°æ’æ¬¡æ•°ï¼Œè€Œå›¾5æ˜¾ç¤ºäº†è®­ç»ƒè¿‡ç¨‹ä¸­ç´¯ç§¯ç¢°æ’æ¬¡æ•°çš„æ¼”å˜ã€‚ç‰¹åˆ«æ˜¯ï¼Œè½¯çº¦æŸMADDPGå±•ç¤ºäº†97.71%<strong>(UI)</strong>ï¼Œä¸æ— çº¦æŸçš„MADDPGç›¸æ¯”ï¼Œç¢°æ’æ¬¡æ•°ä¸ºå‡å°‘äº†97.99%(ED)ã€‚å¦ä¸€æ–¹é¢ï¼Œç¡¬MADDPGåªè¾¾åˆ°äº†84.38%<strong>(UI)</strong>ï¼Œä¸æ— çº¦æŸçš„MADDPGç›¸æ¯”ï¼Œç¢°æ’å‡å°‘äº†42.42%<strong>(ED)</strong>ï¼Œå› ä¸ºä¼˜åŒ–é—®é¢˜çš„ä¸å¯è¡Œæ€§æœ‰æ—¶ä¸å…è®¸å®‰å…¨å…‰çº¤è¿‡æ»¤å™¨è¿›è¡Œå¹²é¢„å’Œçº æ­£æ‰€æè®®çš„æ“ä½œã€‚</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020202949421.png width=auto alt=image-20211020202949421></p><p>To evaluate the impact of the hard-constrained MADDPG,it is essential to investigate the infeasible occurrences, since they represent the critical times when constraints can no longer be satisfified. In our experiments, 20.9% <strong>(UI)</strong>, 56.7 %<strong>(ED)</strong> of the episodes are directly related to infeasible conditions. This motivates the necessity for a soft-constrained safety layer that maintains feasibility and preserves safety in cases where the hard constrained formulation fails to return a solution.ä¸ºäº†è¯„ä¼°ç¡¬çº¦æŸMADDPGçš„å½±å“ï¼Œå¿…é¡»è°ƒæŸ¥ä¸å¯è¡Œçš„æƒ…å†µï¼Œå› ä¸ºå®ƒä»¬ä»£è¡¨äº†çº¦æŸå†ä¹Ÿä¸èƒ½è¢«æ»¡è¶³çš„ä¸´ç•Œæ—¶é—´ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œ20.9%(UI)ã€56.7%(ED)ä¸ä¸å¯è¡Œçš„æ¡ä»¶ç›´æ¥ç›¸å…³ã€‚è¿™æ¿€å‘äº†å»ºç«‹è½¯çº¦æŸå®‰å…¨å±‚çš„å¿…è¦æ€§ï¼Œå³åœ¨ç¡¬çº¦æŸé…æ–¹æœªèƒ½è¿”å›è§£å†³æ–¹æ¡ˆçš„æƒ…å†µä¸‹ï¼Œä¿æŒå¯è¡Œæ€§å’Œä¿æŒå®‰å…¨æ€§ã€‚</p><p>Finally, in order to gain a better understanding of the behaviour of our algorithm after convergence, we ran test simulations of 100 episodes for each agent for 10 different initial random seeds. The cumulative number of the collisions for the 2 different settings is illustrated in Figure 6. It is evident that in both settings, the soft constrained formulation achieves the minimum number of collisions.For visualization purposes, we provide the videos of the test simulation at the following Video Repository.æœ€åï¼Œä¸ºäº†æ›´å¥½åœ°ç†è§£æˆ‘ä»¬çš„ç®—æ³•åœ¨æ”¶æ•›åçš„è¡Œä¸ºï¼Œæˆ‘ä»¬å¯¹10ä¸ªä¸åŒçš„åˆå§‹éšæœºç§å­å¯¹æ¯ä¸ªä»£ç†è¿›è¡Œäº†100ä¸ªepisodeçš„æµ‹è¯•æ¨¡æ‹Ÿã€‚ä¸¤ç§ä¸åŒè®¾ç½®çš„ç´¯ç§¯ç¢°æ’æ¬¡æ•°å¦‚å›¾6æ‰€ç¤ºã€‚å¾ˆæ˜æ˜¾ï¼Œåœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œè½¯çº¦æŸå…¬å¼éƒ½å®ç°äº†æœ€å°çš„ç¢°æ’æ¬¡æ•°ã€‚ä¸ºäº†å¯è§†åŒ–çš„ç›®çš„ï¼Œæˆ‘ä»¬åœ¨ä»¥ä¸‹è§†é¢‘å­˜å‚¨åº“ä¸­æä¾›äº†æµ‹è¯•æ¨¡æ‹Ÿçš„è§†é¢‘ã€‚</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020203609533.png width=auto alt=image-20211020203609533></p><a href=#4-conclusion><h2 id=4-conclusion><span class=hanchor arialabel=Anchor># </span>4 Conclusion</h2></a><p>We proposed an extension of Safe DDPG (Dalal et al., 2018) to multi-agent settings. <strong>From a technical perspective, we relaxed some of the conservative assumptions made in the original single-agent work by introducing soft constraints in the optimization objective.</strong> æˆ‘ä»¬æå‡ºå°†SafeDDPG(Dalalç­‰äººï¼Œ2018)æ‰©å±•åˆ°å¤šæ™ºèƒ½ä½“è®¾ç½®ã€‚ä»æŠ€æœ¯çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨ä¼˜åŒ–ç›®æ ‡ä¸­å¼•å…¥è½¯çº¦æŸï¼Œæ”¾å®½äº†åœ¨åŸå§‹çš„å•ä»£ç†å·¥ä½œä¸­æ‰€åšçš„ä¸€äº›ä¿å®ˆå‡è®¾ã€‚</p><p>This allows us to generalize the approach to settings where more than one constraint is active, which is typically the case for multi-agent environments. è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†è¯¥æ–¹æ³•æ¨å¹¿åˆ°å¤šä¸ªçº¦æŸå¤„äºæ´»åŠ¨çš„è®¾ç½®</p><p>Our empirical results suggest that our soft constrained formulation achieves as dramatic decrease in constraint violations during training when exogenous disturbances and unsafe initialization are encountered, while maintaining the ability to explore and hence solve the desired task successfully. æˆ‘ä»¬çš„å®è¯ç»“æœè¡¨æ˜ï¼Œå½“è®­ç»ƒè¿‡ç¨‹ä¸­é‡åˆ°å¤–æºå¹²æ‰°å’Œä¸å®‰å…¨åˆå§‹åŒ–æ—¶ï¼Œæˆ‘ä»¬çš„è½¯çº¦æŸå…¬å¼å¯ä»¥æ˜¾è‘—å‡å°‘çº¦æŸè¿åï¼ŒåŒæ—¶ä¿æŒæ¢ç´¢ä»è€ŒæˆåŠŸè§£å†³æ‰€éœ€ä»»åŠ¡çš„èƒ½åŠ›ã€‚</p><p>Although this observation does not necessarily generalize to more complex environments, it motivates the practicality of our algorithm in safety-critical deployment under more conservative constraint tightenings. è™½ç„¶è¿™ä¸€è§‚å¯Ÿç»“æœå¹¶ä¸ä¸€å®šé€‚ç”¨äºæ›´å¤æ‚çš„ç¯å¢ƒï¼Œä½†å®ƒæ¿€å‘äº†æˆ‘ä»¬çš„ç®—æ³•åœ¨æ›´ä¿å®ˆçš„çº¦æŸçº¦æŸä¸‹çš„å®‰å…¨ä¸´ç•Œéƒ¨ç½²ä¸­çš„å®ç”¨æ€§ã€‚</p><p>Finally, while our preliminary results are encouraging, we believe there is ample room for improvement and further experimentation. As part of future work, we would like to introduce a reward based on the intervention of the safety fifilter during training, such that we can indirectly propagate the safe behavior to the learnt policies of the agents, which could ultimately eliminate the requirement for using a centralized safety fifilter during test time. æœ€åï¼Œè™½ç„¶æˆ‘ä»¬çš„åˆæ­¥ç»“æœä»¤äººé¼“èˆï¼Œä½†æˆ‘ä»¬ç›¸ä¿¡æœ‰è¶³å¤Ÿçš„æ”¹è¿›ç©ºé—´å’Œè¿›ä¸€æ­¥çš„å®éªŒã€‚ä½œä¸ºæœªæ¥å·¥ä½œçš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬æƒ³åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿å¥–åŠ±åŸºäºå®‰å…¨è¿‡æ»¤å™¨çš„å¹²é¢„ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥é—´æ¥ä¼ æ’­å®‰å…¨è¡Œä¸ºå…³äºagentå·²ç»çš„å­¦ä¹ ç­–ç•¥ï¼Œè¿™å¯èƒ½æœ€ç»ˆæ¶ˆé™¤åœ¨æµ‹è¯•æ—¶é—´ä½¿ç”¨é›†ä¸­å®‰å…¨è¿‡æ»¤å™¨çš„éœ€æ±‚ã€‚</p><p>Additionally, we would like to deploy our approach in more complex environments to explore the true potential of our work.æ­¤å¤–ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨æ›´å¤æ‚çš„ç¯å¢ƒä¸­éƒ¨ç½²æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä»¥æ¢ç´¢æˆ‘ä»¬å·¥ä½œçš„çœŸæ­£æ½œåŠ›ã€‚</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://jieye-ericx.github.io/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Jieye ericx using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2023</p><ul><li><a href=https://jieye-ericx.github.io/>Home</a></li><li><a href=https://github.com/jieye-ericx>Github</a></li></ul></footer></div></div></body></html>