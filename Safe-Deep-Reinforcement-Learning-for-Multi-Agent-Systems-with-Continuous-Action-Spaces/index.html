<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="相关 Safe Exploration in Continuous Action Spaces (Dalal et al., 2018)
Multi-agent actor-critic for mixed cooperative competitive environments (Lowe et al."><meta property="og:title" content><meta property="og:description" content="相关 Safe Exploration in Continuous Action Spaces (Dalal et al., 2018)
Multi-agent actor-critic for mixed cooperative competitive environments (Lowe et al."><meta property="og:type" content="website"><meta property="og:image" content="https://jieye-ericx.github.io/icon.png"><meta property="og:url" content="https://jieye-ericx.github.io/Safe-Deep-Reinforcement-Learning-for-Multi-Agent-Systems-with-Continuous-Action-Spaces/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="相关 Safe Exploration in Continuous Action Spaces (Dalal et al., 2018)
Multi-agent actor-critic for mixed cooperative competitive environments (Lowe et al."><meta name=twitter:image content="https://jieye-ericx.github.io/icon.png"><title>ericx 's 数字花园</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://jieye-ericx.github.io//icon.png><link href=https://jieye-ericx.github.io/styles.80333fa2099c0bee674efa435fde378c.min.css rel=stylesheet><link href=https://jieye-ericx.github.io/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://jieye-ericx.github.io/js/darkmode.48459b7116d092b4e98d2cab704cad80.min.js></script>
<script src=https://jieye-ericx.github.io/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://jieye-ericx.github.io/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://jieye-ericx.github.io/",fetchData=Promise.all([fetch("https://jieye-ericx.github.io/indices/linkIndex.498f8802e86f71aa2af4d45d6c4096d6.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://jieye-ericx.github.io/indices/contentIndex.5368a39ad15318a851f2d088b43f3a36.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://jieye-ericx.github.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://jieye-ericx.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/jieye-ericx.github.io\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=jieye-ericx.github.io src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://jieye-ericx.github.io/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://jieye-ericx.github.io/>ericx 's 数字花园</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated
Unknown
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/Safe%20Deep%20Reinforcement%20Learning%20for%20Multi-Agent%20Systems%20with%20Continuous%20Action%20Spaces.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#相关>相关</a></li><li><a href=#abstract><strong>Abstract</strong></a></li><li><a href=#1-introduction-and-related-work><strong>1 Introduction and Related Work</strong></a><ol><li><a href=#第一段><strong>第一段</strong></a></li><li><a href=#第二段>第二段</a></li><li><a href=#第三段>第三段</a></li><li><a href=#第四段>第四段</a></li><li><a href=#第五段>第五段</a></li></ol></li><li><a href=#2-models-and-methods><strong>2 Models and Methods</strong></a><ol><li><a href=#21-problem-formulation>2.1 <strong>Problem Formulation</strong></a></li><li><a href=#22-safety-signal-model><strong>2.2. Safety Signal Model</strong></a></li><li><a href=#23-safety-layer-optimization><strong>2.3. Safety Layer Optimization</strong></a></li><li><a href=#24-maddpg>2.4. MADDPG</a></li><li><a href=#25-implementation-details><strong>2.5. Implementation Details</strong></a></li></ol></li><li><a href=#3-results>3 Results</a></li><li><a href=#4-conclusion>4 Conclusion</a></li></ol></nav></details></aside><a href=#相关><h2 id=相关><span class=hanchor arialabel=Anchor># </span>相关</h2></a><p>Safe Exploration in Continuous Action Spaces (Dalal et al., 2018)</p><p>Multi-agent actor-critic for mixed cooperative competitive environments (Lowe et al., 2017b)</p><p>凸二次优化问题</p><a href=#abstract><h2 id=abstract><span class=hanchor arialabel=Anchor># </span><strong>Abstract</strong></h2></a><p>多代理控制问题构成了具有连续行动空间的深度强化学习模型的一个有趣的应用领域。然而，这种现实世界的应用通常带有关键的安全约束，不能被违反。为了确保安全，</p><ol><li><strong>我们通过在深度策略网络中添加一个安全层来增强著名的多代理深度确定性策略梯度（MADDPG）框架。</strong></li><li><strong>我们将线性化单步过渡动态的想法扩展到了多代理设置中，正如Safe DDPG（Dalal等人，2018）中针对单代理系统所做的那样。</strong></li><li>我们还提议使用软约束（Kerrigan & Maciejowski，2000）来规避行动修正步骤中的不可行性问题。在温和的假设条件下，精确惩罚函数理论的结果可以用来保证软约束的满足。我们从经验上发现，软约束的表述实现了约束违反的急剧减少，甚至在学习过程中也能保证安全。</li></ol><a href=#1-introduction-and-related-work><h2 id=1-introduction-and-related-work><span class=hanchor arialabel=Anchor># </span><strong>1 Introduction and Related Work</strong></h2></a><a href=#第一段><h3 id=第一段><span class=hanchor arialabel=Anchor># </span><strong>第一段</strong></h3></a><p>In recent years, deep reinforcement learning (Deep RL) with continuous action spaces has received increasing attention in the context of real-world applications such as autonomous driving (Sallab et al., 2017), single- (Gu et al., 2017) and multi robot systems (Hu et al., 2020), as well as data center cooling (Lazic et al., 2018). Contrary to more mature applications of RL such as video games (Mnih et al., 2015), these real-world cases naturally require a set of safety constraints to be fulfifilled (e.g. in the case of robot arms, avoiding obstacles and self-collisions, or limiting angles).</p><p>The main caveat of safety in reinforcement learning is that <strong>the dynamics of the system are a-priori unknown and hence one does not know which actions are safe ahead of time</strong>. Whenever accurate offlfline simulations or a model of the environment are available, safety can be introduced ex-post by correcting a learned policy for example via shielding (Alshiekh et al.,\2018) or via a pre-determined backup controller (Wabersich & Zeilinger, 2018). <strong>Yet, many real-world applications require safety to be enforced during both learning and deployment, and a model of the environment is not always available.</strong></p><p>具有连续动作空间的深度强化学习(DeepRL)在自动驾驶、机器人系统等应用中受到了越来越多的关注。与电子游戏等更成熟的应用相反，这些现实世界的情况自然需要满足一套安全约束(例如，在机器人手臂的情况下，要避免各种碰撞等）</p><p>强化学习中安全的主要警告是，系统的动态是先验未知的，因此人们不提前知道哪些动作是安全的。</p><p>只要有精确的离线模拟或环境模型，就可以通过纠正学习到的策略，例如通过屏蔽(Alshiekh等人，2018)或通过预先确定的备份控制器(Wabersich&泽林格，2018)来预先引入安全性。</p><p>然而，许多现实世界的应用程序在学习和部署期间都需要强制执行安全性，而且环境模型并不总是可用的。</p><a href=#第二段><h3 id=第二段><span class=hanchor arialabel=Anchor># </span>第二段</h3></a><p>A growing line of research addresses the problem of safety of the learning process in model-free settings.</p><p><strong>A traditional approach is reward shaping, where one attempts to encode information on undesirable actions state pairs in the reward function.</strong> 缺点：Unfortunately, this approach comes with the downside that unsafe behavior is discouraged only as long as the relevant trajectories remain stored in the
<a href=https://blog.csdn.net/suoyan1539/article/details/79571010 rel=noopener><strong>experience replay buffer</strong></a>.</p><p>In (Lipton et al., 2018), the authors propose Intrinsic Fear, a framework that mitigates this issue by training a neural network to identify unsafe states, which is then used in shaping the reward function. Although this approach alleviates the problem of periodically revisiting unsafe states, it is still required to visit those states to gather enough information to avoid them.</p><p>第二段作者介绍了越来越多的研究线解决了在无模型环境中学习过程的安全性问题。</p><p>**一种传统的方法是奖励塑造，即人们试图对奖励函数中的不良行为状态对进行编码。不幸的是，这种方法的缺点是，只有当相关的轨迹仍然存储在经验回放缓冲区(experience replay buffer)中时，才不鼓励出现不安全的行为。**在(Liptonetal.，2018)中，作者提出了内在恐惧框架，通过训练神经网络识别不安全状态来缓解这个问题，然后该状态用于塑造奖励函数。虽然这种方法减轻了定期重新访问不安全状态的问题，但仍然需要访问这些状态以收集足够的信息以避免访问它们。</p><a href=#第三段><h3 id=第三段><span class=hanchor arialabel=Anchor># </span>第三段</h3></a><p><strong>Another family of approaches focuses on safety for discrete state/action spaces and thus studies the problem through the lens of finite Constrained Markov Decision Processes</strong> (CMDPs) (Altman, 1998). Along those lines, multiple approaches have been proposed. For example in (Efroni et al.,\2020) the authors propose a framework which focuses on learning the underlying CMDP <strong>based purely on logged historical data</strong>.</p><p>缺点：A common limitation to such approaches is that it is hard to generalize to continuous action spaces, although there exists some work on that direction, as in (Chow et al.,\2019) where the authors leverage Lyapunov functions to handle constraints in continuous settings.</p><p><strong>另一类方法侧重于离散状态/动作空间的安全性</strong>，<strong>从而通过有限约束马尔可夫决策过程(CMDPs)的透镜来研究这个问题</strong>(Altman，1998)。因此，已经提出了多种方法。例如，在(Efroni等人，2020)中，作者提出了一个框架，<strong>专注于纯粹基于记录的历史数据学习底层CMDP</strong>。这种方法的一个共同限制是，很难推广到连续动作空间，尽管在这个方向上存在一些工作，如(Chowetal.，2019)，作者利用李雅普诺夫函数来处理连续设置中的约束。</p><a href=#第四段><h3 id=第四段><span class=hanchor arialabel=Anchor># </span>第四段</h3></a><p>For safe control in physical systems, where actions have relatively short-term consequences.</p><p>(Dalal et al., 2018) propose an off-policy Deep RL method that effificiently exploits single-step transition data to estimate the safety of state-action pairs, thereby successfully eliminating the need of behavior-policy knowledge of traditional off-policy approaches to safe exploration. <strong>In particular, the authors directly add a safety layer to a single agent’s policy that projects unsafe actions onto the safe domain using a linear approximation of the constraint function, which allows the safety layer to be casted as a quadratic program.</strong> This approximation arises from a fifirst-order Taylor approximation of the constraints in the action space, whose sensitivity is parameterized by a neural network, which was pre-trained on logged historical data.</p><p>对于物理系统中的安全控制，动作有相对短期的结果。</p><p>(Dalalaletal.，2018)提出了一种非策略的深度RL方法，有效地排除利用单步过渡数据来估计state-action的安全性，从而成功地消除了传统off-policy方法的行为策略知识的安全探索。特别是，**作者直接向单个代理的策略添加一个安全层，该策略使用约束函数的线性近似将不安全操作投影到安全域上，安全层可看作二次规划（<em>Quadratic programming</em>）。**这种近似来自于动作空间中约束的一阶泰勒近似，其灵敏度由一个神经网络参数化，该神经网络对记录的历史数据进行了预先训练。</p><a href=#第五段><h3 id=第五段><span class=hanchor arialabel=Anchor># </span>第五段</h3></a><p>In this work, we propose a multi-agent extension of the approach presented in (Dalal et al., 2018).</p><p>在这项工作中，我们提出了在(Dalaletal.，2018)中提出的方法的多代理扩展。</p><p>We base our method on the MADDPG framework (Lowe et al., 2017b) and aim at preserving safety for all agents during the whole training procedure. 我们的方法基于MADDPG框架(Lowe等人，2017b)，目的是在整个培训过程中保持所有代理的安全。</p><p>Thereby, <strong>we drop the conservative assumptions made in (Dalal et al., 2018), that the optimization problem that corrects unsafe actions only has one constraint active at a time and is thus always feasible.</strong> In real world problems,the optimization formulation proposed has no guarantees to be recursively feasible and in multi-agent coordination problems where agents impose constraints on one another,one always has more than one constraint active due to the natural symmetry. **因此，我们放弃了(Dalaletal.，2018)中的保守假设，即纠正不安全行为的优化问题一次只有一个约束，因此总是可行的。**在现实世界的问题中，所提出的优化公式不能保证是递归可行的，而在代理相互施加约束的多代理协调问题中，由于自然对称，一个代理总是有多个约束活跃。</p><p>Instead, <strong>we propose to use a specifific soft constrained formulation of the problem that addresses the lack of recursive feasibility guarantees in the hard constrained formulation. This enhances safety signifificantly in practical situations and is general enough to capture the complicated dynamics of multi-agent problems.</strong> 相反，我们建议使用一个特定的软约束公式的问题，以解决在硬约束公式中缺乏递归的可行性保证。这显著提高了实际情况下的安全性，并且足够通用，可以捕捉多智能体问题的复杂动态。</p><p>This approach supersedes the need for a backup policy (as in e.g. (Zhang et al., 2019) and (Khan et al., 2019)) because the optimizer is allowed to loosen the constraints by a penalized margin as proposed in (Kerrigan & Maciejowski, 2000). 这种方法取代了备份策略的需求(例如(Zhang等人，2019)和(Khan等人，2019))，因为优化器可以按照(Kerrigan&macejoski，2000)中提出的惩罚边际放松约束。</p><p>Thus, our approach does not guarantee zero constraint violations in all situations examined, but by tightening the constraints by a tolerance, one could achieve almost safe behavior during training and in fact, we observe only very rare violations in an extensive set of simulations (Section 3).因此，我们的方法并不能保证在所有检查的情况下都不违反约束，但通过公差收紧约束，可以在训练中实现几乎安全的行为，事实上，我们在广泛的模拟中只观察到非常罕见的违反（第3节）。</p><p>In summary, our contribution lies in extending the approach proposed in (Dalal et al., 2018) (Safe DDPG) to a multi-agent setting, while effificiently circumventing infeasibility problems by reformulating the quadratic safety program in a soft-constrained manner.</p><p>总之，我们的贡献在于将(Dalal等人，2018)(Safe DDPG)中提出的方法扩展到多智能体设置，同时通过以软约束的方式重新制定二次安全程序，有效地规避了不可行的问题。</p><a href=#2-models-and-methods><h2 id=2-models-and-methods><span class=hanchor arialabel=Anchor># </span><strong>2 Models and Methods</strong></h2></a><a href=#21-problem-formulation><h3 id=21-problem-formulation><span class=hanchor arialabel=Anchor># </span>2.1 <strong>Problem Formulation</strong></h3></a><p>We consider a discrete-time, fifinite dimensional, decentralized, non-cooperative multi-agent system with <em>N</em> agents,</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020004014392.png width=auto alt=image-20211020004014392></p><p><strong>x</strong> = (<em>x</em>1*, . . . , x<strong>N* )<em>,</em> <strong>a</strong> = (<em>a</em>1*, . . . , a</strong>N* ) and <strong>R</strong> = (<em>R</em>1*, . . . , R**N* )，上标t表示时间</p><p>meaning that each constraint may depend on the state of more than one agent. 这意味着每个约束可能取决于多个代理的状态。</p><p>Finally, we defifine a policy <em>πi</em> to be a function mapping the state of agent <em>i</em> to its local action. In the scope of this work, we consider deterministic policies parameterized by <strong>θ</strong> = (<em>θ</em>1*, &mldr;, θ<strong>N* ), and thus use the notation *π</strong>θ**i* . In this context, we examine the problem of safe exploration in a constrained Markov Game (CMG) and therefore we aim to solve the following optimization <em>problem</em> for each agent:</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020010046067.png width=auto alt=image-20211020010046067></p><p>最后，我们将一个策略πi定义为一个将代理i的状态映射到其局部动作的函数。在本工作的范围内，我们考虑由θ=(θ1，&mldr;，θN)参数化的确定性策略，从而使用符号πθi。在此背景下，我们研究了约束马尔可夫博弈(CMG)中的安全探索问题，因此我们的目标是解决每个代理的以下优化问题：</p><p>The above expectation is taken with respect to all agents future action/state pairs - quantities that depend on the policy of each agent - which gives rise to a well-known problem in multi-agent settings, namely the non-stationarity of the environment from the point of view of an individual agent (Lowe et al., 2017b). 上述期望是针对所有agents未来的行动/状态对——依赖于每个agent策略的数量——这在多代理设置中导致了一个众所周知的问题，即从单个代理的角度来看，环境的非平稳性。</p><p>Alongside the constraint dependence on multiple agents, this is what constitutes the prime diffificulty in guaranteeing safety in decentralized multi-agent environments.除了对多个代理的约束依赖外，这也构成了在分散的多代理环境中保证安全性的主要缺陷。</p><a href=#22-safety-signal-model><h3 id=22-safety-signal-model><span class=hanchor arialabel=Anchor># </span><strong>2.2. Safety Signal Model</strong></h3></a><p>Following (Dalal et al., 2018), we make a first order approximation of the constraint function in (1) with respect to action <strong>a</strong>：跟随(Dalal等人，2018)，我们对（1）中约束函数的动作a进行了一阶近似</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020104954812.png width=auto alt=image-20211020104954812></p><p>x‘ denotes the state that followed <strong>x</strong> after applying action <strong>a</strong></p><p>function <em>g</em> represents a neural network with input <strong>x</strong>, output of the same dimension as the action <strong>a</strong> and weights wj</p><p>This network effificiently learns the constraints’ sensitivity to the applied actions given features of the current state based on a set of single-step transition data
<img src=https://jieye-ericx.github.io//../pics/image-20211020105409590.png width=auto alt=image-20211020105409590>该网络基于一组单步转换数据，有效地学习了当前状态特征下约束条件对应用动作的敏感性</p><p>In our experiments, we generate <em>D</em> by initializing agents with a random state and choosing actions according to a sufficiently exploratory (random) policy for multiple episodes.在我们的实验中，我们通过以随机状态初始化agent并根据充分的探索性（随机）策略选择动作来生成<em>D</em>。</p><p>With the generated data, the sensitivity network can be trained by specifying the loss function for each constraint as
<img src=https://jieye-ericx.github.io//../pics/image-20211020105611202.png width=auto alt=image-20211020105611202>where each constraints’ sensitivity will be trained separately.</p><p>通过生成的数据，可以通过指定每个约束的损失函数来将灵敏度网络训练为:(上图)其中每个约束条件的敏感性将被单独训练</p><a href=#23-safety-layer-optimization><h3 id=23-safety-layer-optimization><span class=hanchor arialabel=Anchor># </span><strong>2.3. Safety Layer Optimization</strong></h3></a><p>Given the one-step safety signals introduced in (2), we augment the policy networks by introducing an additional centralized safety layer, which enhances safety by solving</p><p>考虑到（2）中引入的一步安全信号，我们通过引入一个额外的集中式安全层来增强策略网络，从而通过解决该公式来提高安全性：</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020143746639.png width=auto alt=image-20211020143746639></p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020144127026.png width=auto alt=image-20211020144127026></p><p>This constitutes a quadratic program which computes the (minimum distance) projection of the actions proposed by each of the policy networks πθi (xi) onto the linearized safety set. Figure 1 illustrates the whole pipeline for computing an action from a given state.</p><p>这构成了一个二次程序，它计算每个策略网络πθi(xi)提出的动作在线性化安全集上的（最小距离）投影。图1说明了从给定状态计算操作的整个管道。</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020145219758.png width=auto alt=image-20211020145219758></p><p><em>Figure 1.</em> <strong>An illustration of the safety layer used in combination with the MADDPG networks in order to apply the safe projection of the optimal action</strong>. The individual states of all agents <em>xi</em> are fed into their corresponding policy networks, outputting the evaluation of their current policies at those states which are then concatenated into a single vector. Finally, a <strong>convex quadratic optimization</strong> problem is solved in order to produce the optimal safe action a∗.</p><p>图1。与MADDPG网络结合使用的安全层的说明，以应用最优动作的安全投影。所有代理xi的各个状态被输入它们相应的策略网络，输出它们当前策略的评估，这些状态将连接到单个向量中。最后，求解一个<strong>凸二次优化问题</strong>，得到最优安全作用。</p><p>Due to the strong convexity of the resulting optimization problem, there exists a global unique minimizer to the problem whenever the feasible set is non-empty. In contrast to (Dalal et al., 2018), where recursive feasibility was assumed and therefore a closed form solution using the Lagrangian multipliers was derived, we used a numerical QP-solver to defer from making this rather strong assumption on the existence of the solution, which is not guaranteed for dynamical systems.</p><p>由于所得到的优化问题的强凸性，当可行集非空时，该问题存在一个全局唯一最小化器。与(Dalaletal.，2018)相比，假设递推可行性，因此得到了使用拉格朗日乘子的封闭形式解，我们使用数值qp求解器来推迟对解的存在做出相当强的假设，这对于动态系统是不保证的。</p><p>Due to the generality of the formulation, it is possible that there exists no recoverable action that can guarantee the agents to be taken to a safe state although the previous iteration of the optimization was indeed feasible. The reason for that is that we assume a limited control authority, which further must respect the dynamics of the underlying system.由于公式的通用性，可能不存在可恢复的操作可以保证agent到达安全状态，尽管之前的优化迭代确实是可行的。原因是我们假定我们的控制是有限度的，这也是尊重重底层系统的动态性。</p><p>To take this into account without running into infeasibility problems where the agents would require a backup policy to exit unrecoverable states, we propose a soft constrained formulation, whose solution is equivalent to the original formulation whenever (4) is feasible. Otherwise, the optimizer is allowed to loosen the constraints by a penalized margin as proposed in (Kerrigan & Maciejowski, 2000). We thus reformulate (4) as follows为了考虑到这一点，而不遇到代理需要备份策略来退出不可恢复状态的不可行性问题，我们提出了一个软约束公式，当（4）成立时，其解等价于原始公式。否则，如(Kerrigan&Maciejoski，2000)所述，优化器被允许以惩罚幅度放松约束。因此，我们重新制定了（4）如下</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020152240878.png width=auto alt=image-20211020152240878></p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020152429291.png width=auto alt=image-20211020152429291></p><p>其中=(1，&mldr;，K)为松弛变量，ρ为约束违反惩罚权重。</p><p>我们选择ρ>kλ∗k∞，其中λ∗是（4)中原始问题公式的最优拉格朗日乘数，这保证了当(4)可行的时候，软约束问题会产生等价的解(见(克里根&马西耶夫斯基，2000)）。</p><p>Since exactly quantifying the optimal Lagrange multiplier is time-consuming, we assign a large value of <em>ρ</em> by inspection. It is important to mention that the reformulation in (5) still constitutes a quadratic program when extending the optimization vector into (<strong>a</strong>*,* <strong></strong>) and using an epigraph formulation (Rockafellar, 2015). 由于精确地量化最优拉格朗日乘子是耗时的，我们通过检查来分配一个很大的ρ值。值得一提的是，当将优化向量扩展到(a，)并使用表观公式时，（5）中的重新公式仍然构成了一个二次程序(Rockafellar，2015)。</p><p>Notably, this formulation does not necessarily guarantee zero constraint violations.However, we observe empirically that violations remain very small, when setting a rather high penalty value <em>ρ</em> (see Figure 5).值得注意的是，这个公式并不一定保证没有约束被违反。然而，我们从经验上观察到，当设置一个相当高的惩罚值ρ时，违规行为仍然非常小（见图5）。</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020154433535.png width=auto alt=image-20211020154433535></p><p>说明在两个实验训练中获得的累积碰撞次数。正如预期的那样，在无约束MADDPG训练中，观察到大量的碰撞，而使用硬MADDPG，碰撞减少，但与软约束相比，减少并没有那么显著。</p><a href=#24-maddpg><h3 id=24-maddpg><span class=hanchor arialabel=Anchor># </span>2.4. MADDPG</h3></a><p>For training Deep RL agents in continuous action spaces,the use of policy gradient algorithms, in which the agent’s policy is directly parameterized by a neural network, is particularly well suited as it avoids explicit maximization over continuous actions which is intractable. We thus opt for the Multi-Agent Deep Deterministic Policy Gradient(MADDPG) algorithm (Lowe et al., 2017a) which is a multiagent generalization of the well-known DDPG methods,originally proposed in (Lillicrap et al., 2015).对于在连续动作空间中训练深度RL的agent，使用策略梯度算法，其中agent的策略被神经网络直接参数化，因为它避免了棘手的连续动作的显式最大化。因此，我们选择了多agent深度确定性策略梯度(MADDPG)算法(Lowe等人，2017a)，这是对著名的DDPG方法的多代理推广，最初在(Lillicrap等人，2015)中提出。</p><p>The MADDPG algorithm is in essence a multi-agent variation of the Actor-Critic architecture, <strong>where the problem of the environment’s non-stationarity is addressed by utilizing a series of centralized Q-networks which approximate the agents’ respective optimal Q-value functions using full state and action information</strong>. This unavoidably enforces information exchange during training time, which is sometimes referenced as “centralized training”. On the other hand, the actors employ a policy gradient scheme, where each policy network has access to agent specifific information only.MADDPG算法本质上是Actor-Critic architecture的多agent变体，<strong>其中环境的非平稳性问题是通过利用一系列集中的q网络，使用完整的状态和动作信息近似代理各自的最优q值函数</strong>。这不可避免地会在训练期间加强信息交换，这有时被称为“集中培训”。另一方面，actor采用了一个策略梯度方案，其中每个策略网络只能访问代理特定的信息。</p><p>Once the policy networks converge, only local observations are required to compute each agent’s actions, thus allowing decentralized execution.一旦策略网络收敛，只需要局部观察来计算每个agent的action，从而允许分散执行。</p><p>For stability purposes, MADDPG incorporates ideas from Deep Q Networks, originally introduced in (Mnih et al.,2013). Specififically, a replay buffer <em>R</em> stores historical tuples(<strong>x</strong>*,* <strong>a</strong>*,* <strong>R</strong>*,* <strong>x</strong><em>0</em>), which can be used for off-policy learning and also for breaking the temporal correlation between samples.Furthermore, for each actor and critic network, additional target networks are used to enhance stability of the learning process. 为了稳定性的目的，MADDPG整合了来自深度Q网络的想法，最初引入于(Mnih等人，2013)。具体来说，回放缓冲区R存储历史元组(x、a、R、x0)，它可以用于off-policy学习，也可以用于打破样本之间的时间相关性。此外，对于每个actor 和critic 网络，使用额外的目标网络来增强学习过程的稳定性。</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020164216371.png width=auto alt=image-20211020164216371></p><p>We denote as <em>Q**π</em>i* (<strong>x</strong>*,* <strong>a</strong>; <em>β**i</em>) the critic network, parameterized by <em>β**i</em> , and as <em>π**i</em>(<em>x**i</em>; <em>θ**i</em>) the actor network for agent <em>i</em>, parameterized by <em>θ**i</em> . As for the target networks we denote them as
<img src=https://jieye-ericx.github.io//../pics/image-20211020162954477.png width=auto alt=image-20211020162954477>respectively.</p><p>我们用
<img src=https://jieye-ericx.github.io//../pics/image-20211020164310904.png width=auto alt=image-20211020164310904>表示critic网络，由βi参数化，用
<img src=https://jieye-ericx.github.io//../pics/image-20211020164326930.png width=auto alt=image-20211020164326930>表示agent i的actor网络，由θi参数化。目标网络分别将它们表示为
<img src=https://jieye-ericx.github.io//../pics/image-20211020164400245.png width=auto alt=image-20211020164400245></p><p>Finally, we use <em>τ</em> to denote the convex combination factor for updating the target networks.最后，我们使用τ来表示更新目标网络的凸组合因子。</p><a href=#25-implementation-details><h3 id=25-implementation-details><span class=hanchor arialabel=Anchor># </span><strong>2.5. Implementation Details</strong></h3></a><p>In order to assess the performance of our proposed method we conducted experiments using the multi-agent particle environment, which was previously studied in (Lowe et al.,2017a) and (Mordatch & Abbeel, 2018). In this environment, a fixed number of agents are moving collaboratively in a 2-D grid trying to reach specifific target positions. In our experiments, we used three agents that are constrained to avoid collisions among them. Each agent’s state <em>xi</em> is composed of a vector in R10, containing its position and velocity, the relative distances to the other agents and the target landmark location. Moreover, the actions <em>ai</em> are defined as vectors in R2 containing the acceleration on the two axes.为了评估我们提出的方法的性能，我们使用multi-agent particle 环境进行了实验，这之前在(Lowe等人，2017a)和(莫达奇&Abbeel，2018)中进行了研究。在这种环境下，固定数量的agent在二维网格中协作移动，试图达到特定的目标位置。在我们的实验中，我们使用了三种受约束避免它们之间的碰撞的agent.每个代理的状态xi由R10中的一个向量组成，其中包含其位置和速度、与其他agent的相对距离和目标地标位置。此外，动作ai在r2中作为包含两个轴上的加速度的向量。</p><p>The reward assigned to each agent is proportional to the negative l1 distance of the agent from its corresponding target and furthermore, collisions are being penalized. The agents receive individual rewards based on their respective performance. For the safety layer pre-training, we train a simple fully connected ReLU network <em>g</em>(<strong>x</strong>*, w**i*) with a 10-neuron hidden layer for each of the six existing constraints (two possible collisions for each agent), on the randomly produced dataset <em>D</em>. Note that, due to the pairwise symmetry of the constraints used in the experiment (A colliding with B implies B colliding with A), we could in principle simplify the network design, but for generality we decided to consider them as independent constraints. Based on our empirical results (Section 3), we found it unnecessary to increase the complexity of the network. 分配给每个agent 的奖励与agent 与其相应目标的负l1距离成正比，此外，冲突会受到惩罚。agent 会根据各自的表现获得奖励。对于在随机生成的数据集D上的安全层预训练，我们在随机产生的数据集上训练一个简单的全连接ReLU网络g(x，wi)，6个约束（每个agent 有两个可能的碰撞），每个约束都有一个10个神经元的隐藏层。注意，由于实验中使用的约束的成对对称(与B碰撞意味着B与A碰撞)，我们原则上可以简化网络设计，但为了通用，我们决定将它们视为独立的约束。基于我们的实证结果（第3节），我们发现没有必要增加网络的复杂性。</p><p>We train the model using the popular Adam optimizer (Kingma & Ba, 2015) with a batch size of 256 samples. For solving the QP Problem, we adopted the qpsolvers library, which employs a dual active set algorithm originally proposed in (Goldfarb& Idnani, 1983). We further used a value of <em>ρ</em> = 1000 in the cost function of the soft formulation. For the MADDPG algorithm implementation, we used three pairs of fully connected actor-critic networks. These networks are composed of two hidden layers with 100 and 500 neurons respectively.The choice for all activation functions is ReLU except for the output layer of the actor networks, where tanh was used to compress the actions in the [-1,1] range and represent the agents’ limited control authority. The convex combination rate <em>τ</em> , used when updating the target networks,was set to 0*.*01.我们使用流行的Adam优化器(Kingma&Ba，2015)训练模型，batch size为256个样本。为了求解QP问题，我们采用了qpsolvers ，它采用了(Goldfarb&Idnani&1983)最初提出的双主动集算法（dual active set algorithm）。我们进一步在软约束的成本函数中使用了ρ=1000的值。对于MADDPG算法的实现，我们使用了三对完全连接的actor-critic 网络。这些网络由两个隐藏层组成，分别由100和500个神经元组成。除actor 网络的输出层外，所有激活函数的选择都是ReLU，参与者网络的输出层将tanh用于将动作压缩到[-1,1]范围内，并表示agent有限的控制权限。将更新目标网络时使用的凸组合率τ设置为0.01。</p><p>To evaluate the algorithm’s robustness and its capability of coming up with an implicit recovery policy, we conduct two case studies:为了评估该算法的鲁棒性及其提出隐式恢复策略的能力，我们进行了两个案例研究：</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020213527611.png width=auto alt=image-20211020213527611></p><p>**(ED)**Inject an <strong>E</strong>xogenous uniform <strong>D</strong>isturbance after each step of the environment, which resembles a very common scenario in real life deployment where environment mismatch could lead to such a behaviour.在环境的每一步之后输入一个外源性均匀干扰，这类似于现实生活部署中非常常见的场景，其中环境不匹配可能导致这种行为。</p><p>**(UI)**Allow the environment to be <strong>U</strong>nsafely <strong>I</strong>nitialized,which can also occur in practice.允许环境不安全地初始化，这也可能在实践中发生。</p><a href=#3-results><h2 id=3-results><span class=hanchor arialabel=Anchor># </span>3 Results</h2></a><p>We assess the performance of the proposed algorithm on three metrics: average reward, number of collisions during training, and number of collisions during testing (i.e. aftertraining has converged). An infeasible occurrence appears in case the hard-constrained QP in (4) fails to determine a solution that satisfifies the imposed constraints.我们在三个指标上评估了该算法的性能：平均奖励、训练中的碰撞次数和测试中的碰撞次数（即训练收敛后）。如果（4）中的硬约束QP不能确定满足所施加约束的解，则会出现不可行的情况。</p><p>We benchmark a total of three different Deep RL strategies:我们总共对三种不同的深度RL策略进行了基准测试</p><ul><li>unconstrained MADDPG 优先考虑探索和学习而不是安全，因为约束不是直接施加的</li><li>hard-constrained MADDPG (hard MADDPG) 施加硬状态约束来考虑到安全操作</li><li>soft-constrained MADDPG (soft MADDPG) 按照我们在（5）中的公式，施加了状态约束的放松版本，同时penalizing the amount of slack</li></ul><p>The first approach prioritizes exploration and learning over safety since constraints are not directly imposed, whereas hard MADDPG takes into account safe operation by imposing hard state constraints. Finally, soft MADDPG, as presented in Algorithm 1, imposes a relaxed version of the state constraints while penalizing the amount of slack, following our formulation in (5).第一种方法优先考虑探索和学习而不是安全，因为约束不是直接施加的，而硬MADDPG则通过施加硬状态约束来考虑到安全操作。最后，如算法1所示的软MADDPG，按照我们在（5）中的公式，施加了状态约束的放松版本，同时惩罚松弛量。</p><p>The duration of each experiment is 8000 episodes and, in order to assess uncertainty, we repeat each experiment 10 times using different initial random seeds.每个实验的持续时间为8000集，为了评估不确定性，我们使用不同的初始随机种子重复每个实验10次。</p><p>Under normal operating conditions (safe initialization without disturbances), both the hard and the soft-constrained MADDPG strategies achieve 0 constraint violations in our experiments during the training and the testing phase.在正常操作条件下（安全初始化无干扰），在训练和测试阶段，在我们的实验中，硬约束和软约束MADDPG策略都实现了0个约束违反。</p><p>However,in order to examine the robustness properties of the aforementioned methods, we evaluate our models under the case studies mentioned in the end of Section 2.5. The outcome of the experiments along with the 95% confifidence intervals are summarized in Table 1.为了检验上述方法的鲁棒性，我们在第2.5节末尾提到的案例研究下评估了我们的模型。实验结果和95%五度区间汇总见表1.</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020170221784.png width=auto alt=image-20211020170221784></p><p>表中说明了所提出的算法的实验结果。对于每个setting，我们报告了多次运行中的平均值和相应的95%置信区间。在硬MADDPG中观察到更高的奖励，由于不可行性问题，在训练过程中显示的碰撞次数高于软MADDPG碰撞次数最低的MADDPG。最后，值得注意的是，在测试过程中也存在相同的pattern。</p><p>In Figure 3, the evolution over episodes of the average reward is depicted for the <strong>(ED)</strong> case. The average reward is computed as the mean over the 3 agents in a single episode.Interestingly, we observe that all three presented algorithms have a similar trend, suggesting that introducing the safetyframework (in both hard- and soft- variants) does not negatively affect the ability of the agents to reach their targets. A similar result holds for the case of unsafe itialization <strong>(UI)</strong>,so the respective plot is omitted for brevity.</p><p>在图3中，描述了**(ED)案例的平均奖励的演变。平均奖励被计算为一个事件中3个代理的平均值。有趣的是，我们观察到所有三种算法都有相似的趋势，这表明引入安全框架（包括硬变体和软变体）不会对代理达到目标的能力产生负面影响。类似的结果与不安全初始化(UI)也类似，因此为了简洁起见，省略了各自的图。</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020170649882.png width=auto alt=image-20211020170649882></p><p>Figure 4 shows for each setting, the average number of collisions per episode <em>during training</em>, while Figure 5 presents the evolution of the cumulative number of collisions over the training episodes. In particular, soft MADDPG exhibits 97*.<em>71% <strong>(UI)</strong>, 97</em>.<em>99% <strong>(ED)</strong> fewer collisions compared to the unconstrained MADDPG. On the other hand, hard MADDPG only achieves a 84</em>.<em>38% <strong>(UI)</strong>, 42</em>.*42% <strong>(ED)</strong> reduction in collisions compared to the unconstrained MADDPG,since the infeasibility of the optimization problem does at times not allow the safety fifilter to intervene and correct the proposed actions.图4显示了每个设置，训练期间每集的平均碰撞次数，而图5显示了训练过程中累积碰撞次数的演变。特别是，软约束MADDPG展示了97.71%<strong>(UI)</strong>，与无约束的MADDPG相比，碰撞次数为减少了97.99%(ED)。另一方面，硬MADDPG只达到了84.38%<strong>(UI)</strong>，与无约束的MADDPG相比，碰撞减少了42.42%<strong>(ED)</strong>，因为优化问题的不可行性有时不允许安全光纤过滤器进行干预和纠正所提议的操作。</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020202949421.png width=auto alt=image-20211020202949421></p><p>To evaluate the impact of the hard-constrained MADDPG,it is essential to investigate the infeasible occurrences, since they represent the critical times when constraints can no longer be satisfified. In our experiments, 20.9% <strong>(UI)</strong>, 56.7 %<strong>(ED)</strong> of the episodes are directly related to infeasible conditions. This motivates the necessity for a soft-constrained safety layer that maintains feasibility and preserves safety in cases where the hard constrained formulation fails to return a solution.为了评估硬约束MADDPG的影响，必须调查不可行的情况，因为它们代表了约束再也不能被满足的临界时间。在我们的实验中，20.9%(UI)、56.7%(ED)与不可行的条件直接相关。这激发了建立软约束安全层的必要性，即在硬约束配方未能返回解决方案的情况下，保持可行性和保持安全性。</p><p>Finally, in order to gain a better understanding of the behaviour of our algorithm after convergence, we ran test simulations of 100 episodes for each agent for 10 different initial random seeds. The cumulative number of the collisions for the 2 different settings is illustrated in Figure 6. It is evident that in both settings, the soft constrained formulation achieves the minimum number of collisions.For visualization purposes, we provide the videos of the test simulation at the following Video Repository.最后，为了更好地理解我们的算法在收敛后的行为，我们对10个不同的初始随机种子对每个代理进行了100个episode的测试模拟。两种不同设置的累积碰撞次数如图6所示。很明显，在这两种情况下，软约束公式都实现了最小的碰撞次数。为了可视化的目的，我们在以下视频存储库中提供了测试模拟的视频。</p><p><img src=https://jieye-ericx.github.io//../pics/image-20211020203609533.png width=auto alt=image-20211020203609533></p><a href=#4-conclusion><h2 id=4-conclusion><span class=hanchor arialabel=Anchor># </span>4 Conclusion</h2></a><p>We proposed an extension of Safe DDPG (Dalal et al., 2018) to multi-agent settings. <strong>From a technical perspective, we relaxed some of the conservative assumptions made in the original single-agent work by introducing soft constraints in the optimization objective.</strong> 我们提出将SafeDDPG(Dalal等人，2018)扩展到多智能体设置。从技术的角度来看，我们通过在优化目标中引入软约束，放宽了在原始的单代理工作中所做的一些保守假设。</p><p>This allows us to generalize the approach to settings where more than one constraint is active, which is typically the case for multi-agent environments. 这使我们能够将该方法推广到多个约束处于活动的设置</p><p>Our empirical results suggest that our soft constrained formulation achieves as dramatic decrease in constraint violations during training when exogenous disturbances and unsafe initialization are encountered, while maintaining the ability to explore and hence solve the desired task successfully. 我们的实证结果表明，当训练过程中遇到外源干扰和不安全初始化时，我们的软约束公式可以显著减少约束违反，同时保持探索从而成功解决所需任务的能力。</p><p>Although this observation does not necessarily generalize to more complex environments, it motivates the practicality of our algorithm in safety-critical deployment under more conservative constraint tightenings. 虽然这一观察结果并不一定适用于更复杂的环境，但它激发了我们的算法在更保守的约束约束下的安全临界部署中的实用性。</p><p>Finally, while our preliminary results are encouraging, we believe there is ample room for improvement and further experimentation. As part of future work, we would like to introduce a reward based on the intervention of the safety fifilter during training, such that we can indirectly propagate the safe behavior to the learnt policies of the agents, which could ultimately eliminate the requirement for using a centralized safety fifilter during test time. 最后，虽然我们的初步结果令人鼓舞，但我们相信有足够的改进空间和进一步的实验。作为未来工作的一部分，我们想在训练过程中使奖励基于安全过滤器的干预，这样我们可以间接传播安全行为关于agent已经的学习策略，这可能最终消除在测试时间使用集中安全过滤器的需求。</p><p>Additionally, we would like to deploy our approach in more complex environments to explore the true potential of our work.此外，我们希望在更复杂的环境中部署我们的方法，以探索我们工作的真正潜力。</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://jieye-ericx.github.io/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Jieye ericx using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://jieye-ericx.github.io/>Home</a></li><li><a href=https://github.com/jieye-ericx>Github</a></li></ul></footer></div></div></body></html>