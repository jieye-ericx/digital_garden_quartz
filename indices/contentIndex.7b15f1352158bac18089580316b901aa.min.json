{"/":{"title":"AboutTheGarden","content":"\nhello 👋，我是 jieye 🤐，一个 SE 硕士在读，写过几行代码的码农练习生。\n\n这里是一座「全开放式」的[[数字花园]]，由原子化的卡片笔记编织而成。目前主要在浇灌这几个领域 ⭕：\n\n- [[💻技术学习笔记]]\n- [[💽博文分享]]\n- [[Obsidian高级探索]]\n\n另有更多自动化分类排序索引请见 [[HOMEPAGE]]（Hugo 不一定支持解析）\n虽然有导览，但这绝不是传统的博客，这是我在思考了很多个人知识库方案后作出的实践，所以可能有很多看起来令人费解的折腾内容。当您漫步花园时，这里有 2 个不成熟的小建议 💁：\n\n- 尽量通过鼠标悬浮预览进行[[上下文]]不中断的阅读。\n- 尽量通过底部**反向链接**找回来时的路。\n\n这座数字花园使用[Obsidian](https://obsidian.md/)写作和发布，且毫无保留地**开源**，欢迎来[Github](https://github.com/jieye-ericx/jieye-ericx.github.io)star 一下。\n\n部署本花园离不开前人栽树，感谢 [[PKM]] 大佬 [oldwinter]( https://garden.oldwinter.top/ ) 和脚本提供者[🪴 Quartz 3.3](https://quartz.jzhao.xyz/)\n，让我认识到存在这样的个人知识库方案, 如果你也想部署，这里有我的[[部署总结]]。\n\n最后，如果您发现了令人不适的内容，或我的个人隐私，请告知我，万分感谢 🦀🦀: jieye.ericx@gmail.com。\n","lastmodified":"2023-05-09T16:33:58.287366302Z","tags":["花园"]},"/%E4%B8%8A%E4%B8%8B%E6%96%87":{"title":"上下文","content":"\n上下文\n\nContext 在许多编程语言/框架中都出现，曾几何时，我难以理解这个单词的含义\n","lastmodified":"2023-05-09T16:33:58.295366497Z","tags":["花园"]},"/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4":{"title":"两阶段提交","content":"\n事务提交后，redo log 和 binlog 都要持久化到磁盘，但是这两个是**独立**的逻辑，可能出现半成功的状态，这样就造成两份日志之间的逻辑不一致。\n\n- **如果在将 redo log 刷入到磁盘之后， MySQL 突然宕机了，而 binlog 还没有来得及写入**\n- **如果在将 binlog 刷入到磁盘之后， MySQL 突然宕机了，而 redo log 还没有来得及写入**\n\n**MySQL 为了避免出现两份日志之间的逻辑不一致的问题，使用了「两阶段提交」来解决**，**两阶段提交其实是分布式事务一致性协议**，它可以保证多个逻辑操作要不全部成功，要不全部失败，不会出现半成功的状态。\n\n**两阶段提交把单个事务的提交拆分成了 2 个阶段，分别是「准备（Prepare）阶段」和「提交（Commit）阶段」**，每个阶段都由协调者（Coordinator）和参与者（Participant）共同完成。\n\n## 两阶段提交的过程\n\n在 MySQL 的 InnoDB 存储引擎中，开启 binlog 的情况下，MySQL 会同时维护 binlog 日志与 InnoDB 的 redo log，为了保证这两个日志的一致性，MySQL 使用了**内部 XA 事务**（是的，也有外部 XA 事务，跟本文不太相关，我就不介绍了），内部 XA 事务**由 binlog 作为协调者，存储引擎是参与者。**\n\n当客户端执行 commit 语句或者在自动提交的情况下，MySQL 内部开启一个 XA 事务，**分两阶段来完成 XA 事务的提交**，如下图：\n![两阶段提交](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4.drawio.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n从图中可看出，事务的提交过程有两个阶段，就是**将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入 binlog**，具体如下：\n\n- **prepare 阶段**：将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘（innodb_flush_log_at_trx_commit = 1 的作用）；\n- **commit 阶段**：把 XID 写入到 binlog，然后将 binlog 持久化到磁盘（sync_binlog = 1 的作用），接着调用引擎的提交事务接口，将 redo log 状态设置为 commit，此时该状态并不需要持久化到磁盘，只需要 write 到文件系统的 page cache 中就够了，因为只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，一样会被认为事务已经执行成功；\n","lastmodified":"2023-05-09T16:33:58.299366595Z","tags":["mysql"]},"/%E5%88%86%E5%B8%83%E5%BC%8F":{"title":"分布式","content":"\n关于分布式 20230314 也看过不少面试题和博客了，但感觉一直无法完全理解，所以有了这篇总结[[分布式系统]]  \n\n[[分布式系统]] 是直接由 xmind 导出，格式有些混乱，所以还是看这张图吧  \n通过极客时间专栏、pdai、javaguide 等渠道学习制作了此xmind  \n![dist](https://cdn.jsdelivr.net/gh/jieye-ericx/rax-picbed/PicList/obsidian/dist.jpg)  \n不定期更新图片  \n[分布式数据库](分布式数据库.md)\n[[kafka]]","lastmodified":"2023-05-09T16:33:58.299366595Z","tags":[]},"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93":{"title":"分布式数据库","content":"\nhttps://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/24%E8%AE%B2%E5%90%83%E9%80%8F%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AE%8C/01%20%20%E5%AF%BC%E8%AE%BA%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%9F%E8%81%8A%E8%81%8A%E5%AE%83%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F.md\n\n![](Pasted%20image%2020230314165709.png)\n\n**分布式数据库的核心——数据分片、数据同步**\n\n1. 数据分片  \n该特性是分布式数据库的技术创新。它可以突破中心化数据库单机的容量限制，从而将数据分散到多节点，以更灵活、高效的方式来处理数据。这是分布式理论带给数据库的一份礼物。  \n分片方式包括两种。\n- 水平分片：按行进行数据分割，数据被切割为一个个数据组，分散到不同节点上。\n- 垂直分片：按列进行数据切割，一个数据表的模式（Schema）被切割为多个小的模式。\n2. 数据同步  \n它是**分布式数据库的底线**。由于数据库理论传统上是建立在单机数据库基础上，而引入分布式理论后，一致性原则被打破。因此需要引入数据库同步技术来帮助数据库恢复一致性。  \n简而言之，就是使分布式数据库用起来像“正常的数据库”。所以数据同步背后的推动力，就是人们对数据“一致性”的追求。这两个概念相辅相成，互相作用。\n\n### sql 与 nosql\n\n![](Pasted%20image%2020230314170435.png)\n\n## 1 数据分片 -提高数据容量和性能\n\n1. 水平分片：在不同的数据库节点中存储同一表的不同行。\n2. 垂直分片：在不同的数据库节点中存储表不同的表列。\n\n### 分片算法\n\n分片算法一般指代水平分片所需要的算法。经过多年的演化，其已经在大型系统中得到了广泛的实践。下面我将介绍两种最常见的水平分片算法，并简要介绍一些其他的分片算法优化思路。\n\n#### 哈希分片\n\n哈希分片，首先需要获取分片键，然后根据特定的哈希算法计算它的哈希值，最后使用哈希值确定数据应被放置在哪个分片中。数据库一般对所有数据使用统一的哈希算法（例如 ketama），以促成哈希函数在服务器之间均匀地分配数据，从而降低了数据不均衡所带来的热点风险。通过这种方法，数据不太可能放在同一分片上，从而使数据被随机分散开。\n\n这种算法非常适合随机读写的场景，能够很好地分散系统负载，但弊端是不利于范围扫描查询操作。下图是这一算法的工作原理。  \n![](Pasted%20image%2020230314171613.png)\n\n#### 范围分片\n\n范围分片根据数据值或键空间的范围对数据进行划分，相邻的分片键更有可能落入相同的分片上。每行数据不像哈希分片那样需要进行转换，实际上它们只是简单地被分类到不同的分片上。下图是范围分片的工作原理。  \n![Drawing 2.png](https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/24%E8%AE%B2%E5%90%83%E9%80%8F%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AE%8C/assets/Cip5yGABUXSATworAABCLehE-pM870.png)  \n范围分片需要选择合适的分片键，这些分片键需要尽量不包含重复数值，也就是其候选数值尽可能地离散。同时数据不要单调递增或递减，否则，数据不能很好地在集群中离散，从而造成热点。\n\n范围分片非常适合进行范围查找，但是其随机读写性能偏弱。\n\n#### 融合算法\n\n这时我们应该意识到，以上介绍的哈希和范围的分片算法并不是非此即彼，二选一的。相反，我们可以灵活地组合它们。\n\n例如，我们可以建立一个多级分片策略，该策略在最上层使用哈希算法，而在每个基于哈希的分片单元中，数据将按顺序存储。\n\n这个算法相对比较简单且灵活，下面我们再说一个地理位置算法。\n\n#### 地理位置算法\n\n该算法一般用于 NewSQL 数据库，提供全球范围内分布数据的能力。\n\n在基于地理位置的分片算法中，数据被映射到特定的分片，而这些分片又被映射到特定区域以及这些区域中的节点。\n\n然后在给定区域内，使用哈希或范围分片对数据进行分片。例如，在美国、中国和日本的 3 个区域中运行的集群可以依靠 User 表的 Country_Code 列，将特定用户（User）所在的数据行映射到符合位置就近规则的区域中。\n\n那么以上就是几种典型的分片算法，下面我们接着讨论如何将分片算法应用到实际的场景中。\n\n### 分片案例\n\n[Apache ShardingShpere](https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/24%E8%AE%B2%E5%90%83%E9%80%8F%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AE%8C/03%20%20%E6%95%B0%E6%8D%AE%E5%88%86%E7%89%87%EF%BC%9A%E5%A6%82%E4%BD%95%E5%AD%98%E5%82%A8%E8%B6%85%E5%A4%A7%E8%A7%84%E6%A8%A1%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%9F.md#:~:text=ShardingShpere%20%E9%A6%96%E5%85%88%E6%8F%90%E4%BE%9B%E4%BA%86%E5%88%86%E5%B8%83)  \n[TiDB](https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/24%E8%AE%B2%E5%90%83%E9%80%8F%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AE%8C/03%20%20%E6%95%B0%E6%8D%AE%E5%88%86%E7%89%87%EF%BC%9A%E5%A6%82%E4%BD%95%E5%AD%98%E5%82%A8%E8%B6%85%E5%A4%A7%E8%A7%84%E6%A8%A1%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%9F.md#:~:text=TiDB%20%E5%B0%B1%E6%98%AF-,%E4%B8%80%E4%B8%AA,-%E5%9E%82%E7%9B%B4%E4%B8%8E%E6%B0%B4%E5%B9%B3)\n\n## 2 高可用-重要根基：复制\n\n### 单主复制\n\n单主复制，也称主从复制。写入主节点的数据都需要复制到从节点，即存储数据库副本的节点。当客户要写入数据库时，他们**必须将请求发送给主节点**，而后主节点将这些数据转换为复制日志或修改数据流发送给其所有从节点。**从使用者的角度来看，从节点都是只读的**。下图就是经典的主从复制架构。\n\n![Drawing 0.png](https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/24%E8%AE%B2%E5%90%83%E9%80%8F%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AE%8C/assets/Ciqc1GAJV6SADprzAACli5qqAMo678.png)\n\n这种模式是最早发展起来的复制模式，不仅被广泛应用在传统数据库中，如 PostgreSQL、MySQL、Oracle、SQL Server；  \n它也被广泛应用在一些分布式数据库中，如 MongoDB、RethinkDB 和 Redis 等。\n\n那么接下来，我们就从复制同步模式、复制延迟、复制与高可用性以及复制方式几个方面来具体说说这个概念。\n\n#### 复制同步模式\n\n复制是一个非常耗费时间而且很难预测完成情况的操作。虽然其受影响的因素众多，但一个复制操作是同步发生还是异步发生，被认为是极为重要的影响因素，可以从以下三点来分析。\n\n1. 同步复制：如果由于从库已崩溃，存在网络故障或其他原因而没有响应，则主库也无法写入该数据。\n2. 半同步复制：其中部分从库进行同步复制，而其他从库进行异步复制。也就是，如果其中一个从库同步确认，主库可以写入该数据。\n3. 异步复制：不管从库的复制情况如何，主库可以写入该数据。而此时，如果主库失效，那么还未同步到从库的数据就会丢失。  \n可以看到**不同的同步模式是在性能和一致性上做平衡**，三种模式对应不同场景，并没有好坏差异。用户需要根据自己的业务场景来设置不同的同步模式。\n\n#### 复制延迟\n\n如果我们想提高数据库的查询能力，最简便的方式是向数据库集群内添加足够多的从节点。这些从节点都是只读节点，故查询请求可以很好地在这些节点分散开。  \n但是如果使用同步复制，每次写入都需要同步所有从节点，会造成一部分从节点已经有数据，但是主节点还没写入数据。而异步复制的问题是从节点的数据可能不是最新的。  \n**以上这些问题被称为“复制延迟”，在一般的材料中，我们会听到诸如“写后读”“读单增”等名词来解决复制延迟。但是这些概念其实是数据一致性模型的范畴**。\n\n#### 复制与高可用性\n\n高可用（High availablity）是一个 IT 术语，指系统无中断地执行其功能的能力。系统中的任何节点都可能由于各种出其不意的故障而造成计划外停机；同时为了要维护系统，我们也需要一些计划内的停机。采用主从模式的数据库，可以防止单一节点挂起导致的可用性降低的问题。\n\n系统可用程度一般使用小数点后面多个 9 的形式，如下表所示。  \n**可用性 年故障时间**  \n99.9999% 32秒  \n99.999% 5分15秒  \n99.99% 52分34秒  \n99.9% 8小时46分  \n99% 3天15小时36分  \n一般的生产系统都会至少有两个 9 的保证，追求三个9，想要做到4个9是非常最具有挑战的。  \n在主从模式下，为了支撑高可用，就需要进行故障处理。我这里总结了两种可能的故障及其处理方案。\n\n1. **从节点故障**。由于**每个节点都复制了从主库那里收到的数据更改日志**，因此它知道在发生故障之前已处理的最后一个事务，由此可以凭借此信息从主节点或其他从节点那里恢复自己的数据。\n2. **主节点故障**。在这种情况下，需要在从节点中选择一个成为新的主节点，此过程称为故障转移，可以手动或自动触发。其典型过程为：第一步根据超时时间确定主节点离线；第二步选择新的主节点，这里注意**新的主节点通常应该与旧的主节点数据最为接近**；第三步是重置系统，让它成为新的主节点。\n\n#### 复制方式\n\n为了灵活并高效地复制数据，下面我介绍几种常用的复制方式。  \n**1. 基于语句的复制**  \n主库记录它所执行的每个写请求（一般以 SQL 语句形式保存），每个从库解析并执行该语句，就像从客户端收到该语句一样。但这种复制会有一些潜在问题，如语句使用了获取当前时间的函数，复制后会在不同数据节点上产生不同的值。  \n另外如自增列、触发器、存储过程和函数都可能在复制后产生意想不到的问题。但可以通过预处理规避这些问题。使用该复制方式的分布式数据库有 VoltDB、Calvin。  \n**2. 日志（WAL）同步**  \nWAL 是一组字节序列，其中包含对数据库的所有写操作。它的内容是一组低级操作，如向磁盘的某个页面的某个数据块写入一段二进制数据，主库通过网络将这样的数据发送给从库。\n\n这种方法避免了上面提到的语句中部分操作复制后产生的一些副作用，但要求主从的数据库引擎完全一致，最好版本也要一致。如果要升级从库版本，那么就需要计划外停机。PostgreSQL 和 Oracle 中使用了此方法。  \n**3. 行复制**  \n它由一系列记录组成，这些记录描述了以行的粒度对数据库表进行的写操作。它与特定存储引擎解耦，并且第三方应用可以很容易解析其数据格式。  \n**4. ETL 工具**  \n该功能一般是最灵活的方式。用户可以根据自己的业务来设计复制的范围和机制，同时在复制过程中还可以进行如过滤、转换和压缩等操作。但性能一般较低，故适合处理子数据集的场景。  \n关于单主复制就介绍到这里，下面我们再来说说多主复制。\n\n### 多主复制\n\n也称为主主复制。数据库集群内存在多个对等的主节点，它们可以同时接受写入。每个主节点同时充当主节点的从节点。  \n多主节点的架构模式最早来源于 DistributedSQL 这一类多数据中心，跨地域的分布式数据库。在这样的物理空间相距甚远，有多个数据中心参与的集群中，每个数据中心内都有一个主节点。而在每个数据中心的内部，却是采用常规的单主复制模式。  \n这么设计该类系统的目的在于以下几点。\n\n1. 获得更好的写入性能：使数据可以就近写入。\n2. 数据中心级别的高可用：每个数据中心可以独立于其他数据中心继续运行。\n3. 更好的数据访问性能：用户可以访问到距离他最近的数据中心。\n\n但是，此方法的最大缺点是，**存在一种可能性，即两个不同的主节点同时修改相同的数据**。这其实是非常危险的操作，应尽可能避免。这就需要**一致性模型**，配合冲突解决机制来规避。\n\n还有一种情况是处理客户端离线操作的一致性问题。为了提高性能，数据库客户端往往会缓存一定的写入操作，而后批量发送给服务端。这种情况非常**类似于大家使用协作办公文档工具的场景。在这种情况下，每个客户端都可以被看作是具有主节点属性的本地数据库，并且多个客户端之间存在一种异步的多主节点复制的过程**。这就需要数据库可以协调写操作，并处理可能的数据冲突。\n\n典型的多主复制产品有 MySQL 的 Tungsten Replicator、PostgreSQL 的 BDR 和 Oracle 的 GoldenGate。\n\n目前，**大部分 NewSQL、DistributedSQL 的分布式数据库都支持多主复制，但是大部分是用 Paxos 或 Raft 等协议来构建复制组，保证写入线性一致或顺序一致性**；同时传统数据库如 MySQL 的 MGR 方案也是使用类似的方式，可以看到**该方案是多主复制的发展方向**。\n\n### MySQL 复制技术的发展\n\nMySQL 由于其单机机能的限制，很早就发展了数据复制技术以提高性能。同时依赖该技术，MySQL 可用性也得到了长足的发展。  \n截止到现在，该技术经历了四代的发展。第一代为传统复制，使用 MHA（Master High Available）架构；第二代是基于 GTID 的复制，即 GTID+Binlog server 的模式；第三代为增强半同步复制，GTID+增强半同步复制；**第四代为 MySQL 原生高可用，即 MySQL InnoDB Cluster**。  \n数据库的复制技术需要考虑两个因素：数据一致 RPO 和业务连续性 RTO。所以，就像前面的内容所强调的，复制与一致性是一对如影随形的概念，本讲内容聚焦于复制，但是会提到关于一致性相关的概念。  \n下面我就从第一代复制技术开始说起。\n\n#### MHA 复制控制\n\n下图是 MHA 架构图。  \n![Drawing 1.png](https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/24%E8%AE%B2%E5%90%83%E9%80%8F%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AE%8C/assets/Cip5yGAJV9qAVnjXAAC85xLxhaU613.png)  \nMHA 作为第一代复制架构，有如下适用场景：\n\n1. MySQL 的版本≤5.5，这一点说明它很古老；\n2. 只用于异步复制且一主多从环境；\n3. 基于传统复制的高可用。\n\nMHA 尽最大能力做数据补偿，但并不保证一定可以成功；它也尽最大努力在实现 RPO，有 RTO 概念支持。可以看到它只是一个辅助工具，本身的架构与机制对 RPO 和 RTO 没有任何保障。  \n那么由此可知，它会存在如下几个问题：\n\n1. 它的 GTID 模型强依赖 binlog server，但是对于 5.7 后的 binlog 却不能识别，同时对并行复制支持不好；\n2. 服务 IP 切换依赖自行编写的脚本，也可以与 DNS 结合，其运维效果取决于运维人员的经验；\n3. 运维上需要做 SSH 信任、切换判断等人工操作，总体上处于“刀耕火种”的状态，自动化程度较低，维护难度高；\n4. 现在项目基本无维护。\n\n从上述问题中可以看到，MHA 作为第一代复制架构，功能相对原始，但已经为复制技术的发展开辟了道路，特别是对 GTID 和 binlog 的应用。但如果不是维护比较古老的 MySQL 集群，目前已经不推荐采用它了。\n\n#### 半同步复制\n\n这是第二代复制技术，它与第一代技术的差别表现在以下几点。\n\n1. binlog 使用半同步，而第一代是异步同步。它保障了数据安全，一般至少要同步两个节点，保证数据的 RPO。\n2. 同时保留异步复制，保障了复制性能。并通过监控复制的延迟，保证了 RTO。\n3. 引入配置中心，如 consul。对外提供健康的 MySQL 服务。\n4. 这一代开始需要支持跨 IDC 复制。需要引入监控 Monitor，配合 consul 注册中心。多个 IDC 中 Monitor 组成分布式监控，把健康的 MySQL 注册到 consul 中，同时将从库复制延迟情况也同步到 consul 中。\n\n下图就是带有 consul 注册中心与监控模块的半同步复制架构图。\n\n![Drawing 2.png](https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/24%E8%AE%B2%E5%90%83%E9%80%8F%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AE%8C/assets/Cip5yGAJV-KAAg5HAAF8syZ9vQM483.png)\n\n第二代复制技术也有自身的一些缺陷。\n\n1. 存在幻读的情况。当事务同步到从库但没有 ACK 时，主库发生宕机；此时主库没有该事务，而从库有。\n2. MySQL 5.6 本身半同步 ACK 确认在 dump_thread 中，dump_thread 存在 IO 瓶颈问题。\n\n基于此，第三代复制技术诞生。\n\n#### 增强半同步复制\n\n这一代需要 MySQL 是 5.7 以后的版本。有一些典型的框架来支持该技术，如 MySQL Replication Manager、GitHub-orchestrator 和国内青云开源的 Xenon 等。\n\n这一代复制技术采用的是增强半同步。首先主从的复制都是用独立的线程来运行；其次主库采用 binlog group commit，也就是组提交来提供数据库的写入性能；而从库采用并行复制，它是基于事务的，通过数据参数调整线程数量来提高性能。这样主库可以并行，从库也可以并行。\n\n这一代技术体系强依赖于增强半同步，利用半同步保证 RPO，对于 RTO，则取决于复制延迟。\n\n下面我们用 Xenon 来举例说明，请看下图（图片来自官网）。\n\n![Drawing 3.png](https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/24%E8%AE%B2%E5%90%83%E9%80%8F%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AE%8C/assets/CgpVE2AJV-mAE6vWAAB_JZptW8Y497.png)\n\n从图中可以看到。每个节点上都有一个独立的 agent，这些 agent 利用 raft 构建一致性集群，利用 GTID 做索引选举主节点；而后主节点对外提供写服务，从节点提供读服务。\n\n当主节点发生故障后，agent 会通过 ping 发现该故障。由于 GTID 和增强半同步的加持，从节点与主节点数据是一致的，因此很容易将从节点提升为主节点。\n\n第三代技术也有自身的缺点，如增强半同步中存在幽灵事务。这是由于数据写入 binlog 后，主库掉电。由于故障恢复流程需要从 binlog 中恢复，那么这份数据就在主库。但是如果它没有被同步到从库，就会造成从库不能切换为主库，只能去尝试恢复原崩溃的主库。\n\n#### MySQL 组复制\n\n组复制是 MySQL 提供的新一代高可用技术的重要组成。其搭配 MySQL Router 或 Proxy，可以实现原生的高可用。\n\n从这一代开始，MySQL 支持多主复制，同时保留单主复制的功能。其单主高可用的原理与第三代技术类似，这里我们不做过多分析了。\n\n现在说一下它的多主模式，原理是使用 MySQL Router 作为数据路由层，来控制读写分离。而后组内部使用 Paxos 算法构建一致性写入。\n\n它与第三代复制技术中使用的一致性算法的作用不同。三代中我们只使用该算法来进行选主操作，数据的写入并不包含在其中；而组复制的多主技术需要 Paxos 算法深度参与，并去决定每一次数据的写入，解决写入冲突。  \n组复制有如下几个优点。\n\n- 高可用分片：数据库节点动态添加和移除。分片实现写扩展，每个分片是一个复制组。可以结合上一讲中对于 TiDB 的介绍，原理类似。\n- 自动化故障检测与容错：如果一个节点无法响应，组内大多数成员认为该节点已不正常，则自动隔离。\n- 方案完整：前面介绍的方案都需要 MySQL 去搭配一系列第三方解决方案；而组复制是原生的完整方案，不需要第三方组件接入。  \n当然，组复制同样也有一些限制。主要集中在需要使用较新的特性，一些功能在多组复制中不支持，还有运维人员经验缺乏等。  \n相信随着 MySQL 的发展，将会有越来越多的系统迁移到组复制中，多主模式也会逐步去替代单主模式。  \n**复制往往需要与一致性放在一起讨论**\n\n## 3 一致性-CAP\n\n为了使系统高度可用，系统需要被设计成允许一个或多个节点的崩溃或不可访问。为此，我们需要引入[2 高可用-重要根基：复制](#2%20高可用-重要根基：复制)技术，其核心就是使用多个冗余的副本来提高系统的可用性。但是，一旦添加了这些副本，**我们将面临使多个数据副本保持同步的问题，并且遭遇故障后如何恢复系统的问题**。  \n这就是 MySQL 复制发展历程所引入的 RPO 概念，也就是系统不仅仅要可用，而且数据还需要一致。所以**高可用必须要尽可能满足业务连续性和数据一致性这两个指标**。\n","lastmodified":"2023-05-09T16:33:58.299366595Z","tags":[]},"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F":{"title":"分布式系统","content":"# 分布式系统\n\n## 分布式数据库\n\n### 数据分片\n（水平分片\n垂直分片）\n\n- 分片算法\n\n### 高可用\n\n- 复制\n\n\t- 单主复制\n\t- 多主复制\n\n### 错误侦测\n\n- 心跳检测法\n- Gossip 协议检测\n- φ 值检测\n\n### 存储引擎 LSM树\n\n- 评价存储引擎\n\n\t- 缓存形式\n\t- 可变/不可变数据\n\t- 排序\n\n### 分布式索引\n\n- 数据文件 索引数据表SSTable\n- 数据缓冲 跳表\n- 查询路径 布隆过滤\n\n### 分布式事务\n\n- 两阶段提交\n\n\t- 协调器与参与者\n\n- 三阶段提交\n\n\t- 三阶段相比于两阶段主要是解决协调器在准备阶段失败中描述的阻塞状态。它的解决方案是在两阶段中间插入一个阶段，第一阶段还是进行投票，第二阶段将投票后的结果分发给所有参与者，第三阶段是提交操作。其关键点是在第二阶段，如果协调者在第二阶段之前崩溃无法恢复，参与者可以通过超时机制来释放该事务。一旦所有节点通过第二阶段，那么就意味着它们都知道了当前事务的状态，此时，不管协调者还是参与者崩溃都不会影响事务执行。\n\t- 问题：在第二阶段的时候，一些参与者与协调器失去联系，它们由于超时机制会中断事务。而如果另外一些参与者已经收到可以提交的指令，就会提交数据，从而造成脑裂的情况。\n\n- Percolator 乐观事务\n\n\t- TiDB 乐观事务冲突处理\n\n- Spanner\n\n\t- TrueTime 和 Paxos Group\n\n\t\t- 读写事务：该事务是通过分布式锁实现的，并发性是最差的。且数据写入每个分片 Paxos Group 的主节点。\n\t\t- 只读事务：该事务是无锁的，可以在任意副本集上进行读取。但是，如果想读到最新的数据，需要从主节点上进行读取。主节点可以从 Paxos Group 中获取最新提交的时间节点。\n\t\t- 快照读：顾名思义，Spanner 实现了 MVCC 和快照隔离，故读取操作在整个事务内部是一致的。同时这也暗示了，Spanner 可以保存同一份数据的多个版本。\n\n\t- 隔离方面，Spanner 实现了 SSI，也就是序列化的快照隔离。其方法就是上文提到的 lock table。该锁是完全的排他锁，不仅仅能阻止并发写入数据，写入也可以阻止读取，从而解决快照隔离写偏序的问题。\n\n- Calvin 与 FaunaDB\n\n\t- Calvin 的方案是让事务在每个副本上的执行顺序达到一致，那么执行结果也肯定是一致的。这样做的好处是避免了众多事务之间的锁竞争，从而大大提高了高并发度事务的吞吐量。同时，节点崩溃不影响事务的执行。因为事务执行步骤已经分配，节点恢复后从失败处接着运行该事务即可，这种模式使分布式事务的可用性也大大提高。目前实现了 Calvin 事务模式的数据库是 FaunaDB。\n\t- 同时 Calvin 事务有 read set 和 write set 的概念。前者表示事务需要读取的数据，后者表示事务影响的数据。这两个集合需要在事务开始前就进行确定，故Calvin 不支持在事务中查询动态数据而后影响最终结果集的行为。这一点很重要，是这场战争的核心。\n\n- 基于消息队列\n\n\t- 先让订单系统把要发送的消息持久化到本地数据库里，然后将这条消息记录的状态设置为代发送，紧接着订单系统再投递消息到消息队列，优惠券系统消费成功后，也会向消息队列发送一个通知消息。当订单系统接收到这条通知消息后，再把本地持久化的这条消息的状态设置为完成\n\n### Etcd\n\n- 高可用、强一致\n\n\t- 主要分为四个部分：HTTP Server、Store、Raft 以及 WAL（预写式日志）。\n\n\t\t- Store：用于处理 Etcd 支持的各类功能的事务，包括数据索引、节点状态变更、监控与反馈、事件处理与执行等等，是 Etcd 对用户提供的大多数 API 功能的具体实现。\n\t\t- Raft：Raft 强一致性算法的具体实现，是 Etcd 的核心。\n\t\t- WAL：Write Ahead Log（预写式日志），是 Etcd 的数据存储方式。除了在内存中存有所有数据的状态以及节点的索引，Etcd 还通过 WAL 进行持久化存储。WAL 中，所有的数据提交前都会事先记录日志。Snapshot 是为了防止数据过多而进行的状态快照。Entry 表示存储的具体日志内容。\n\t\t- HTTP Server：用于处理客户端发送的 API 请求以及其它 Etcd 节点的同步与心跳信息请求。\n\t\t- 通常，一个用户的请求发送过来，会经由 HTTP Server 转发给 Store 进行具体的事务处理；如果涉及到节点的修改，则交给 Raft 模块进行状态的变更、日志的记录，然后再同步给别的 Etcd 节点以确认数据提交；最后进行数据的提交，再次同步。\n\n\t- 租约机制（TTL，Time To Live），Etcd 可以为存储的 Key-Value 对设置租约，当租约到期，Key-Value 将失效删除；同时也支持续约，通过客户端可以在租约到期之前续约，以避免 Key-Value 对过期失效；此外，还支持解约，一旦解约，与该租约绑定的 Key-Value 将失效删除；\n\t- Prefix 机制：即前缀机制，也称目录机制，如两个 Key 命名如下：key1=“/mykey/key1”，key2=\"/mykey/key2\"，那么，可以通过前缀“/mykey”查询，返回包含两个 Key-Value 对的列表；\n\t- Watch 机制：即监听机制，Watch 机制支持监听某个固定的 Key，也支持监听一个范围（前缀机制），当被监听的 Key 或范围发生变化，客户端将收到通知；\n\t- Revision 机制：每个 Key 带有一个 Revision 号，每进行一次事务便加一，因此它是全局唯一的，如初始值为 0，进行一次 Put 操作，Key 的 Revision 变为1，同样的操作，再进行一次，Revision 变为 2；换成 Key1 进行 Put 操作，Revision 将变为 3。这种机制有一个作用，即通过 Revision 的大小就可知道写操作的顺序，这对于实现公平锁，队列十分有益。\n\n- 应用场景\n\n\t- 服务发现\n\t- 消息发布和订阅\n\t- 分布式锁\n\n\t\t- 媲美业界“名宿”ZooKeeper\n\t\t- 通过前缀“/mylock” 查询，返回包含两个 Key-Value 对的 Key-Value 列表，同时也包含它们的 Revision，通过 Revision 大小，客户端可以判断自己是否获得锁，如果抢锁失败，则等待锁释放（对应的 Key 被删除或者租约过期），然后再判断自己是否可以获得锁\n\t\t- 多个客户端同时抢锁，根据 Revision 号大小依次获得锁，可以避免 “羊群效应” （也称“惊群效应”），实现公平锁\n\t\t- 如果抢锁失败，可通过 Prefix 机制返回的 Key-Value 列表获得 Revision 比自己小且相差最小的 Key（称为 Pre-Key），对 Pre-Key 进行监听，因为只有它释放锁，自己才能获得锁\n\t\t- Lease 机制可以保证分布式锁的安全性，为锁对应的 Key 配置租约，即使锁的持有者因故障而不能主动释放锁，锁也会因租约到期而自动释放\n\n\t- 集群监控与 Leader 竞选\n\n## 数据库中间件\n\n### 分片\n\n- 范围分片和哈希分片\n\n\t- 一致性 Hash 算法\n\n### 全局唯一主键\n\n- Twitter 的 Snowflake（又名“雪花算法”）\n\n\t- 生成的是 64 位唯一 ID（由 41 位的 timestamp + 10 位自定义的机器码 + 13 位累加计数器组成）\n\n- UUID/GUID（一般应用程序和数据库均支持）\n- MongoDB ObjectID（类似 UUID 的方式）\n\n### 跨分片查询\n\n### 分布式事务\n\n## 一致性算法\n失败模型、失败侦测、领导选举和一致性的合体\n\n### 原子广播与 ZAB\n\n- 广播协议是一类将数据从一个节点同步到多个节点的协议。特别是其中的 Gossip 协议可以保障大规模的数据同步，而 Gossip 在正常情况下就是采用广播模式传播数据的\n\n\t- 原子广播协议：Zookeeper Atomic Broadcast（ZAB）。\n\n### Paxos\n\n- Proposer：Proposer 可以有多个，Proposer 提出议案（value）。所谓 value，可以是任何操作，比如“设置某个变量的值为 value”。不同的 Proposer 可以提出不同的 value。但对同一轮 Paxos 过程，最多只有一个 value 被批准。\n- Acceptor：Acceptor 有 N 个，Proposer 提出的 value 必须获得 Quorum 的 Acceptor 批准后才能通过。Acceptor 之间完全对等独立。\n- Learner：上面提到只要 Quorum 的 Accpetor 通过即可获得通过，那么 Learner 角色的目的就是把通过的确定性取值同步给其他未确定的 Acceptor。\n- Multi-Paxos\n\n\t- 如果完全执行上面描述的过程，那性能消耗是任何生产系统都无法承受的，因此我们一般使用的是 Multi-Paxos 可以并发执行多个 Paxos 协议，它优化的重点是把 Propose 阶段进行了合并，这就引入了一个 Leader 的角色，也就是领导节点\n\t- replicated log：值被提交后写入到日志中。这种日志结构除了提供持久化存储外，更重要的是保证了消息保存的顺序性。而 Paxos 算法的目标是保证每个节点该日志内容的强一致性。\n\t- state snapshot：由于日志结构保存了所有值，随着时间推移，日志会越来越大。故算法实现了一种状态快照，可以保存最新的日志消息。当快照生成后，我们就可以安全删除快照之前的日志了。\n\t- 缺点：Multi-Paxos 随机性使得没有一个节点有完整的最新的数据，因此其恢复流程非常复杂，需要同步节点间的历史记录\n\n### Raft\n\n- 节点三种状态Leader、 Follower、Candidate\n- 一致性问题\n\n\t- 选举（Leader Election）当 Leader 宕机或者集群初创时，一个新的 Leader 需要被选举出来；\n\n\t\t- Candidate 收到超过半数节点的投票（N/2 + 1），它将获胜成为 Leader\n\t\t- 请求节点的 Term 大于自己的 Term，且自己尚未投票给其它节点，则接受请求，把票投给它，否则投给自己\n\n\t- 日志复制（Log Replication）Leader 接收来自客户端的请求并将其以日志条目的形式复制到集群中的其它节点，并且强制要求其它节点的日志和自己保持一致\n\n\t\t- 只有 Leader 节点能够处理客户端的请求（如果客户端的请求发到了 Follower，Follower 将会把请求重定向到 Leader），客户端的每一个请求都包含一条被复制状态机执行的指令。Leader 把这条指令作为一条新的日志条目（Entry）附加到日志中去，然后并行得将附加条目发送给 Followers，让它们复制这条日志条目。\n\n\t\t\t- 这时就会把 Follower 冲突的日志条目全部删除并且加上 Leader 的日志。一旦附加日志成功，那么 Follower 的日志就会和 Leader 保持一致\n\n\t- 安全性（Safety）：如果有任何的服务器节点已经应用了一个确定的日志条目到它的状态机中，那么其它服务器节点不能在同一个日志索引位置应用一个不同的指令。\n\n\t\t- 日志条目的传送是单向的，只从 Leader 传给 Follower，并且 Leader 从不会覆盖自身本地日志中已经存在的条目\n\n## 分布式缓存Redis\n\n### 主从复制模式\n\n- 主从服务器之间采用的是「读写分离」的方式，所有的数据修改只在主服务器上进行，然后将最新的数据同步给从服务器，这样就使得主从服务器的数据是一致的\n- 主从服务器间的第一次同步的过程可分为三个阶段：第一阶段是建立链接、协商同步，第二阶段是主服务器同步数据给从服务器，第三阶段是主服务器发送新写操作命令给从服务器。之后双方之间就会维护一个 TCP 连接\n- 网络断开又恢复后，从主从服务器会采用增量复制的方式继续同步，也就是只会把网络断开期间主服务器接收到的写操作命令，同步给从服务器\n- 从节点是无法自动升级为主节点的，这个过程需要人工处理\n\n### 哨兵机制\n\n- 哨兵其实是一个运行在特殊模式下的 Redis 进程，所以它也是一个节点。从“哨兵”这个名字也可以看得出来，它相当于是“观察者节点”，观察的对象是主从节点：监控、选主、通知。\n- 哨兵节点通过 Redis 的发布者/订阅者机制，哨兵之间可以相互感知，相互连接，然后组成哨兵集群，同时哨兵又通过 INFO 命令，在主节点里获得了所有从节点连接信息，于是就能和从节点建立连接，并进行监控了\n\n### Redis-Cluster \n\n- 实现基础：分片 Slot（客户端分片和代理分片）采用哈希槽（Hash Slot），来处理数据和节点之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384 个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中\n- 节点通信原理（分布式一致性协议）：Gossip 算法ping、pong消息通信，通信节点选择过多虽然可以做到信息及时交换但成本过高。节点选择过少则会降低集群内所有节点彼此信息交换的频率，Gossip 协议需要兼顾信息交换实时性和成本开销。\n- 故障转移：选举，但是有脑裂问题，由于网络问题，集群节点之间失去联系。主从数据不同步；重新平衡选举，产生两个主服务。等网络恢复，旧主节点会降级为从节点，再与新主节点进行同步复制的时候，由于会从节点会清空自己的缓冲区，所以导致之前客户端写入的数据丢失了\n解决：当主节点发现从节点下线或者通信超时的总数量小于阈值时，那么禁止主节点进行写数据，直接把错误返回给客户端\n- 集群规模超过百节点级别后，Gossip 协议的效率将会显著下降，通信成本越来越高 Gossip算法又被称为反熵（Anti-Entropy），熵是物理学上的一个概念，代表杂乱无章，而反熵就是在杂乱无章中寻求一致，这充分说明了Gossip的特点：在一个有界网络中，每个节点都随机地与其他节点通信，经过一番杂乱无章的通信，最终所有节点的状态都会达成一致。\n\n### 分布式锁\n\n- SET 命令有个 NX 参数可以实现「key不存在才插入」，EX/PX 设置其过期时间以预防死锁，key区分不同客户端\n- 1 判断锁的 unique_value 是否为加锁客户端，是的话，才将 lock_key 键删除，解锁是有两个操作，Lua 脚本来保证解锁的原子性。\n2 如果持有的锁已经因过期而释放（或者过期释放后又被其它客户端持有），则 Key 对应的 Value 将改变，释放锁的事务将不会被执行。\n- 1 超时时间不好设置，如果锁的超时时间设置过长，会影响性能，如果设置的超时时间过短会保护不到共享资源。（先给锁设置一个超时时间，然后启动一个守护线程，让守护线程在一段时间后，重新设置这个锁的超时时间，实现复杂）\n2 主从复制模式中数据是异步复制的，导致分布式锁的不可靠性。主节点获取到锁后，在没有同步到从节点时，主节点宕机了，此时新的主节点依然可以获取锁，所以多个应用服务就可以同时获取到锁。\n- RedLock\n\n\t- 客户端和多个独立的主节点依次请求申请加锁，如果客户端能够和半数以上（\u003e=N/2+1）的节点成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁，否则加锁失败（t2-t1 \u003c 锁的过期时间）（因此成功是两个条件）\n\t- 向所有 Redis 节点发起释放锁的操作lua脚本\n\n## 消息队列中间件\n\n### Kafka\n\n### ActiveMQ\n\n### RabbitMQ\n\n### RocketMQ\n\n## Zookeeper\n\n### 分布式锁\n\n- ZooKeeper分布式锁（如InterProcessMutex），能有效的解决分布式问题，不可重入问题，使用起来也较为简单\n- ZooKeeper实现的分布式锁，性能并不太高。每次在创建锁和释放锁的过程中，都要动态创建、销毁瞬时节点来实现锁功能。大家知道，ZK中创建和删除节点只能通过Leader服务器来执行，然后Leader服务器还需要将数据同不到所有的Follower机器上，这样频繁的网络通信，性能的短板是非常突出的。\n\n","lastmodified":"2023-05-09T16:33:56.631325871Z","tags":[]},"/%E5%88%A9%E7%94%A8Element%E5%AE%9E%E7%8E%B0%E5%93%8D%E5%BA%94%E5%BC%8F%E5%AF%BC%E8%88%AA%E6%A0%8F":{"title":"利用Element实现响应式导航栏","content":"\n### 开始之前\n\n按照计划，前端使用Vue.js+Element UI，但在设计导航栏时，发现element没有提供传统意义上的页面顶部导航栏组件，只有一个可以用在很多需要选择tab场景的**导航菜单**，便决定在其基础上改造，由于我认为实现移动端良好的体验是必须的，所以便萌生了给其增加响应式功能的想法。\n\n### 需求分析与拆解\n\n假设我们的导航栏有logo和四个`el-menu-item`。\n\n给`window`绑定监听事件，当宽度小于a时，四个链接全部放入右侧`el-submenu`的子菜单：\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rad-figure-bed/PicGo/blogs/elementuimenu/20200507123454.png)\n\n当宽度大于a时，右侧`el-submenu`不显示，左侧`el-menu-item`正常显示：\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rad-figure-bed/PicGo/blogs/elementuimenu/20200507123504.png)\n\n所以，先创建一个数组，存储所有所需的item：\n\n```javascript\nnavItems: [\n  { name: \"Home\", indexPath: \"/home\", index: \"1\" },\n  { name: \"Subscribe\", indexPath: \"/subscribe\", index: \"2\"},\n  { name: \"About\", indexPath: \"/about\", index: \"3\" },\n  { name: \"More\", indexPath: \"/more\", index: \"4\" }\n]\n```\n\n### 监听宽度\n\n很明显功能实现的关键是随时监听窗口的变化，根据对应的宽度做出响应，在`data`中，我使用`screenWidth`变量来存储窗口大小,保存初始打开页面时的宽度:\n\n```javascript\ndata() {\n  return {\n    screenWidth: document.body.clientWidth\n    ......\n  }\n}\n```\n\n接下来在`mounted`中绑定屏幕监听事件，将最新的可用屏幕宽度赋给`screenWidth`：\n\n```javascript\nmounted() {\n  window.onresize = () =\u003e {\n    this.screenWidth = document.body.clientWidth\n  }\n}\n```\n\n(关于document和window中N多的关于高度和宽度的属性，可以参考[这篇文章](https://segmentfault.com/a/1190000007515034)。)\n\n为了防止频繁触发resize函数导致页面卡顿，可以使用一个定时器，控制下`screenWidth`更新的频率：\n\n```javascript\nwatch: {\n  screenWidth(newValue) {\n    // 为了避免频繁触发resize函数导致页面卡顿，使用定时器\n    if (!this.timer) {\n      // 一旦监听到的screenWidth值改变，就将其重新赋给data里的screenWidth\n      this.screenWidth = newValue;\n      this.timer = true;\n      setTimeout(() =\u003e {\n        //console.log(this.screenWidth);\n        this.timer = false;\n      }, 400);\n    }\n  }\n}\n```\n\n### 显示\n\n有了屏幕宽度的实时数据后，就可以以computed的方式控制menuItem了。\n\n```javascript\ncomputed: {\n  ...\n  leftNavItems: function() {\n    return this.screenWidth \u003e= 600 ? this.navItems : {};\n  },\n  rightNavItems: function() {\n    return this.screenWidth \u003c 600 ? this.navItems : {};\n  }\n},\n```\n\n通过简单的判断即可在窗口宽度变化时，将菜单里的内容放入预先设置的正常菜单或者当宽度小于600时显示的右侧下拉菜单,附上html部分代码：\n\n```html\n\u003cel-menu text-color=\"#2d2d2d\" id=\"navid\" class=\"nav\" mode=\"horizontal\" @select=\"handleSelect\"\u003e\n  \u003cel-menu-item class=\"logo\" index=\"0\" route=\"/home\"\u003e\n    \u003cimg class=\"logoimg\" src=\"../assets/img/logo.png\" alt=\"logo\" /\u003e\n  \u003c/el-menu-item\u003e\n  \u003cel-menu-item\n    :key=\"key\"\n    v-for=\"(item,key) in leftNavItems\"\n    :index=\"item.index\"\n    :route=\"item.activeIndex\"\n  \u003e{{item.name}}\u003c/el-menu-item\u003e\n  \u003cel-submenu\n    style=\"float:right;\"\n    class=\"right-item\"\n    v-if=\"Object.keys(rightNavItems).length === 0?false:true\"\n    index=\"10\"\n  \u003e\n    \u003ctemplate slot=\"title\"\u003e\n      \u003ci class=\"el-icon-s-fold\" style=\"font-size:28px;color:#2d2d2d;\"\u003e\u003c/i\u003e\n    \u003c/template\u003e\n    \u003cel-menu-item\n      :key=\"key\"\n      v-for=\"(item,key) in rightNavItems\"\n      :index=\"item.index\"\n      :route=\"item.activeIndex\"\n    \u003e{{item.name}}\u003c/el-menu-item\u003e\n  \u003c/el-submenu\u003e\n\u003c/el-menu\u003e\n```\n\n### 总结\n\n总的来说，一个丐版就算完成了，这里只提供了一种可能的思路，如需实践可以增加更多判断规则及功能。（主要是已经转用Vuetify啦~）\n","lastmodified":"2023-05-09T16:33:58.299366595Z","tags":[]},"/%E5%89%8D%E7%AB%AF":{"title":"前端","content":"\n前端是学的最久最多的了，bilibili的实习也是前端居多，在转后端面前，这些好像又失去了作用\n\n浏览器相关  \n[[browser]]\n\nWeb安全  \n[[同源策略]]  \n[[CSRFXSS]]\n\n[[包管理工具]]  \n[[移动Web与响应式]]  \n\n框架  \n[[react]]  \n[[Vue2基础]]  \n[[Vue3基础]]  \n[[Vuex]]  \n[[Diff]]\n\n构建工具  \n[[webpack4]]  \nwebpack5相关在notion中，等待搬运\n\n基础知识  \n[[CSS3 no]]  \n[[CSS基础]]  \n[[nodejs]]  \n[[Javascript基础]]  \n[[typescript]]  \n[[Canvas]]  \n[[Html基础]]\n\n[[浏览器指纹 notion]]  \n[[前端工程化]]  \n[[组件库设计]]  \n[[ESLint]]  \n[[web-test]]  \n[[Web性能]]\n","lastmodified":"2023-05-09T16:33:58.299366595Z","tags":[]},"/%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%8C%96":{"title":"前端工程化","content":"\n前端工程本质上是软件工程的一种。软件工程化关注的是性能、稳定性、可用性、可维护性等方面，注重基本的开发效率、运行效率的同时，思考维护效率。一切以这些为目标的工作都是\"前端工程化\"。工程化是一种思想而不是某种技术。\n\n## 模块化\n\n简单来说，模块化就是将一个大文件拆分成相互依赖的小文件，再进行统一的拼装和加载。（方便了多人协作）。\n\n分而治之是软件工程中的重要思想，是复杂系统开发和维护的基石，这点放在前端开发中同样适用。模块化是目前前端最流行的分治手段。\n\n模块化开发的最大价值应该是分治！  \n不管你将来是否要复用某段代码，你都有充分的理由将其分治为一个模块。\n\n### JS模块化方案\n\nAMD/CommonJS/UMD/ES6 Module等等。\n\nCommonJS的核心思想是把一个文件当做一个模块，要在哪里使用这个模块，就在哪里require这个模块，然后require方法开始加载这个模块并且执行其中的代码，最后会返回你指定的export对象。\n\n```javascript\nmodule.export = function() {\n    hello: function() {\n        alert(\"你好\");\n    }\n}\n\nvar a = require('./xxx/a.js');\na.hello(); // ==\u003e 弹窗“你好”\n```\n\nCommonJS 加载模块是同步的，所以只有加载完成才能执行后面的操作，不能非阻塞的并行加载多个模块。\n\nAMD（异步模块定义，Asynchronous Module Definition），特点是可以实现异步加载模块，等所有模块都加载并且解释执行完成后，才会执行接下来的代码。\n\n```javascript\n// 通过AMD载入模块\n// define(\n//     module_id /*可选*/, \n//     [dependencies] 可选, \n//     definition function /*回调 用来初始化模块或对象的函数*/\n// );\ndefine(['myModule', 'myOtherModule'], function(myModule, myOtherModule) {\n    console.log(myModule.hello());\n    //会先并行加载所有的模块a b 并执行其中模块的代码后，在执行逐步执行下面的 console\n    require(\"a\");\n    console.log(\"a required\");\n\n    require(\"b\");\n    console.log(\"b required\");\n\n    console.log(\"all modules have been required\");\n});\n```\n\n在一些同时需要AMD和CommonJS功能的项目中，你需要使用另一种规范：Universal Module Definition（通用模块定义规范）。UMD创造了一种同时使用两种规范的方法，并且也支持全局变量定义。所以UMD的模块可以同时在客户端和服务端使用。\n\n幸运的是在JS的最新规范ECMAScript 6 (ES6)中，引入了模块功能。  \nES6 的模块功能汲取了CommonJS 和 AMD 的优点，拥有简洁的语法并支持异步加载，并且还有其他诸多更好的支持（例如导入是实时只读的。（CommonJS 只是相当于把导出的代码复制过来））。\n\n```javascript\n// CommonJS代码\n// lib/counter.js\nvar counter = 1;\nfunction increment() {\n  counter++;\n}\nfunction decrement() {\n  counter--;\n}\nmodule.exports = {\n  counter: counter,\n  increment: increment,\n  decrement: decrement\n};\n// src/main.js\nvar counter = require('../../lib/counter');\ncounter.increment();\nconsole.log(counter.counter); // 1\n```\n\n```javascript\n// 使用 es6 modules 通过 import 语句导入\n// lib/counter.js\nexport let counter = 1;\nexport function increment() {\n  counter++;\n}\nexport function decrement() {\n  counter--;\n}\n// src/main.js\nimport * as counter from '../../counter';\nconsole.log(counter.counter); // 1\ncounter.increment();\nconsole.log(counter.counter); // 2\n```\n\n### CSS模块化方案\n\n在less、sass、stylus等预处理器的import/mixin特性支持下实现、css modules。\n\n虽然SASS、LESS、Stylus等预处理器实现了CSS的文件拆分，但没有解决CSS模块化的一个重要问题：选择器的全局污染问题;\n\nCSS in JS是彻底抛弃CSS，使用JS或JSON来写样式。这种方法很激进，不能利用现有的CSS技术，而且处理伪类等问题比较困难；\n\nCSS Modules 原理：使用JS 来管理样式模块，它能够最大化地结合CSS生态和JS模块化能力，通过在每个 class 名后带一个独一无二 hash 值，这样就不有存在全局命名冲突的问题了。\n\nwebpack 自带的 css-loader 组件，自带了 CSS Modules，通过简单的配置即可使用。\n\n```javascript\n{\n    test: /\\.css$/,\n    loader: \"css?modules\u0026localIdentName=[name]__[local]--[hash:base64:5]\"\n}\n```\n\n## 组件化\n\n前端作为一种GUI软件，光有JS/CSS的模块化还不够，对于UI组件的分治也有着同样迫切的需求。分治的确是非常重要的工程优化手段。\n\n![前端组件化开发](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171810.png)\n\n\u003e 页面上的每个 独立的 可视/可交互区域视为一个组件；  \n\u003e ==每个组件对应一个工程目录==，组件所需的各种资源都在这个目录下就近维护；  \n\u003e 由于组件具有独立性，因此组件与组件之间可以 自由组合；  \n\u003e 页面只不过是组件的容器，负责组合组件形成功能完整的界面；  \n\u003e 当不需要某个组件，或者想要替换组件时，可以整个目录删除/替换。\n\n由于系统功能被分治到独立的模块或组件中，粒度比较精细，组织形式松散，开发者之间不会产生开发时序的依赖，大幅提升并行的开发效率，理论上允许随时加入新成员认领组件开发或维护工作，也更容易支持多个团队共同维护一个大型站点的开发。\n\n## “智能”加载静态资源（性能优化）\n\n模块化/组件化开发之后，我们最终要解决的，就是模块/组件加载的技术问题。然而前端与客户端GUI软件有一个很大的不同：前端是一种远程部署，运行时增量下载的GUI软件。\n\n如果用户第一次访问页面就强制其加载全站静态资源再展示，相信会有很多用户因为失去耐心而流失。根据“增量”的原则，我们应该精心规划每个页面的资源加载策略，使得用户无论访问哪个页面都能按需加载页面所需资源，没访问过的无需加载，访问过的可以缓存复用，最终带来流畅的应用体验。\n\n这正是Web应用“免安装”的魅力所在。\n\n由“增量”原则引申出的前端优化技巧几乎成为了性能优化的核心。\n\n\u003e  有加载相关的按需加载、延迟加载、预加载、请求合并等策略；  \n\u003e 有缓存相关的浏览器缓存利用，缓存更新、缓存共享、非覆盖式发布等方案；\n\n还有复杂的BigRender、BigPipe、Quickling、PageCache等技术。  \n这些优化方案无不围绕着如何将增量原则做到极致而展开。\n\n- 一种静态网页资源管理和优化技术。  \n  静态资源管理系统 = 资源表 + 资源加载框架\n\n资源表是一份数据文件（比如JSON），是项目中所有静态资源（主要是JS和CSS）的构建信息记录，通过构建工具扫描项目源码生成，是一种k-v结构的数据，以每个资源的id为key，记录了资源的类别、部署路径、依赖关系、打包合并等内容。\n\n```json\n{\n    \"res\" : {\n        \"widget/a/a.css\" : \"/widget/a/a_1688c82.css\",\n        \"widget/a/a.js\"  : \"/widget/a/a_ac3123s.js\",\n        \"widget/b/b.css\" : \"/widget/b/b_52923ed.css\",\n        \"widget/b/b.js\"  : \"/widget/b/b_a5cd123.js\",\n        \"widget/c/c.css\" : \"/widget/c/c_03cab13.css\",\n        \"widget/c/c.js\"  : \"/widget/c/c_bf0ae3f.js\",\n        \"jquery.js\"      : \"/jquery_9151577.js\",\n        \"bootstrap.css\"  : \"/bootstrap_f5ba12d.css\",\n        \"bootstrap.js\"   : \"/bootstrap_a0b3ef9.js\"\n    },\n    \"pkg\" : {\n        \"p0\" : {\n            \"url\" : \"/pkg/lib_cef213d.js\",\n            \"has\" : [ \"jquery.js\", \"bootstrap.js\" ]\n        },\n        \"p1\" : {\n            \"url\" : \"/pkg/lib_afec33f.css\",\n            \"has\" : [ \"bootstrap.css\" ]\n        },\n        \"p2\" : {\n            \"url\" : \"/pkg/widgets_22feac1.js\",\n            \"has\" : [\n                \"widget/a/a.js\",\n                \"widget/b/b.js\",\n                \"widget/c/c.js\"\n            ]\n        },\n        \"p3\" : {\n            \"url\" : \"/pkg/widgets_af23ce5.css\",\n            \"has\" : [\n                \"widget/a/a.css\",\n                \"widget/b/b.css\",\n                \"widget/c/c.css\"\n            ]\n        }\n    }\n}\n```\n\n在查表的时候，如果一个静态资源有pkg字段(用来记录web应用中一个页面加载过的静态资源，当下个页面用到这个资源就无需加载了，有效利用缓存)，那么就去加载pkg字段所指向的打包文件，否则加载资源本身。\n\n## 规范化\n\n规范化其实是工程化中很重要的一个部分，项目初期规范制定的好坏会直接影响到后期的开发质量。\n\n- 目录结构的制定\n- 编码规范\n- 前后端接口规范\n- 文档规范\n- 组件管理\n- Git分支管理\n- Commit描述规范\n- 定期CodeReview\n- 视觉图标规范\n\n\u003cimg src=\"Engineering.assets/upload_9525e99d8347c7183040a707b6310229.png\" alt=\"img\" style=\"zoom:150%;\" /\u003e\n\n如上图所示，一个基本的研发流程闭环，一般是需求导入 - 需求拆解 - 技术方案制定 - 本地编码 - 联调 - 自测优化 - 提测修复 Bug - 打包 - 部署 - 数据收集\u0026分析复盘 - 迭代优化 —— 即新一轮的需求导入。\n\n在这个基础的闭环中，每一个节点都有其进一步的内部环节，每一个环节相连，组成了一个研发周期。这个周期顺，研发流程就顺。这个周期中每一个环节的**阻塞点越少，研发效率就越高**。最初期的基建，就是从这些耽误研发时间的阻塞点入手，按照普遍性 + 高频的优先级标准，挨个突破。\n\n**提效、体验、稳定性**，是基建要解决的最重要的目标，通用的公式是 **标准化 + 规范化 + 工具化 + 自动化**，能力完备后可以进一步提升到平台化 + 产品化。在方向方面，从下面的 8 个主要方向进行归类和建设，供大家参考：\n\n- 开发规范：这一部分沉淀的是团队的标准化共识，标准化是团队有效协作的必备前提。\n- 研发流程：标准化流程直接影响上下游的协作分工和效率，优秀的流程能带来更专业的协作。\n- 基础资产：在我们团队，资产体系包括了工具链、团队标准 DSL、物料库（组件、区块、模板等）。\n- 工程管理：面向应用全生命周期的低成本管控，从应用的创建到本地环境配置到低代码搭建到打包部署。\n- 性能体验：自动化工具化的方式发现页面性能瓶颈，提供优化建议。\n- 安全防控：三方包依赖安全、代码合规性检查、安全风险检测等防控机制。\n- 统计监控：埋点方案、数据采集、数据分析、线上异常监控等。\n- 质量保障：自测 CheckList、单测、UI 自动化测试、链路自动化测试等。\n\n## 自动化\n\n任何简单机械的重复劳动都应该让机器去完成。\n\n- 图标合并\n- 持续集成\n- 自动化构建\n- 自动化部署\n- 自动化测试\n\n## ZOO基建范例\n\n### 1. 规范\u0026文档（Docs）\n\n规范是最应该先行的，始皇帝初统六国即“书同文车同轨”，规范意味着标准，是团队的共识，是沟通协作的基础。而文档，是最容易被忽略的事情之一，除了明面上重要的技术文档、业务稳定之外，还包括了行间的有效注释。想想，有多少时间是花在琢磨别人的代码逻辑，或刚接手某个业务得问多少人才能搞明白你面前那几个仓库是怎么回事，又有多少故障是因为不清楚前任留下的坑在哪里不小心踩雷。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171811.png)\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171812.jpg)\n\n对于规范的制定，需要强调的一点，是规范的产出应是团队内大部分同学的共识，应该是集体审美。规范一旦确定就应该严格执行，要能形成团队行为的**一致性**。对于文档，为了写而写的文档是垃圾，不如不写。文档的重点在**说人话**，在于**有效性**，在于直观、省事、不饶。想想一个 UI 组件库的文档，先给你看可交互的 Demo 再提供 API 信息，和直接开头就罗列一大堆的 API 文字介绍，哪种对阅读者的感受更好、心理成本更低？\n\n### 2. 本地工程化环境（CLI）\n\n本地开发环境，相信是任何一个团队都会做的标配，省事的可能直接拥抱框架选型对应的全家桶，如 Vue 全家桶，或者用 Webpack 撸一个脚手架。能力多一些的会再为脚手架提供一些插件服务，如 Lint 或者 Mock。从简单的一个本地脚手架，到复杂的一个工程化套件系统，其目的都是为了本地开发流程的去人肉化、自动化。\n\n我们团队的本地开发环境基建，是一个工程化套件环境，核心理念就是尽量 “一步搞定所有事”，把本地环境的配置和使用尽量变的傻瓜化无脑化。比如本地初始化一个应用的环境，从 CLI 命令行的操作出发的话（实际上政采云前端团队现在已完全 GUI 化），一个 `zoo init` 命令就能搞定全部的本地环境搭建，这个全部是指在终端执行回车后，从仓库本地目录的生成到 npm 依赖的自动化安装到脚手架插件的初始化再到唤起一个浏览器窗口，都是自动化执行的。是的，连 `npm install` 和 `dev` 什么的都不用执行，能省一步操作就省一步，楚王好细腰，少就是性感。下图是 CLI 本地工程套件的架构图：\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171813.png)\n\n### 3. 可视化工程系统（GUI）\n\n其实目前团队的日常研发，已经基本上脱离了 CLI 操作，统一到了团队自研的桌面客户端 “敦煌” 平台。基于客户端的能力，能将分散的工程能力进行聚合，并形成链路的串联能力，结合 GUI 的直观和简便操作，进一步的省事。通过桌面客户端，可以将日常的前端研发链路上的操作都聚合进来，从组件开发到模板开发再到应用开发；从唤起编辑器到启动调试环境、进行包更新到打包部署发布。同时桌面端系统还能和其他的研发系统进行打通，形成更多的能力。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171814.png)\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171815.png)\n\n\\###4. 组件开发与管理\n\n一般情况下，前端团队都会完善自己的组件库体系，有些情况下一些 UI 组件库可能采用社区开源的优秀三方库，如 antd，但多多少少还会有自己的业务组件库需要封装。工具的价值在于**抹平**差异，将基础标准一致化。对于组件开发，前面所述的 CLI 工具链是这里的底层依赖，同理还有后面介绍的模板开发与使用，以及应用的开发。通过工具进行组件的开发和管理，可以较好的实现诸如组件命名标准化、版本标准化、查找便利性、开发流程简化等，还能实现组件的应用场景统计和版本覆盖率等涉及到组件在接入场景更新成本相关的必要统计。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171816.png)\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171817.png)\n\n### 5. 模板开发与管理\n\n同飞冰类似，我们也沉淀了一套类似的模板化能力，便于中后台业务场景的快速开发。因为中后台的业务场景相对固化，诸如表单、列表等居多，基于模板的方式可以省掉很大一部分制作 Demo 和实现交互的成本。模板的前提是 UI 组件库的完备，和标准的中后台交互、视觉设计，基于此沉淀标准化的业务模板库，根据场景选择合适的模板，配置下页面信息和路径后，就可以一键安装到本地并自动化配置好路由，安装好依赖。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171818.png)\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171819.png)\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171820.png)\n\n### 6. 项目创建与管理\n\n项目的创建与管理，从一开始我们的目标就是 “去耦合，单人可全流程 Hold” —— 意思是在项目的创建、本地环境的搭建（也包括了环境的升级）、分支管理、构建、部署等环节，前端同学可以完全一人搞定。不需要因为权限的问题找人帮忙建仓库；不需要因为组件、区块、模板、应用（SPA/MPA）、选型（React/Vue）导致本地开发环境的标准不一致进而每次都得学新的；不需要头疼不同业务的版本流程不一致导致还得问这问那；不需要还得人肉的去配置打包脚本；不需要每次部署都得找人（或者是运维）帮忙… 总之，我们希望借助工具抹平日常中太多的不对称，将开发者的专注力重新尽量拉回简单纯粹的编码中。即使是一个对 Git、命令行、应用管理流程不太明白的校园新人，在桌面端可视化工程的系统辅助下，也能很愉快的开始编码。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171821.png)\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171822.png)\n\n### 7. 前端基础资产\n\n前面我们提到了前端团队的规范（标准化）、工具链（CLI）、基于工具链之上的可视化辅助客户端（GUI），提到了组件（模块）、模板、应用。对工具的抽象和业务的可复用抽象，是一个团队的基础资产。简化到 Webpack 撸一个脚手架 + 一套开源三方 UI 组件库，剩下的拼装式生产全靠人肉；复杂些诸如阿里系正在突破的 UI2Code 、编辑器等能力，将标准、流程更自动化，进一步的去人肉。基础资产这部分，我们团队目前业务阶段下的建设分层如下：\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171823.png)\n\n### 8. CI/CD 自动化构建部署\n\n前端具备自己的构建部署系统，便于专业化方面更好的流程控制。政采云前端团队在 2019 年下半年建设了自己的构建部署系统，实现了云打包、云检测和自动化部署（打通对接运维的部署系统）。新的独立系统在设计之初，重点就是希望能实现一种 Flow 的流式机制，以便实现代码的合规性静态检测能力。这部分在系统中最终实现了一套插件化机制，可以按需配置不同的检测项，如某检测项检测不通过，最终会阻塞发布流程，这些检测项有诸如：\n\n- *Lint 检测\n- 兼容性 API 检测\n- HTTPS 检测\n- 包检测（黑名单、包版本）\n- 合法性检测（域、链接）\n- 404 检测\n- 基础的 UI 检测（如是否缺少吊顶）\n- …\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171824.png)\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171825.jpg)\n\n### 9. 可视化搭建系统\n\n可视化搭建系统是进一步高效利用组件的上层建筑。页面是由组件（业务模块）组成，搭建系统将组件的拼装由本地人肉操作，产品化到系统中可视化拼图，将页面搭建、数据配置、构建部署、数据埋点等等产品化，赋能给产品、运营等协作方。前端产出组件，运营搭建页面，既能节省前端的人效，也能让运营能力前置拓展，在营销场景中进一步释放运营的业务能力，实现共赢。\n\n关乎可视化搭建系统的更多，可以查看我们团队之前输出的这篇文章：《[前端工程实践之可视化搭建系统](https://mp.weixin.qq.com/s/tPcIXCCQkdSXr_gTi8KT6A)》\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171826.png)\n\n系统架构图：\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171827.png)\n\n部署流程图：\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171828.png)\n\n### 10. 数据埋点与分析\n\n在很多公司，数据埋点与分析往往是 BI 部门的事情。在政采云，因为公司前期 BI 能力相对不足，前端团队首先发起并推动了面向业务的 Web 数据埋点收集和数据分析、可视化相关的全系统建设。前后实现了埋点规范、埋点 SDK、数据收集及分析、PV/UV、链路分析、转化分析、用户画像、可视化热图、坑位粒度数据透出等数据化能力。\n\n更多数据埋点与分析相关，可以查看我们团队之前输出的这篇文章：《[前端工程实践之数据埋点分析系统](https://mp.weixin.qq.com/s/SnAVuXis1fOtc7VFBN4ckQ)》\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171829.png)\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171830.png)\n\n### 11. 页面性能自动化分析\n\n页面性能，90% 在前端。尤其是像我们公司现阶段 toB 为主的业务，不同于我的老东家（淘宝、蘑菇街）早已移动端占绝对主导，我们依然是 PC 场景占大头，在页面性能方面问题还是比较突出。过去 1 年时间内，给大家可参考的路径是，我们首先发起了图体积优化，针对占据页面体积大头、请求数大头的图片首先发起优化策略，采用规范+工具的方式帮助业务快速实现图体积的优化，相关沉淀可见早先团队的这篇《[为你重新系统梳理下， Web 体验优化中和图有关的那些事](https://mp.weixin.qq.com/s/euvdMHkYUXHmgkV9D334NQ)》。后来，我们逐步基于 Node 能力将梳理出的影响页面性能的点，实现了自动化检测能力，并依据不同的业务场景区分设计检测模型，再后来做了定时任务来实现性能的连贯性分析及数据走势能力，再之后又增加了业务性能数据大盘和每周的红黑榜。关于页面性能自动化分析系统的更多细节，可阅读我们团队早前的文章 《[自动化 Web 性能分析之 Puppeteer 爬虫实践](https://mp.weixin.qq.com/s/tfC7SAJ2r2UclFUj4vE-tQ)》、《[自动化 Web 性能优化分析方案](https://mp.weixin.qq.com/s/2CHA5ewWz_SIlBrdEuVv7w)》。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171831.png)\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171832.png)\n\n### 12. 2019 基建里程碑\n\n上面介绍的一部分政采云前端团队的技术基础建设，基本上都是在 2019 年一年内逐步建设落地并取得结果的。下图是在上一年周期中的建设里程碑，能看出对应的建设周期和节奏。在我这个团队，没有单独设立独立的前端架构组，前端下的团队都是业务团队，我们同学从业务支撑中沉淀问题，针对问题进行思考和聚敛，从业务问题出发针对性的推进对应的建设。\n\n对于研发同学来说，**身价取决于解决问题的能力**，取决于面对不同的业务问题是否具备解决问题的方案。我们团队很庆幸的点在于，业务处于快速发展期，问题很多，既有的沉淀很少，我们很幸运的可以在帮业务解决问题、跟随业务快速发展的过程中，几乎是从零开始做这些建设。这是很可贵、很难得的一段经历，因为大部分的公司要么是体量还没到需要做这些的地步，要么是早就做完了轮不到，无法全程看到整个体系的发展。**公司要为员工创造环境，但员工的成长最终是靠自己**。所以同学们都认可用业余的时间参与建设、甚至是主导某个方向，对自身的成长是个宝贵的机会。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171833.png)\n\n## 基建之外\n\n### 1. 凡是建设，必有数据\n\n凡是建设，必须要有对应的数据收集和分析，数据说明基建带来的改变，说明投入产出比。数据指标的设计，需要在某专项建设的前期即设计好并进行采集，并在整个推动周期和落地后持续收集，这样可以得到一个相对完整的变化曲线，用以作证工作的成效。数据不见得一定是完全精准的，但数据一定要能**说明趋势**，**直观化，反馈准确**。\n\n### 2. 从场景出发找方案\n\n对于人力方面，任何情况下人力都是缺失的。但很多时候我们的建设推不下去，往往不是因为人力的问题，而是**没想清楚**。《庄子·列寇传》有一则寓言，“朱评漫学屠龙于支离益，单千金之家，三年技成而无所用其巧”。 讲的是一个人散尽家资学习屠龙之技，学成却发现世界上本没有龙。对于研发同学，同样会存在从方案出发找场景的问题，如想学习 Node 不知道如何学习，照着书中的例子学，最后发现都忘了效果很不好。没有一个作家是看小说看成的，也没有一个语言学家是看字典看成的，同理技术专家也不会是通过看技术书籍养成的。在实践中学习，从来都是最快的方式。有价值的事从来都是从业务本身的问题出发。问题就是机会，问题就是长萝卜的坑。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171834.png)\n\n### 3. 不设限，拓展能力边界\n\n前端在研发体系中话语权偏低的现状，从前端这个职能出现那一刻就存在了。不排除个别研发团队，因其业务模式的原因，对前端的依赖较深，前端的话语权相对偏高。绝大部分的研发团队中，前端的工作，在其他研发眼中，往往是 “技术含量低”、“很薄的一层” 等情况。这个现状的背后，看看下图就知道了：\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171835.jpeg)\n\n横、纵，2 个维度。右边的 “纵”，参考网络应用系统的分层体系，前端的传统工作范畴，都是集中在 “用户界面层”，很少能往下深入，深入到网关 ~ 基础设施层。后端则不同。从这个角度看，前端确实很 “薄”。现在良性的一面是，Node 能力为前端提供了向下渗透的服务端能力。一些团队也基于 Node 横向扩展自身的工程化能力，和向业务纵深去拓展前端的系统化能力。\n\n我们再看左边的 “横”。只有很少的前端团队，能较完善的去建设和发展技术体系。对于有了较完善体系的前端团队而言，其技术体系也更多是局限于前端自身的职能范畴，没能较好的互动渗透到业务侧，更多是在自嗨，业务的感知力是很弱的。将技术带来的工程收益，转变为业务收益；将部门内的技术影响，转变为业务影响；将技术场景，升级到业务场景；将团队的基础能力，变为业务能力。**跳出前端，围绕并深入业务**，这是每一个正在推动团队体系建设的同学要更多想想的事。\n\n### 4. 业务的阶段匹配性\n\n基建的内容，是和业务阶段相匹配的。不同团队服务的业务阶段不同，基建的内容和广深度也会不同。高下之分不在于多寡，而在于对业务的理解和支持程度。**如果你只需要一根针，千万不要去磨铁棒**。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171836.png)\n\n### 5. 技术的价值，在于解决业务问题\n\n技术的价值，在于解决业务问题；人的身价，在于解决问题的能力。但解决问题，技术基建绝不是银弹，甚至在我来看，都不是排在前三位的。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/engineering/20210410171837.png)\n","lastmodified":"2023-05-09T16:33:58.299366595Z","tags":[]},"/%E5%89%8D%E7%AB%AF%E9%9D%A2%E8%AF%95%E9%A2%98":{"title":"前端面试题","content":"\n[[浏览器相关]]  \n[[css]]  \n[[html]]  \n[[javascript]]  \n[[vue\u0026react]]\n","lastmodified":"2023-05-09T16:33:58.299366595Z","tags":[]},"/%E5%89%91%E6%8C%87Offer":{"title":"剑指Offer","content":"## 剑指Offer\n\n### 1. 替换空格\n\n请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。\n\n```c++\nclass Solution {\npublic:\n\tvoid replaceSpace(char *str,int length) {\n\t}\n};\n```\n\n\u003chr\u003e\n这题其实就是在给定地str中找到空格，并替换成%20，由于是一个字符替换成三个字符，所以需要移动元素，如果从前往后移动，假设有n个空格，则第n个空格后面的元素都要被移动n次，很浪费时间，所以从后往前移动，并且不需要额外的数组，先计算出填补后的长度`newlength=oldnumber+n*2`,然后从后遍历直接`str[pNewlength--]=str[pOldlength];`\n\n```c++\nclass Solution {\npublic:\n\tvoid replaceSpace(char *str,int length) {\n        if (str == NULL || length \u003c 0)\n      return;\n        int i=0,oldLength=strlen(str),newLength=0,blanks=0;\n        while(str[i]!='\\0'){\n            //oldLength++;\n            if(str[i]==' ')\n            blanks++;\n            i++;\n        }\n        newLength=oldLength+blanks*2;\n        if(newLength\u003elength) return;\n        while(newLength\u003eoldLength\u0026\u0026oldLength\u003e=0){\n            if(str[oldLength]==' '){\n                str[newLength--]='0';\n                str[newLength--]='2';\n                str[newLength--]='%';\n            }else{\n                str[newLength]=str[oldLength];\n                newLength--;\n            }\n            oldLength--;\n        }\n\t}\n};\n```\n\n### 2. 从头到尾打印链表\n\n输入一个链表，按链表从尾到头的顺序返回一个ArrayList。\n\n\u003chr\u003e\n```c++\nclass Solution {\npublic:\n    vector\u003cint\u003e printListFromTailToHead(ListNode* head) {\n        vector\u003cint\u003e arr;\n        while(head){\n            arr.insert(arr.begin(),head-\u003eval);\n            head=head-\u003enext;\n        }\n        return arr;\n    }\n};\n```\n\n### 3. 重建二叉树\n\n输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。\n\n```c++\n/**\n * Definition for binary tree\n * struct TreeNode {\n *     int val;\n *     TreeNode *left;\n *     TreeNode *right;\n *     TreeNode(int x) : val(x), left(NULL), right(NULL) {}\n * };\n */\nclass Solution {\npublic:\n    TreeNode* reConstructBinaryTree(vector\u003cint\u003e pre,vector\u003cint\u003e vin) {\n\n    }\n};\n```\n\n\u003chr\u003e\n\n\n![图片说明](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/剑指Offer20210214204350.png)\n\n![图片说明](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/剑指Offer20210214204401.png)\n\n思路：\n\n1. 由先序序列第一个**`pre[0]`**在中序序列中找到根节点位置**`gen`** \n\n2. 以\n\n   `gen`\n\n   为中心遍历\n\n   - `0~gen`\n\n     左子树\n\n     - 子中序序列：**`0~gen-1`**，放入**`vin_left[]`** \n     - 子先序序列：**`1~gen`**放入**`pre_left[]`**，**`+1`**可以看图，因为头部有根节点 \n\n   - `gen+1~vinlen`\n\n     为右子树\n\n     - 子中序序列：**`gen+1 ~ vinlen-1`**放入**`vin_right[]`** \n     - 子先序序列：**`gen+1 ~ vinlen-1`**放入**`pre_right[]`** \n\n3. 由先序序列**`pre[0]`**创建根节点 \n\n4. 连接左子树，按照左子树子序列递归（**`pre_left[]`**和**`vin_left[]`**） \n\n5. 连接右子树，按照右子树子序列递归（**`pre_right[]`**和**`vin_right[]`**） \n\n6. 返回根节点\n\n```c++\n/**\n * Definition for binary tree\n * struct TreeNode {\n *     int val;\n *     TreeNode *left;\n *     TreeNode *right;\n *     TreeNode(int x) : val(x), left(NULL), right(NULL) {}\n * };\n */\nclass Solution {\npublic:\n    TreeNode* reConstructBinaryTree(vector\u003cint\u003e pre,vector\u003cint\u003e vin) {\n        int mid=0,len=vin.size();\n        if(len==0) return NULL;\n        vector\u003cint\u003e preL,preR,vinL,vinR;\n        \n        TreeNode* root=new TreeNode(pre[0]);\n        \n        for(int i=0;i\u003clen;i++){\n            if(vin[i]==pre[0]){\n                mid=i;\n                break;\n            }\n        }\n        \n        for(int i=0;i\u003cmid;i++){\n            preL.push_back(pre[i+1]);\n            vinL.push_back(vin[i]);\n        }\n        for(int i=mid+1;i\u003clen;i++){\n            preR.push_back(pre[i]);\n            vinR.push_back(vin[i]);\n        }\n        root-\u003eleft=reConstructBinaryTree(preL,vinL);\n        root-\u003eright=reConstructBinaryTree(preR,vinR);\n        return root;\n    }\n};\n```\n\n### 4. 两个栈实现队列\n\n用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。\n\n\u003chr\u003e\n```c++\nclass Solution\n{\npublic:\n    void push(int node) {\n        if(stack2.empty()\u0026\u0026stack1.empty()){\n            stack2.push(node);\n        }else\n        stack1.push(node);\n    }\n\n    int pop() {\n        if(!stack2.empty()){\n            \n        }else{\n            if(stack1.empty()){\n                return -1;\n            }n\n            trans();\n        }\n        int ans= stack2.top();\n        stack2.pop();\n        if(stack2.empty()){\n            trans();\n        }\n        return ans;\n    }\n    \n    void trans(){\n        while(!stack1.empty()){\n            stack2.push(stack1.top());\n            stack1.pop();\n        }\n    }\n\nprivate:\n    stack\u003cint\u003e stack1;\n    stack\u003cint\u003e stack2;\n};\n```\n\n### 5.二进制中1的个数\n\n输入一个整数，该数二进制表示中1的个数。其中负数用补码表示。\n\n\u003chr\u003e\n1.利用一个结论：一个二进制数n减1后与原二进制数进行\u0026运算( 即n\u0026(n-1) )会消去最右边的1。\n2.这个结论怎么来的？\n\n- 假设二进制数101进行减1运算，刚好最右边是1，则得到100，此时用100跟101做\u0026运算，得到的是100，故消去了101左右边的1。 \n- 100减1呢？最低位是0,跟十进制减法一样啊，向高位借。(可以脑补一下十进制100减1的过程) \n\n**所以二进制100减1的运算过程如下：** \n\n- 最右边的0向右数第二位借1得：2-1=1， \n- 右数第二位还是0，却要借给最右边那位1，所以它也得向高位借。 \n- 这样右数第三位的1借给它1之后变成0，右数第二位借1得：2-1=1。 \n- 所以得到新数为011。 \n\n**观察一下刚刚的运算过程可以发现：** \n\n- 如果最右边刚好是1(如101)，进行减1运算就不用向高位借，直接得0，高位则保持原样不变(得100)。 \n- 再把减1后得到的数与原数\u0026运算(即101\u0026100=100)可知高位都不变那就是消去最右边的1！(由这可能还不太明显是消去最右边的1，继续看下面的) \n- 如果最右边是0(如100)，进行减1运算，则需要像高位借，而最终会导致最近的高位1因为被借走1而变0，而比它高的高位保持原样不变(得011),再跟原来的数进行\u0026运算(即100\u0026011=000)； \n- 所以由以上两点可知二进制数每次减1后与原数进行\u0026运算会消去最右边的1。\n\n```c++\nclass Solution {\npublic:\n     int  NumberOf1(int n) {\n         int count=0;\n         while(n!=0)\n         {\n             n=n\u0026(n-1);\n             count++;\n         }\n         return count;\n     }\n};\n\n```\n\n### 6. 调整数组顺序\n\n输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。\n\n\u003chr\u003e\n\n\n我写了个很弱智的方法,时间复杂度不高空间挺复杂的\n\n```c++\nclass Solution {\npublic:\n    void reOrderArray(vector\u003cint\u003e \u0026array) {\n        vector\u003cint\u003e tmp;\n        vector\u003cint\u003e tmp2;\n        for(int i=0;i\u003carray.size();i++){\n            if(array[i]\u00261){\n                tmp.push_back(array[i]);\n            }else{\n                tmp2.push_back(array[i]);\n            }\n        }\n        for(int i=0;i\u003ctmp2.size();i++){\n            tmp.push_back(tmp2[i]);\n        }\n        array.swap(tmp);\n    }\n};\n```\n\n### 7. 链表中倒数第k个结点\n\n输入一个链表，输出该链表中倒数第k个结点。\n\n\u003chr\u003e\n```c++\nclass Solution {\npublic:\n    ListNode* FindKthToTail(ListNode* pListHead, unsigned int k) {\n        int cnt=0;\n        ListNode * L;\n        ListNode * ans;\n        L=pListHead;ans=pListHead;\n        while(L){\n            cnt++;\n            if(cnt\u003ek){\n                ans=ans-\u003enext;\n            }\n            L=L-\u003enext;\n        }\n        if(cnt\u003ck){\n            return NULL;\n        }\n        return ans;\n    }\n};\n```\n\n没啥难度，但是指针定义那一块要小心` ListNode * L,ans;`是错误的，应为` ListNode * L,*ans;`\n\n### 8. 反转链表\n\n输入一个链表，反转链表后，输出新链表的表头。\n\n\u003chr\u003e\n```c++\nclass Solution {\npublic:\n    ListNode* ReverseList(ListNode* pHead) {\n        ListNode * p0=nullptr,*p1=pHead,*p2=nullptr;\n        while(p1){\n            p2=p1-\u003enext;\n            p1-\u003enext=p0;\n            p0=p1;\n            p1=p2;\n        }\n        return p0;\n    }\n};\n```\n\n没啥难度，但要小心。\n\n![反转链表](../../../../../Desktop/Algorithm.assets/815472960_1566642098849_C3B4D3532589FC2990420BCEA62A8DBF.jfif)\n\n### 9. 合并两个排序的链表\n\n输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。\n\n\u003chr\u003e\n搜集了一个递归写法，不算太懂\n\n```c++\nclass Solution {\npublic:\n    ListNode* Merge(ListNode* pHead1, ListNode* pHead2)\n    {\n        if(pHead1==NULL)\n            return pHead2;\n        else if(pHead2==NULL)\n            return pHead1;\n        ListNode* p1=pHead1;\n        ListNode* p2=pHead2;\n        ListNode* newHead=NULL;\n        if(p1-\u003eval\u003cp2-\u003eval){\n            newHead=p1;\n            newHead-\u003enext=Merge(p1-\u003enext,p2);\n        }\n        else{\n            newHead=p2;\n            newHead-\u003enext=Merge(p2-\u003enext,p1);\n        }\n        return newHead;\n    }\n};\n```\n\n这其实就是归并排序，等回头数据结构学完得好好写一下。\n\n### 10. 顺时针打印矩阵\n\n输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下4 X 4矩阵： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 则依次打印出数字1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10.\n\n\u003chr\u003e\n```c++\nclass Solution {\npublic:\n    vector\u003cint\u003e printMatrix(vector\u003cvector\u003cint\u003e \u003e matrix) {\n        int dir[4][2]={{0,1},{1,0},{0,-1},{-1,0}},di=0,i=0,j=0,cnt=0;\n        int vis[matrix.size()][matrix[0].size()],num=matrix.size()*matrix[0].size();\n        vector\u003cint\u003e arr;\n        if(num==0){\n            return arr;\n        }\n\n        while(1){\n                //cout\u003c\u003cmatrix[i][j];\n            arr.push_back(matrix[i][j]);\n            vis[i][j]=1;\n            i+=dir[di][0];\n            j+=dir[di][1];\n          //判断增加后是否超出数组范围或者下一个元素已被访问，则转向\n            if(vis[i][j]==1||i==matrix.size()||j==matrix[0].size()||i\u003c0||j\u003c0){\n                    i-=dir[di][0];\n                    j-=dir[di][1];\n                    di=(di+1)%4;\n                    i+=dir[di][0];\n                    j+=dir[di][1];\n            }\n            if(++cnt==num){//最后判断输出个数达到数组元素个数时则结束\n                break;\n            }\n        }\n    return arr;\n    }\n};\n```\n\n其实有点找回以前刷题时用dir方向数组+vis数组的感觉了，题目在以前算简单，现在居然花了好一会才出来。\n\n### 11.包含min函数的栈\n\n定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数（时间复杂度应为O（1））。\n\n\u003chr\u003e\n```c++\nclass Solution {\npublic:\n    void push(int value) {\n        if(ptr\u003e=9999) return;\n        if(value\u003cminNum) {\n            stack[++ptr]=minNum;\n            minNum=value;\n        }\n        stack[++ptr]=value;\n    }\n    void pop() {\n        if(ptr==-1) return;\n        if(stack[ptr]==minNum){\n            ptr--;\n            minNum=stack[ptr--];\n        }else{\n            ptr--;\n        }\n    }\n    int top() {\n        if(ptr==-1) return -1;\n        return stack[ptr];\n    }\n    int min() {\n        return minNum==0xffffff?-1:minNum;\n    }\n    int minNum=0xffffff;\n    int ptr=-1;\n    int stack[10000];\n    \n};\n```\n\n本来对于目前的最小值被pop后如何处理没什么办法，查看题解后明白可以在插入时判断，如果此值为新的最小值，则在插入此值之前先插入上一个最小值，在pop时判断取出的是不是当前最小值，若是则pop两次，第二次pop出的则为当前的最小值。\n\n### 12. 栈的压入弹出\n\n要看一个序列是否可以作为一个给定入栈序列的弹出序列，可以用一个栈进行模拟（也可用数组代替，我就偷个懒），设一个指向popV第一个元素的指针j，按照pushV的顺序插入，插入时检索插入的值和popV[J]是否一样，若一样则说明当前出栈序列正好出到这个值，那就直接弹出，j++（匹配出栈序列的下一个），最后pushV全插入后，若j未到popV末尾，说明栈中剩下的内容一次性全部弹出，直接边弹出边和popV中的作比较即可。\n\n```c++\nclass Solution {\npublic:\n    bool IsPopOrder(vector\u003cint\u003e pushV,vector\u003cint\u003e popV) {\n        stack\u003cint\u003e sta;\n        int i=0,j=0;\n        while(i!=pushV.size()){\n            sta.push(pushV[i]);\n            if(pushV[i]==popV[j]){\n                sta.pop();\n                j++;\n            }\n            i++;\n        }\n        for(;j\u003cpopV.size();j++){\n            if(sta.top()!=popV[j]){\n                return false;\n            }else{\n                sta.pop();\n            }\n        }\n        return true;\n    }\n};\n```\n\n### 13.字符串的排列\n\n输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。\n\n\u003chr\u003e\n```c++\nclass Solution {\npublic:\n    vector\u003cstring\u003e Permutation(string str) {\n        if (str.empty()) return {};\n        sort(str.begin(), str.end());\n        vector\u003cstring\u003e ans;\n        ans.push_back(str);\n        while (next_permutation(str.begin(), str.end()))\n            ans.push_back(str);\n        return ans;\n    }\n};\n```\n\n### 14. 数组中出现次数超过一半的数字\n\n数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。\n\n\u003chr\u003e\n```c++\nclass Solution {\npublic:\n    int MoreThanHalfNum_Solution(vector\u003cint\u003e numbers) {\n        sort(numbers.begin(),numbers.end());\n        int len=numbers.size()/2;\n        int tmp,j;\n        for(int i=0;i\u003cnumbers.size();i++){\n            j=i+len;\n            if(j\u003cnumbers.size()){\n                if(numbers[j]==numbers[i]){\n                    return numbers[i];\n                }\n            }else{\n                return 0;\n            }\n        }\n    }\n};\n```\n\n这个办法不是很满意，看了看他人的，普遍有找中间数的方法\n\n```java\npublic int MoreThanHalfNum_Solution(int [] array) {\n    Arrays.sort(array);\n    int i=array[array.length/2];\n    return IntStream.of(array).filter(k-\u003ek==i).count()\u003earray.length/2?i:0;\n}\n```\n\n或者用count++\n\n```c++\nclass Solution {\npublic:\n    int MoreThanHalfNum(vector\u003cint\u003e numbers) {\n        int n = 0;\n        int ret;\n        for(size_t i=0;i\u003cnumbers.size();i++){\n            if(n == 0){\n                ret = numbers[i];\n                n = 1;\n            }else{\n                if(ret == numbers[i])\n                    n ++;\n                else\n                    n--;  \n            }\n        }\n        return ret;\n    }\n};\n```\n\n### 15.连续子数组的最大和\n\nHZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。给一个数组，返回它的最大连续子序列的和，你会不会被他忽悠住？(子向量的长度至少是1)\n\n\u003chr\u003e\n```c++\nclass Solution {\npublic:\n    int FindGreatestSumOfSubArray(vector\u003cint\u003e array) {\n        int dp[array.size()],maxn=array[0];\n        dp[0]=array[0];\n        for(int i=1;i\u003carray.size();i++){\n            dp[i]=max(array[i],dp[i-1]+array[i]);\n            maxn=max(maxn,dp[i]);\n        }\n        return maxn;\n    }\n}; \n```\n\n最简单的动态规划，dp作为规划数组，dp[i]存储从前面最佳位置到array[i]的和，由于dp[i-1]存储从前面最佳位置到array[i-1]的和，所以如果dp[i-1]+array[i]\u003carray[i]，即前面最佳位置到i-1的值是负的，说明还不如从i位置重新开始寻找，此时dp[i]=array[i]，同时maxn密切追踪最大值，最后的值就是整个的最大值。\n\n也可以待循环结束后遍历整个dp数组找到最大值。\n\n### 16. 丑数\n\n把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。\n\n\u003chr/\u003e\n\n```cpp\nclass Solution {\npublic:\n    int GetUglyNumber_Solution(int index) {\n        int ans[index+1];\n        //ans[0]=0;\n        memset(ans,0,sizeof(ans));\n        ans[1]=1;\n        int p2=1,p3=1,p5=1;\n        for(int i=2;i\u003c=index;i++){\n            ans[i]=min(ans[p2]*2,min(ans[p3]*3,ans[p5]*5));\n            if(ans[p2]*2==ans[i]) p2++;\n            if(ans[p3]*3==ans[i]) p3++;\n            if(ans[p5]*5==ans[i]) p5++;\n        }\n        return ans[index];\n    }\n};\n```\n\n\n利用动态规划，由于每个丑数可以理解为``2^x3^y5^z`的形式,用ans[i]表示第i个丑数，则ans[i]等于ans[a]\\*2、ans[b]\\*3、ans[c]*5中最小的一个，a、b、c均小于i。所以用三个指针记录a、b、c即可，由于有可能ans[a]\\*2==ans[b]\\*3，所以最后指针++时得用三个独立的if。\n\n### 17.数字在排序数组中出现的次数\n\n统计一个数字在排序数组中出现的次数。\n\n```c++\nclass Solution {\npublic:\nint GetNumberOfK(vector\u003cint\u003e data, int k)\n{\n};\n```\n\n\u003chr\u003e一道三分钟的题被我写成了三小时。\n自己想法是先二分找到这个数然后再遍历寻找个数。主要是好久不写二分，一些细节都忘了。\n\n```c++\nclass Solution {\npublic:\nint GetNumberOfK(vector\u003cint\u003e data, int k)\n{\n  int le = 0, ri = data.size() - 1;\n  int mid, cnt = 0;\n  while ( le \u003c= ri){\n    mid = (ri + le) \u003e\u003e 1;\n    if (data[mid] \u003e k){\n      ri = mid - 1;//注意需要-1，不然可能出现13+14/2=13一直访问不到14的情况\n    }\n    else if (data[mid] \u003c k){\n      le = mid + 1;\n    }else{\n      break;\n    }\n  }\n  if (mid\u003e=data.size()||mid\u003c0)//判断是找到退出还是le\u003eri退出时，不能用data[mid]==k，因为mid可能\u003c0或==data.size()造成越界\n    return cnt;\n  else\n    cnt = 0;\n  for (int i = mid; i \u003c data.size() \u0026\u0026 data[i] == k; i++)\n  {\n    cnt++;\n  }\n  for (int i = mid - 1; i \u003e= 0 \u0026\u0026 data[i] == k; i--)\n  {\n    cnt++;\n  }\n  return cnt;\n}\n};\n```\n\n还有更好的方法是，直接upper_bound-lower_bound就是个数，相当于两次都用二分。\n\n```java\nprivate int upper_bound(int[] array, int val) {\n        int l = 0, r = array.length - 1, mid;\n        while (l \u003c= r) {\n            mid = (l + r) \u003e\u003e 1;\n            if (array[mid] \u003c= val) {\n                l = mid + 1;\n            } else {\n                r = mid - 1;\n            }\n        }\n        return l;\n    }\n \n    private int lower_bound(int[] array, int val) {\n        int l = 0, r = array.length - 1, mid;\n        while (l \u003c= r) {\n            mid = (l + r) \u003e\u003e 1;\n            if (array[mid] \u003c val) {\n                l = mid + 1;\n            } else {\n                r = mid - 1;\n            }\n        }\n        return l;\n    }\n```\n\n### 18.二叉树深度\n\n输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。\n\n```c++\n/*\nstruct TreeNode {\n\tint val;\n\tstruct TreeNode *left;\n\tstruct TreeNode *right;\n\tTreeNode(int x) :\n\t\t\tval(x), left(NULL), right(NULL) {\n\t}\n};*/\nclass Solution {\npublic:\n    int maxDeep=0;\n    void getD(TreeNode* pRoot,int dep){\n        if(pRoot==NULL){\n            maxDeep=max(maxDeep,dep);\n            return;\n        }\n        dep+=1;\n        getD(pRoot-\u003eright,dep);\n        getD(pRoot-\u003eleft,dep);\n    };\n    int TreeDepth(TreeNode* pRoot)\n    {\n        maxDeep=0;\n        if(pRoot){\n            getD(pRoot,0);\n            return maxDeep;\n        }else{\n            return 0;\n        }\n    };\n};\n```\n\n19.判断是否平衡二叉树\n\n```c++\nclass Solution {\npublic:\n    int getH(TreeNode* pRoot){\n        if(pRoot==NULL) return 0;\n        int leftH=0,rightH=0;\n        if(pRoot-\u003eleft!=NULL){\n            leftH=getH(pRoot-\u003eleft);\n        }\n        if(pRoot-\u003eright!=NULL){\n            rightH=getH(pRoot-\u003eright);\n        }\n        if(rightH==-1||leftH==-1||abs(rightH-leftH)\u003e1){\n            return -1;\n        }\n        return max(rightH,leftH)+1;\n    }\n    bool IsBalanced_Solution(TreeNode* pRoot) {\n        return getH(pRoot)==-1?false:true;\n    }\n    \n};\n```\n\n### 19.剪绳子\n\n题目描述\n\n给你一根长度为n的绳子，请把绳子剪成整数长的m段（m、n都是整数，n\u003e1并且m\u003e1），每段绳子的长度记为k[0],k[1],...,k[m]。请问k[0]xk[1]x...xk[m]可能的最大乘积是多少？例如，当绳子的长度是8时，我们把它剪成长度分别为2、3、3的三段，此时得到的最大乘积是18。\n\n输入描述:\n\n```\n输入一个数n，意义见题面。（2 \u003c= n \u003c= 60）\n```\n\n输出描述:\n\n```\n输出答案。\n```\n\n示例1\n\n输入\n\n```\n8\n```\n\n输出\n\n```\n18\n```\n\n思路\n\n这题我是通过模拟了n=1-16的情况（挺快的，几分钟），发现4-6分两段、7-9分三段、10-12分四段。。。每段长度最接近时成绩最大，以此类推，从而利用数学特性解决，貌似题目初衷是使用动态规划，一位老哥给予了数学证明。\n\n我的\n\n```javascript\nfunction cutRope(number) {\n  if (number === 2 || number === 3) {\n    return number - 1\n  }\n  // write code here\n  const m = number % 3 === 0 ? number / 3 : Math.ceil(number / 3),\n    base = Math.floor(number / m),\n    remains = number % m\n  return Math.pow(base, m - remains) * Math.pow(base + 1, remains)\n}\n```\n\n动态规划\n\n```c++\nclass Solution {\npublic:\n    int cutRope(int number) {\n        if (number == 2) {\n            return 1;\n        }\n        else if (number == 3) {\n            return 2;\n        }\n\n        vector\u003cint\u003e f(number + 1, -1);\n        for (int i = 1; i \u003c= 4; ++i) {\n            f[i] = i;\n        }\n        for (int i = 5; i \u003c= number; ++i) {\n            for (int j = 1; j \u003c i; ++j) {\n                f[i] = max(f[i], j * f[i - j]);\n            }\n        }\n        return f[number];\n    }\n};\n```\n\n数学证明\n\nhttps://www.nowcoder.com/questionTerminal/57d85990ba5b440ab888fc72b0751bf8?answerType=1\u0026f=discussion\n\n### 20.判断子结构\n\n输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构）\n\n根据题意可知，需要一个函数判断树A和树B是否有相同的结构。显然是个递归程序。可考察递归程序3部曲。\n\n1. 递归函数的功能：判断2个数是否有相同的结构，如果相同，返回true，否则返回false \n2. 递归终止条件： \n\n- 如果树B为空，返回true，此时，不管树A是否为空，都为true \n- 否则，如果树B不为空，但是树A为空，返回false，此时B还没空但A空了，显然false \n\n1. 下一步递归参数： \n\n- 如果A的根节点和B的根节点不相等，直接返回false \n- 否则，相等，就继续判断A的左子树和B的左子树，A的右子树和B的右子树\n\n代码\n\n```c++\nbool HasSubtree(TreeNode* pRoot1, TreeNode* pRoot2)\n{\n    if (!pRoot1 || !pRoot2) return false;\n    return dfs(pRoot1, pRoot2) || HasSubtree(pRoot1-\u003eleft, pRoot2) ||\n    HasSubtree(pRoot1-\u003eright, pRoot2);\n}\nbool dfs(TreeNode *r1, TreeNode *r2) {\n    if (!r2) return true;\n    if (!r1) return false;\n    return r1-\u003eval==r2-\u003eval \u0026\u0026 dfs(r1-\u003eleft, r2-\u003eleft) \u0026\u0026 dfs(r1-\u003eright, r2-\u003eright);\n}\n```\n\n```javascript\n/* function TreeNode(x) {\n    this.val = x;\n    this.left = null;\n    this.right = null;\n} */\nfunction HasSubtree(r1, r2)\n{\n  \tif (!r1 || !r2) return false\n    return test(r1,r2)||HasSubtree(r1.left,r2)||HasSubtree(r1.right,r2)\n}\n\nfunction test(r1,r2){\n  \tif(!r2) return true\n  \tif(!r1) return false\n    return r1.val===r2.val\u0026\u0026test(r1.left,r2.left)\u0026\u0026test(r1.right,r2.right)\n}\n```\n\n可能是20200612写的时候很困，看了半天才写出来，分为两步吧，第一步先遍历每个结点，如果这两个结点的值相同，则r1和r2都需要在检查左、右儿子是否相同，即`r1.val===r2.val\u0026\u0026test(r1.left,r2.left)\u0026\u0026test(r1.right,r2.right)`，如果值不同，则检查r1的左右儿子是否与r2的值相同，即`HasSubtree(r1.left,r2)||HasSubtree(r1.right,r2)`（r2不需要。left或。right），也仅在`test(r1,r2)||…`中test返回false时才会运行后面的函数。\n\n### 21.二叉树镜像\n\n操作给定的二叉树，将其变换为源二叉树的镜像。\n\n```c++\n/*\nstruct TreeNode {\n    int val;\n    struct TreeNode *left;\n    struct TreeNode *right;\n    TreeNode(int x) :\n            val(x), left(NULL), right(NULL) {\n    }\n};*/\nclass Solution {\npublic:\n    void Mirror(TreeNode *pRoot) {\n        if(!pRoot) return;\n        if(pRoot-\u003eleft==NULL\u0026\u0026pRoot-\u003eright==NULL){\n            return;\n        }\n        TreeNode * tmp;\n        if(pRoot-\u003eleft!=NULL){\n            tmp=pRoot-\u003eright;\n            pRoot-\u003eright=pRoot-\u003eleft;\n            pRoot-\u003eleft=NULL;\n            if(tmp){\n                pRoot-\u003eleft=tmp;\n            }\n        }else{\n            pRoot-\u003eleft=pRoot-\u003eright;\n            pRoot-\u003eright=NULL;\n        }\n        Mirror(pRoot-\u003eleft);\n        Mirror(pRoot-\u003eright);\n    }\n};\n```\n\n","lastmodified":"2023-05-09T16:33:56.635325968Z","tags":[]},"/%E5%8C%85%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7":{"title":"包管理工具","content":"\n## npm\n\n````bash\n# 查看文件路径\nnpm config get userconfig\nnpm config get globalconfig\n````\n\n### proxy\n\n```bash\nnpm config set proxy http://127.0.0.1:7890\nnpm config set https-proxy http://127.0.0.1:7890\n \nnpm config delete proxy \nnpm config delete https-proxy\n```\n\n### registry\n\n```bash\nnpm config set registry https://registry.npm.taobao.org/\nnpm config set registry https://registry.npmjs.org/\n```\n\n## yarn\n\n### 基本使用\n\n```bash\nyarn config list\t\t# 查看yarn配置\n\nyarn add [package] #添加包，会自动安装最新版本，注意会覆盖指定版本号！！！\nyarn add [package]@[version] #带版本号安装\nyarn remove [package] #移除某个包\n#更新一个包\nyarn upgrade [package]\nyarn upgrade [package]@[version]\nyarn upgrade [package]@[tag]\n```\n\n**初始化一个新项目**\n\n```bash\nyarn init\n```\n\n**添加依赖包**\n\n```bash\nyarn add [package]\nyarn add [package]@[version]\nyarn add [package]@[tag]\n```\n\n**将依赖项添加到不同依赖项类别中**\n\n分别添加到 `devDependencies`、`peerDependencies` 和 `optionalDependencies` 类别中：\n\n```bash\nyarn add [package] --dev\nyarn add [package] --peer\nyarn add [package] --optional\n```\n\n### proxy\n\n```bash\nyarn config set proxy http://127.0.0.1:7890\nyarn config set https-proxy http://127.0.0.1:7890\n```\n\n```bash\nyarn config delete proxy  \nyarn config delete https-proxy\n```\n\n### registry\n\n```bash\nregistry=https://npmmirror.com/mirrors/\nelectron_mirror=https://npmmirror.com/mirrors/electron/\nyarn config set registry https://registry.npmjs.org/\n\nyarn config set registry https://registry.yarnpkg.com\n```\n\n## npm vs yarn\n\n### CLI commands comparison\n\n| npm (v5)                                | Yarn                            |\n| :-------------------------------------- | :------------------------------ |\n| `npm install`                           | `yarn add`                      |\n| ***(N/A)\\***                            | `yarn add --flat`               |\n| ***(N/A)\\***                            | `yarn add --har`                |\n| `npm install --no-package-lock`         | `yarn add --no-lockfile`        |\n| ***(N/A)\\***                            | `yarn add --pure-lockfile`      |\n| `npm install [package] --save`          | `yarn add [package]`            |\n| `npm install [package] --save-dev`      | `yarn add [package] --dev`      |\n| ***(N/A)\\***                            | `yarn add [package] --peer`     |\n| `npm install [package] --save-optional` | `yarn add [package] --optional` |\n| `npm install [package] --save-exact`    | `yarn add [package] --exact`    |\n| ***(N/A)\\***                            | `yarn add [package] --tilde`    |\n| **`npm install [package] --global`**    | **`yarn global add [package]`** |\n| `npm update --global`                   | `yarn global upgrade`           |\n| `npm rebuild`                           | `yarn add --force`              |\n| `npm uninstall [package]`               | `yarn remove [package]`         |\n| `npm cache clean`                       | `yarn cache clean [package]`    |\n| `rm -rf node_modules \u0026\u0026 npm install`    | `yarn upgrade`                  |\n| `npm version major`                     | `yarn version --major`          |\n| `npm version minor`                     | `yarn version --minor`          |\n| `npm version patch`                     | `yarn version --patch`          |\n\n## nvm\n\n1. 安装node\n\n   ```bash\n   nvm ls-remote --lts #查看远程lts仓库\n   nvm install --lts=XXX #安装指定lts版本\n   nvm use \u003c版本号\u003e or --lts=\u003cLTS name\u003e\n   ```\n","lastmodified":"2023-05-09T16:33:58.299366595Z","tags":[]},"/%E5%8D%9A%E6%96%87%E5%88%86%E4%BA%AB":{"title":"💽博文分享","content":"\n这里展示了本人摸鱼划水时写的一些以发布为目的的博客\n\n[[JavaScript 之混淆的“类”]]  \n[[hexo之看板娘]]  \n[[利用Element实现响应式导航栏]]\n\n```dataviewjs\nvar list=dv.pages(`\"Notes/报告\"`)\nfor(let e of list){\nif(e.title!==\"💽博文分享\"){\ndv.paragraph(`[[${e.title}]]`)\n}\n}\ndv.paragraph(`\n\n❕ 此目录由dataview自动生成`)\n```\n","lastmodified":"2023-05-09T16:33:58.311366888Z","tags":[]},"/%E5%90%8C%E6%BA%90%E7%AD%96%E7%95%A5":{"title":"同源策略","content":"\n## 同源的定义\n\n**同源策略**是一个重要的安全策略，它用于限制一个[origin](https://developer.mozilla.org/en-US/docs/Glossary/Origin)的文档或者它加载的脚本如何能与另一个源的资源进行交互。它能帮助阻隔恶意文档，减少可能被攻击的媒介。\n\n\u003e **Origin**\n\u003e\n\u003e Web内容的源由用于访问它的[URL](https://developer.mozilla.org/en-US/docs/Glossary/URL) 的方案(协议)，主机(域名)和端口定义。只有当方案，主机和端口都匹配时，两个对象具有相同的起源。\n\u003e\n\u003e [同源的例子](https://developer.mozilla.org/zh-CN/docs/Glossary/Origin#同源的例子)\n\u003e\n\n\u003e | `http://example.com/app1/index.html` `http://example.com/app2/index.html` | same origin because same scheme (`http`) and host (`example.com`) |\n\u003e | ------------------------------------------------------------ | ------------------------------------------------------------ |\n\u003e | `http://Example.com:80` `http://example.com`                 | same origin because a server delivers HTTP content through port 80 by default |\n\n\u003e\n\u003e [不同源的例子](https://developer.mozilla.org/zh-CN/docs/Glossary/Origin#不同源的例子)\n\u003e\n\n\u003e | `http://example.com/app1` `https://example.com/app2`         | different schemes |\n\u003e | ------------------------------------------------------------ | ----------------- |\n\u003e | `http://example.com` `http://www.example.com` `http://myapp.example.com` | different hosts   |\n\u003e | `http://example.com` `http://example.com:8080`               | different ports   |\n\n如果两个 URL 的 [protocol](https://developer.mozilla.org/en-US/docs/Glossary/Protocol)、[port](https://developer.mozilla.org/en-US/docs/Glossary/Port) (如果有指定的话)和 [host](https://developer.mozilla.org/en-US/docs/Glossary/Host) 都相同的话，则这两个 URL 是*同源*。这个方案也被称为“协议/主机/端口元组”，或者直接是 “元组”。（“元组” 是指一组项目构成的整体，双重/三重/四重/五重/等的通用形式）。\n\n在页面中通过 `about:blank` 或 `javascript:` URL 执行的脚本会继承打开该 URL 的文档的源，因为这些类型的 URLs 没有包含源服务器的相关信息。\n\n## 源的更改\n\n满足某些限制条件的情况下，页面是可以修改它的源。脚本可以将 [`document.domain`](https://developer.mozilla.org/zh-CN/docs/Web/API/Document/domain) 的值设置为其**当前域**或其**当前域的父域**。如果将其设置为其当前域的父域，则这个较短的父域将用于后续源检查。\n\n\u003e 例如：假设 http://store.company.com/dir/other.html 文档中的一个脚本执行以下语句：\n\u003e\n\u003e ```javascript\n\u003e document.domain = \"company.com\";\n\u003e ```\n\u003e\n\u003e 这条语句执行之后，页面将会成功地通过与 `http://company.com/dir/page.html` 的同源检测（假设`http://company.com/dir/page.html` 将其 `document.domain` 设置为“`company.com`”，以表明它希望允许这样做 - 更多有关信息，请参阅 [`document.domain`](https://developer.mozilla.org/zh-CN/docs/Web/API/Document/domain) ）。然而，`company.com` 不能设置 `document.domain` 为 `othercompany.com`，因为它不是 `company.com` 的父域。\n\u003e\n\u003e 端口号是由浏览器另行检查的。任何对`document.domain`的赋值操作，包括 `document.domain = document.domain` 都会导致端口号被重写为 `null` 。因此 `company.com:8080` **不能**仅通过设置 `document.domain = \"company.com\"` 来与`company.com` 通信。必须在他们双方中都进行赋值，以确保端口号都为 `null` 。\n\u003e\n\u003e 注意：使用 `document.domain` 来允许子域安全访问其父域时，您需要在父域和子域中设置 document.domain 为相同的值。这是必要的，即使这样做只是将父域设置回其原始值。不这样做可能会导致权限错误。\n\n## 导致跨域的操作\n\n同源策略控制不同源之间的交互，例如在使用[`XMLHttpRequest`](https://developer.mozilla.org/zh-CN/docs/Web/API/XMLHttpRequest) 或`\u003cimg\u003e`标签时则会受到同源策略的约束。这些交互通常分为三类：\n\n- 跨域**写操作**（Cross-origin writes）*一般是被允许的*。例如链接（links），重定向以及表单提交。特定少数的HTTP请求需要添加 [preflight](https://developer.mozilla.org/zh-CN/docs/HTTP/Access_control_CORS#Preflighted_requests)。\n- 跨域**资源嵌入**（Cross-origin embedding）一般是被允许（后面会举例说明）。\n- 跨域**读操作**（Cross-origin reads）*一般是不被允许的*，但常可以通过内嵌资源来巧妙的进行读取访问。例如，你可以读取嵌入图片的高度和宽度，调用内嵌脚本的方法，或[availability of an embedded resource](https://grepular.com/Abusing_HTTP_Status_Codes_to_Expose_Private_Information).\n\n以下是可能嵌入跨源的资源的一些示例：\n\n- `\u003cscript src=\"…\"\u003e\u003c/script\u003e` 标签嵌入跨域脚本。语法错误信息只能被同源脚本中捕捉到。\n- `\u003clink rel=\"stylesheet\" href=\"…\"\u003e` 标签嵌入CSS。由于CSS的[松散的语法规则](http://scarybeastsecurity.blogspot.dk/2009/12/generic-cross-browser-cross-domain.html)，CSS的跨域需要一个设置正确的 HTTP 头部 `Content-Type` 。不同浏览器有不同的限制： [IE](http://msdn.microsoft.com/zh-CN/library/ie/gg622939(v=vs.85).aspx), [Firefox](http://www.mozilla.org/security/announce/2010/mfsa2010-46.html), [Chrome](http://code.google.com/p/chromium/issues/detail?id=9877), [Safari](http://support.apple.com/kb/HT4070) (跳至CVE-2010-0051)部分 和 [Opera](http://www.opera.com/support/kb/view/943/)。\n- 通过 [`\u003cimg\u003e`](https://developer.mozilla.org/zh-CN/docs/Web/HTML/Element/img) 展示的图片。支持的图片格式包括PNG,JPEG,GIF,BMP,SVG,…\n- 通过 [`\u003cvideo\u003e`](https://developer.mozilla.org/zh-CN/docs/Web/HTML/Element/video) 和 [`\u003caudio\u003e`](https://developer.mozilla.org/zh-CN/docs/Web/HTML/Element/audio) 播放的多媒体资源。\n- 通过 `\u003cobject\u003e`、 [`\u003cembed\u003e`](https://developer.mozilla.org/zh-CN/docs/HTML/Element/embed) 和 `\u003capplet\u003e` 嵌入的插件。\n- 通过 `@font-face` 引入的字体。一些浏览器允许跨域字体（ cross-origin fonts），一些需要同源字体（same-origin fonts）。\n- 通过 `\u003ciframe\u003e` 载入的任何资源。站点可以使用 [X-Frame-Options](https://developer.mozilla.org/zh-CN/docs/HTTP/X-Frame-Options) 消息头来阻止这种形式的跨域交互。\n\n## 跨域API\n\nJavaScript 的 API 中，如 `iframe.contentWindow`、 [`window.parent`](https://developer.mozilla.org/zh-CN/docs/Web/API/Window/parent)、[`window.open`](https://developer.mozilla.org/zh-CN/docs/Web/API/Window/open) 和 [`window.opener`](https://developer.mozilla.org/zh-CN/docs/Web/API/Window/opener) 允许文档间直接相互引用。当两个文档的源不同时，这些引用方式将对 [Window](http://www.whatwg.org/specs/web-apps/current-work/multipage/browsers.html#security-window) 和 [Location](http://www.whatwg.org/specs/web-apps/current-work/multipage/history.html#security-location)对象的访问添加限制，如下两节所述。\n\n为了能让不同源中文档进行交流，可以使用 [`window.postMessage`](https://developer.mozilla.org/zh-CN/docs/Web/API/Window/postMessage)。\n\n## 跨域数据存储\n\n访问存储在浏览器中的数据，如 [localStorage](https://developer.mozilla.org/zh-CN/docs/Web/Guide/API/DOM/Storage) 和 [IndexedDB](https://developer.mozilla.org/zh-CN/docs/IndexedDB)，是以源进行分割。每个源都拥有自己单独的存储空间，一个源中的 JavaScript 脚本不能对属于其它源的数据进行读写操作。\n\n[Cookies](https://developer.mozilla.org/en-US/docs/Glossary/Cookie) 使用不同的源定义方式。一个页面可以为本域和其父域设置 cookie，只要是父域不是公共后缀（public suffix）即可。Firefox 和 Chrome 使用 [Public Suffix List](http://publicsuffix.org/) 检测一个域是否是公共后缀（public suffix）。Internet Explorer 使用其内部的方法来检测域是否是公共后缀。不管使用哪个协议（HTTP/HTTPS）或端口号，浏览器都允许给定的域以及其任何子域名(sub-domains) 访问 cookie。当你设置 cookie 时，你可以使用 `Domain`、`Path`、`Secure`、和 `HttpOnly` 标记来限定其可访问性。当你读取 cookie 时，你无法知道它是在哪里被设置的。 即使您只使用安全的 https 连接，您看到的任何 cookie 都有可能是使用不安全的连接进行设置的。\n\n## 解决跨域\n\n### JSONP\n\n在HTML标签里，一些标签比如script、img这样的获取资源的标签是没有跨域限制的，利用这一点，我们可以这样干：\n\n后端写个小接口\n\n```javascript\n// 处理成功失败返回格式的工具\nconst {successBody} = require('../utli')\nclass CrossDomain {\n  static async jsonp (ctx) {\n    // 前端传过来的参数\n    const query = ctx.request.query\n    // 设置一个cookies\n    ctx.cookies.set('tokenId', '1')\n    // query.cb是前后端约定的方法名字，其实就是后端返回一个直接执行的方法给前端，由于前端是用script标签发起的请求，所以返回了这个方法后相当于立马执行，并且把要返回的数据放在方法的参数里。\n    ctx.body = `${query.cb}(${JSON.stringify(successBody({msg: query.msg}, 'success'))})`\n  }\n}\nmodule.exports = CrossDomain\n```\n\n简单版前端\n\n```javascript\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003cscript type='text/javascript'\u003e\n      // 后端返回直接执行的方法，相当于执行这个方法，由于后端把返回的数据放在方法的参数里，所以这里能拿到res。\n      window.jsonpCb = function (res) {\n        console.log(res)\n      }\n    \u003c/script\u003e\n    \u003cscript src='http://localhost:9871/api/jsonp?msg=helloJsonp\u0026cb=jsonpCb' type='text/javascript'\u003e\u003c/script\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n```\n\n简单封装一下前端这个套路\n\n```javascript\n/**\n * JSONP请求工具\n * @param url 请求的地址\n * @param data 请求的参数\n * @returns {Promise\u003cany\u003e}\n */\nconst request = ({url, data}) =\u003e {\n  return new Promise((resolve, reject) =\u003e {\n    // 处理传参成xx=yy\u0026aa=bb的形式\n    const handleData = (data) =\u003e {\n      const keys = Object.keys(data)\n      const keysLen = keys.length\n      return keys.reduce((pre, cur, index) =\u003e {\n        const value = data[cur]\n        const flag = index !== keysLen - 1 ? '\u0026' : ''\n        return `${pre}${cur}=${value}${flag}`\n      }, '')\n    }\n    // 动态创建script标签\n    const script = document.createElement('script')\n    // 接口返回的数据获取\n    window.jsonpCb = (res) =\u003e {\n      document.body.removeChild(script)\n      delete window.jsonpCb\n      resolve(res)\n    }\n    script.src = `${url}?${handleData(data)}\u0026cb=jsonpCb`\n    document.body.appendChild(script)\n  })\n}\n// 使用方式\nrequest({\n  url: 'http://localhost:9871/api/jsonp',\n  data: {\n    // 传参\n    msg: 'helloJsonp'\n  }\n}).then(res =\u003e {\n  console.log(res)\n})\n2.空iframe加for\n```\n\n### **空iframe加form**\n\nJSONP只能发GET请求，因为本质上script加载资源就是GET，那么如果要发POST请求怎么办呢？\n\n后端写个小接口\n\n```javascript\n// 处理成功失败返回格式的工具\nconst {successBody} = require('../utli')\nclass CrossDomain {\n  static async iframePost (ctx) {\n    let postData = ctx.request.body\n    console.log(postData)\n    ctx.body = successBody({postData: postData}, 'success')\n  }\n}\nmodule.exports = CrossDomain\n```\n\n前端\n\n```javascript\nconst requestPost = ({url, data}) =\u003e {\n  // 首先创建一个用来发送数据的iframe.\n  const iframe = document.createElement('iframe')\n  iframe.name = 'iframePost'\n  iframe.style.display = 'none'\n  document.body.appendChild(iframe)\n  const form = document.createElement('form')\n  const node = document.createElement('input')\n  // 注册iframe的load事件处理程序,如果你需要在响应返回时执行一些操作的话.\n  iframe.addEventListener('load', function () {\n    console.log('post success')\n  })\n\n  form.action = url\n  // 在指定的iframe中执行form\n  form.target = iframe.name\n  form.method = 'post'\n  for (let name in data) {\n    node.name = name\n    node.value = data[name].toString()\n    form.appendChild(node.cloneNode())\n  }\n  // 表单元素需要添加到主文档中.\n  form.style.display = 'none'\n  document.body.appendChild(form)\n  form.submit()\n```\n\n### **CORS** (推荐)\n\nCORS是一个W3C标准，全称是\"跨域资源共享\"（Cross-origin resource sharing）[跨域资源共享 CORS 详解](http://www.ruanyifeng.com/blog/2016/04/cors.html)。看名字就知道这是处理跨域问题的标准做法。CORS有两种请求，简单请求和非简单请求。\n\n\u003e 这里引用上面链接阮一峰老师的文章说明一下简单请求和非简单请求。  \n\u003e 浏览器将CORS请求分成两类：简单请求（simple request）和非简单请求（not-so-simple request）。\n\n只要同时满足以下两大条件，就属于简单请求。  \n（1) 请求方法是以下三种方法之一：\n\n- HEAD\n- GET\n- POST\n\n（2）HTTP的头信息不超出以下几种字段：\n\n- Accept\n- Accept-Language\n- Content-Language\n- Last-Event-ID\n- Content-Type：只限于三个值application/x-www-form-urlencoded、multipart/form-data、text/plain\n\n1.简单请求  \n后端\n\n```javascript\n// 处理成功失败返回格式的工具\nconst {successBody} = require('../utli')\nclass CrossDomain {\n  static async cors (ctx) {\n    const query = ctx.request.query\n    // *时cookie不会在http请求中带上\n    ctx.set('Access-Control-Allow-Origin', '*')\n    ctx.cookies.set('tokenId', '2')\n    ctx.body = successBody({msg: query.msg}, 'success')\n  }\n}\nmodule.exports = CrossDomain\n```\n\n前端什么也不用干，就是正常发请求就可以，如果需要带cookie的话，前后端都要设置一下，下面那个非简单请求例子会看到。\n\n```javascript\nfetch(`http://localhost:9871/api/cors?msg=helloCors`).then(res =\u003e {\n  console.log(res)\n})\n```\n\n2.非简单请求  \n非简单请求会发出一次预检测请求，返回码是204，预检测通过才会真正发出请求，这才返回200。这里通过前端发请求的时候增加一个额外的headers来触发非简单请求。  \n![clipboard.png](bVbdz9K.png)\n\n后端\n\n```javascript\n// 处理成功失败返回格式的工具\nconst {successBody} = require('../utli')\nclass CrossDomain {\n  static async cors (ctx) {\n    const query = ctx.request.query\n    // 如果需要http请求中带上cookie，需要前后端都设置credentials，且后端设置指定的origin\n    ctx.set('Access-Control-Allow-Origin', 'http://localhost:9099')\n    ctx.set('Access-Control-Allow-Credentials', true)\n    // 非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为\"预检\"请求（preflight）\n    // 这种情况下除了设置origin，还需要设置Access-Control-Request-Method以及Access-Control-Request-Headers\n    ctx.set('Access-Control-Request-Method', 'PUT,POST,GET,DELETE,OPTIONS')\n    ctx.set('Access-Control-Allow-Headers', 'Origin, X-Requested-With, Content-Type, Accept, t')\n    ctx.cookies.set('tokenId', '2')\n\n    ctx.body = successBody({msg: query.msg}, 'success')\n  }\n}\nmodule.exports = CrossDomain\n```\n\n一个接口就要写这么多代码，如果想所有接口都统一处理，有什么更优雅的方式呢？见下面的koa2-cors。\n\n```javascript\nconst path = require('path')\nconst Koa = require('koa')\nconst koaStatic = require('koa-static')\nconst bodyParser = require('koa-bodyparser')\nconst router = require('./router')\nconst cors = require('koa2-cors')\nconst app = new Koa()\nconst port = 9871\napp.use(bodyParser())\n// 处理静态资源 这里是前端build好之后的目录\napp.use(koaStatic(\n  path.resolve(__dirname, '../dist')\n))\n// 处理cors\napp.use(cors({\n  origin: function (ctx) {\n    return 'http://localhost:9099'\n  },\n  credentials: true,\n  allowMethods: ['GET', 'POST', 'DELETE'],\n  allowHeaders: ['t', 'Content-Type']\n}))\n// 路由\napp.use(router.routes()).use(router.allowedMethods())\n// 监听端口\napp.listen(9871)\nconsole.log(`[demo] start-quick is starting at port ${port}`)\n```\n\n前端\n\n```javascript\nfetch(`http://localhost:9871/api/cors?msg=helloCors`, {\n  // 需要带上cookie\n  credentials: 'include',\n  // 这里添加额外的headers来触发非简单请求\n  headers: {\n    't': 'extra headers'\n  }\n}).then(res =\u003e {\n  console.log(res)\n})\n```\n\n### **代理** (Nginx)\n\n想一下，如果我们请求的时候还是用前端的域名，然后有个东西帮我们把这个请求转发到真正的后端域名上，不就避免跨域了吗？这时候，Nginx出场了。  \nNginx配置\n\n```nginx\nserver{\n    # 监听9099端口\n    listen 9099;\n    # 域名是localhost\n    server_name localhost;\n    #凡是localhost:9099/api这个样子的，都转发到真正的服务端地址http://localhost:9871 \n    location ^~ /api {\n        proxy_pass http://localhost:9871;\n    }    \n}\n```\n\n前端就不用干什么事情了，除了写接口，也没后端什么事情了\n\n```javascript\n// 请求的时候直接用回前端这边的域名http://localhost:9099，这就不会跨域，然后Nginx监听到凡是localhost:9099/api这个样子的，都转发到真正的服务端地址http://localhost:9871 \nfetch('http://localhost:9099/api/iframePost', {\n  method: 'POST',\n  headers: {\n    'Accept': 'application/json',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    msg: 'helloIframePost'\n  })\n})\n```\n\nNginx转发的方式似乎很方便！但这种使用也是看场景的，如果后端接口是一个公共的API，比如一些公共服务获取天气什么的，前端调用的时候总不能让运维去配置一下Nginx，如果兼容性没问题（IE 10或者以上），CROS才是更通用的做法吧。\n\n## 同源策略限制下Dom查询的正确打开方式\n\n**1.postMessage**  \n`window.postMessage()` 是HTML5的一个接口，专注实现不同窗口不同页面的跨域通讯。  \n为了演示方便，我们将hosts改一下：127.0.0.1 crossDomain.com，现在访问域名crossDomain.com就等于访问127.0.0.1。\n\n这里是[http://localhost](http://localhost/):9099/#/crossDomain，发消息方\n\n```vue\n\u003ctemplate\u003e\n  \u003cdiv\u003e\n    \u003cbutton @click=\"postMessage\"\u003e给http://crossDomain.com:9099发消息\u003c/button\u003e\n    \u003ciframe name=\"crossDomainIframe\" src=\"http://crossdomain.com:9099\"\u003e\u003c/iframe\u003e\n  \u003c/div\u003e\n\u003c/template\u003e\n\n\u003cscript\u003e\nexport default {\n  mounted () {\n    window.addEventListener('message', (e) =\u003e {\n      // 这里一定要对来源做校验\n      if (e.origin === 'http://crossdomain.com:9099') {\n        // 来自http://crossdomain.com:9099的结果回复\n        console.log(e.data)\n      }\n    })\n  },\n  methods: {\n    // 向http://crossdomain.com:9099发消息\n    postMessage () {\n      const iframe = window.frames['crossDomainIframe']\n      iframe.postMessage('我是[http://localhost:9099], 麻烦你查一下你那边有没有id为app的Dom', 'http://crossdomain.com:9099')\n    }\n  }\n}\n\u003c/script\u003e\n```\n\n这里是[http://crossdomain.com](http://crossdomain.com/):9099，接收消息方\n\n```\n\u003ctemplate\u003e\n  \u003cdiv\u003e\n    我是http://crossdomain.com:9099\n  \u003c/div\u003e\n\u003c/template\u003e\n\n\u003cscript\u003e\nexport default {\n  mounted () {\n    window.addEventListener('message', (e) =\u003e {\n      // 这里一定要对来源做校验\n      if (e.origin === 'http://localhost:9099') {\n        // http://localhost:9099发来的信息\n        console.log(e.data)\n        // e.source可以是回信的对象，其实就是http://localhost:9099窗口对象(window)的引用\n        // e.origin可以作为targetOrigin\n        e.source.postMessage(`我是[http://crossdomain.com:9099]，我知道了兄弟，这就是你想知道的结果：${document.getElementById('app') ? '有id为app的Dom' : '没有id为app的Dom'}`, e.origin);\n      }\n    })\n  }\n}\n\u003c/script\u003e\n```\n\n结果可以看到：\n\n![clipboard.png](bVbdBwQ.png)\n\n**2.document.domain**  \n这种方式只适合主域名相同，但子域名不同的iframe跨域。  \n比如主域名是[http://crossdomain.com](http://crossdomain.com/):9099，子域名是[http://child.crossdomain.com](http://child.crossdomain.com/):9099，这种情况下给两个页面指定一下document.domain即document.domain = crossdomain.com就可以访问各自的window对象了。\n\n**3.canvas操作图片的跨域问题**  \n这个应该是一个比较冷门的跨域问题，张大神已经写过了我就不再班门弄斧了[解决canvas图片getImageData,toDataURL跨域问题](https://www.zhangxinxu.com/wordpress/2018/02/crossorigin-canvas-getimagedata-cors/)\n\n## HTTPS的页面发送不了HTTP请求\n\n有些人说是跨域问题，真的是这样吗？\n\n\u003e 同源策略：1. 协议相同 2. 域名相同 3.端口相同\n\n尽管HTTPS访问HTTP确实不符合同源策略中的协议相同，但是在现代浏览器里，即使是域名相同的请求，也是会出现以下报错，而不是跨域报错。\n\n![image-20210430001209252](image-20210430001209252.png)这也很好理解，毕竟混合内容的安全策略是在浏览器端判定的，而是否能跨域要看服务器返回的Response头，请求都被浏览器block掉了，也就不存在是否跨域的问题。\n\n那什么是混合内容？\n\n\u003e **混合内容：初始 HTML 内容通过安全的 HTTPS 连接加载，但其他资源（例如，图像、视频、样式表、脚本）则通过不安全的 HTTP 连接加载[1]。因为页面通过 HTTPS 加载的初始请求是安全的，但是又加载了不安全的HTTP内容，因此称之为混合内容。**\n\n因为HTTPS的S本身就是Secure的意思，现代浏览器最初会针对此类型的内容显示警告，以向用户表明此页面包含不安全的资源。但是即使显示警告，页面也已经加载，用户的安全仍然受到了威胁。所以没过多久，Chrome和Firefox就直接阻断掉了这类的请求。\n\n这就是HTTPS页面为什么发送不了HTTP的原因。\n\n**突破方式**\n\n尽管现在主流浏览器都已经block掉了HTTPS页面上的HTTP请求，但是我们还是可以通过被动混合内容来发送get请求。\n\n\u003e 被动混合内容:指的是不与页面其余部分进行交互的内容，包括图像、视频和音频内容，以及无法与页面其余部分进行交互的其他资源。\n\u003e\n\u003e **主动混合内容: 指的是能与页面交互的内容，包括浏览器可下载和执行的脚本、样式表、iframe、flash 资源及其他代码。**[1]\n\n因为攻击者可以通过不安全的HTTP内容来攻击安全的HTTPS页面，所以这类请求被严格阻断掉了————这也是为什么我们的Fetch请求被干掉了。所以我们**可以在迫不得已的情况下，用img.src的方式来发送请求。当然，请求方法只能是get。**\n\n```\nsendHttpRequest: () =\u003e {\n    const img = new Image();\n    img.src = 'http://xxx.com//你的请求'\n} \n```\n\n这时候，浏览器只会在控制台报warning，而不会block我们的请求。\n\n**出于HTTPS的安全策略，浏览器会阻断HTTPS上的非安全请求（HTTP）请求，但是我们可以使用被动混合内容的方式来跨越这个安全策略。**\n","lastmodified":"2023-05-09T16:33:58.299366595Z","tags":[]},"/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E6%9D%8E%E5%AE%8F%E6%AF%85":{"title":"Reinforcement Learning","content":"\n# Reinforcement Learning\n\n## 0 从监督学习到强化学习\n\n那什么是 Reinforcement Learning 呢,到目前为止，我们讲的几乎都是 Supervised Learning,假设你要做一个 Image 的 Classifier,你不只要告诉机器,它的 Input 是什么,你还要告诉机器,它应该输出什么样的 Output,然后接下来呢,你就可以 Train 一个 Image 的 Classifier\n\n![image-20210911230950586](image-20210911230950586.png)\n\n那在多数这门课讲到目前为止的技术,基本上都是基于 Supervised Learning 的方法,就算是我们在讲 Self Supervised Learning 的时候,我们其实也是,很类似 Supervised Learning 的方法,只是我们的 Label,不需要特别僱用人力去标记,它可以自动产生，或者是我们在讲 Auto-encoder 的时候,我们虽然说它是一个 Unsupervised 的方法,我们没有用到人类的标记,但事实上,我们还是有一个 Label,只是这个 Label,不需要耗费人类的力量来产生而已\n\n但是 RL 就是另外一个面向的问题了,在 RL 裡面,我们遇到的问题是这样子的,我们,**机器当我们给它一个输入的时候,我们不知道最佳的输出应该是什么**\n\n举例来说,假设你要叫机器学习下围棋\n\n![image-20210911231032976](image-20210911231032976.png)\n\n用 Supervised Learning 的方法,好像也可以做,你就是告诉机器说,看到现在的盘势长这个样子的时候,下一步应该落子的位置在哪裡,但是问题是,下一步应该落子的位置到底应该在哪裡呢,哪一个是最好的下一步呢,哪一步是神之一手呢,可能人类根本就不知道\n\n当然你可以说,让机器阅读很多职业棋士的棋谱,让机器阅读很多高段棋士的棋谱,也许这些棋谱裡面的答案,也许这些棋谱裡面给某一个盘势,人类下的下一步,就是一个很好的答案,但它是不是最好的答案呢,我们不知道,在这个**你不知道正确答案是什么的情况下,往往就是 RL 可以派上用场的时候**,所以当你今天,你发现你要收集有标注的资料很困难的时候,正确答案人类也不知道是什么的时候,也许就是你可以考虑使用 RL 的时候\n\n但是 **RL 在学习的时候,机器其实也不是一无所知的**,我们虽然不知道正确的答案是什么,但是机器会知道什么是好,什么是不好,机器会跟环境去做互动,得到一个叫做 **Reward** 的东西,这我们等一下都还会再细讲。\n\n所以机器会知道,它现在的输出是好的还是不好的,来藉由跟环境的互动,藉由知道什么样的输出是好的,什么样的输出是不好的,机器还是可以学出一个模型\n\n## 1 What is RL?(Three steps in ML)\n\n![image-20210911231230408](image-20210911231230408.png)\n\n首先呢,我们会从最基本的 RL 的概念开始,那在介绍这个 RL 概念的时候,有很多不同的切入点啦,也许你比较常听过的切入点是这样,比如说从 Markov Decision Process 开始讲起\n\n那我们这边选择了一个比较不一样的切入点,我要告诉你说,虽然如果你自己读 RL 的文献的话,你会觉得,哇 RL 很複杂哦,跟一般的 Machine Learning 好像不太一样哦,但是我这边要告诉你说,**RL 它跟我们这一门课学的 Machine Learning,是一样的框架**\n\n我们在今天这个这学期,一开始的第一堂课就告诉你说,Machine Learning 就是三个步骤,那 RL 呢,RL 也是一模一样的三个步骤,等一下会再跟大家说明\n\n### 1.1 Machine Learning ≈ Looking for a Function\n\n在今天,在这个本学期这一门课的第一开始,就告诉你说,什么是机器学习,**机器学习就是找一个 Function**,Reinforcement Learning,RL 也是机器学习的一种,那它也在找一个 Function,它在找什么样的 Function 呢\n\n那 Reinforcement Learning 裡面呢,我们会有一个 Actor,还有一个 Environment,那这个 **Actor 跟 Environment,会进行互动**\n\n![image-20210911232111496](image-20210911232111496.png)\n\n- 你的这个 Environment,你的这个环境啊,会给 Actor 一个 Observation,会给,那这个 Observation 呢,就是 Actor 的输入\n- 那 Actor 呢,看到这个 Observation 以后呢,它会有一个输出,这个输出呢,叫做 Action,那这个 Action 呢,会去影响 Environment\n- 这个 Actor 採取 Action 以后呢,Environment 就会给予新的 Observation,然后 Actor 呢,会给予新的 Action,那这个 Observation 是 Actor 的输入,那这个 Action 呢,是 Actor 的输出\n\n所以 **Actor 本身啊,它就是一个 Function**,其实 Actor,它就是我们要找的 Function,这个 Function 它的轮入,就是环境给它的 Observation,输出就是这个 Actor 要採取的 Action,而今天在这个互动的过程中呢,**这个 Environment,会不断地给这个 Actor 一些 Reward**,告诉它说,你现在採取的这个 Action,它是好的还是不好的\n\n而我们今天要找的这个 Actor,我们今天要找的这个 Function,可以拿 Observation 当作 Input,Actor 当作 Output 的 Function,这个 Function 的目标,是要去 Maximizing,我们可以从 Environment,获得到的 Reward 的总和,我们希望呢,找一个 Function,那用这个 Function 去跟环境做互动,**用 Observation 当作 Input,输出 Action,最终得到的 Reward 的总和,可以是最大的,这个就是 RL 要找的 Function**\n\n### 1.2 Example: Playing Video Game\n\n那我知道这样讲,你可能还是觉得有些抽象,所以我们举更具体的例子,那等一下举的例子呢,都是用 Space Invader 当作例子啦,那 Space Invader 就是一个非常简单的小游戏,那 RL 呢,最早的几篇论文,也都是玩,让那个机器呢,去玩这个 Space Invader 这个游戏\n\n在 Space Invader 裡面呢\n\n![image-20210912211107238](image-20210912211107238.png)\n\n- 你要**操控的是下面这个绿色的东西**,这个下面这个绿色的东西呢,是你的太空梭,你可以採取的行为,也就是 Action 呢 有三个,**左移 右移跟开火**,就这三个行为,然后你现在要做的事情啊,就是杀掉画面上的这些外星人\n- 画面上这些黄色的东西,也就是**外星人**啦,然后你开火,击中那些外星人的话,那外星人就死掉了\n- 那前面这些东西是什么呢,那个是你的**防护罩**,如果你不小心打到自己的防护罩的话,你的防护罩呢,也是会被打掉的,那你可以躲在防护罩后面,你就可以挡住外星人的攻击\n- 然后接下来呢 会有分数,那在萤幕画面上会有分数,当你杀死外星人的时候,你会得到**分数**,或者是在有些版本的 Space Invader 裡面,会有一个补给包,从上面横过去 飞过去,那你打到补给包的话,会被加一个很高的分数,那这个 Score 呢,就是 Reward,就是环境给我们的 Reward\n\n那这个游戏呢,它是会终止的,那什么时候终止呢,当所有的外星人都被杀光的时候就终止,或者是呢,外星人其实也会对你的母舰开火啦,外星人击中你的母舰 你也是会,这个你就被摧毁了,那这个游戏呢,也就终止了,好 那这个是介绍一下,Space Invader 这一个游戏,\n\n那如果你今天呢,要用 Actor 去玩 Space Invader,大概会像是什么样子呢\n\n现在你的 Actor 啊,**Actor 虽然是一个机器,但是它是坐在人的这一个位置,它是站在人这一个角度,去操控摇杆**,去控制那个母舰,去跟外星人对抗,而你的环境是什么,**你的环境呢,是游戏的主机**,游戏的主机这边去操控那些外星人,外星人去攻击你的母舰,所以 Observation 是游戏的画面\n\n![image-20210912211546018](image-20210912211546018.png)\n\n所以对 Actor 来说,它看到的,其实就跟人类在玩游戏的时候,看到的东西是一样的,就看到一个游戏的画面\n\n那输出呢,就是 Actor 可以採取的行为,那可以採取哪些行为,通常是事先定义好的,在这个游戏裡面,就只有向左 向右跟开火,三种可能的行为而已,好 那当你的 Actor 採取向右这个行为的时候,那它会得到 Reward\n\n那因为在这个游戏裡面,只有**杀掉外星人会得到分数,而我们就是把分数定义成我们的 Reward**,那向左 向右其实并不会,不可能杀掉任何的外星人,所以你得到的 Reward 呢,就是 0 分,好 那你採取一个 Action 以后呢,游戏的画面就变了\n\n- 游戏的画面变的时候,就代表了有了新的 Observation 进来\n- 有了新的 Observation 进来,你的 Actor 就会决定採取新的 Action\n\n![image-20210912211850357](image-20210912211850357.png)\n\n你的 Actor 是一个 Function,这个 Function 会根据输入的 Observation,输出对应的 Action,那新的画面进来,假设你的 Actor,现在它採取的行为是开火,而开火这个行为正好杀掉一隻外星人的时候,你就会得到分数,那这边假设得到的分数是 5 分,杀那个外星人,得到的分数是 5 分,那你就得到 Reward 等于 5\n\n那这个呢,就是拿 Actor 去玩,玩这个 Space Invader 这个游戏的状况,好 那这个 Actor 呢,它想要学习的是什么呢,我们在玩游戏的过程中,会不断地得到 Reward,那在刚才例子裡面,做第一个行为的时候,向右的时候得到的是 0 分,做第二个行为,开火的时候得到的是 5 分,那接下来你採取了一连串行为,都有可能给你分数\n\n而 Actor 要做的事情,我**们要学习的目标,我们要找的这个 Actor** 就是,我们想要 Learn 出一个 Actor,这个 Actor,这个 Function,我们使用它在这个游戏裡面的时候,**可以让我们得到的 Reward 的总和会是最大的**,那这个就是拿 Actor 去,这个就是 RL 用在玩这个小游戏裡面的时候,做的事情\n\n### 1.3 Example: Learning to play Go\n\n那其实如果把 RL 拿来玩围棋,拿来下围棋,其实做的事情跟小游戏,其实也没有那麽大的差别,只是规模跟问题的複杂度不太一样而已\n\n那如果今天你要让机器来下围棋,那你的 Actor 就是就是 AlphaGo,那你的环境是什么,你的环境就是 AlphaGo 的人类对手\n\n你的 Actor 的输入就是棋盘,棋盘上黑子跟白子的位置,那如果是在游戏的一开始,棋盘上就空空的,空空如也,上面什么都没有,没有任何黑子跟白子\n\n那这个 Actor 呢,看到这个棋盘呢,它就要产生输出,它就要决定它下一步,应该落子在哪裡,那如果是围棋的话,你的输出的可能性就是有 19×19 个可能性,那这 19×19 个可能性,每一个可能性,就对应到棋盘上的一个位置\n\n那假设现在你的 Actor,决定要落子在这个地方\n\n![image-20210912212426858](image-20210912212426858.png)\n\n那这一个结果,就会输入给你的环境,那其实就是一个棋士,然后呢 这个环境呢,就会再产生新的 Observation,因为这个李世石这个棋士呢,也会再落一子,那现在看到的环境又不一样了,那你的 Actor 看到这个新的 Observation,它就会产生新的 Action,然后就这样反覆继续下去\n\n![image-20210912212456020](image-20210912212456020.png)\n\n你就可以让机器做下围棋这件事情,好 那在这个,在这个下围棋这件事情裡面的 Reward,是怎麽计算的呢\n\n**在下围棋裡面,你所採取的行为,几乎都没有办法得到任何 Reward**,在下围棋这个游戏裡,在下围棋这件事情裡面呢,你会定义说,如果赢了,就得到 1 分,如果输了就得到 -1 分\n\n![image-20210912212536103](image-20210912212536103.png)\n\n也就是说在下围棋这整个,这个你的 Actor 跟环境互动的过程中,其实**只有游戏结束,只有整场围棋结束的最后一子,你才能够拿到 Reward**,就你最后,最后 Actor 下一子下去,赢了,就得到 1 分,那最后它落了那一子以后,游戏结束了,它输了,那就得到 -1 分,那在中间整个互动的过程中的 Reward,就都算是 0 分,没有任何的 Reward,那这个 Actor 学习的目标啊,就是要去最大化,它可能可以得到的 Reward。\n\n### 1.4 三步走\n\n刚才讲的也许你都已经听过了,那这个是 RL 最常见的一种解说方式,那接下来要告诉你说,RL 跟机器学习的 Framework,它们之间的关係是什么\n\n![image-20210912212810370](image-20210912212810370.png)\n\n开学第一堂课就告诉你说,Machine Learning 就是三个步骤\n\n1. 第一个步骤,你有一个 Function,那个 Function 裡面有一些未知数,Unknown 的 Variable,这些未知数是要被找出来的\n2. 第二步,订一个 Loss Function,第三步,想办法找出未知数去最小化你的 Loss\n3. 第三步就是 Optimization\n\n而 RL 其实也是一模一样的三个步骤\n\n#### Step 1: Function with Unknown\n\n第一个步骤,我们现在有未知数的这个 Function,到底是什么呢,这个有未知数的 Function,就是我们的 Actor,那在 RL 裡面,**你的 Actor 呢,就是一个 Network**,那我们**现在通常叫它 Policy 的 Network。**\n\n![image-20210912213733742](image-20210912213733742.png)\n\n那在过去啊,**在还没有把 Deep Learning 用到 RL 的时候,通常你的 Actor 是比较简单的,它不是 Network,它可能只是一个 Look-Up-Table**,告诉你说看到什么样的输入,就产生什么样的输出,那今天我们都知道要用 Network,来当做这个 Actor,那这个 Network,其实就是一个很複杂的 Function\n\n![image-20210912213816733](image-20210912213816733.png)\n\n这个複杂的 Function 它的输入是什么呢,它的**输入就是游戏的画面**,就是游戏的画面,这个游戏画面上的 Pixel,像素,就是这一个 Actor 的输入\n\n那它的输出是什么呢,它的**输出就是,每一个可以採取的行为**,它的分数,每一个可以採取的 Action 它的分数,举例来说 输入这样的画面,给你的 Actor,你的 Actor 其实就是一个 Network,它的输出可能就是给,向左 0.7 分,向右 0.2 分,开火 0.1 分\n\n那事实上啊,这件事情**跟分类是没有什么两样**的,你知道分类就是输入一张图片,输出就是决定这张图片是哪一个类别,那你的 Network 会给每一个类别,一个分数,你可能会通过一个 Softmax Layer,然后每一个类别都有个分数,而且这些分数的总和是 1\n\n那其实在 RL 裡面,你的 Actor 你的 Policy Network,跟分类的那个 Network,其实是一模一样的,你就是输入一张图片,输出其实最后你也会有个 Softmax Layer,然后呢,你就会 Left、Right 跟 Fire,三个 Action 各给一个分数,那这些分数的总和,你也会让它是 1\n\n![image-20210912213951581](image-20210912213951581.png)\n\n那至于这个 **Network 的架构呢,那你就可以自己设计了**,要设计怎麽样都行,比如说如果输入是一张图片,欸 也许你就会想要用 CNN 来处理\n\n不过在助教的程式裡面,其实不是用 CNN 来处理啦,因为在我们的作业裡面,其实在玩游戏的时候,不是直接让我们的 Machine 去看游戏的画面,让它直接去看游戏的,让它直接去看游戏的画面比较难做啦,所以我们是让,看这个跟现在游戏的状况有关的一些参数而已,所以在这个助教的,在这个作业的这个 Sample Code 裡面呢,还没有用到 CNN 那麽複杂,就是一个简单的 Fully Connected Network,但是假设你要让你的 Actor,它的输入真的是游戏画面,欸 那你可能就会採取这个 CNN,你可能就用 CNN 当作你的 Network 的架构\n\n**甚至你可能说**,我不要只看现在这一个时间点的游戏画面,我要看整场游戏到目前为止发生的所有事情,可不可以呢？可以,那过去你可能会用 RNN 考虑,现在的画面跟过去所有的画面,那现在你可能会想要**用 Transformer,考虑所有发生过的事情**,所以 Network 的架构是你可以自己设计的,只要能够输入游戏的画面,输出类似像类别这样的 Action 就可以了,那最后机器会决定採取哪一个 Action,取决于每一个 Action 取得的分数\n\n常见的做法啊,是直接把这个分数,就当做一个机率,然后按照这个机率,去 Sample,去随机决定要採取哪一个 Action\n\n![image-20210912213951581](image-20210912213951581.png)\n\n举例来说 在这个例子裡面,向左得到 0.7 分,那就是有 70% 的机率会採取向左,20% 的机率会採取向右,10% 的机率会採取开火\n\n那你可能会问说,为什么不是用argmax呢,为什么不是看 Left 的分数最高,就直接向左呢,你也可以这麽做,但是在助教的程式裡面,还有多数 RL 应用的时候 你会发现,我们都是採取 Sample,\n\n採取 **Sample 有一个好处是说,今天就算是看到同样的游戏画面,你的机器每一次採取的行为,也会略有不同**,那在很多的游戏裡面这种随机性,也许是重要的,比如说你在做剪刀石头布的时候,如果你总是会出石头,就跟小叮噹一样,那你就很容易被打爆,如果你有一些随机性,就比较不容易被打爆\n\n那其实之所以今天的输出,是用随机 Sample 的,还有另外一个很重要的理由,那这个我们等一下会再讲到,好 所以这是第一步,我们有一个 Function,这个 Function 有 Unknown 的 Variable,我们有一个 Network,那裡面有参数,这个参数就是 Unknown 的 Variable,就是要被学出来的东西,这是第一步\n\n#### Step 2: Define “Loss”\n\n然后接下来第二步,我们要定义 Loss,在 RL 裡面,我们的 Loss 长得是什么样子呢,我们再重新来看一下,我们的机器跟环境互动的过程,那只是现在用不一样的方法,来表示刚才说过的事情\n\n![image-20210912214755516](image-20210912214755516.png)\n\n首先有一个初始的游戏画面,这个初始的游戏画面,被作为你的 Actor 的输入\n\n你的 Actor 那就输出了一个 Action,比如说向右,输入的游戏画面呢,我们叫它 s1,然后输出的 Action 呢,就叫它 a1\n\n那现在会得到一个 Reward,这边因为向右没有做任何事情,没有杀死任何的外星人,所以得到的 Reward 可能就是 0 分\n\n![image-20210912214950357](image-20210912214950357.png)\n\n採取向右以后,会看到新的游戏画面,这个叫做 s2,根据新的游戏画面 s2,你的 Actor 会採取新的行为,比如说开火,这边用 a2,来表示看到游戏画面 s2 的时候,所採取的行为\n\n那假设开火恰好杀死一隻外星人,和你的 Actor 就得到 Reward,这个 Reward 的分数呢,是 5 分,然后採取开火这个行为以后\n\n![image-20210912215036248](image-20210912215036248.png)\n\n接下来你会看到新的游戏画面,那机器又会採取新的行为,那这个互动的过程呢,就会反覆持续下去,直到机器在採取某一个行为以后,游戏结束了,那什么时候游戏结束呢,就看你游戏结束的条件是什么嘛\n\n举例来说,採取最后一个行为以后,比如说向右移,正好被外星人的子弹打中,那你的飞船就毁了,那游戏就结束了,或者是最后一个行为是开火,把最后一隻外星人杀掉,那游戏也就结束了,就你执行某一个行为,满足游戏结束的条件以后,游戏就结束了\n\n![image-20210912215129396](image-20210912215129396.png)\n\n那**从游戏开始到结束的这整个过程啊,被称之为一个 Episode**,那在整个游戏的过程中,机器会採取非常多的行为,每一个行为都可能得到 Reward,把所有的 Reward 通通集合起来,我们就得到一个东西,叫做整场游戏的 Total Reward,\n\n![image-20210912215203605](image-20210912215203605.png)\n\n那这个 Total Reward 呢,就是从游戏一开始得到的 r1,一直得,一直加,累加到游戏最后结束的时候,得到的 rt,假设这个游戏裡面会互动,T 次,那麽就得到一个 Total Reward,我们这边用 R,来表示 Total Reward,其实这个 Total Reward 又有另外一个名字啊,叫做 Return 啦,你在这个 RL 的文献上,常常会同时看到 Reward 跟 Return,这两个词会出现,那 Reward 跟 Return 其实有点不一样,Reward 指的是你採取某一个行为的时候,立即得到的好处,这个是 Reward,把整场游戏裡面所有的 Reward 通通加起来,这个叫做 Return\n\n但是我知道说,很快你就会忘记 Reward 跟 Return 的差别了,所以我们等一下就不要再用 Return 这个词彙,我们直接告诉你说,整场游戏的 Reward 的总和,就是 Total 的 Reward,而这个 Total 的 Reward 啊,就是我们想要去最大化的东西,就是我们训练的目标\n\n那你可能会说,欸 这个跟 Loss 不一样啊,Loss 是要越小越好啊,这个 Total Reward 是要越大越好啊,所以有点不一样吧,但是我们可以说在 RL 的这个情境下,我们把那个 Total Reward 的负号,**负的 Total Reward,就当做我们的 Loss,Total Reward 是要越大越好,那负的 Total Reward,当然就是要它越小越好**吧,就我们完全可以说负的 Total Reward,就是我们的 Loss,就是 RL 裡面的 Loss\n\n#### Step 3: Optimization\n\n那我们再把这个环境跟,Agent 互动的这一件事情啊,再用不一样的图示,再显示一次\n\n![image-20210912220011837](image-20210912220011837.png)\n\n这个是你的环境,你的环境呢,输出一个 Observation,叫做 s1\n\n- 这个 s1 呢,会变成你的 Actor 的输入\n- 你的 Actor 呢,接下来就是输出 a1\n- 然后这个 a1 呢,又变成环境的输入\n- 你的环境呢,看到 a1 以后,又输出 s2\n\n然后这个互动的过程啊,就会继续下去,s2 又输入给 Actor,它就输出 a2,a2 又输入给 Environment,它就输出给,它就产生 s3\n\n它这个互动呢,一直下去,直到满足游戏中止的条件,好 那这个 s 跟 a 所形成的这个 Sequence,就是 s1 a1 s2 a2 s3 a3 **这个 Sequence,又叫做 Trajectory**,那我们用 τ来表示 Trajectory\n\n![image-20210912220319849](image-20210912220319849.png)\n\n那根据这个互动的过程,Machine 会得到 Reward,你其实可以把 Reward 也想成是一个 Function,我们这边用一个绿色的方块来代表,这个 Reward 所构成的 Function\n\n那这个 Reward 这个 Function,有不同的表示方法啦,在有的游戏裡面,也许你的 Reward,只需要看你採取哪一个 Action 就可以决定,不过通常我们在决定 Reward 的时候,光看 Action 是不够的,你还要看现在的 Observation 才可以,因为并不是每一次开火你都一定会得到分数,开火要正好有击到外星人,外星人正好在你前面,你开火才有分数\n\n所以通常 **Reward Function 在定义的时候,不是只看 Action,它还需要看 Observation**,同时看 Action 跟 Observation,才能够知道现在有没有得到分数,所以 Reward 是一个 Function,\n\n这个 Reward 的 Function,它拿 a1 跟 s1 当作输入,然后它产生 r1 作为输出,它拿 a2 跟 s2 当作输入,产生 r2 作为输出,把所有的 r 通通结合起来,把 r1 加 r2 加 r3 一直加到 T,全部结合起来就得到 R,这个就是 Total Reward,也就是 Return,这个是我们要最大化要去 Maximize 的对象\n\n 这个 Optimization 的问题是这个样子,你要去找一个 Network,其实是 Network 裡面的参数,你要去 Learn 出一组参数,这一组参数放在 Actor 的裡面,它可以让这个 R 的数值越大越好,就这样,结束了,整个 Optimization 的过程就是这样,你要去找一个 Network 的参数,让这边产生出来的 R 越大越好\n\n![image-20210912220319849](image-20210912220319849.png)\n\n那乍看之下,如果这边的,这个 Environment Actor 跟 Reward,它们**都是 Network 的话,这个问题其实也没有什么难的**,这个搞不好你现在都可以解,它看起来就有点像是一个 Recurrent Network,这是一个 Recurrent Network,然后你的 Loss 就是这个样子,那只是这边是 Reward 不是 Loss,所以你是要让它越大越好,你就去 Learn 这个参数,用 Gradient Descent 你就可以让它越大越好\n\n但是 RL 困难的地方是,**这不是一个一般的 Optimization 的问题**,因为你的 Environment,这边有很多问题导致说,它跟一般的 Network Training 不太一样\n\n第一个问题是,**你的 Actor 的输出是有随机性的**\n\n![image-20210912221135459](image-20210912221135459.png)\n\n这个 **a1 它是用 Sample 产生的**,你定同样的 s1 每次产生的 a1 不一定会一样,所以假设你把 Environment Actor 跟 Reward,合起来当做是一个巨大的 Network 来看待,这个 Network 可不是一般的 Network,**这个 Network 裡面是有随机性的**,这个 Network 裡面的某一个 Layer 是,每次产生出来结果是不一样的,**这个 Network 裡面某一个 Layer 是,它的输出每次都是不一样的**\n\n另外还有一个更大的问题就是,**你的 Environment 跟 Reward,它根本就不是 Network 啊,它只是一个黑盒子而已**,你根本不知道裡面发生了什么事情,Environment 就是游戏机,那这个游戏机它裡面发生什么事情你不知道,你只知道说你输入一个东西会输出一个东西,你採取一个行为它会有对应的回应,但是到底是怎麽产生这个对应的回应,我们不知道,它只是一个黑盒子,\n\n**Reward 可能比较明确,但它也不是一个 Network,它就是一条规则**嘛,它就是一个规则说,看到这样子的 Optimization 跟这样的 Action,会得到多少的分数,它就只是一个规则而已,所以它也不是 Network\n\n而且更麻烦的地方是,**往往 Reward 跟 Environment,它也是有随机性的**,如果是在电玩裡面,通常 Reward 可能比较不会有随机性,因为规则是定好的,对有一些 RL 的问题裡面,Reward 是有可能有随机性的\n\n但是在 Environment 裡面,就算是在电玩的这个应用中,它也是有随机性的,你给定同样的行为,到底游戏机会怎麽样回应,它裡面可能也是有乱数的,它可能每次的回应也都是不一样,如果是下围棋,你落同一个子,你落在,你落子在同一个位置,你的对手会怎麽样回应,每次可能也是不一样\n\n![image-20210912230423421](image-20210912230423421.png)\n\n所以环境很有可能也是有随机性的,所以这不是一个一般的 Optimization 的问题,你可能不能够用我们这门课已经学过的,训练 Network 的方法来找出这个 Actor,来最大化 Reward\n\n**所以 RL 真正的难点就是,我们怎麽解这一个 Optimization 的问题**,怎麽找到一组 Network 参数,可以让 R 越大越好\n\n其实你再仔细想一想啊,这整个问题**跟 GAN 其实有异曲同工之妙**,它们有一样的地方,也有不一样的地方\n\n- 先说它们**一样**的地方在哪裡,你记不记得在训练 GAN 的时候,在训练 Generator 的时候,你会把 Generator 跟 Discriminator 接在一起,然后你希望去调整 Generator 的参数,让 Discriminator 的输出越大越好,今天在 RL 裡面,我们也可以说这个 Actor 就像是 Generator,Environment 跟 Reward 就像是 Discriminator,我们要去**调整 Generator 的参数,让 Discriminator 的输出越大越好**,所以它跟 GAN 有异曲同工之妙\n- 但什么地方**不一样**呢,在 GAN 裡面你的 Discriminator,也是一个 Neural Network,你了解 Discriminator 裡面的每一件事情,它也是一个 Network,你可以用 Gradient Descent,来 train 你的 Generator,让 Discriminator 得到最大的输出,但是在 RL 的问题裡面,你的 **Reward 跟 Environment,你可以把它们当 Discriminator 来看,但它们不是 Network,它们是一个黑盒子,所以你没有办法用,一般 Gradient Descent 的方法来调整你的参数,来得到最大的输出**,所以这是 RL,跟一般 Machine Learning不一样的地方\n\n但是我们还是可以把 RL 就看成三个阶段,只是在 Optimization 的时候,在你怎麽 Minimize Loss,也就怎麽 Maximize Reward 的时候,跟之前我们学到的方法是不太一样的\n\n## 2 Policy Gradient\n\n接下来啊,我们就要讲一个拿来解 RL,拿来做 Optimization 那一段常用的一个演算法,叫做 Policy Gradient\n\n![image-20210912231113114](image-20210912231113114.png)\n\n那如果你真的想知道,Policy Gradient 是哪裡来的,你可以参见过去上课的[录影](https://youtu.be/W8XF3ME8G2I),对 Policy Gradient 有比较详细的推导,那今天我们是从另外一个角度,来讲 Policy Gradient 这件事情\n\n### 2.1 How to control your actor\n\n那在讲 Policy Gradient 之前,我们先来想想看,我们要怎麽操控一个 Actor 的输出,我们要怎麽让一个 Actor,在看到某一个特定的 Observation 的时候,採取某一个特定的行为呢,我们怎麽让一个 Actor,它的输入是 s 的时候,它就要输出 Action a^ 呢\n\n![image-20210912231731634](image-20210912231731634.png)\n\n那你其实完全可以把它**想成一个分类的问题**,也就是说假设你要让 Actor 输入 s,输出就是 a^,假设 a^ 就是向左好了,假设你要让,假设你已经知道,假设你就是要教你的 Actor 说,看到这个游戏画面向左就是对的,你就是给我向左,那你要怎麽让你的 Actor 学到这件事呢\n\n![image-20210912231832863](image-20210912231832863.png)\n\n那也就说 s 是 Actor 的输入,a^ 就是我们的 Label,就是我们的 Ground Truth,就是我们的正确答案,而接下来呢,你就可以计算你的 Actor,它的输出跟 Ground Truth 之间的 Cross-entropy,那接下来你就可以定义一个 Loss\n\n假设你希望你的 Actor,它**採取 a^ 这个行为的话**,你就定一个 Loss,这个 Loss 等于 Cross-entropy\n\n![image-20210912232003035](image-20210912232003035.png)\n\n然后呢,你再去 Learn 一个 θ,你再去 Learn 一个 θ,然后这个 θ 可以让 Loss 最小,那你就可以让这个 Actor 的输出,跟你的 Ground Truth 越接近越好\n\n你就可以让你的 Actor 学到说,看到这个游戏画面的时候,它就是要向左,这个是要让你的 Actor,採取某一个行为的时候的做法\n\n但是假设你想要让你的 Actor,**不要採取某一个行为**的话,那要怎麽做呢,假设你希望做到的事情是,你的 Actor 看到某一个 Observation s 的时候,我就千万不要向左的话怎麽做呢,其实很容易,你**只需要把 Loss 的定义反过来就好**\n\n![image-20210912232223834](image-20210912232223834.png)\n\n你希望你的 Actor 採取 a^ 这个行为,你就定义你的大 L 等于 Cross-entropy,然后你要 Minimize Cross-entropy,假设你要让你的 Actor,不要採取 a^ 这个行为的话,那你就把你就定一个 Loss,叫做负的 Cross-entropy,Cross-entropy 乘一个负号,那你去 Minimize 这个 L,你去 Minimize 这个 L,就是让 Cross-entropy 越大越好,那也就是让 a 跟 a^ 的距离越远越好,那你就可以避免你的 Actor 在看到 s 的时候,去採取 a^ 这个行为,所以我们有办法控制我们的 Actor,做我们想要做的事,只要我们给它适当的 Label 跟适当的 Loss,\n\n所以假设我们要让我们的 Actor,**看到 s 的时候採取 a^,看到 s' 的时候不要採取 a^'** 的话,要怎麽做呢\n\n这个时候你就会说,Given s 这个 Observation,我们的 Ground Truth 叫做 a^,Given s' 这个 Observation 的时候,我们有个 Ground Truth 叫做 a^',那对这两个 Ground Truth, 我们都可以去计算 Cross-entropy,e1 跟 e2\n\n![image-20210912232542292](image-20210912232542292.png)\n\n然后接下来呢,我们就定义说我们的 Loss,就是 e1 减 e2,也就是说我们要让这个 Case,它的 Cross-entropy 越小越好,这个 Case 它的 Cross-entropy 越大越好\n\n然后呢,我们去找一个 θ 去 Minimize Loss,得到 θ⋆,那就是一个可以在 s,可以在看到 s 的时候採取 a^,看到 s' 的时候採取 a^' 的 Actor,所以藉由很像是在,Train 一个 Classifier 的这种行为,藉由很像是现在 Train 一个 Classifier,的这种 Data,我们可以去控制一个 Actor 的行为,\n\n有一个同学问了一个非常好的问题\n\n- **Q:**就是如果以 Alien 的游戏来说的话,因为只有射中 Alien 才会有 Reward,这样 Model 不是就会一直倾向于射击吗\n\n  **A:**对 这个问题我们等一下会来解决它,之后的投影片就会来解决它\n\n然后又有另外一个同学,问了一个非常好的问题就是\n\n- **Q:**哇 这样不就回到 Supervised Learning 了嘛,这个投影片上看起来,就是在训练一个 Classifier 而已啊,我们就是在训练 Classifier,你只是告诉它说,看到 s 的时候就要输出 a^,看到 s' 的时候就不要输出 a^,a^',这不就是 Supervised Learning 吗\n- **A:**这就是 Supervised Learning,这个就是跟 Supervised Learning Train 的,Image Classifier 是一模一样的,但等下我们会看到它跟,一般的 Supervised Learning 不一样在哪裡,\n\n那所以呢,如果我们要训练一个 Actor,我们其实就需要收集一些训练资料,就收集训练资料说,我希望在 s1 的时候採取 a^1,我希望在 s2 的时候不要採取 a^2\n\n![image-20210912233019856](image-20210912233019856.png)\n\n但可能会问说,欸 这个训练资料哪来的,这个我们等一下再讲训练资料哪来的\n\n所以你就**收集一大堆的资料,这个跟 Train 一个 Image 的 Classifier 很像的**,这个 s 你就想成是 Image,这个 a^ 你就想成是 Label,只是现在有的行为是想要被採取的,有的行为是不想要被採取的,你就收集一堆这种资料,你就可以去定义一个 Loss Function,有了这个 Loss Function 以后,你就可以去训练你的 Actor,去 **Minimize 这个 Loss Function,就结束了**,你就可以训练一个 Actor,期待它执行我们的行为,期待它执行的行为是我们想要的\n\n而你甚至还可以更进一步,你可以说**每一个行为并不是只有好或不好**,并不是有想要执行跟不想要执行而已,**它是有程度的差别的**,有执行的非常好的,有 Nice to have 的,有有点不好的,有非常差的\n\n所以刚才啊,我们是说每一个行为就是要执行 不要执行,这是一个 Binary 的问题,这是我们就用 ±1 来表示\n\n![image-20210912233324323](image-20210912233324323.png)\n\n但是现在啊,我们改成**每一个 s 跟 a 的 Pair,它有对应的一个分数**,这个分数代表说,我们多希望机器在看到 s1 的时候,执行 a^1 这个行为\n\n那比如说这边第一笔资料跟第三笔资料,我们分别是定 +1.5 跟 +0.5,就代表说我们期待机器看到 s1 的时候,它可以做 a^1,看到 s3 的时候它可以做 a^3,但是我们期待它看到 s1 的时候,做 a^1 的这个期待更强烈一点,比看到 s3 做 a^3 的期待更强烈一点\n\n那我们希望它在看到 s2 的时候,不要做 a^2,我们期待它看到 sN 的时候,不要做 a^N,而且我们非常不希望,它在看到 sN 的时候做 a^N\n\n有了这些资讯,你一样可以定义一个 Loss Function,你只是在你的原来的 Cross-entropy 前面,本来是 Cross-entropy 前面,要嘛是 +1 要嘛是 -1\n\n现在改成乘上 An 这一项,改成乘上 An 这一项,告诉它说有一些行为,我们非常期待 Actor 去执行,有一些行为我们非常不期待 Actor 去执行,有一些行为如果执行是比较好的,有一些行为希望儘量不要执行比较好,但就算执行了也许伤害也没有那麽大\n\n所以我们**透过这个 An 来控制说,每一个行为我们多希望 Actor 去执行**,然后接下来有这个 Loss 以后,一样 Train 一个 θ,Train 下去你就找一个 θ⋆,你就有个 Actor 它的行为是符合我们期待的\n\n![image-20210912233513464](image-20210912233513464.png)\n\n那接下来的难点就是,要怎麽定出这一个 a 呢,这个就是我们接下来的难点,就是我们接下来要面对的问题,我们还有另外一个要面对的问题是,怎麽产生这个 s 跟 a 的 Pair 呢,怎麽知道在 s1 的时候要执行 a1,或在 s2 的时候不要执行 a2 呢,那这个也是等一下我们要处理的问题,\n\n讲到目前为止,你可能觉得跟 Supervised LearNing,没有什么不同,那确实就是没有什么不同,接下来真正的重点是,在我们怎么定义 a 上面：\n\n### 2.2 Version 0\n\n那先讲一个最简单的,但是其实是不正确的版本,那这个其实也是,助教的 Sample Code 的版本,那这个不正确的版本是怎么做的呢\n\n首先我们还是需要收集一些训练资料,就是需要**收集 s 跟 a 的 Pair**\n\n怎么收集这个 s 跟 a 的 Pair 呢?\n\n你需要先有一个 Actor,这个 Actor 去跟环境做互动,它就可以收集到 s 跟 a 的 Pair\n\n那这个 Actor 是哪裡来的呢,你可能觉得很奇怪,我们今天的目标,不就是要训练一个 Actor 吗,那你又说你需要拿一个 Actor,去跟环境做互动,然后把这个 Actor,它的 s 跟 a 记录下来,那这个 Actor 是哪裡来的呢?\n\n你先把这个 Actor,想成就是一个**随机的 Actor** 好了,就它是一个,它就是一个随机的东西,那看到 s1,然后它执行的行为就是乱七八糟的,就是随机的,但是我们会把它在,每一个 s 执行的行为 a,通通都记录下来,好 那通常我们在这个收集资料的话,你不会只把 Actor 跟环境做一个 Episode,通常会做多个 Episode,然后期待你可以收集到足够的资料,比如说在助教 Sample Code裡面,可能就是跑了 5 个 Episode,然后才收集到足够的资料\n\n![image-20211025092210834](image-20211025092210834.png)\n\n所以我们就是去观察,某一个 Actor 它跟环境互动的状况,那把这个 Actor,它在每一个 Observation,执行的 Action 都记录下来,然后接下来,我们就去**评价每一个 Action,它到底是好还是不好**,评价完以后,我们就可以拿我们评价的结果,来训练我们的 Actor\n\n那怎么评价呢,我们刚才有说,我们会用 A 这一个东西,来评价在每一个 Step,我们希不希望我们的 Actor,採取某一个行为,那最简单的评价方式是,假设在某一个 Step s1,我们执行了 a1,然后得到 Reward r1\n\n- 那 Reward 如果如果是正的,那也许就代表这个 Action 是好的\n- 那如果 Reward 是负的,那也许就代表这个 Action 是不好的\n\n那我们就把这个 Reward r1 r2,当做是 a,A1 就是 r1,A2 就是 r2,A3 就是 r3,AN 就是 rN,那这样等同于你就告诉 machine 说,如果我们执行完某一个 Action,a1 那得到的 **Reward 是正的,那这就是一个好的 Action**,你以后看到 s1 就要执行 a1,如果今天在 s2 执行 a2,得到 Reward 是负的,就代表 a2 是不好的 a2,就代表所以以后看到 s2 的时候,就不要执行 a2\n\n![image-20211025092531818](image-20211025092531818.png)\n\n那这个,那这个 Version 0,它并不是一个好的版本,为什么它不是一个好的版本呢,因为你用这一个方法,你把 a1 设为 r1,A2 设为 r2,这个方法认出来的 Network,它是一个**短视近利的 Actor**,它就是一个只知道会一时爽的 Actor,它**完全没有长程规划的概念**\n\n![image-20211025092851736](image-20211025092851736.png)\n\n- 我们知道说每一个行为,其实都会影响互动接下来的发展,也就是说 Actor 在 s1 执行 a1 得到 r1,这个并不是互动的全部,因为 a1 影响了我们接下来会看到 s2,s2 会影响到接下来会执行 a2,也影响到接下来会产生 r2,所以 a1 也会影响到,我们会不会得到 r2,所以**每一个行为并不是独立的,每一个行为都会影响到接下来发生的事情**,\n- 而且我们今天在跟环境做互动的时候,有一个问题叫做,Reward Delay,就是有时候你需要**牺牲短期的利益,以换取更长程的目标**,如果在下围棋的时候,如果你有看天龙八部的时候你就知道说,这个虚竹在破解珍珑棋局的时候,堵死自己一块子,让自己被杀了很多子以后,最后反而赢了整局棋\n\n  那如果是在这个,Space Invaders 的游戏裡面,你可能需要先左右移动一下进行瞄准,然后射击才会得到分数,而左右移动这件事情是,没有任何 Reward 的,左右移动这件事情得到的 Reward 是零,只有射击才会得到 Reward,但是并不代表左右移动是不重要的,我们会先需要左右移动进行瞄准,那我们的射击才会有效果,所以有时候我们会,需要牺牲一些近期的 Reward,而换取更长程的 Reward\n\n- 所以今天假设我们用 Version 0,会发生说今天 Machine,只要是採取向左跟向右,它得到的 Reward 会是 0,如果它採取开火,它得到的 Reward 就会,只有开火的时候,它得到的 Reward 才会是正的,才会是正的,那这样 Machine 就会学到,它只有疯狂狂开火才是对的,因为只有开火这件事才会得到 Reward,其它行为都不会得到 Reward,所以其它行为都是不被鼓励的,只有开火这件事是被鼓励的,那个 Version 0 就只会学到疯狂开火而已,那 Version 0 是助教的范例程式,那这个当然也是可以执行的,那只是它的结果不会太好而已,那助教范例十,程式之所以是 Version 0 是因为,我不知道为什么这个 Version 0,好像是大家,如果你自己在 Implement rl 的时候,你特别容易犯的错误,你特别容易拿自己 Implement 的时候,就直接使用 Version 0,但是得到一个很差的结果\n\n所以接下来怎么办呢,我们开始正式进入 rl 的领域,真正来看 Policy Gradient 是怎么做的,所以我们需要有 Version 1\n\n### 2.3 Version 1\n\n在 Version 1 裡面,a1 它有多好,不是在取决于 r1,而是取决于 **a1 之后所有发生的事情**,我们会把 a1,执行完 a1 以后,所有得到的 Reward,r1 r2 r3 到 rN,通通集合起来,通通加起来,得到一个数值叫做 G1,然后我们会说 a1 就等于 G1,我们拿这个 G1,来当作评估一个 Action 好不好的标准\n\n![image-20211025095137691](image-20211025095137691.png)\n\n刚才是直接拿 r1 来评估,现在不是,拿 G1 来评估,那接下来所有发生的 r 通通加起来,拿来评估 a1 的好坏,因为我们执行完 a1 以后,就发生这么一连串的事情,那这么一连串的事情加起来,也许就可以评估 a1,到底是不是一个好的 Action\n\n所以以此类推,a2 它有多好呢,就把执行完 a2 以后,所有的 r,r2 到 rN,通通加起来得到 G2,然后那 a3 它有多好呢,就把执行完 a3 以后,所有的 r 通通加起来,就得到 G3,所以把这些东西通通都加起来,就把那 这些这个 G,叫做 Cumulated Reward,叫做累积的 Reward,把未来所有的 Reward 加起来,来评估一个 Action 的好坏,那像这样子的方法听起来就合理多了\n\nGt 是什么呢,就是从 t 这个时间点开始,我们把 rt 一直加到 rN,全部合起来就是,Cumulated 的 Reward Gt,那当我们用,Cumulated 的 Reward 以后,我们就可以解决 Version 0 遇到的问题,因为你可能向右移动以后进行瞄准,接下来开火,就有打中外星人,那这样向右这件事情,它也有 Accumulate Reward,虽然向右这件事情没有立即的 Reward,假设 a1 是向右,那 r1 可能是 0,但接下来可能会因为向右这件事,导致有瞄准,导致有打到外星人,那 Cumulated 的 Reward 就会正的,那我们就会知道说,其实向右也是一个好的 Action,这个是 Version 1\n\n但是你仔细想一想会发现,Version 1 好像也有点问题\n\n**假设这个游戏非常地长**,你**把 rN 归功于 a1 好像不太合适**吧,就是当我们採取 a1 这个行为的时候,立即有影响的是 r1,接下来有影响到 r2,接下来影响到 r3,那假设这个过程非常非常地长的话,那我们说因为有做 a1,导致我们可以得到 rN,这个可能性应该很低吧,也许得到 rN 的功劳,不应该归功于 a1,好 所以怎么办呢\n\n### 2.4 Version 2\n\n有第二个版本的 Cumulated 的 Reward,我们这边用 G',来表示 Cumulated 的 Reward,好 这个我们会在 r 前面,乘一个 Discount 的 Factor\n\n![image-20211025095219890](image-20211025095219890.png)\n\n这个 Discount 的 Factor γ,也会设一个小于 1 的值,有可能会设,比如说 0.9 或 0.99 之类的,所以这个 G'1 相较于 G1 有什么不同呢,G1 是 r1 加 r2 加 r3,那 G'1 呢,是 r1 加 γr2 加 γ 平方 r3,就是距离採取这个 Action 越远,我们 γ 平方项就越多,所以 r2 距离 a1 一步,就乘个 γ,r3 距离 a1 两步,就乘 γ 平方,那这样一直加到 rN 的时候,rN 对 G'1 就几乎没有影响力了,因为你 γ 乘了非常非常多次了,γ 是一个小于 1 的值,就算你设 0.9,0.9 的比如说 10 次方,那其实也很小了\n\n所以你今天用这个方法,就可以**把离 a1 比较近的那些 Reward,给它比较大的权重**,离我比较远的那些 Reward,给它比较小的权重,所以我们现在有一个新的 A,这个新的 A 这个评估,这个 Action 好坏的这个 A,我们现在用 G'1 来表示它,那它的式子可以写成这个样子,这个 G't 就是 Summention over,n 等于 t 到 N,然后我们把 rN 乘上 γ 的 n-t 次方,所以离我们现在,採取的 Action 越远的那些 Reward,它的 γ 就被乘越多次,它对我们的 G' 的影响就越小,这是第二个版本,听到这边你是不是觉得合理多了呢\n\n### Q\u0026A\n\nQ1:一个大括号是一个 Episode,还是这样蓝色的框住的多个大括号,是一个 Episode\n\nA1: 一个大括号不是一个 Episode,一个大括号是,我们在这一个 Observation,执行这一个 Action 的时候,这个是一笔资料,它不是一个 Episode,Episode 是很多的,很多次的 Observation,跟很多次的 Action 才合起来,才是一个 Episode\n\nQ2: G1 需不需要做标准化之类的动作\n\nA2: 这个问题太棒了,为什么呢,因为这个就是 Version 3\n\nQ3: 越早的动作就会累积到越多的分数吗,越晚的动作累积的分数就越少\n\nA3: 对 没错 是,在这个设计的情境裡面,是,越早的动作就会累积到越多的分数,但是这个其实也是一个合理的情境,因为你想想看,比较早的那些动作对接下来的影响比较大,到游戏的终局,没什么外星人可以杀了,你可能做什么事对结果影响都不大,所以比较早的那些 Observation,它们的动作是我们可能需要特别在意的,不过像这种 A 要怎么决定,有很多种不同的方法,如果你不想要比较早的动作 Action 比较大,你完全可以改变这个 A 的定义,事实上不同的 rl 的方法,其实就是在 A 上面下文章,有不同的定义 A 的方法\n\nQ4: 看来仍然不适合用在围棋之类的游戏,毕竟围棋这种游戏只有结尾才有分数\n\nA4: 这是一个好问题,这个我们现在讲的这些方法,到底能不能够处理,这种结尾才有分数的游戏呢,其实也是可以的,怎么说呢,假设今天只有 rN 有分数,其它通通都是 0,那会发生什么事,那会发生说,今天我们採取一连串的 Action,**只要最后赢了,这一串的 Action 都是好的,如果输了,这一连串的 Action,通通都算是不好的**,而你可能会觉得这样做,感觉 Train Network 应该会很难 Train,确实很难 Train,但是就我所知,最早的那个版本的 AlphaGo,它是这样 Train 的,很神奇,它就是这样做的,它裡面有用到这样子的技术,当然还有一些其它的方法,比如说 Value Network 等等,那这个等一下也会讲到,那最早的 AlphaGo,它有採取这样子的技术来做学习,它有试著採取这样的技术,看起来是学得起来的,拿预估的误差当 Reward,拿,有一个同学说那其实 AlphaGo,可以拿预估的误差当 Reward,那你要有一个办法先预估误差,那你才拿它来当 Reward,那有没有办法事先预估,我们接下来会得到多少的 Reward 呢,有 那这个在之后的版本裡面,会有这样的技术,但我目前还没有讲到那一块,好 那我们接下来就讲 Version 3,\n\n### 2.5 Version 3\n\nVersion 3 就是像刚才同学问的,要不要做标准化呢?\n\n要,因为好或坏是相对的,**好或坏是相对的**,怎么说好或坏是相对的呢,假设所有,假设今天在这个游戏裡面,你每次採取一个行动的时候,最低分就预设是 10 分,那你其实得到 10 分的 Reward,根本算是差的,就好像说今天你说你得,在某一门课得到 60 分,这个叫做好或不好,还是不好呢,没有人知道\n\n因为那要看别人得到多少分数,如果别人都是 40 分,你是全班最高分,那你很厉害,如果别人都是 80 分,你是全班最低分,那就很不厉害,所以 Reward 这个东西是相对的\n\n![image-20211025102800545](image-20211025102800545.png)\n\n所以如果我们只是单纯的把 G 算出来,你可能会遇到一个问题,假设这个游戏裡面,可能永远都是拿到正的分数,每一个行为都会给我们正的分数,只是有大有小的不同,那你这边 G 算出来通通都会是正的,有些行为其实是不好的,但是你 仍然会鼓励你的 Model,去採取这些行为\n\n所以怎么办,我们需要做一下标准化,那这边先讲一个最简单的方法就是,**把所有的 G' 都减掉一个 b**,这个 b 在这边叫做,在 rl 的文献上通常叫做 Baseline,那这个跟我们作业的 Baseline 有点不像,但是反正在 rl 的文献上,就叫做 Baseline 就对了,我们把所有的 G' 都减掉一个 b,目标就是让 G' 有正有负,特别高的 G' 让它是正的,特别低的 G' 让它是负的\n\n但是这边会有一个问题就是,那要**怎么样设定这个 Baseline** 呢,我们怎么设定一个好的 Baseline,让 G' 有正有负呢,那这个我们在接下来的版本裡面还会再提到,但目前为止我们先讲到这个地方\n\nQ: 需要个比较好的,Heuristic Function\n\nA: 对 需要个,就是说在下围棋的时候,假设今天你的 Reward 非常地 Sparse,那你可能会需要一个好的,Heuristic Function,如果你有看过那个最原始的,那个深蓝的那篇 Paper,就是在这个机器下围棋打爆人类之前,其实已经在西洋棋上打爆人类了,那个就叫深蓝,深蓝就有蛮多 Heuristic 的 Function,它就不是只有下到游戏的中盘,才知道 才得到 Reward,中间会有蛮多的状况它都会得到 Reward,好\n\n### 2.6 Policy Gradient\n\n接下来就会实际告诉你说,Policy Gradient 是怎么操作的,那你可以仔细读一下助教的程式,助教就是这么操作的\n\n![image-20211025103507578](image-20211025103507578.png)\n\n首先你要先 Random 初始化,随机初始化你的 Actor,你就给你的 Actor 一个随机初始化的参数,叫做 θ0,然后接下来你进入你的 Training Iteration,假设你要跑 T 个 Training Iteration,好 那你就拿你的这个,现在手上有的 Actor,一开始是这个 θ0\n\n一开始很笨 它什么都不会,它採取的行为都是随机的,但它会越来越好,你拿你的 Actor 去跟环境做互动,那你就得到一大堆的 s 跟 a,你就得到一大堆的 s 跟 a,就把它互动的过程记录下来,得到这些 s 跟 a,那接下来你就要进行评价,你用 A1 到 AN 来决定说,这些 Action 到底是好还是不好\n\n你先拿你的 Actor 去跟环境做互动,收集到这些观察,接下来你要进行评价,看这些 Action 是好的还是不好的,那你真正需要这个在意的地方,你最需要把它改动的地方,就是在评价这个过程裡面,那助教程式这个 A 就直接设成,Immediate Reward,那你写的要改这一段,你才有可能得到好的结果\n\n设完这个 A 以后,就结束了\n\n你就把 Loss 定义出来,然后 Update 你的 Model,这个 Update 的过程,就跟 Gradient Descent 一模一样的,会去计算 L 的 Gradient,前面乘上 Learning Rate,然后拿这个 Gradient 去 Update 你的 Model,就把 θi−1 Update 成 θi,\n\n但是这边有一个神奇的地方是,一般的 training,在我们到目前为止的 Training,Data Collection 都是在 For 循环之外,比如说我有一堆资料,然后把这堆资料拿来做 Training,拿来 Update,Model 很多次,然后最后得到一个收敛的参数,然后拿这个参数来做 Testing\n\n![image-20211025104111127](image-20211025104111127.png)\n\n但在 RL 裡面不是这样,你发现**收集资料这一段,居然是在 For 循环裡面**,假设这个 For 循环,你打算跑 400 次,那你就得收集资料 400 次,或者是我们用一个图像化的方式来表示\n\n![image-20211025104803588](image-20211025104803588.png)\n\n这个是你收集到的资料,就是你观察了某一个 Actor,它在每一个 State 执行的 Action,然后接下来你给予一个评价,但要用什么评价 要用哪一个版本,这个是你自己决定的,你给予一个评价,说每一个 Action 是好或不好,你有了这些资料 这些评价以后,拿去训练你的 Actor,你拿这些评价可以定义出一个 Loss,然后你可以更新你的参数一次\n\n但是有趣的地方是,你**只能更新一次而已,一旦更新完一次参数以后,接下来你就要重新去收集资料了**,登记一次参数以后,你就要重新收集资料,才能更新下一次参数,所以这就是为什么 RL,往往它的训练过程非常花时间\n\n收集资料这件事情,居然是在 For 循环裡面的,你每次 Update 完一次参数以后,你的资料就要重新再收集一次,再去 Update 参数,然后 Update 完一次以后,又要再重新收集资料,如果你参数要 Update 400 次,那你资料就要收集 400 次,那这个过程显然非常地花时间,那你接下来就会问说,那为什么会这样呢\n\n**为什么我们不能够一组资料,就拿来 Update 模型 Update 400 次,然后就结束了呢,为什么每次 Update 完我们的模型参数以后,Update Network 参数以后,就要重新再收集资料呢**\n\n那我们,那这边一个比较简单的比喻是,你知道**一个人的食物,可能是另外一个人的毒药**\n\n![image-20211025105544243](image-20211025105544243.png)\n\n这些资料是由 θi−1 所收集出来的,这是 θi−1 跟环境互动的结果,这个是 θi−1 的经验,这些经验可以拿来更新 θi−1,可以拿来 Update θi−1 的参数,但它不一定适合拿来 Update θi 的参数\n\n或者是我们举一个具体的例子,这个例子来自棋魂的第八集,大家看过棋魂吧,我应该就不需要解释棋魂的剧情了吧\n\n![image-20211025105804642](image-20211025105804642.png)\n\n这个是进藤光,然后他在跟佐为下棋,然后进藤光就下一步,在大马 现在在小马步飞,这小马步飞具体是什么,我其实也没有非常地确定,但这边有解释一下,就是棋子斜放一格叫做小马步飞,斜放好几格叫做大马步飞,好 阿光下完棋以后,佐为就说这个时候不要下小马步飞,而是要下大马步飞,然后阿光说为什么要下大马步飞呢,我觉得小马步飞也不错\n\n![image-20211025105831934](image-20211025105831934.png)\n\n这个时候佐为就解释了,如果大马步飞有 100 手的话,小马步飞只有 99 手,接下来是重点,之前走小马步飞是对的,因为小马步飞的后续比较容易预测,也比较不容易出错,但是大马步飞的下法会比较複杂,但是阿光假设想要变强的话,他应该要学习下大马步飞,或者是阿光变得比较强以后,他应该要下大马步飞,所以你知道说同样的一个行为,同样是做下小马步飞这件事,对不同棋力的棋士来说,也许它的好是不一样的,对于比较弱的阿光来说,下小马步飞是对的,因为他比较不容易出错,但对于已经变强的阿光来说,应该要下大马步飞比较好,下小马步飞反而是比较不好的\n\n所以**同一个 Action 同一个行为,对于不同的 Actor 而言,它的好是不一样的**\n\n![image-20211025110056680](image-20211025110056680.png)\n\n所以今天假设我们用 θi−1,收集了一堆的资料,这个是 θi−1 的 Trajectory,这些资料只能拿来训练 θi−1,你不能拿这些资料来训练 θi,为什么不能拿这些资料来训练 θi 呢\n\n因为假设 假设就算是从 θi−1 跟 θi,它们在 s1 都会採取 a1 好了,但之后到了 s2 以后,它们**可能採取的行为就不一样了**,所以假设对 θ,假设今天 θi,它是看 θi−1 的这个 Trajectory,那 θi−1 会执行的这个 Trajectory,跟 θi 它会採取的行为根本就不一样,所以你拿著 θi−1 接下来会得到的 Reward,来评估 θi 接下来会得到的 Reward,其实是不合适的\n\n所以如果再回到刚才棋魂的那个例子,同样是假设这个 a1 就是下小马步飞,那对于变强以前的阿光,这是一个合适的走法,但是对于变强以后的阿光,它可能就不是一个合适的走法\n\n所以今天我们在收集资料,来训练你的 Actor 的时候,你要注意就是**收集资料的那个 Actor,要跟被训练的那个 Actor,最好就是同一个**,那当你的 **Actor 更新以后,你就最好要重新去收集资料**,这就是为什么 RL它非常花时间的原因\n\n#### On-policy v.s. Off-policy\n\n刚才我们说,这个要被训练的 Actor,跟要拿来跟环境互动的 Actor,最好是同一个,当我们训练的 Actor,跟互动的 Actor 是同一个的时候,这种叫做 On-policy Learning,那我们刚才示范的那个,Policy Gradient 的整个 Algorithm,它就是 On-policy 的 Learning,那但是还有另外一种状况叫做,Off-policy Learning,\n\n![image-20211025110753839](image-20211025110753839.png)\n\nOff-policy 的 Learning 我们今天就不会细讲,Off-policy 的 Learning,期待能够做到的事情是,我们能不能够**让要训练的那个 Actor,还有跟环境互动的那个 Actor,是分开的两个 Actor 呢**,我们要训练的 Actor,能不能够根据其他 Actor 跟环境互动的经验,来进行学习呢\n\nOff-policy 有一个非常显而易见的好处,你就**不用一直收集资料了**,刚才说 Reinforcement Learning,一个很卡的地方就是,每次更新一次参数就要收集一次资料,你看助教的示范历程是更新 400 次参数,400 次参数相较于你之前 Train 的 Network,没有很多,但我们要收集 400 次资料,跑起来也已经是很卡了,那如果我们可以收一次资料,就 Update 参数很多次,这样不是很好吗,所以 Off-policy 它有不错的优势\n\n#### Off-policy → Proximal Policy Optimization(PPO)\n\n但是 Off-policy 要怎么做呢,我们这边就不细讲,有一个非常经典的 Off-policy 的方法,叫做 **Proximal Policy Optimization,缩写是 PPO**,那这个是今天蛮常使用的一个方法,它也是一个蛮强的方法,蛮常使用的方法\n\nOff-policy 的重点就是,你**在训练的那个 Network,要知道自己跟别人之间的差距,它要有意识的知道说,它跟环境互动的那个 Actor 是不一样的**,那至于细节我们就不细讲,那我有留那个上课的录影的[连结](https://disp.cc/b/115-bLHe),在投影片的下方,等一下大家如果有兴趣的话,再自己去研究 PPO\n\n![image-20211025111501212](image-20211025111501212.png)\n\n那如果要举个比喻的话,就好像是你去问克里斯伊凡 就是美国队长,怎么追一个女生,然后克里斯伊凡就告诉你说,他就示范给你看,他就是 Actor To Interact,他就是负责去示范的那个 Actor,他说他只要去告白,从来没有失败过,但是你要知道说,你跟克里斯伊凡其实还是不一样,人帅真好 人丑吃草,你跟克里斯伊凡是不一样的,所以克里斯伊凡可以採取的招数,你不一定能够採取,你可能要打一个折扣,那这个就是 Off-policy 的精神\n\n你的 Actor To Train,要知道 Actor To Interact,跟它是不一样的,**所以 Actor To Interact 示范的那些经验,有些可以採纳,有些不一定可以採纳**,至于细节怎么做,那过去的上课录影留在这边,给大家参考\n\n#### Collection Training Data: Exploration\n\n那还有另外一个很重要的概念,叫做 Exploration,Exploration 指的是什么呢,我们刚才有讲过说,我们今天的,我们今天的这个 **Actor,它在採取行为的时候,它是有一些随机性的**\n\n而这个随机性其实非常地重要,很多时候你随机性不够,你会 Train 不起来,为什么呢,举一个最简单的例子,假设你一开始初始的 Actor,它永远都只会向右移动,它从来都不会知道要开火,如果它从来没有採取开火这个行为,你就永远不知道开火这件事情,到底是好还是不好,唯有今天某一个 Actor,去试图做开火这件事得到 Reward,你才有办法去评估这个行为好或不好,假设有一些 Action 从来没被执行过,那你根本就无从知道,这个 Action 好或不好\n\n![image-20211025111831335](image-20211025111831335.png)\n\n所以你今天在训练的过程中,这个拿去跟环境的互动的这个 Actor,它本身的随机性是非常重要的,你其实会期待说跟环境互动的这个 Actor,它的**随机性可以大一点**,这样我们才能够收集到,比较多的 比较丰富的资料,才不会有一些状况,它的 Reward 是从来不知道,那为了要让这个 Actor 的随机性大一点,甚至你在 Training 的时候,你会刻意加大它的随机性\n\n比如说 Actor 的 Output,不是一个 Distribution 吗,有人会刻意加大,那个 Distribution 的 Entropy,那让它在训练的时候,比较容易 Sample 到那些机率比较低的行为,或者是有人会直接在这个 Actor,它的那个参数上面加 Noise,直接在 Actor 参数上加 Noise,让它每一次採取的行为都不一样,好 那这个就是 Exploration,那 Exploration,其实也是 RL Training 的过程中,一个非常重要的技巧,如果你在训练过程中,你没有让 Network 尽量去试不同的 Action,你很有可能你会 Train 不出好的结果\n\n那我们来看一下,其实这个 PPO 这个方法,DeepMind 跟 Open AI,都同时提出了 PPO 的想法\n\n那我们来看一下,DeepMind 的 PPO 的 Demo 的影片https://youtu.be/gn4nRCC9TwQ,它看起来是这样子的,好 那这个是 DeepMind 的 PPO,那就是可以用 PPO 这个方法,用这个 Reinforcement Learning 的方法,去 Learn什么,蜘蛛型的机器人或人形的机器人,做一些动作,比如说跑起来 或者是蹦跳,或者是跨过围牆等等\n\n那接下来是 OpenAI 的 PPOhttps://blog.openai.com/openai-baselines-ppo/,它这个影片就没有刚才那个潮,它没有那个配音,不过我帮它配个音好了,这个影片我叫它,修机器学习的你,好 我修了一门课叫做机器学习,但在这门课裡面,有非常多的障碍 我一直遇到挫折,那个红色的球是 Baseline,而这个 Baseline 一个接一个,永远都不会停止,然后我 Train 一个 Network 很久,我 collate 它就掉线啦,Train 了三个小时的 Model 不见,但我仍然是爬起来继续地向前,我想开一个比较大的模型,看看可不可以 Train 得比较好一点,但是结果发生什么事情呢,Out Of Memory,那个圈圈一直在转,它就是不跑,怎么办,但我还是爬起来,继续向前,结果 Private Set 跟 Public Set,结果不一样,真的是让人觉得非常地生气,这个影片到这边就结束了吗\n\n没关係 我们最后还是要给它一个正面的结尾,就算是遭遇到这么多挫折,我仍然努力向前好好地学习,这个就是 PPO,好 那讲到这边正好告一个段落\n\n## 3 Critic\n\n那上一次 RL 的部分,我们讲说我们要 Learn 一个 Actor,那这一次,我们要 Learn 另外一个东西,这个东西叫做 Critic\n\n我会先解释 Critic 是什麽,然后我们再来讲说,这个 Critic 对 Learn Actor 这个东西,有什麽样的帮助\n\n![image-20211027150436777](image-20211027150436777.png)\n\n**Critic 它的工作是要来评估一个 Actor 的好坏**,就你现在已经有一个 Actor,它的参数叫 θ,那 Critic 的工作就是,它要评估说如果这个 Actor,它看到某个样子的 Observation,看到某一个游戏画面,接下来它**可能会得到多少的 Reward**\n\n那 Critic 有好多种不同的变形,有的 Critic 是只看游戏画面来判断,有的 Critic 是说采取某,看到某一个游戏画面,接下来又发现 Actor 採取某一个 Action,在这两者都具备的前提下,那接下来会得到多少 Reward\n\n![image-20211027150753604](image-20211027150753604.png)\n\n那这样讲,还是有点抽象,所以我们讲的更具体一点,我们直接介绍一个,我们等一下会真的被用上,你在作业裡面真的派得上用场的,这个 Critic 叫做 Value Function,那这个 Value Function,我们这边用大写的 Vθ(S) 来表示\n\n它的**输入是 s**,也就是现在游戏的状况,比如说游戏的画面,那这边要特别注意一下 V,它是有一个**上标 θ** 的\n\n![image-20211027151809732](image-20211027151809732.png)\n\n**这个上标 θ 代表这个 V ,它观察的对象是 θ 这个 Actor**,它观察的这个 Actor 它的参数是 θ,那这个 V ,Vθ就是一个 Function,它的输入是 S,那输出是一个 Scalar,这边用 Vθ(S) 来表示这一个 Scalar\n\n那 Scalar这个数值的含义是,这一个 Actor θ,放在上标的这个 Actor θ,它如果看到 Observation S,如果看到输入的这个 S 的游戏画面,接下来它得到的,Discounted Cumulated Reward 是多少\n\n![image-20211027152150748](image-20211027152150748.png)\n\n这个的 Value Function 它的工作,就是要去估测说,对某一个 Actor 来说,如果现在它已经看到某一个游戏画面,那接下来会得到的,Discounted Cumulated Reward 应该是多少\n\n当然 Discounted Cumulated Reward,你可以直接透过**把游戏玩到底**,就你看到你已经有了 Actor θ,那假设它看到这个 State s,那最后它到底会得到多少的这个 G' ,你就把这个游戏玩完你就知道了\n\n但是这些这个 Value Function,它的能力就是它要**未卜先知,未看先猜**,游戏还没有玩完,只光看到 S 就要预测这个 Actor,它可以得到什麽样的表现,那这个就是 Value Function 要做的事情\n\n举例来说,假设你给 Value Function 这一个游戏画面,它就要直接预测说,看到这个游戏画面,接下来应该会得到很高的 Cumulated Reward,为什麽,因为游戏,这个游戏画面裡面还有很多的外星人\n\n![image-20211027152406211](image-20211027152406211.png)\n\n假设你的这个 Actor 它很厉害,它是一个好的 Actor,它是能杀得了外星人的 Actor,那接下来它就会得到很多的 Reward\n\n那像这个画面,这已经是游戏的中盘\n\n![image-20211027152442705](image-20211027152442705.png)\n\n游戏的残局,游戏快结束了,剩下的外星人没几隻了,那可以得到的 Reward 就比较少,那这些数值,你把整场游戏玩完你也会知道,但是 Value Function 想要做的事情,就是未卜先知,在游戏没玩完之前,就先猜应该会得到多少的,Discounted Cumulated Reward\n\n那这边有一件要跟大家特别强调的事情是,这个 Value Function 是有一个上标 θ 的,这个 **Value Function,跟我们观察的 Actor 是有关係的**,同样的 Observation,同样的游戏画面,不同的 Actor,它应该要得到不同的,Discounted Cumulated Reward\n\n我刚才在举例子的时候我说,假设我们有一个好的 Actor,看到这个游戏画面会有高的 Value,看到这个游戏画面会有低的 Value,但是假设你的 Actor 其实很烂,它很容易被外星人杀死,那也许看到这个画面,它的 Value 也是低的,因为有一堆外星人,它随便动两下它就被杀死了,它根本得不到 Reward,这个烂的 Actor 在这个画面,它可能拿到的 V 也是低的,所以 Value Function 的数值,是跟观察的对象有关係的,好 这个是 Critic\n\n### 3.1 How to estimate Vθ(s)\n\n#### Monte-Carlo (MC) based approach\n\n那在讲 Critic 要怎麽被使用,在 Reinforcement Learning 之前,我们来讲一下 Critic 是怎麽被训练出来的,那有两种常用的训练方法,第一种方法,是 Monte Carlo Based 的方法,这边缩写成 MC\n\n![image-20211027153610879](image-20211027153610879.png)\n\n如果是用 MC 的方法的话,你就**把 Actor 拿去跟环境互动**,互动很多轮,那 Actor 跟环境互动以后,Actor 去玩这个游戏以后,你就会得到一些游戏的记录\n\n你就会发现说,那这个时候,你的 Value Function 就得到一笔训练资料,这笔训练资料告诉它说,如果看到 sa 作为输入,它的输出,这个 Vθ(sa) 应该要跟 G'a 越接近越好\n\n那假设你今天 sample 到另外一个 Observation,看到另外一个游戏画面,把游戏玩完之后发现,得到的 Cumulated Reward 是 G'b,那这个时候,你的这个 Value Function ,输入 sb 它就应该得到 Vθ(sb),那这个 Vθ(sb) 就应该跟 G'b 越接近越好\n\n那这个非常直觉,你就去观察 Actor,会得到的 Cumulated Reward,那观察完你就有训练资料,直接拿这些训练资料来训练 Value Function,好 这个 MC ,是一个很直觉的作法\n\n#### Temporal-difference (TD) approach\n\n接下来我们来看另外一个,没有那麽直觉的作法,这个作法叫做 Temporal-Difference Approach,缩写是 TD\n\n那 Temporal-Difference Approach,它希望做到的事情是,**不用玩完整场游戏**,才能得到训练 Value 的资料,你**只要在某一个 Observation st 的,看到 st 的时候,你的 Actor 执行了 At 得到 Reward rt,**然后接下来再看到 St+1 这样的游戏画面,光看到这样一笔资料,就能够训练 Vπ(S) 了,光看到这样子的资料,就可以拿来更新 Vπ(S) 的参数了\n\n那如果光看这样一笔资料,就可以更新 Vπ(S) 的参数有什麽样的好处,它的好处是你想在 MC 裡面,你要玩完整场游戏,你才能得到一笔训练资料,那**有的游戏其实很长,甚至有的游戏也许,它根本就没有不会结束**,它永远它都一直继续下去,它永远都不会结束,那像这样子的游戏,你用 MC 就非常地不适合。那这个时候,你可能就希望採用 TD 的方法,好 那怎麽只看到这样子的资料,就拿来训练 Vπ(S)\n\n这边举一个例子,我们先来看一下,Vθ(st) 跟 Vθ(st+1) 它们之间的关係\n\n![image-20211027155628077](../../../../../../../c:\\Users\\10131\\AppData\\Roaming\\Typora\\typora-user-images\\image-20211027155628077.png)\n\n我们说Vθ(st),就是看到 st之后的 Cumulated Reward,所以Vθ(st) 就是 ²rt+γrt+1+γ²rt+2 以此类推\n\n然后 Vθ(st+1) 就是 rt+1+γrt+2 以此类推\n\n那你发现说这两个 Vθ,Vθ(st) 跟 Vθ(st+1),它们之间是有关係的\n\n你可以把它写成这样一个式子,把 Vθ(st+1) 乘上 γ 再加 rt,把 Vθ(st+1) 每一项都乘 γ 再加上 rt,就会变成Vθ(st),所以Vθ(st) 跟 Vθ(st+1) 中间,有这样子的关係\n\n我们现在,有这样一笔资料以后,我们就可以拿来训练我们的 Value Function,希望 Value Function 可以满足,这边我们所写的这个式子\n\n![image-20211027160335508](image-20211027160335508.png)\n\n那什麽意思,就假设我们现在有这样一笔资料,我们就把 St,代到 Value Function 裡面得到Vθ(st),我们有 st+1 代到 Value Function 裡面,得到 Vθ(st+1),虽然我们不知道Vθ(st) 是多少,我们也不知道 Vθ(st+1) 应该是多少,我们没有这两个东西的标准答案,但我们**知道它们相减应该是多少**\n\n根据上面这一个式子,我们把 Vθ(st+1) 乘上 γ,然后再去减Vθ(st),把Vθ(st) 减掉 γ 乘 Vθ(st+1),应该要跟 rt 越接近越好,rt 在这边,我们是有蒐集到 rt 这一笔资料的\n\n我们又知道Vθ(st),跟这个 Vθ(st+1) 之间的关係,所以我们知道Vθ(st) 减掉 γ 乘上 Vθ(st+1),应该跟 rt 越接近越好,所以你就有了这样子训练资料,输入st,输入 st+1,它们都通过 Vθ,然后把它们相减,然后要跟 rt 越接近越好\n\n那这个就是 TD 的方法\n\n#### MC v.s. TD\n\n这两个方法,其实你拿来计算同样的,观察到的结果,同样的资料,同样的 θ,你用 MC 跟 TD 来观察,你算出来的 Value Function,很有可能会是不一样的\n\n那这边,就举一个例子,这个例子是这样子的,我们观察某一个 Actor,这个 Actor ,跟环境互动玩了某一个游戏八次,当然这边为了简化计算,我们假设这些游戏都非常简单,都一个回合,就到两个回合就结束了\n\n![image-20211029140442476](image-20211029140442476.png)\n\n- 所以那个 Actor 第一次玩游戏的时候,它先看到 sa 这个画面,得到 Reware 0\n- 接下来看到 sb 这个画面,得到 Reware 0 游戏结束\n- 接下来,这个有连续六场游戏,都是看到 sb 这个画面,得到 Reward 1 就结束了\n- 最后一场游戏,看到 sb 这个画面,得到 Reward 0 就结束了\n\n那我们这边,先无视 Actor,为了简化起见无视 Actor,我们也假设,γ 就等于 1,也就是没有做 Discount,好 那这个 sb 应该是多少,Vθ(sb) 应该是多少\n\n我们知道这个 Vθ(sb),它的意思就是这个**看到 sb 这一个画面,你会得到的 Reward 的期望值**,那 sb 这个画面,我们在这八次游戏中,总共看到了八次,每次游戏都有看到 sb 这个画面,看到 sb 这个画面之后会得到多少 Reward\n\n**八次游戏裡面,有六次得到 1 分**,两次得到 0 分,所以平均是 3/4 分没有问题,所以 Vθ(sb) 就是 3/4,妥妥的没有争议\n\n那 Vθ(sa) 应该是多少,你觉得看到 sa,接下来应该要得到多少 Reward ,根据这八笔训练资料,看到 sa 接下来该得到多少 Reward\n\n几乎没有其他答案,所有人都说是 0,好 多数同学都说是 0,**0 是不是一个正确的答案,它既对也不对**\n\n其实还有另外一个可能的答案是 3/4,我看没有人写 3/4,等一下来解释,为什麽有可能算出 3/4,但 0 也是一个合理的答案,为什麽你会觉得是应该是 0 ,**0 是用 Monte-Carlo 的想法得到的**\n\n为什麽是 0,因为我们看到 sa 只有一次,看到 sa 以后会得到多少 Reward,这是 0,看到 sa 以后得到 Reward 0,再看到 sb 得到 Reward 还是 0,所以 Cumulated Reward 就是 0,所以如果从 Monte-Carlo 的角度来看,我们看到 sa,接下来算出来的 G 应该是多少,就是 0 ,所以 Vθ(sa) 应该就是 0,妥妥的没问题,几乎所有同学都得到了正确答案\n\n但如果你**用 TD,你算出来的,可会是不一样的结果**\n\n![image-20211029141053625](image-20211029141053625.png)\n\n因为 Vθ(sa) 跟 Vθ(sb) 中间,有这样子的一个关係,这个 Vθ(sa) 应该要等于 Vθ(sb) 加上 Reward,就是你在看到 sa 之后得到 Reward,接下来进入 sb,那这个 Vθ(sa),应该等于 Vθ(sb) 加上这一个 Reward\n\n所以按照这个想法,Vθ(sb) 是3/4,这个 r 是 0,但 Vθ(sa) 应该是 3/4 对不对,按照 TD 的想法,Vθ(sa) 应该是 3/4\n\n![image-20211029141247164](image-20211029141247164.png)\n\n你可能会问说,那到底 Monte-Carlo 跟 TD,谁算出来是对的,**都可以说是对的,它们只是背后的假设是不同的**,对 Monte-Carlo 而言,它就是直接看我们观察到的资料,sa 之后接 sb 得到的,Cumulated Reward 就是 0,所以 Vθ(sa) 当然是 0\n\n但对于 TD 而言,它背后的假设是这个 **sa 跟 sb 是没有关係**的,看到 sa 之后再看到 sb,并不会影响看到 sb 的 Reward,你现在看这八笔训练资料,给你一种错觉,觉得说 Vθ(sa) 应该是 0,那只是因为**你 sample 到的资料太少了**,看到 sb,应该可以期望的 Reward 是 3/4,只是因为今天正好运气不好,看完 sa 以后再看 sb,正好 r 是 0,但是期望值应该是 3/4,你只是正好运气不好看到 r 是 0,你才会觉得 sa 是 0\n\n但是 sb,看到 sb 以后得到的期望 Reward 应该是 3/4,所以看到 sa 以后你会看到 sb,那你得到的这个期望的 Reward 也应该是 3/4,所以从 TD 的角度来看,sb 会得到多少 Reward,跟 sa 是没有关係的\n\n所以你应该,所以 sa 的这个 Cumulated Reward 应该是 3/4\n\n所以总之用 MC 来计算,用 TD 来计算,会有微妙的差异\n\n### 3.2 Version 3.5\n\nCritic 怎麽被用在训练 Actor 上面,还记不记得我们上一次,最后我们讲到这个 Actor 的方法的时候,我们说怎麽训练一个 Actor,你就先把 Actor 跟环境互动,得到一些 Reward,然后你得到一堆这个 Observation,跟这个 Action 的 Pair\n\n这个在 s1 执行 A1 的时候多好,得到一个分数 A1,那我们说这个 A1 ,它是 Cumulative 的 Reward,那上週也有同学问到说,难道 Cumulative 的 Reward,不需要做 Normalization 吗,需要做 Normalization,所以我们说,这个减掉一个 b 当做 Normalization\n\n![image-20211029144710982](image-20211029144710982.png)\n\n但这个 b 的值应该设多少,就不好说,那我这边 告诉大家说,一个 V 合理的设法,是把它设成 Vθ(S)\n\n![image-20211029144900910](image-20211029144900910.png)\n\n你现在 Learn 出这个 Critic 以后,这个 Critic 给它一个 Step,它就会产生一个分数,那你把这个分数 当做 B,,所以 G1' 就是要减掉 Vθ(s1),G2' 就是减掉 Vθ(s2),以此类推\n\n那再来的问题就是,**为什麽减掉 V 是一个合理的选择**,那我们在下一页投影片,来跟大家解释一下\n\n我们已经知道说这个 At 代表s，a 这个 Pair 有多好,我们是用 G' 减掉Vθ(st),来定义这个 A,好 那我们先来看一下这个Vθ(st),到底代表什麽意思\n\n![image-20211030105849219](image-20211030105849219.png)\n\n**Vθ(st)是看到某一个画面 St 以后,接下来会得到的 Reward**\n\n它其实是一个**期望值**,因为假设你今天看到同一个画面,接下来再继续玩游戏,**游戏有随机性**,你每次得到的 Reward 都不太一样的话,那Vθ(st) 其实是一个期望值\n\n那在这个时候,在**看到 St 的时候,你的 Actor 不一定会执行At 这一个 Action**\n\n因为 Actor 本身是有随机性的,在训练的过程中,我们甚至鼓励 Actor 是有随机性的,所以同样的 S,你的 Actor ,它会输出的这个 Action 不一定是一样的\n\n我们说 **Actor 的输出其实是一个 Probability Distribution**,是一个在这个 Action 的 space 上面的,Probability Distribution,它还给每一个 Action 一个分数,你按照这个分数去做sample,有些 Action 被sample 到的机率高,有些 Action 被sample 到的机率低,但每一次sample 出来的 Action,并不保证一定要是一样的,\n\n所以看到 St 之后,接下来有很多的可能 很多的可能,所以你会算出不同的 Cumulative 的 Reward\n\n![image-20211030110507941](image-20211030110507941.png)\n\n那当然如果你有 Discount 的话,就是 Discounted 的 Cumulative Reward,那我们这边,是把 Discount 这件事情暂时省略掉，那**把这些可能的结果平均起来,就是Vθ(st)**,这是Vθ(st) 这一项的含义\n\n那 Gt' 这一项的含义是什麽？\n\nGt' 这一项的含义是,在 St 这个位置 **在 St 这个画面下,执行 At 以后,接下来会得到的 Cumulative Reward**\n\n![image-20211030132243867](image-20211030132243867.png)\n\n所以你**执行 At 以后,接下来 再一路玩下去,你会得到一个结果 得到一个 Reward,就是 Gt'**\n\n- 如果 At 大于 0 代表说,Gt' 大于Vθ(st),这个时候代表说,这个 Action 是比,我们 Random sample 到的 Action 还要好的,在这边得到 Gt' 的时候,我们确定是执行了 At,那在 St 在算这个Vθ(st) 的时候,我们不确定我们会执行哪一个 Action\n\n  所以我们执行 Action At 的时候,得到的 Reward大于随便执行一个 Action 得到的 Reward,所以当 At 大于 0 的时候代表说,**At 大于随便执行的一个 Action,那这个时候这个 Action At 它就是好的**,所以我们给它一个大于 0 的 At\n\n- 如果 At 小于 0 代表说,这个平均的 Reward,大过执行 At 得到的 Reward,你随机採取的 Action,按照某一个 Distribution,sample 出来的 Action,得到的这个 Cumulative Reward 的期望值,大过採取 At 这个 Action 所得到的 Reward,那这个时候 At 就是坏的,所要给它负的大 At\n\n  所以这样就非常地直觉,为什麽我们应该把 Gt' 减掉Vθ(st),但讲到这边,你有没有觉得有一些地方有点违和,什麽地方有点违和,这个 Gt' 它是一个sample 的结果,它是执行 At 以后,一直玩玩玩 玩到游戏结束,某一个sample 出来的结果,而Vθ(st) 是很多条路径 很多个可能性,平均以后的结果,我们把一个sample 去减掉平均,这样会准吗,**也许这个sample 特别好或特别坏,我们为什麽不是拿平均去减掉平均**\n\n### 3.3 Version 4\n\n所以我们这一门课要讲的最后一个版本,就是拿平均去减掉平均\n\n我们执行完 At 以后 得到 Reward rt,然后跑到下一个画面 St+1,把这个 St+1 接下来一直玩下去,有很多不同的可能,每个可能通通会得到一个 Reward,把这些 Reward 平均起来\n\n![image-20211030132330419](image-20211030132330419.png)\n\n把这些 Cumulative 的 Reward 平均起来,其实就是 Vθ(st+1),本来你会需要玩很多场游戏,才能够得到这个平均值,\n\n但没关係,假设你训练出一个好的 Critic,那你直接代 Vθ(st+1),你就知道说,在 St+1 这个画面下,接下来会得到的,Cumulative Reward 的期望值应该多少\n\n而接下来 你再加上 rt,接下来再加上 rt,代表说在 St 这个位置採取 at\n\n![image-20211030132914309](image-20211030132914309.png)\n\n跳到 St+1以后,会得到的 Reward 的期望值,因为我们已经知道说,在 St 这边採取 at 会得到 Reward rt,再跳到 St+1,然后 St+1 会得到期望值,期望的 Reward 是 Vθ(st+1)\n\n所以我们这边,再给它加上 rt,代表说在 St 这边执行 At 以后,会得到的 Reward 的期望值,接下来再把这两个东西相减,再把 rt+Vθ(st+1) 减掉Vθ(st)\n\n![image-20211030133441524](image-20211030133441524.png)\n\n也就是我们把 G' 换成 rt+Vθ(st+1),再减掉Vθ(st)\n\n我们就知道说,採取 at 这个 Action得到的期望 Reward,减掉根据某个 Distribution sample 一个 Action得到的 Reward,两者的期望值差距有多大\n\n那**如果 rt+Vθ(st+1) 比较大,就代表 at 比较好**,它比随便 sample Reward 好\n\n rt+Vθ(st+1) 小于Vθ(st),就代表 at 它是 Lower Than Average,它比从一个 Distribution,sample 到的 Action 还要差\n\n所以今天,这个就是大名鼎鼎的一个常用的方法,叫做 Advantage Actor-Critic,在 Advantage Actor-Critic 裡面,你是怎麽定义 at 的,也就是 rt+Vθ(st+1) 减掉, rt+Vθ(st+1) -Vθ(st),就是我们的 At 了\n\n### 3.4 Tip of Actor-Critic\n\n这边有一个训练 Actor-Critic 的小技巧,那你在作业裡面也不妨使用这个技巧\n\n![image-20211030134011817](image-20211030134011817.png)\n\nActor 是一个 Network,Critic 也是一个 Network,Actor 这个 Network,是一个游戏画面当做输入,它的输出是每一个 Action 的分数,Critic 是一个游戏画面当做输入,输出是一个数值,代表接下来会得到的 Cumulative 的 Reward\n\n这边有**两个 Network,它们的输入是一样的东西**,所以这两个 Network,它们应该**有部分的参数可以共用**吧,尤其假设你的输入又是一个非常複杂的东西,比如说游戏画面的时候,前面几层应该都需要是 CNN 吧,要了解这个游戏画面需要用的 CNN,也许是差不多的吧\n\n所以 Actor 跟 Critic,它们可以共用前面几个 Layer,所以你今天在实作的时候往往,你会把你的 Actor-Critic 设计成这个样子,Actor-Critic ,它们有共用大部分的 Network,然后只是最后,输出不同的 Action,就是 Actor,输出一个 Scalar,就是 Critic,好 那这是一个训练 Actor-Critic 的小技巧\n\n### 3.5 Outlook: Deep Q Network (DQN)\n\n那其实今天讲的,并不是 Reinforcement Learning 的全部,那其实在 Reinforcement Learning 裡面,还有一个犀利的做法,是直接採取 Critic,也就是**直接用 Critic,就可以决定要用什麽样的 Action**\n\n![image-20211030134218607](image-20211030134218607.png)\n\n那其中最知名的就是,Deep Q Network (DQN),那不过 这边我们就不细讲 DQN 了,如果你真的想知道 DQN 的话,可以参考过去上课的录影,那 DQN 哇 有非常非常多的变形\n\n这边 就是找一个非常,有一篇非常知名的 Paper 叫做 Rainbow,裡面 就是试著去尝试了各种 DQN 的变形,试了七种 然后再把这七种变形集合起来,因为有七种变形集合起来,所以他说它是一个彩虹,所以他把它的方法叫做 Rainbow,那我也把这个 Paper 留在这边给你参考,那如果你想知道 Rainbow 裡面的,每一个小技巧是怎麽做的话,你就参见上课录影,过去的课程,有把 Rainbow 裡面的每一个小技巧,都讲过一遍\n\n### Q\u0026A\n\nQ1: sa 后面接的不一定是 sb 吧,这样怎麽办\n\nA1: 这是一个很好的问题,sa 后面不一定接 sb,那这个问题,在刚才我们看到的那个例子裡面,就没有办法处理,因为在刚才那个,我们看到那个只有 8 个 Episode 的例子裡面,sa 后面就只会接 sb,所以我们观察 没有观察到其它的可能性,所以我们没办法处理这个问题,所以这就告诉我们说,在做 Reinforcement Learning 的时候,sample 这件事情是非常重要的,你 Reinforcement Learning,最后 Learn 得好不好,跟你在 sample 的时候 sample 得好不好,关係非常大,喔 所以这个 Reinforcement Learning,是一个非常吃人品的方法啦,所以你在作业裡面你可以体验一下,就你 sample 到的结果,对你最后 Training 的结果,有非常大的影响\n\nQ2: 每一个 V,都需对应到固定的环境发生顺序吗\n\nA2: 我没有很确定你的问题,但是我试著回答一下,就是每一个 V 它不会固定,它不会对应到固定的环境发生顺序,如果你的游戏有随机性的话,那 V 其实是代表了一个期望值,它想要算的就是,给某一个 Observation,看到某一个游戏画面以后,接下来你会得到的 Cumulative Reward 的平均值,它的期望值,如果你的游戏有随机性的话,V 代表的是期望值,你看到某一个游戏画面以后,然后接下来会发生什麽事情,不见得是一样的,但把所有的可能性都平均起来,取它的期望值,这个就是 V 所代表的意思\n\nQ3: 后面出现的 S 应该是不固定的,这样怎麽代公式\n\nA3: 好 那个我想我刚才应该算是有回答到了,后面出现的 看到某一个这个 Observation,后面出现的 Observation 确实是不固定的,那如果有些状况,某些 Observation 你没观察到的话,哇 那你真的就没办法训练\n\nQ4: 就是拿 V 当一般人的实力,超过它就是猛,没超过就是烂吗\n\nA4: 对 就是这样,V 就是平均的实力,超过 V 就是好\n\nQ5: 想请问这个 Distribution 要从哪裡知道\n\nA5: 我想你这个 Distribution 问的是那个,Actor 的 Distribution 啦对不对,我们说,Distribution Action 的 Distribution,Action 是从某一个 Distribution,sample 出来的,那个 Distribution 是谁,那个 Distribution 是这样,就是你的 Actor 不是像是一个 Classifier 吗,你的 Actor 像是一个 Classifier,然后你把 S 丢进去,每一个 Action 都会有一个分数,那你把这个分数,通过 Soft Mess,就做一个 Normalize,它就变得像机率一样,然后按照那个机率去做 sample,那这个就是 Actor,从一个 Distribution sample 出来的,这句话的意思,\n","lastmodified":"2023-05-09T16:33:58.30736679Z","tags":[]},"/%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0":{"title":"💻技术学习笔记","content":"\n从18年大二开始整理技术笔记至今，看了看文件目录，记录了太多从未看过的内容，借这次重新发布[[数字花园]]的机会，在这里梳理下需要继续学习维护的部分\n\n想想从大二到研二，好像什么都做过，但又好像什么都做不精\n\n![[前端]]\n\n![[分布式]]\n\n![[Python]]\n\n![[数据库、部署、运维]]\n\n![[Linux]]\n\n![[网络与媒体]]\n\n![[面试题]]","lastmodified":"2023-05-09T16:33:58.311366888Z","tags":[]},"/%E6%8E%92%E5%9D%91":{"title":"排坑","content":"\n## locale 的报错 LC_CTYPE / LC_ALL 问题\n\n   如果输入 locale 查看系统字体编码，可能会出现如下报错\n\n   ```bash\n   locale: Cannot set LC_CTYPE to default locale: No such file or directory\n   locale: Cannot set LC_ALL to default locale: ?????????\n   LANG=zh_CN.UTF-8\n   LC_CTYPE=UTF-8\n   LC_NUMERIC=\"zh_CN.UTF-8\"\n   LC_TIME=\"zh_CN.UTF-8\"\n   LC_COLLATE=\"zh_CN.UTF-8\"\n   LC_MONETARY=\"zh_CN.UTF-8\"\n   LC_MESSAGES=\"zh_CN.UTF-8\"\n   LC_PAPER=\"zh_CN.UTF-8\"\n   LC_NAME=\"zh_CN.UTF-8\"\n   LC_ADDRESS=\"zh_CN.UTF-8\"\n   LC_TELEPHONE=\"zh_CN.UTF-8\"\n   LC_MEASUREMENT=\"zh_CN.UTF-8\"\n   LC_IDENTIFICATION=\"zh_CN.UTF-8\"\n   LC_ALL=\n   ```\n\n   **解决办法**\n\n   ```bash\n   # 中文\n   # vim /etc/profile.d/locale.sh\n   export LC_CTYPE=zh_CN.UTF-8\n   export LC_ALL=zh_CN.UTF-8\n    \n   # vim /etc/locale.conf\n   LANG=zh_CN.UTF-8\n    \n   # vim /etc/sysconfig/i18n\n   LANG=zh_CN.UTF-8\n    \n   # vim /etc/environment\n   LANG=zh_CN.UTF-8\n   LC_ALL=zh_CN.UTF-8\n   \n   # 英文\n   # vim /etc/profile.d/locale.sh\n   export LC_CTYPE=en_US.UTF-8\n   export LC_ALL=en_US.UTF-8\n    \n   # vim /etc/locale.conf\n   LANG=en_US.UTF-8\n    \n   # vim /etc/sysconfig/i18n\n   LANG=en_US.UTF-8\n    \n   # vim /etc/environment\n   LANG=en_US.UTF-8\n   LC_ALL=en_US.UTF-8\n   \n   source /etc/profile.d/locale.sh\n   source /etc/locale.conf\n   source /etc/sysconfig/i18n\n   source /etc/environment\n   ```\n","lastmodified":"2023-05-09T16:33:58.30736679Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F":{"title":"操作系统","content":"\n## 进程间通信\n\n1、无名管道( pipe )；2、高级管道(popen)；3、有名管道 (named pipe)；4、消息队列( message queue )；5、信号量( semophore )；7、共享内存( shared memory )；8、套接字( socket )。\n\n### 线程间通信\n\n1.锁机制：包括互斥锁、条件变量、读写锁 2.信号量机制(Semaphore) 3.信号机制(Signal)\n\n## 内核态与用户态的信息交互如何实现\n\n出于效率和代码大小的考虑，内核程序不能使用标准库函数（当然还有其它的顾虑，详细原因请查阅参考资料2）因此内核开发不如用户程序开发那么方便。\n\n内核中启动用户程序还是要通过execve这个系统调用原形，只是此时的调用发生在内核空间，而一般的系统调用则在用户空间进行。\n\n### procfs(/proc)\u003c--sysctl\n\nprocfs 是 进程文件系统 的缩写，它本质上是一个伪文件系统，为什么说是 伪 文件系统呢？因为它不占用外部存储空间，只是占用少量的内存，通常是挂载在 /proc 目录下。  \n我们在该目录下看到的一个文件，实际上是一个内核变量。内核就是通过这个目录，以文件的形式展现自己的内部信息，相当于 /proc 目录为用户态和内核态之间的交互搭建了一个桥梁，用户态读写 /proc 下的文件，就是读写内核相关的配置参数。\n\n### netlink\u003c--iproute2\n\nnetlink 是 Linux 用户态与内核态通信最常用的一种方式。Linux kernel 2.6.14 版本才开始支持。它本质上是一种 socket，常规 socket 使用的标准 API，在它身上同样适用。\n\n## IO原理 epoll等\n\n### IO数据到达网卡之后的流程\n\n1. 输入Frames通过**DMA模块**把数据拷贝到内存（不需要消耗CPU资源），直接进行内存映射。之所以这样做，是因为网卡没有大量的内存空间，只能做简单的缓冲，所以必须赶紧将它们保存下来。\n2. Buffer是一个双向链表作为缓冲区，看上去像一个有很多个凹槽的线性结构，每个凹槽（节点）可以存储一个封包，这个封包可以从网络层看（IP 封包），也可以从传输层看（TCP 封包）。触发CPU**中断**交给操作系统处理不断地从Buffer中取出数据，数据通过一个**协议栈**，你可以把它理解成很多个协议的集合。协议栈中数据封包找到对应的协议程序处理完之后，就会形成Socket文件，一个 Socket 文件内部类似一个双向的管道，进程读取 Socket 文件，可以从 Buffer 中对应节点读走数据。\n\n    高并发的请求量级实在太大，有可能把 Buffer 占满，此时，操作系统就会拒绝服务  \n![](Pasted%20image%2020230329001817.png)\n\n### select/poll/epoll\n\n**进程如何监听关注集合的状态变化，比如说在有数据进来，如何通知到这个进程**  \n一个线程需要处理所有关注的 Socket 产生的变化，或者说消息。实际上一个线程要处理很多个文件的 I/O。**所有关注的 Socket 状态发生了变化，都由一个线程去处理，构成了 I/O 的多路复用问题**。  \n**一个 Socket 文件，可以由多个进程使用；而一个进程，也可以使用多个 Socket 文件。进程和 Socket 之间是多对多的关系。**  \n这样在进程内部就需要一个数据结构来描述自己会关注哪些 Socket 文件的哪些事件（读、写、异常等）。  \n**一种是利用线性结构**，比如说数组、链表等，这类结构的查询需要遍历。每次内核产生一种消息，就遍历这个线性结构，**select** 和 poll 都采用线性结构，每次 select 操作会阻塞当前线程，在阻塞期间所有操作系统产生的每个消息，都会通过遍历的手段查看是否在集合中。**poll** 虽然优化了编程模型，但是从性能角度分析，它和 select 差距不大。因为内核在产生一个消息之后，依然需要遍历 poll 关注的所有文件描述符来确定这条消息是否跟用户程序相关。  \n**另一种是索引结构**，内核发生了消息可以通过索引结构马上知道这个消息进程关不关注。**epoll 将进程关注的文件描述符存入一棵二叉搜索树，通常是红黑树的实现**。\n\n### 总结与同步异步阻塞非阻塞\n\n**总结一下，select/poll 是阻塞模型，epoll 是非阻塞模型**。**当然，并不是说非阻塞模型性能就更好。在多数情况下，epoll 性能更好是因为内部有红黑树的实现**。\n\n上面的模型当中，select/poll 是阻塞（Blocking）模型，epoll 是非阻塞（Non-Blocking）模型。**阻塞和非阻塞强调的是线程的状态**，所以阻塞就是触发了线程的阻塞状态，线程阻塞了就停止执行，并且切换到其他线程去执行，直到触发中断再回来。\n\n还有一组概念是同步（Synchrounous）和异步（Asynchrounous），select/poll/epoll 三者都是同步调用。\n\n**同步强调的是顺序，所谓同步调用，就是可以确定程序执行的顺序的调用。比如说执行一个调用，知道调用返回之前下一行代码不会执行。这种顺序是确定的情况，就是同步。\n\n而异步调用则恰恰相反，**异步调用不明确执行顺序**。比如说一个回调函数，不知道何时会回来。异步调用会加大程序员的负担，因为我们习惯顺序地思考程序。因此，我们还会发明像协程的 yield 、迭代器等将异步程序转为同步程序。\n\n由此可见，**非阻塞不一定是异步，阻塞也未必就是同步**。比如一个带有回调函数的方法，阻塞了线程 100 毫秒，又提供了回调函数，那这个方法是异步阻塞。\n\n## 零拷贝\n\n![](Pasted%20image%2020230329003814.png)  \n从上面可以看出，应用进程的每一次写操作，都会把数据写到用户空间的缓冲区中，再由CPU将数据拷贝到系统内核的缓冲区中，之后再由DMA将这份数据拷贝到网卡中，最后由网卡发送出去。也就是说，一次写操作数据要拷贝两次才能通过网卡发送出去，而用户进程的读操作则是将整个流程反过来，数据同样会拷贝两次才能让应用程序读取到数据。  \n![](Pasted%20image%2020230329003959.png)  \n**零拷贝技术**是指计算机执行操作时，CPU不需要先将数据从某处内存复制到另一个特定区域，所谓的零拷贝，就是取消用户空间和内核空间之间的数据拷贝操作，应用程序每一次的读写操作，可以通过一种方式，直接将数据写入内核或者从内核中读取数据，再通过DMA将内核中的数据拷贝到网卡，或者将网卡中的数据copy到内核。\n\n### 实现方式\n\n![](Pasted%20image%2020230329004142.png)  \n分为三种：\n\n1. 让数据拷贝完全在内核里进行，通过增加新的系统调用，比如mmap()，sendfile() 以及 splice()，其核心原理都是通过虚拟内存来解决的。\n2. **绕过内核的直接IO**，内核在传输过程中只负责一些管理和辅助的工作\n3. **内核缓冲区和用户缓冲区之间的传输优化**：这种方式侧重于在用户进程的缓冲区和操作系统的页缓存之间的 CPU 拷贝的优化。这种方法延续了以往那种传统的通信方式，但更灵活。\n","lastmodified":"2023-05-09T16:33:58.30736679Z","tags":[]},"/%E6%95%B0%E5%AD%97%E8%8A%B1%E5%9B%AD":{"title":"数字花园","content":"\n## 定义\n\n数字花园是通过借助网络数字工具，对想法、笔记和思考等一切你感兴趣的信息或进行收集、整理和创作，文字之间用标签或链接创建连接，形成一座独具个人浓厚色彩的信息与知识型花园。\n\n这个花园是开放的，不可避免地至少要经常打理。\n\n**数字花园介于笔记本和博客之间的交叉空间，具有半公开性质**。\n\n不像笔记内容的纯私人性一样，数字花园鼓励用户发布自己的想法、草稿，这在很大程度上降低了我们的发布压力。\n\n同时，数字花园不像博客那样完全公开，以建立个人品牌作为内容发布的主要目的。\n","lastmodified":"2023-05-09T16:33:58.30736679Z","tags":["花园"]},"/%E6%95%B0%E6%8D%AE%E5%BA%93%E9%83%A8%E7%BD%B2%E8%BF%90%E7%BB%B4":{"title":"数据库、部署、运维","content":"\n[[Docker基础]]\n\n[[PM2]]  \n[[Nginx]]  \n[[Redis原理]]\n\n[[Mysql 原理]]  \n[[Mysql 命令速查]]\n","lastmodified":"2023-05-09T16:33:58.30736679Z","tags":[]},"/%E6%A0%87%E6%B3%A8":{"title":"标注","content":"\n```\n\u003e [!NOTE] Note title\n\u003e Information\n```\n\n\u003e [!NOTE] Note title  \n\u003e Information\n\n```\n\u003e [!WARNING] A warning\n\u003e This is a warning\n```\n\n\u003e [!WARNING] A warning  \n\u003e This is a warning\n\n```\n\u003e [!NOTE]+ Open by default\n\u003e Folding/Collapsable callout\n```\n\n\u003e [!NOTE]+ Open by default  \n\u003e Folding/Collapsable callout\n\n```\n\u003e [!FAQ]-Closed by default\n\u003e Folding/Collapsable callout\n```\n\n\u003e [!FAQ]- Closed by default  \n\u003e Folding/Collapsable callout\n\n```\n\u003e [!TIP] Nested callouts\n\u003e Text inside the tip callout\n\u003e \u003e [!EXAMPLE] Inner callout\n\u003e \u003e Multiple nesting layers\n\u003e \u003e \u003e [!TODO] Inner inner callout\n```\n\n\u003e [!TIP] Nested callouts  \n\u003e Text inside the tip callout\n\u003e\n\u003e \u003e [!EXAMPLE] Inner callout  \n\u003e \u003e Multiple nesting layers\n\u003e \u003e\n\u003e \u003e \u003e [!TODO] Inner inner callout\n","lastmodified":"2023-05-09T16:33:58.30736679Z","tags":["花园"]},"/%E6%B5%8F%E8%A7%88%E5%99%A8%E6%8C%87%E7%BA%B9-notion":{"title":"浏览器指纹 notion","content":"\n## 什么是浏览器指纹?\n\n我们常说的指纹，都是指人们手指上的指纹，因具有唯一性，所以可以被用来标识一个人的唯一身份。而浏览器指纹是指仅通过浏览器的各种[信息](http://mp.weixin.qq.com/s?__biz=MjM5MTA1MjAxMQ==\u0026mid=200264691\u0026idx=3\u0026sn=f790116607b7a8372a0af1039ffa8a1e\u0026scene=21#wechat_redirect)，如CPU核心数、显卡信息、系统字体、屏幕分辨率、浏览器插件等组合成的一个字符串，就能近乎绝对定位一个[用户](http://mp.weixin.qq.com/s?__biz=MjM5MTA1MjAxMQ==\u0026mid=200035502\u0026idx=1\u0026sn=8d4183f93cef06d394202b5bebee6c77\u0026scene=21\u0026subscene=126#wechat_redirect)，就算使用浏览器的隐私窗口模式，也无法避免。\n\n这是一个被动的识别方式。也就是说，理论上你访问了某一个网站，那么这个网站就能识别到你，虽然不知道你是谁，但你有一个唯一的指纹，将来无论是广告投放、精准推送、安全防范，还是其他一些关于隐私的事情，都非常方便。\n\n## 实现浏览器指纹的技术点有哪些?\n\n### 1、基本指纹\n\n浏览器基本指纹是任何浏览器都具有的特征标识，比如屏幕分辨率、硬件类型、操作系统、用户代理（User agent）、系统字体、语言、浏览器插件 、浏览器扩展、浏览器设置 、时区差等众多信息，这些指纹信息“类似”人类的身高、年龄等，有很大的冲突概率，只能作为辅助识别。可以在该网址进行查看本地浏览器的基本特征，https://www.whatismybrowser.com/\n\n### 2、高级指纹\n\n浏览器高级指纹与基本指纹的区别是，基本指纹就像是人的外貌特征，外貌可以用男女、身高、体重区分，然而这些特征不能对某个人进行唯一性标识，仅使用基本指纹也无法对客户端进行唯一性判定，基于HTML5的诸多高级功能就能[生成](http://mp.weixin.qq.com/mp/appmsg/show?__biz=MjM5MTA1MjAxMQ==\u0026appmsgid=10012058\u0026itemidx=1\u0026sign=c4200f3a5feb6021da8c18cdbce1d7d8\u0026scene=21\u0026subscene=126#wechat_redirect)高级指纹了。\n\n#### Canvas指纹\n\n说到高级指纹，不得不提Canvas指纹，Canvas（画布）是HTML5中一种动态绘图的标签，可以使用其生成甚至处理高级图片。\n\nCanvas指纹的原理大致如下：\n\n相同的HTMLCanvasElement元素绘制操作，在不同操作系统、不同浏览器上，产生的图片内容不完全相同。在图片格式上，不同浏览器使用了不同的图形处理引擎、不同的图片导出选项、不同的默认压缩级别等。在像素级别来看，操作系统各自使用了不同的设置和算法来进行抗锯齿和子像素渲染操作。即使相同的绘图操作，产生的图片数据的CRC检验也不相同。Canvas几乎已被所有主流浏览器支持，可以通过大部分的PC、平板、智能手机访问。\n\n在线测试地址：https://www.browserleaks.com/canvas，可查看浏览器的Canvas唯一性字符串。\n\n#### WebGL指纹\n\n通过HTMLCanvasElement元素可以获取到Webgl对象（canvas.getContext(\"webgl\")）,通过此对象可以获取到用户的硬件信息，比如显卡名称、显卡型号、显卡制造商等，比如：ANGLE (NVIDIA GeForce GTX 1050 Ti Direct3D11 vs50 ps50)，Google Inc.。\n\n由于硬件一般是不会随意更换的，有些是电脑买来到电脑报废就没更换过硬件，电脑硬件种类也比较多，虽然非常大的碰撞率，但是依然可以被用来当做用户指纹的一部分，收集用户的信息也多，就越能代表用户的唯一指纹，这点不可忽视。\n\n#### AudioContext指纹\n\nHTML5提供给JavaScript编程用的Audio API则让开发者有能力在代码中直接操作原始的音频流数据，对其进行任意生成、加工、再造，诸如提高音色，改变音调，音频分割等多种操作，甚至可称为网页版的Adobe Audition。\n\nAudioContext指纹原理大致如下：\n\n方法一：生成音频信息流(三角波)，对其进行FFT变换，计算SHA值作为指纹。\n\n方法二：生成音频信息流（正弦波），进行动态压缩处理，计算MD5值。\n\n两种方法都是在音频输出到音频设备之前进行清除，用户根本就毫无察觉就被获取了指纹。\n\nAudioContext指纹基本原理：\n\n主机或浏览器硬件或软件的细微差别，导致音频信号的处理上的差异，相同器上的同款浏览器产生相同的音频输出，不同机器或不同浏览器产生的音频输出会存在差异。\n\n从上可以看出AudioContext和Canvas指纹原理很类似，都是利用硬件或软件的差异，前者生成音频，后者生成图片，然后计算得到不同哈希值来作为标识。音频指纹测试地址：https://audiofingerprint.openwpm.com\n\n#### WebRTC指纹\n\nWebRTC（网页实时通信，Web Real Time Communication），是可以让浏览器有音视频实时通信的能力，它提供了三个主要的API来让JS可以实时获取和交换音视频数据，MediaStream、RTCPeerConnection和RTCDataChannel。当然如果要使用WebRTC获得通信能力，用户的真实ip就得暴露出来（NAT穿透），所以RTCPeerConnection就提供了这样的API，直接使用JS就可以拿到用户的IP地址。用户的内网IP地址也是大多数情况下不会改变，所以也是可以用来当做用户指纹的其中一个因子。\n\n## 综合指纹\n\n上面几点都说了浏览器指纹大致有哪些，还没有完全说完，只是一部分，但是零散的指纹信息并不能真正的定位到唯一用户，并不能用来代表一个用户的唯一身份（用户指纹）。\n\n综合指纹是指将所有的用户浏览器信息组合起来，就可以近乎99%以上的准确率定位标识用户，综合指纹大致有如下：\n\n- 基本指纹（UserAgent、屏幕分辨率、CPU核心数、内存大小、插件信息、语言等）\n- 高级指纹部分（Canvas指纹、Webgl指纹、AudioContext指纹、WebRTC指纹、字体指纹等）\n- 地理位置、时区、DNS、SSL证书等信息。\n\n将以上几点组合起来就可以生成综合指纹（用户指纹），就可以达到前面说的99%以上可以定位唯一用户。\n\n## 如何防止被生成“用户指纹”？\n\n前面我们说了一大堆网站如何使用各种技术来“生成”用户指纹，来标识唯一用户，那么下面我们来说说，如何避免被网站“生成”唯一用户指纹。\n\n常用的手段是，通过浏览器的扩展插件，阻止网站获取各种信息，或者返回个假的数据，这种方式是在网页加载前就执行一段JS代码，更改、重写、HOOK了js的各个函数来实现的，因为JS的灵活性给这种方式提供的可能。但是这种方式始终是表层的，使用JS修改是能防止大部分网站的生成唯一指纹，但是是有手段可以检测出来是否“作弊”的。\n\n更好的手段是从浏览器底层做处理，从浏览器底层修改API使得这些在js层获取的信息并不唯一，不管如何组合都不能生成一个唯一的代表用户的指纹。比如：猫头鹰浏览器\n\n猫头鹰浏览器是基于chromium代码修改编译的浏览器，从底层对各种API做了修改，可以交给用户自定义返回各种数据，比如Canvas、Webgl、AudioContext、WebRTC、字体、UserAgent、屏幕分辨率、CPU核心数、内存大小、插件信息、语言等信息，这样就可以完全避免被“生成”唯一用户指纹了。\n\n## 常用检测网站\n\n- BrowserLeaks - Web Browser Fingerprinting - Browsing Privacy\n- 浏览器环境校验\n- AmIUnique\n\n####  \n","lastmodified":"2023-05-09T16:33:58.30736679Z","tags":[]},"/%E6%B5%8F%E8%A7%88%E5%99%A8%E7%9B%B8%E5%85%B3":{"title":"浏览器相关","content":"\n## **1. 如何实现浏览器内多个标签页之间的通信?**\n\n1. WebSocket SharedWorker\n2. 也可以调用 localstorge、cookies 等本地存储方式。 localstorge 在另一个浏览上下文里被添加、修改或删除时，它都会触发一个事件，我们通过监听事件，控制它的值来进行页面信息通信。\n\n注意：Safari 在无痕模式下设置 localstorge 值时会抛出QuotaExceededError 的异常~~~~\n\n## **2.webSocket如何兼容低浏览器？**\n\n1. Adobe Flash Socket ActiveX HTMLFile (IE) 基于 multipart 编码发送 XHR 基于长轮询的 XHR\n2. 引用WebSocket.js这个文件来兼容低版本浏览器。\n\n## **3.页面可见性（Page Visibility）API 可以有哪些用途？**\n\n1. 通过visibility state的值得检测页面当前是否可见，以及打开网页的时间。\n2. 在页面被切换到其他后台进程时，自动暂停音乐或视频的播放。\n\n## 4.前端性能优化？\n\n```\n前端性能优化主要是为了提高页面的加载速度，优化用户的访问体验。我认为可以从这些方面来进行优化。\n\n 第一个方面是页面的内容方面\n\n （1）通过文件合并、css 雪碧图、使用 base64 等方式来减少 HTTP 请求数，避免过多的请求造成等待的情况。\n\n （2）通过 DNS 缓存等机制来减少 DNS 的查询次数。\n\n （3）通过设置缓存策略，对常用不变的资源进行缓存。\n\n （4）使用延迟加载的方式，来减少页面首屏加载时需要请求的资源。延迟加载的资源当用户需要访问时，再去请求加载。\n\n （5）通过用户行为，对某些资源使用预加载的方式，来提高用户需要访问资源时的响应速度。\n\n 第二个方面是服务器方面\n\n （1）使用 CDN 服务，来提高用户对于资源请求时的响应速度。\n\n （2）服务器端启用 Gzip、Deflate 等方式对于传输的资源进行压缩，减小文件的体积。\n\n （3）尽可能减小 cookie 的大小，并且通过将静态资源分配到其他域名下，来避免对静态资源请求时携带不必要的 cookie\n\n 第三个方面是 CSS 和 JavaScript 方面\n\n （1）把样式表放在页面的 head 标签中，减少页面的首次渲染的时间。\n\n （2）避免使用 @import 标签。\n\n （3）尽量把 js 脚本放在页面底部或者使用 defer 或 async 属性，避免脚本的加载和执行阻塞页面的渲染。\n\n （4）通过对 JavaScript 和 CSS 的文件进行压缩，来减小文件的体积。\n```\n","lastmodified":"2023-05-09T16:33:58.30736679Z","tags":[]},"/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B":{"title":"1. Scikit-learn与特征工程","content":"\n![image-20210306160955457](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170111.png)\n\n# 1. Scikit-learn与特征工程\n\n数据决定了机器学习的上限，而算法只是尽可能逼近这个上限”，这句话很好的阐述了数据在机器学习中的重要性。大部分直接拿过来的数据都是特征不明显的、没有经过处理的或者说是存在很多无用的数据，那么需要进行一些特征处理，特征的缩放等等，满足训练数据的要求。\n\n我们将初次接触到Scikit-learn这个机器学习库的使用\n\n![image-20200623201306065](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170112.png)\n\n![image-20200623200615498](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170113.png)\n\n\u003e Scikit-learn\n\u003e\n\u003e - Python语言的机器学习工具\n\u003e - 所有人都适用，可在不同的上下文中重用\n\u003e - 基于NumPy、SciPy和matplotlib构建\n\u003e - 开源、商业可用 - BSD许可\n\u003e - 目前稳定版本0.18\n\n自2007年发布以来，scikit-learn已经成为最给力的Python机器学习库（library）了。scikit-learn支持的机器学习算法包括分类，回归，降维和聚类。还有一些特征提取（extracting features）、数据处理（processing data）和模型评估（evaluating models）的模块。作为Scipy库的扩展，scikit-learn也是建立在Python的NumPy和matplotlib库基础之上。NumPy可以让Python支持大量多维矩阵数据的高效操作，matplotlib提供了可视化工具，SciPy带有许多科学计算的模型。     \n\nscikit-learn文档完善，容易上手，丰富的API，使其在学术界颇受欢迎。开发者用scikit-learn实验不同的算法，只要几行代码就可以搞定。scikit-learn包括许多知名的机器学习算法的实现，包括LIBSVM和LIBLINEAR。还封装了其他的Python库，如自然语言处理的NLTK库。另外，scikit-learn内置了大量数据集，允许开发者集中于算法设计，节省获取和整理数据集的时间。\n\n安装的话参考下面步骤： 创建一个基于Python3的虚拟环境：\n\n```\nmkvirtualenv -p /usr/local/bin/python3.6 ml3\n```\n\n在ubuntu的虚拟环境当中运行以下命令\n\n```\npip3 install Scikit-learn\n```\n\n然后通过导入命令查看是否可以使用：\n\n```python\nimport sklearn\n```\n\n### 数据的特征工程\n\n从数据中抽取出来的对预测结果有用的信息，通过专业的技巧进行数据处理，是的特征能在机器学习算法中发挥更好的作用。优质的特征往往描述了数据的固有结构。 最初的原始特征数据集可能太大，或者信息冗余，因此在机器学习的应用中，一个初始步骤就是选择特征的子集，或构建一套新的特征集，减少功能来促进算法的学习，提高泛化能力和可解释性。\n\n例如：你要查看不同地域女性的穿衣品牌情况，预测不同地域的穿衣品牌。如果其中含有一些男性的数据，是不是要将这些数据给去除掉\n\n![image-20200623201321325](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170114.png)\n\n### 特征工程的意义\n\n- 更好的特征意味着更强的鲁棒性\n- 更好的特征意味着只需用简单模型\n- 更好的特征意味着更好的结果\n\n### 特征工程之特征处理\n\n特征工程中最重要的一个环节就是特征处理，特征处理包含了很多具体的专业技巧\n\n- 特征预处理\n  - 单个特征\n    - 归一化\n    - 标准化\n    - 缺失值\n  - 多个特征\n    - 降维\n      - PCA\n\n### 特征工程之特征抽取与特征选择\n\n如果说特征处理其实就是在对已有的数据进行运算达到我们目标的数据标准。特征抽取则是将任意数据格式（例如文本和图像）转换为机器学习的数字特征。而特征选择是在已有的特征中选择更好的特征。后面会详细介绍特征选择主要区别于降维。\n\n## 1.1 数据的来源与类型\n\n大部分的数据都来自已有的数据库，如果没有的话也可以交给很多爬虫工程师去采集，来提供。也可以来自平时的记录，反正数据无处不在，大都是可用的。\n\n### 数据的类型\n\n按照**机器学习的数据分类**我们可以将数据分成：\n\n- 标称型：标称型目标变量的结果只在有限目标集中取值，如真与假(标称型目标变量主要用于分类)\n- 数值型：数值型目标变量则可以从无限的数值集合中取值，如0.100，42.001等 (数值型目标变量主要用于回归分析)\n\n按照**数据的本身分布特性**\n\n- 离散型\n- 连续型\n\n那么什么是离散型和连续型数据呢？首先连续型数据是有规律的,离散型数据是没有规律的\n\n- 离散变量是指其数值只能用自然数或整数单位计算的则为离散变量.例如，班级人数、进球个数、是否是某个类别等等\n- 连续型数据是指在指定区间内可以是任意一个数值,例如，票房数据、花瓣大小分布数据\n\n## 1.2 数据的特征抽取\n\n现实世界中多数特征都不是连续变量，比如分类、文字、图像等，为了对非连续变量做特征表述，需要对这些特征做数学化表述，因此就用到了特征提取. `sklearn.feature_extraction`提供了特征提取的很多方法\n\n### 分类特征变量提取\n\n我们将城市和环境作为字典数据，来进行特征的提取。\n\n`sklearn.feature_extraction.DictVectorizer(sparse = True)`\n\n将映射列表转换为Numpy数组或scipy.sparse矩阵\n\n- sparse 是否转换为scipy.sparse矩阵表示，默认开启\n\n![image-20210316201130096](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170115.png)\n\n**方法**\n\n**fit_transform(X,y)**\n\n应用并转化映射列表X，y为目标类型\n\n**inverse_transform(X[, dict_type])**\n\n将Numpy数组或scipy.sparse矩阵转换为映射列表\n\n```python\nfrom sklearn.feature_extraction import DictVectorizer\nonehot = DictVectorizer()  # 如果结果不用toarray，请开启sparse=False\n    instances = [{'city': '北京', 'temperature': 100}, {'city': '上海', 'temperature': 60},\n                 {'city': '深圳', 'temperature': 30}]\n    X = onehot.fit_transform(instances).toarray()\n    print(onehot.inverse_transform(X))\n    print(onehot.get_feature_names())\n    print(X)\n```\n\n### 文本特征提取（只限于英文）\n\n文本的特征提取应用于很多方面，比如说文档分类、垃圾邮件分类和新闻分类。那么文本分类是通过词是否存在、以及词的概率（重要性）来表示。\n\n**(1)文档的中词的出现**\n\n数值为1表示词表中的这个词出现，为0表示未出现\n\n**sklearn.feature_extraction.text.CountVectorizer()**\n\n将文本文档的集合转换为计数矩阵（scipy.sparse matrices）\n\n**方法**\n\n**fit_transform(raw_documents,y)**\n\n学习词汇词典并返回词汇文档矩阵\n\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\ncontent = [\"life is short,i like python\",\"life is too long,i dislike python\"]\nvectorizer = CountVectorizer()\nprint(vectorizer.fit_transform(content).toarray())\n```\n\n![image-20210316200811022](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170116.png)\n\n需要toarray()方法转变为numpy的数组形式\n\n\u003e 温馨提示：每个文档中的词，只是整个语料库中所有词，的很小的一部分，这样造成特征向量的稀疏性（很多值为0）为了解决存储和运算速度的问题，使用Python的scipy.sparse矩阵结构\n\n**(2)TF-IDF表示词的重要性**\n\nTfidfVectorizer会根据指定的公式将文档中的词转换为概率表示。（朴素贝叶斯介绍详细的用法）\n\n**class sklearn.feature_extraction.text.TfidfVectorizer()**\n\n**方法**\n\n**fit_transform(raw_documents,y)**\n\n学习词汇和idf，返回术语文档矩阵。\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ncontent = [\"life is short,i like python\",\"life is too long,i dislike python\"]\nvectorizer = TfidfVectorizer(stop_words='english')\nprint(vectorizer.fit_transform(content).toarray())\nprint(vectorizer.vocabulary_)\n```\n\n### 文本特征提取（中文）\n\n![image-20200623201427282](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170117.png)\n\n![image-20210306161347051](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170118.png)\n\n```python\ndef cutword():\n    con1 = jieba.cut(\"今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今。\")\n    con2 = jieba.cut(\"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\")\n    con3 = jieba.cut(\"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\")\n    # 转换成列表\n    content1 = list(con1)\n    content2 = list(con2)\n    content3 = list(con3)\n    # 吧列表转换成字符串\n    c1 = ' '.join(content1)\n    c2 = ' '.join(content2)\n    c3 = ' '.join(content3)\n    return c1, c2, c3\n```\n\n![image-20210316201043435](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170119.png)\n\n## 1.3 数据的特征预处理\n\n![image-20210306161652247](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170120.png)\n\n### 单个特征\n\n**（1）归一化**\n\n归一化首先在特征（维度）非常多的时候，可以防止某一维或某几维对数据影响过大，也是为了把不同来源的数据统一到一个参考区间下，这样比较起来才有意义，其次可以程序可以运行更快。 例如：一个人的身高和体重两个特征，假如体重50kg，身高175cm,由于两个单位不一样，数值大小不一样。如果比较两个人的体型差距时，那么身高的影响结果会比较大，k-临近算法会有这个距离公式。\n\n**min-max方法**\n\n常用的方法是通过对原始数据进行线性变换把数据映射到[0,1]之间，变换的函数为：\n\n![image-20210306161529396](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170121.png)\n\n![image-20210306161734975](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170122.png)\n\n其中min是样本中最小值，max是样本中最大值，注意在**数据流场景下最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景**。\n\n![image-20210306161914681](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170123.png)\n\n`sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)…)`\n\n- min-max自定义处理\n\n这里我们使用相亲约会对象数据在MatchData.txt，这个样本时男士的数据，三个特征，玩游戏所消耗时间的百分比、每年获得的飞行常客里程数、每周消费的冰淇淋公升数。然后有一个 所属类别，被女士评价的三个类别，不喜欢、魅力一般、极具魅力。 首先导入数据进行矩阵转换处理.\n\n![image-20200623201658337](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170124.png)\n\n```python\nimport numpy as np\n\ndef data_matrix(file_name):\n  \"\"\"\n  将文本转化为matrix\n  :param file_name: 文件名\n  :return: 数据矩阵\n  \"\"\"\n  fr = open(file_name)\n  array_lines = fr.readlines()\n  number_lines = len(array_lines)\n  return_mat = zeros((number_lines, 3))\n  # classLabelVector = []\n  index = 0\n  for line in array_lines:\n    line = line.strip()\n    list_line = line.split('\\t')\n    return_mat[index,:] = list_line[0:3]\n    # if(listFromLine[-1].isdigit()):\n    #     classLabelVector.append(int(listFromLine[-1]))\n    # else:\n    #     classLabelVector.append(love_dictionary.get(listFromLine[-1]))\n    # index += 1\n  return return_mat\n```\n\n输出结果为\n\n```python\n[[  4.09200000e+04   8.32697600e+00   9.53952000e-01]\n [  1.44880000e+04   7.15346900e+00   1.67390400e+00]\n [  2.60520000e+04   1.44187100e+00   8.05124000e-01]\n ...,\n [  2.65750000e+04   1.06501020e+01   8.66627000e-01]\n [  4.81110000e+04   9.13452800e+00   7.28045000e-01]\n [  4.37570000e+04   7.88260100e+00   1.33244600e+00]]\n```\n\n我们查看数据集会发现，有的数值大到几万，有的才个位数，同样如果计算两个样本之间的距离时，其中一个影响会特别大。也就是说飞行里程数对于结算结果或者说相亲结果影响较大，**但是统计的人觉得这三个特征同等重要**，所以需要将数据进行这样的处理。\n\n这样每个特征任意的范围将变成[0,1]的区间内的值，或者也可以根据需求处理到[-1,1]之间，我们再定义一个函数，进行这样的转换。\n\n```python\ndef feature_normal(data_set):\n    \"\"\"\n    特征归一化\n    :param data_set:\n    :return:\n    \"\"\"\n    # 每列最小值\n    min_vals = data_set.min(0)\n    # 每列最大值\n    max_vals = data_set.max(0)\n    ranges = max_vals - min_vals\n    norm_data = np.zeros(np.shape(data_set))\n    # 得出行数\n    m = data_set.shape[0]\n    # 矩阵相减\n    norm_data = data_set - np.tile(min_vals, (m,1))\n    # 矩阵相除\n    norm_data = norm_data/np.tile(ranges, (m, 1)))\n    return norm_data\n```\n\n输出结果为\n\n```python\n[[ 0.44832535  0.39805139  0.56233353]\n [ 0.15873259  0.34195467  0.98724416]\n [ 0.28542943  0.06892523  0.47449629]\n ...,\n [ 0.29115949  0.50910294  0.51079493]\n [ 0.52711097  0.43665451  0.4290048 ]\n [ 0.47940793  0.3768091   0.78571804]]\n```\n\n这样得出的结果都非常相近，这样的数据可以直接提供测试验证了。\n\n**注意在特定场景下最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。**\n\n- min-max的scikit-learn处理\n\nscikit-learn.preprocessing中的类MinMaxScaler，将数据矩阵缩放到[0,1]之间\n\n```python\n\u003e\u003e\u003e X_train = np.array([[ 1., -1.,  2.],\n...                     [ 2.,  0.,  0.],\n...                     [ 0.,  1., -1.]])\n...\n\u003e\u003e\u003e min_max_scaler = preprocessing.MinMaxScaler()\n\u003e\u003e\u003e X_train_minmax = min_max_scaler.fit_transform(X_train)\n\u003e\u003e\u003e X_train_minmax\narray([[ 0.5       ,  0.        ,  1.        ],\n       [ 1.        ,  0.5       ,  0.33333333],\n       [ 0.        ,  1.        ,  0.        ]])\n```\n\n**（3）标准化**\n\n特点：通过对原始数据进行变换把数据变换到均值为0,方差为1范围内。\n\n常用的方法是z-score标准化，经过处理后的数据均值为0，标准差为1，处理方法是：\n\n![image-20210306162207644](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170125.png)\n\n![image-20210306162246784](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170126.png)\n\n其中*μ*是样本的均值，*σ*是样本的标准差，它们可以通过现有的样本进行估计，在已有的样本足够多的情况下比较稳定，适合嘈杂的数据场景。\n\nsklearn中提供了StandardScalar类实现列标准化:\n\n```python\nIn [2]: import numpy as np\n\nIn [3]: X_train = np.array([[ 1., -1.,  2.],[ 2.,  0.,  0.],[ 0.,  1., -1.]])\n\nIn [4]: from sklearn.preprocessing import StandardScaler\n\nIn [5]: std = StandardScaler()\n\nIn [6]: X_train_std = std.fit_transform(X_train)\n\nIn [7]: X_train_std\nOut[7]:\narray([[ 0.        , -1.22474487,  1.33630621],\n       [ 1.22474487,  0.        , -0.26726124],\n       [-1.22474487,  1.22474487, -1.06904497]])\n```\n\n![image-20200623201857964](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170127.png)\n\n**（3）缺失值**\n\n由于各种原因，许多现实世界的数据集包含缺少的值，通常编码为空白，NaN或其他占位符。然而，这样的数据集与scikit的分类器不兼容，它们假设数组中的所有值都是数字，并且都具有和保持含义。使用不完整数据集的基本策略是丢弃包含缺失值的整个行和/或列。然而，这是以丢失可能是有价值的数据（即使不完整）的代价。更好的策略是估算缺失值，即从已知部分的数据中推断它们。\n\n(1)填充缺失值 使用sklearn.preprocessing中的Imputer类进行数据的填充(旧API)\n\n新：`from sklearn.impute import SimpleImputer`\n\n```python\nclass Imputer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)\n    \"\"\"\n    用于完成缺失值的补充\n\n    :param param missing_values: integer or \"NaN\", optional (default=\"NaN\")\n        丢失值的占位符，对于编码为np.nan的缺失值，使用字符串值“NaN”\n\n    :param strategy: string, optional (default=\"mean\")\n        插补策略\n        如果是“平均值”，则使用沿轴的平均值替换缺失值\n        如果为“中位数”，则使用沿轴的中位数替换缺失值\n        如果“most_frequent”，则使用沿轴最频繁的值替换缺失\n\n    :param axis: integer, optional (default=0)\n        插补的轴\n        如果axis = 0，则沿列排列\n        如果axis = 1，则沿行排列\n    \"\"\"\n\u003e\u003e\u003e import numpy as np\n\u003e\u003e\u003e from sklearn.impute import SimpleImputer\n\u003e\u003e\u003e imp = Imputer(missing_values='NaN', strategy='mean')\n\u003e\u003e\u003e imp.fit([[1, 2], [np.nan, 3], [7, 6]])\nImputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)\n\u003e\u003e\u003e X = [[np.nan, 2], [6, np.nan], [7, 6]]\n\u003e\u003e\u003e print(imp.transform(X))                          \n[[ 4.          2.        ]\n [ 6.          3.666...]\n [ 7.          6.        ]]\n```\n\n### 多个特征\n\n#### **降维**\n\n高维度数据容易出现的问题:特征之间通常是线性相关的。\n\nPCA（Principal component analysis），主成分分析。特点是保存数据集中对方差影响最大的那些特征，PCA极其容易受到数据中特征范围影响，所以在运用PCA前一定要做特征标准化，这样才能保证每维度特征的重要性等同。\n\n目的：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。\n\n作用：可以削减回归分析或者聚类分析中特征的数量\n\n**sklearn.decomposition.PCA**\n\n![image-20210306163131804](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170128.png)\n\n```python\nclass PCA(sklearn.decomposition.base)\n   \"\"\"\n   主成成分分析\n   :param n_components: int, float, None or string\n   这个参数可以帮我们指定希望PCA降维后的特征维度数目。最常用的做法是直接指定降维到的维度数目，此时n_components是一个大于1的整数。\n   我们也可以用默认值，即不输入n_components，此时n_components=min(样本数，特征数)\n\t:param whiten: bool, optional (default False)\n      判断是否进行白化。所谓白化，就是对降维后的数据的每个特征进行归一化。对于PCA降维本身来说一般不需要白化,如果你PCA降维后有后续的数据处理动作，可以考虑白化，默认值是False，即不进行白化\n\n   :param svd_solver:\n      选择一个合适的SVD算法来降维,一般来说，使用默认值就够了。\n    \"\"\"\n```\n\n通过一个例子来看\n\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\npca = PCA(n_components=2)\npca.fit(X)\nPCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n  svd_solver='auto', tol=0.0, whiten=False)\nprint(pca.explained_variance_ratio_)\n# [ 0.99244...  0.00755...]\n```\n\n## 1.4 数据的特征选择\n\n降维本质上是从一个维度空间映射到另一个维度空间，特征的多少别没有减少，当然在映射的过程中特征值也会相应的变化。举个例子，现在的特征是1000维，我们想要把它降到500维。降维的过程就是找个一个从1000维映射到500维的映射关系。原始数据中的1000个特征，每一个都对应着降维后的500维空间中的一个值。假设原始特征中有个特征的值是9，那么降维后对应的值可能是3。而对于特征选择来说，有很多方法：\n\n- Filter(过滤式):VarianceThreshold\n- Embedded(嵌入式)：正则化、决策树\n- Wrapper(包裹式)\n\n其中过滤式的特征选择后，数据本身不变，而数据的维度减少。而嵌入式的特征选择方法也会改变数据的值，维度也改变。Embedded方式是一种自动学习的特征选择方法，后面讲到具体的方法的时候就能理解了。\n\n特征选择主要有两个**功能**：\n\n（1）减少特征数量，降维，使模型泛化能力更强，减少过拟合\n\n（2）增强特征和特征值之间的理解\n\n**sklearn.feature_selection**\n\n**去掉取值变化小的特征（删除低方差特征）**\n\nVarianceThreshold 是特征选择中的一项基本方法。它会移除所有方差不满足阈值的特征。默认设置下，它将移除所有方差为0的特征，即那些在所有样本中数值完全相同的特征。\n\n![image-20210306162950853](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170129.png)\n\n假设我们要移除那些超过80%的数据都为1或0的特征：\n\n```python\nVarianceThreshold(threshold = 0.0)\n删除所有低方差特征\n\nVariance.fit_transform(X,y)       \nX:numpy array格式的数据[n_samples,n_features]\n返回值：训练集差异低于threshold的特征将被删除。\n默认值是保留所有非零方差特征，即删除所有样本\n中具有相同值的特征。\n```\n\n```python\nfrom sklearn.feature_selection import VarianceThreshold\nX = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\nsel = VarianceThreshold(threshold=(.8 * (1 - .8)))\nsel.fit_transform(X)\narray([[0, 1],\n       [1, 0],\n       [0, 0],\n       [1, 1],\n       [1, 0],\n       [1, 1]])\n```\n\n# 2. sklearn数据集与机器学习组成\n\n### 机器学习组成：模型、策略、优化\n\n《统计机器学习》中指出：机器学习=模型+策略+算法。其实机器学习可以表示为：Learning= Representation+Evalution+Optimization。我们就可以将这样的表示和李航老师的说法对应起来。机器学习主要是由三部分组成，即：表示(模型)、评价(策略)和优化(算法)。\n\n**表示(或者称为：模型)：Representation**\n\n表示主要做的就是建模，故可以称为模型。模型要完成的主要工作是转换：将实际问题转化成为计算机可以理解的问题，就是我们平时说的建模。类似于传统的计算机学科中的算法，数据结构，如何将实际的问题转换成计算机可以表示的方式。这部分可以见“简单易学的机器学习算法”。给定数据，我们怎么去选择对应的问题去解决，选择正确的已有的模型是重要的一步。\n\n**评价(或者称为：策略)：Evalution**\n\n评价的目标是判断已建好的模型的优劣。对于第一步中建好的模型，评价是一个指标，用于表示模型的优劣。这里就会是一些评价的指标以及一些评价函数的设计。在机器学习中会有针对性的评价指标。\n\n- 分类问题\n\n**优化：Optimization**\n\n优化的目标是评价的函数，我们是希望能够找到最好的模型，也就是说评价最高的模型。\n\n### 开发机器学习应用程序的步骤\n\n**（1）收集数据**\n\n我们可以使用很多方法收集样本护具，如：制作网络爬虫从网站上抽取数据、从RSS反馈或者API中得到信息、设备发送过来的实测数据。\n\n**（2）准备输入数据**\n\n得到数据之后，还必须确保数据格式符合要求。\n\n**（3）分析输入数据**\n\n这一步的主要作用是确保数据集中没有垃圾数据。如果是使用信任的数据来源，那么可以直接跳过这个步骤\n\n**（4）训练算法**\n\n机器学习算法从这一步才真正开始学习。如果使用无监督学习算法，由于不存在目标变量值，故而也不需要训练算法，所有与算法相关的内容在第（5）步\n\n**（5）测试算法**\n\n这一步将实际使用第（4）步机器学习得到的知识信息。当然在这也需要评估结果的准确率，然后根据需要重新训练你的算法\n\n**（6）使用算法**\n\n转化为应用程序，执行实际任务。以检验上述步骤是否可以在实际环境中正常工作。如果碰到新的数据问题，同样需要重复执行上述的步骤\n\n## 2.1 scikit-learn数据集\n\n我们将介绍sklearn中的数据集类，模块包括用于加载数据集的实用程序，包括加载和获取流行参考数据集的方法。它还具有一些人工数据生成器。\n\n### sklearn.datasets\n\n**（1）datasets.load_\\*()**\n\n获取小规模数据集，数据包含在datasets里\n\n**（2）datasets.fetch_\\*()**\n\n获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home，表示数据集下载的目录，默认是 ~/scikit_learn_data/，要修改默认目录，可以修改环境变量SCIKIT_LEARN_DATA\n\n![image-20210306164558336](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170130.png)\n\n**（3）datasets.make_\\*()**\n\n本地生成数据集\n\n**load\\**和 fetch\\** 函数返回的数据类型是 datasets.base.Bunch，本质上是一个 dict，它的键值对可用通过对象的属性方式访问。主要包含以下属性：**\n\n- data：特征数据数组，是 n_samples * n_features 的二维 numpy.ndarray 数组\n- target：标签数组，是 n_samples 的一维 numpy.ndarray 数组\n- DESCR：数据描述\n- feature_names：特征名\n- target_names：标签名\n\n**数据集目录可以通过datasets.get_data_home()获取，clear_data_home(data_home=None)删除所有下载数据**\n\n**(4)datasets.get_data_home(data_home=None)**\n\n返回scikit学习数据目录的路径。这个文件夹被一些大的数据集装载器使用，以避免下载数据。默认情况下，数据目录设置为用户主文件夹中名为“scikit_learn_data”的文件夹。或者，可以通过“SCIKIT_LEARN_DATA”环境变量或通过给出显式的文件夹路径以编程方式设置它。'〜'符号扩展到用户主文件夹。如果文件夹不存在，则会自动创建。\n\n**(5)sklearn.datasets.clear_data_home(data_home=None)**\n\n删除存储目录中的数据\n\n### 获取小数据集\n\n**用于分类**\n\n- sklearn.datasets.load_iris\n\n```python\nclass sklearn.datasets.load_iris(return_X_y=False)\n  \"\"\"\n  加载并返回虹膜数据集\n\n  :param return_X_y: 如果为True，则返回而不是Bunch对象，默认为False\n\n  :return: Bunch对象，如果return_X_y为True，那么返回tuple，（data,target）\n  \"\"\"\nIn [12]: from sklearn.datasets import load_iris\n    ...: data = load_iris()\n    ...:\n\nIn [13]: data.target\nOut[13]:\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\nIn [14]: data.feature_names\nOut[14]:\n['sepal length (cm)',\n 'sepal width (cm)',\n 'petal length (cm)',\n 'petal width (cm)']\n\nIn [15]: data.target_names\nOut[15]:\narray(['setosa', 'versicolor', 'virginica'],\n      dtype='|S10')\n\nIn [17]: data.target[[1,10, 100]]\nOut[17]: array([0, 0, 2])\n```\n\n| 名称         | 数量 |\n| :----------- | ---: |\n| 类别         |    3 |\n| 特征         |    4 |\n| 样本数量     |  150 |\n| 每个类别数量 |   50 |\n\n- sklearn.datasets.load_digits\n\n```python\nclass sklearn.datasets.load_digits(n_class=10, return_X_y=False)\n    \"\"\"\n    加载并返回数字数据集\n\n    :param n_class: 整数，介于0和10之间，可选（默认= 10，要返回的类的数量\n\n    :param return_X_y: 如果为True，则返回而不是Bunch对象，默认为False\n\n    :return: Bunch对象，如果return_X_y为True，那么返回tuple，（data,target）\n    \"\"\"\nIn [20]: from sklearn.datasets import load_digits\n\nIn [21]: digits = load_digits()\n\nIn [22]: print(digits.data.shape)\n(1797, 64)\n\nIn [23]: digits.target\nOut[23]: array([0, 1, 2, ..., 8, 9, 8])\n\nIn [24]: digits.target_names\nOut[24]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\nIn [25]: digits.images\nOut[25]:\narray([[[  0.,   0.,   5., ...,   1.,   0.,   0.],\n        [  0.,   0.,  13., ...,  15.,   5.,   0.],\n        [  0.,   3.,  15., ...,  11.,   8.,   0.],\n        ...,\n        [  0.,   4.,  11., ...,  12.,   7.,   0.],\n        [  0.,   2.,  14., ...,  12.,   0.,   0.],\n        [  0.,   0.,   6., ...,   0.,   0.,   0.]],\n\n        [[  0.,   0.,  10., ...,   1.,   0.,   0.],\n        [  0.,   2.,  16., ...,   1.,   0.,   0.],\n        [  0.,   0.,  15., ...,  15.,   0.,   0.],\n        ...,\n        [  0.,   4.,  16., ...,  16.,   6.,   0.],\n        [  0.,   8.,  16., ...,  16.,   8.,   0.],\n        [  0.,   1.,   8., ...,  12.,   1.,   0.]]])\n```\n\n| 名称     | 数量 |\n| :------- | ---: |\n| 类别     |   10 |\n| 特征     |   64 |\n| 样本数量 | 1797 |\n\n**用于回归**\n\n- sklearn.datasets.load_boston\n\n```python\nclass  sklearn.datasets.load_boston(return_X_y=False)\n  \"\"\"\n  加载并返回波士顿房价数据集\n\n  :param return_X_y: 如果为True，则返回而不是Bunch对象，默认为False\n\n  :return: Bunch对象，如果return_X_y为True，那么返回tuple，（data,target）\n  \"\"\"\nIn [34]: from sklearn.datasets import load_boston\n\nIn [35]: boston = load_boston()\n\nIn [36]: boston.data.shape\nOut[36]: (506, 13)\n\nIn [37]: boston.feature_names\nOut[37]:\narray(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n       'TAX', 'PTRATIO', 'B', 'LSTAT'],\n      dtype='|S7')\n\nIn [38]:\n```\n\n| 名称     | 数量 |\n| :------- | ---: |\n| 目标类别 | 5-50 |\n| 特征     |   13 |\n| 样本数量 |  506 |\n\n- sklearn.datasets.load_diabetes\n\n```python\nclass sklearn.datasets.load_diabetes(return_X_y=False)\n  \"\"\"\n  加载和返回糖尿病数据集\n\n  :param return_X_y: 如果为True，则返回而不是Bunch对象，默认为False\n\n  :return: Bunch对象，如果return_X_y为True，那么返回tuple，（data,target）\n  \"\"\"\nIn [13]:  from sklearn.datasets import load_diabetes\n\nIn [14]: diabetes = load_diabetes()\n\nIn [15]: diabetes.data\nOut[15]:\narray([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n         0.01990842, -0.01764613],\n       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n        -0.06832974, -0.09220405],\n       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n         0.00286377, -0.02593034],\n       ...,\n       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n        -0.04687948,  0.01549073],\n       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n         0.04452837, -0.02593034],\n       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n        -0.00421986,  0.00306441]])\n```\n\n| 名称     |   数量 |\n| :------- | -----: |\n| 目标范围 | 25-346 |\n| 特征     |     10 |\n| 样本数量 |    442 |\n\n### 获取大数据集\n\n- sklearn.datasets.fetch_20newsgroups\n\n```python\nclass sklearn.datasets.fetch_20newsgroups(data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True)\n  \"\"\"\n  加载20个新闻组数据集中的文件名和数据\n\n  :param subset: 'train'或者'test','all'，可选，选择要加载的数据集：训练集的“训练”，测试集的“测试”，两者的“全部”，具有洗牌顺序\n\n\n  :param data_home: 可选，默认值：无，指定数据集的下载和缓存文件夹。如果没有，所有scikit学习数据都存储在'〜/ scikit_learn_data'子文件夹中\n\n  :param categories: 无或字符串或Unicode的集合，如果没有（默认），加载所有类别。如果不是无，要加载的类别名称列表（忽略其他类别）\n\n  :param shuffle: 是否对数据进行洗牌\n\n  :param random_state: numpy随机数生成器或种子整数\n\n  :param download_if_missing: 可选，默认为True，如果False，如果数据不在本地可用而不是尝试从源站点下载数据，则引发IOError\n\n  :param remove: 元组\n  \"\"\"\nIn [29]: from sklearn.datasets import fetch_20newsgroups\n\nIn [30]: data_test = fetch_20newsgroups(subset='test',shuffle=True, random_sta\n    ...: te=42)\n\nIn [31]: data_train = fetch_20newsgroups(subset='train',shuffle=True, random_s\n    ...: tate=42)\n```\n\n- sklearn.datasets.fetch_20newsgroups_vectorized\n\n```python\nclass sklearn.datasets.fetch_20newsgroups_vectorized(subset='train', remove=(), data_home=None)\n  \"\"\"\n  加载20个新闻组数据集并将其转换为tf-idf向量，这是一个方便的功能; 使用sklearn.feature_extraction.text.Vectorizer的默认设置完成tf-idf 转换。对于更高级的使用（停止词过滤，n-gram提取等），将fetch_20newsgroup与自定义Vectorizer或CountVectorizer组合在一起\n\n  :param subset: 'train'或者'test','all'，可选，选择要加载的数据集：训练集的“训练”，测试集的“测试”，两者的“全部”，具有洗牌顺序\n\n  :param data_home: 可选，默认值：无，指定数据集的下载和缓存文件夹。如果没有，所有scikit学习数据都存储在'〜/ scikit_learn_data'子文件夹中\n\n  :param remove: 元组\n  \"\"\"\nIn [57]: from sklearn.datasets import fetch_20newsgroups_vectorized\n\nIn [58]: bunch = fetch_20newsgroups_vectorized(subset='all')\n\nIn [59]: from sklearn.utils import shuffle\n\nIn [60]: X, y = shuffle(bunch.data, bunch.target)\n    ...: offset = int(X.shape[0] * 0.8)\n    ...: X_train, y_train = X[:offset], y[:offset]\n    ...: X_test, y_test = X[offset:], y[offset:]\n    ...:\n```\n\n### 获取本地生成数据\n\n生成本地分类数据：\n\n- sklearn.datasets.make_classification\n\n  ```python\n  class make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n  \"\"\"\n  生成用于分类的数据集\n  \n  :param n_samples:int，optional（default = 100)，样本数量\n  \n  :param n_features:int，可选（默认= 20），特征总数\n  \n  :param n_classes:int，可选（default = 2),类（或标签）的分类问题的数量\n  \n  :param random_state:int，RandomState实例或无，可选（默认=无）\n    如果int，random_state是随机数生成器使用的种子; 如果RandomState的实例，random_state是随机数生成器; 如果没有，随机数生成器所使用的RandomState实例np.random\n  \n  :return :X,特征数据集；y,目标分类值\n  \"\"\"\n  ```\n\n```python\nfrom sklearn.datasets.samples_generator import make_classification\nX,y= datasets.make_classification(n_samples=100000, n_features=20,n_informative=2, n_redundant=10,random_state=42)\n```\n\n生成本地回归数据：\n\n- sklearn.datasets.make_regression\n\n```python\nclass make_regression(n_samples=100, n_features=100, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)\n  \"\"\"\n  生成用于回归的数据集\n\n  :param n_samples:int，optional（default = 100)，样本数量\n\n  :param  n_features:int,optional（default = 100)，特征数量\n\n  :param  coef:boolean，optional（default = False），如果为True，则返回底层线性模型的系数\n\n  :param random_state:int，RandomState实例或无，可选（默认=无）\n    如果int，random_state是随机数生成器使用的种子; 如果RandomState的实例，random_state是随机数生成器; 如果没有，随机数生成器所使用的RandomState实例np.random\n\n  :return :X,特征数据集；y,目标值\n  \"\"\"\nfrom sklearn.datasets.samples_generator import make_regression\nX, y = make_regression(n_samples=200, n_features=5000, random_state=42)\n```\n\n## 2.2 模型的选择\n\n算法是核心，数据和计算是基础。这句话很好的说明了机器学习中算法的重要性。那么我们开看下机器学习的几种分类：\n\n- 监督学习\n  - 分类 k-近邻算法、决策树、贝叶斯、逻辑回归(LR)、支持向量机(SVM)\n  - 回归 线性回归、岭回归\n  - 标注 隐马尔可夫模型(HMM)\n- 无监督学习\n  - 聚类 k-means\n\n### 如何选择合适的算法模型\n\n在解决问题的时候，必须考虑下面两个问题：一、使用机器学习算法的目的，想要算法完成何种任务，比如是预测明天下雨的概率是对投票者按照兴趣分组；二、需要分析或者收集的数据时什么\n\n首先考虑使用机器学习算法的目的。如果想要预测目标变量的值，则可以选择监督学习算法，否则可以选择无监督学习算法，确定选择监督学习算法之后，需要进一步确定目标变量类型，如果目标变量是离散型，如是／否、1/2/3，A/B/C/或者红／黑／黄等，则可以选择**分类**算法；如果目标变量是连续的数值，如0.0～100.0、-999～999等，则需要选择**回归**算法\n\n如果不想预测目标变量的值，则可以选择**无监督**算法。进一步分析是否需要将数据划分为离散的组。如果这是唯一的需求，则使用**聚类**算法。\n\n当然在大多数情况下，上面给出的选择办法都能帮助读者选择恰当的机器学习算法，但这也并非已成不变。也有分类算法可以用于回归。\n\n其次考虑的是数据问题，我们应该充分了解数据，对实际数据了解的越充分，越容易创建符合实际需求的应用程序，主要应该了解数据的一下特性：特征值是**离散型变量** 还是 **连续型变量** ，特征值中是否存在缺失的值，何种原因造成缺失值，数据中是够存在异常值，某个特征发生的频率如何，等等。充分了解上面提到的这些数据特性可以缩短选择机器学习算法的时间。\n\n### 监督学习中三类问题的解释\n\n**（1）分类问题** 分类是监督学习的一个核心问题，在监督学习中，当输出变量取有限个离散值时，预测问题变成为分类问题。这时，输入变量可以是离散的，也可以是连续的。监督学习从数据中学习一个分类模型活分类决策函数，称为分类器。分类器对新的输入进行输出的预测，称为分类。最基础的便是二分类问题，即判断是非，从两个类别中选择一个作为预测结果；除此之外还有多酚类的问题，即在多于两个类别中选择一个。\n\n分类问题包括学习和分类两个过程，在学习过程中，根据已知的训练数据集利用有效的学习方法学习一个分类器，在分类过程中，利用学习的分类器对新的输入实例进行分类。图中(X1,Y1),(X2,Y2)…都是训练数据集，学习系统有训练数据学习一个分类器P(Y|X)或Y=f(X);分类系统通过学习到的分类器对于新输入的实例子Xn+1进行分类，即预测术其输出的雷标记Yn+1\n\n![image-20210107105644583](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170131.png)\n\n分类在于根据其特性将数据“分门别类”，所以在许多领域都有广泛的应用。例如，在银行业务中，可以构建一个客户分类模型，按客户按照贷款风险的大小进行分类；在网络安全领域，可以利用日志数据的分类对非法入侵进行检测；在图像处理中，分类可以用来检测图像中是否有人脸出现；在手写识别中，分类可以用于识别手写的数字；在互联网搜索中，网页的分类可以帮助网页的抓取、索引和排序。\n\n即一个分类应用的例子，文本分类。这里的文本可以是新闻报道、网页、电子邮件、学术论文。类别往往是关于文本内容的。例如政治、体育、经济等；也有关于文本特点的，如正面意见、反面意见；还可以根据应用确定，如垃圾邮件、非垃圾邮件等。文本分类是根据文本的特征将其划分到已有的类中。输入的是文本的特征向量，输出的是文本的类别。通常把文本的单词定义出现取值是1，否则是0；也可以是多值的，，表示单词在文本中出现的频率。直观地，如果“股票”“银行““货币”这些词出现很多，这个文本可能属于经济学，如果“网球””比赛“”运动员“这些词频繁出现，这个文本可能属于体育类。\n\n**（2）回归问题**\n\n回归是监督学习的另一个重要问题。回归用于预测输入变量和输出变量之间的关系，特别是当初如变量的值发生变化时，输出变量的值随之发生的变化。回归模型正式表示从输入到输出变量之间映射的函数。回归稳日的学习等价与函数拟合：选择一条函数曲线使其更好的拟合已知数据且很好的预测位置数据\n\n![image-20210107105911365](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170132.png)\n\n回归问题按照输入变量的个数，分为一元回归和多元回归；按照输入变量和输出变量之间关系的类型即模型的类型，分为线性回归和非线性回归。\n\n许多领域的任务都可以形式化为回归问题，比如，回归可以用于商务领域，作为市场趋势预测、产品质量管理、客户满意度调查、偷袭风险分析的工具。\n\n**（3）标注问题**\n\n标注也是一个监督学习问题。可以认为标注问题是分类问题的一个推广，标注问题又是更复杂的结构预测问题的简单形式。标注问题的输入是一个观测序列，输出是一个标记序列或状态序列。标注问题在信息抽取、自然语言处理等领域广泛应用，是这些领域的基本问题。例如，自然语言处理的词性标注就是一个典型的标注，即对一个单词序列预测其相应的词性标记序\n\n![image-20210107110012298](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170133.png)\n\n\u003e 当然我们主要关注的是分类和回归问题，并且标注问题的算法复杂\n\n## 2.3 模型检验-交叉验证\n\n一般在进行模型的测试时，我们会将数据分为训练集和测试集。在给定的样本空间中，拿出大部分样本作为训练集来训练模型，剩余的小部分样本使用刚建立的模型进行预测。\n\n### 训练集与测试集\n\n训练集与测试集的分割可以使用cross_validation中的train_test_split方法，大部分的交叉验证迭代器都内建一个划分数据前进行数据索引打散的选项，train_test_split 方法内部使用的就是交叉验证迭代器。默认不会进行打散，包括设置cv=some_integer（直接）k折叠交叉验证的cross_val_score会返回一个随机的划分。如果数据集具有时间性，千万不要打散数据再划分！\n\n- sklearn.cross_validation.train_test_split\n\n```python\ndef train_test_split(*arrays,**options)\n  \"\"\"\n  :param arrays:允许的输入是列表，数字阵列\n\n  :param test_size:float，int或None（默认为无）,如果浮点数应在0.0和1.0之间，并且表示要包括在测试拆分中的数据集的比例。如果int，表示测试样本的绝对数\n\n  :param train_size:float，int或None（默认为无）,如果浮点数应在0.0到1.0之间，表示数据集包含在列车拆分中的比例。如果int，表示列车样本的绝对数\n\n  :param random_state:int或RandomState,用于随机抽样的伪随机数发生器状态，参数 random_state 默认设置为 None，这意为着每次打散都是不同的。\n  \"\"\"\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nprint iris.data.shape,iris.target.shape\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=42)\nprint X_train.shape,y_train.shape\nprint X_test.shape,y_test.shape\n```\n\n上面的方式也有局限。因为只进行一次测试，并不一定能代表模型的真实准确率。因为，模型的准确率和数据的切分有关系，在数据量不大的情况下，影响尤其突出。所以还需要一个比较好的解决方案。\n\n模型评估中，除了训练数据和测试数据，还会涉及到验证数据。使用训练数据与测试数据进行了交叉验证，只有这样训练出的模型才具有更可靠的准确率，也才能期望模型在新的、未知的数据集上，能有更好的表现。这便是模型的推广能力，也即泛化能力的保证。\n\n### holdout method\n\n评估模型泛化能力的典型方法是holdout交叉验证(holdout cross validation)。holdout方法很简单，我们只需要将原始数据集分割为训练集和测试集，前者用于训练模型，后者用于评估模型的性能。一般来说，Holdout 验证并非一种交叉验证，因为数据并没有交叉使用。 随机从最初的样本中选出部分，形成交叉验证数据，而剩余的就当做训练数据。 一般来说，少于原本样本三分之一的数据被选做验证数据。所以这种方法得到的结果其实并不具有说服性\n\n### k-折交叉验证\n\nK折交叉验证，初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。\n\n![image-20210107110259176](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170134.png)\n\n例如5折交叉验证，全部可用数据集分成五个集合，每次迭代都选其中的1个集合数据作为验证集，另外4个集合作为训练集，经过5组的迭代过程。交叉验证的好处在于，可以保证所有数据都有被训练和验证的机会，也尽最大可能让优化的模型性能表现的更加可信。\n\n使用交叉验证的最简单的方法是在估计器和数据集上使用cross_val_score函数。\n\n- sklearn.cross_validation.cross_val_score\n\n```python\ndef cross_val_score(estimator, X, y=None, \tgroups=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')\n  \"\"\"\n  :param estimator:模型估计器\n\n  :param X:特征变量集合\n\n  :param y:目标变量\n\n  :param cv:int，使用默认的3折交叉验证，整数指定一个（分层）KFold中的折叠数\n\n  :return :预估系数\n  \"\"\"\nfrom sklearn.cross_validation import cross_val_score\ndiabetes = datasets.load_diabetes()\nX = diabetes.data[:150]\ny = diabetes.target[:150]\nlasso = linear_model.Lasso()\nprint(cross_val_score(lasso, X, y))\n```\n\n使用交叉验证方法的目的主要有2个：\n\n- 从有限的学习数据中获取尽可能多的有效信息；\n- 可以在一定程度上避免过拟合问题。\n\n### 超参数搜索-网格搜索\n\n通常情况下，有很多参数是需要手动指定的（如k-近邻算法中的K值），\n\n这种叫超参数。但是手动过程繁杂，所以需要对模型预设几种超参数组\n\n合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建\n\n立模型。\n\n![image-20210307234523125](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170135.png)\n\n**sklearn.model_selection.GridSearchCV**\n\n![image-20210307234552494](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170136.png)\n\n## 2.4 estimator的工作流程\n\n![image-20210306164823141](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170137.png)\n\n![image-20210306164928862](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170138.png)\n\n![image-20210306164949155](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170139.png)\n\n在sklearn中，估计器(estimator)是一个重要的角色，分类器和回归器都属于estimator。在估计器中有有两个重要的方法是fit和transform。\n\n- fit方法用于从训练集中学习模型参数\n- transform用学习到的参数转换数据\n\n![image-20210107110717711](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170140.png)\n\n# 3. Scikit-learn的分类器算法\n\n## 3.1 分类算法之k-近邻\n\n定义：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。\n\n来源：KNN算法最早是由Cover和Hart提出的一种分类算法。\n\nk-近邻算法采用测量不同特征值之间的距离来进行分类\n\n\u003e 优点：精度高、对异常值不敏感、无数据输入假定\n\u003e\n\u003e 缺点：计算复杂度高、空间复杂度高，必须指定K值，K值选择不当则分类精度不能保证\n\u003e\n\u003e 使用数据范围：数值型和标称型\n\n加快搜索速度——基于算法的改进KDTree,API接口里面有实现\n\n### 一个例子弄懂k-近邻\n\n电影可以按照题材分类，每个题材又是如何定义的呢？那么假如两种类型的电影，动作片和爱情片。动作片有哪些公共的特征？那么爱情片又存在哪些明显的差别呢？我们发现动作片中打斗镜头的次数较多，而爱情片中接吻镜头相对更多。当然动作片中也有一些接吻镜头，爱情片中也会有一些打斗镜头。所以不能单纯通过是否存在打斗镜头或者接吻镜头来判断影片的类别。那么现在我们有6部影片已经明确了类别，也有打斗镜头和接吻镜头的次数，还有一部电影类型未知。\n\n|         电影名称          | 打斗镜头 | 接吻镜头 | 电影类型 |\n| :-----------------------: | :------: | :------: | :------: |\n|      California Man       |    3     |   104    |  爱情片  |\n| He's not Really into dues |    2     |   100    |  爱情片  |\n|      Beautiful Woman      |    1     |    81    |  爱情片  |\n|      Kevin Longblade      |   101    |    10    |  动作片  |\n|     Robo Slayer 3000      |    99    |    5     |  动作片  |\n|         Amped II          |    98    |    2     |  动作片  |\n|             ?             |    18    |    90    |   未知   |\n\n那么我们使用K-近邻算法来分类爱情片和动作片：存在一个样本数据集合，也叫训练样本集，样本个数M个，知道每一个数据特征与类别对应关系，然后存在未知类型数据集合1个，那么我们要选择一个测试样本数据中与训练样本中M个的距离，排序过后选出最近的K个，这个取值一般不大于20个。选择K个最相近数据中次数最多的分类。那么我们根据这个原则去判断未知电影的分类\n\n|         电影名称          | 与未知电影的距离 |\n| :-----------------------: | :--------------: |\n|      California Man       |       20.5       |\n| He's not Really into dues |       18.7       |\n|      Beautiful Woman      |       19.2       |\n|      Kevin Longblade      |      115.3       |\n|     Robo Slayer 3000      |      117.4       |\n|         Amped II          |      118.9       |\n\n我们假设K为3，那么排名前三个电影的类型都是爱情片，所以我们判定这个未知电影也是一个爱情片。那么计算距离是怎样计算的呢？\n\n![image-20210306165428737](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170141.png)\n\n### sklearn.neighbors\n\nsklearn.neighbors提供监督的基于邻居的学习方法的功能，sklearn.neighbors.KNeighborsClassifier是一个最近邻居分类器。那么KNeighborsClassifier是一个类，我们看一下实例化时候的参数\n\n![image-20210306165606032](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170142.png)\n\n```python\nclass sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)**\n  \"\"\"\n  :param n_neighbors：int，可选（默认= 5），k_neighbors查询默认使用的邻居数\n\n  :param algorithm：{'auto'，'ball_tree'，'kd_tree'，'brute'}，可选用于计算最近邻居的算法：'ball_tree'将会使用 BallTree，'kd_tree'将使用 KDTree，“野兽”将使用强力搜索。'auto'将尝试根据传递给fit方法的值来决定最合适的算法。\n\n  :param n_jobs：int，可选（默认= 1),用于邻居搜索的并行作业数。如果-1，则将作业数设置为CPU内核数。不影响fit方法。\n\n  \"\"\"\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\n\nneigh = KNeighborsClassifier(n_neighbors=3)\n```\n\n### Method\n\n**fit(X, y)**\n\n使用X作为训练数据拟合模型，y作为X的类别值。X，y为数组或者矩阵\n\n```python\nX = np.array([[1,1],[1,1.1],[0,0],[0,0.1]])\ny = np.array([1,1,0,0])\nneigh.fit(X,y)\n```\n\n**kneighbors(X=None, n_neighbors=None, return_distance=True)**\n\n找到指定点集X的n_neighbors个邻居，return_distance为False的话，不返回距离\n\n```python\nneigh.kneighbors(np.array([[1.1,1.1]]),return_distance= False)\n\nneigh.kneighbors(np.array([[1.1,1.1]]),return_distance= False,an_neighbors=2)\n```\n\n**predict(X)**\n\n预测提供的数据的类标签\n\n```python\nneigh.predict(np.array([[0.1,0.1],[1.1,1.1]]))\n```\n\n**predict_proba(X)**\n\n返回测试数据X属于某一类别的概率估计\n\n```python\nneigh.predict_proba(np.array([[1.1,1.1]]))\n```\n\n## 3.2 k-近邻算法案例分析\n\n本案例使用最著名的”鸢尾“数据集，该数据集曾经被Fisher用在经典论文中，目前作为教科书般的数据样本预存在Scikit-learn的工具包中。\n\n![image-20210306170053370](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170143.png)\n\n![image-20210306170125343](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170144.png)\n\n**读入Iris数据集细节资料**\n\n```python\nfrom sklearn.datasets import load_iris\n# 使用加载器读取数据并且存入变量iris\niris = load_iris()\n\n# 查验数据规模\niris.data.shape\n\n# 查看数据说明（这是一个好习惯）\nprint iris.DESCR\n```\n\n通过上述代码对数据的查验以及数据本身的描述，我们了解到Iris数据集共有150朵鸢尾数据样本，并且均匀分布在3个不同的亚种；每个数据样本有总共4个不同的关于花瓣、花萼的形状特征所描述。由于没有制定的测试集合，因此按照惯例，我们需要对数据进行随即分割，25%的样本用于测试，其余75%的样本用于模型的训练。\n\n由于不清楚数据集的排列是否随机，可能会有按照类别去进行依次排列，这样训练样本的不均衡的，所以我们需要分割数据，已经默认有随机采样的功能。\n\n**对Iris数据集进行分割**\n\n```python\nfrom sklearn.cross_validation import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(iris.data,iris.target,test_size=0.25,random_state=42)\n```\n\n**对特征数据进行标准化**\n\n```python\nfrom sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.fit_transform(X_test)\n```\n\nK近邻算法是非常直观的机器学习模型，我们可以发现K近邻算法没有参数训练过程，也就是说，我们没有通过任何学习算法分析训练数据，而只是根据测试样本训练数据的分布直接作出分类决策。因此，K近邻属于无参数模型中非常简单一种。\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\n\ndef knniris():\n    \"\"\"\n    鸢尾花分类\n    :return: None\n    \"\"\"\n\n    # 数据集获取和分割\n    lr = load_iris()\n\n    x_train, x_test, y_train, y_test = train_test_split(lr.data, lr.target, test_size=0.25)\n\n    # 进行标准化\n\n    std = StandardScaler()\n\n    x_train = std.fit_transform(x_train)\n    x_test = std.transform(x_test)\n\n    # estimator流程\n    knn = KNeighborsClassifier()\n\n    # # 得出模型\n    # knn.fit(x_train,y_train)\n    #\n    # # 进行预测或者得出精度\n    # y_predict = knn.predict(x_test)\n    #\n    # # score = knn.score(x_test,y_test)\n\n    # 通过网格搜索,n_neighbors为参数列表\n    param = {\"n_neighbors\": [3, 5, 7]}\n\n    gs = GridSearchCV(knn, param_grid=param, cv=10)\n\n    # 建立模型\n    gs.fit(x_train,y_train)\n\n    # print(gs)\n\n    # 预测数据\n\n    print(gs.score(x_test,y_test))\n\n    # 分类模型的精确率和召回率\n\n    # print(\"每个类别的精确率与召回率：\",classification_report(y_test, y_predict,target_names=lr.target_names))\n\n    return None\n\nif __name__ == \"__main__\":\n    knniris()\n```\n\n## 3.3 朴素贝叶斯\n\n朴素贝叶斯（Naive Bayes）是一个非常简单，但是实用性很强的分类模型。朴素贝叶斯分类器的构造基础是贝叶斯理论。\n\n![image-20210307234131617](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170145.png)\n\n### 概率论基础\n\n概率定义为一件事情发生的可能性。事情发生的概率可以 通过观测数据中的事件发生次数来计算，事件发生的概率等于改事件发生次数除以所有事件发生的总次数。举一些例子：\n\n- 扔出一个硬币，结果头像朝上\n- 某天是晴天\n- 某个单词在未知文档中出现\n\n我们将事件的概率记作P(X)，那么假设这一事件为X属于样本空间中的一个类别，那么0≤*P*(*X*)≤1。\n\n**联合概率与条件概率**\n\n- **联合概率**\n\n是指两件事情同时发生的概率。那么我们假设样本空间有一些天气数据：\n\n| 编号 | 星期几 | 天气 |\n| :--: | :----: | :--: |\n|  1   |   2    | 晴天 |\n|  2   |   1    | 下雨 |\n|  3   |   3    | 晴天 |\n|  4   |   4    | 晴天 |\n|  5   |   1    | 下雨 |\n|  6   |   2    | 下雪 |\n|  7   |   3    | 下雪 |\n\n那么天气被分成了三类，那么P(X=sun)=3/7，假如说天气=下雪且星期几=2？这个概率怎么求？这个概率应该等于两件事情为真的次数除以所有事件发生 的总次数。我们可以看到只有一个样本满足天气=下雪且星期几=2，所以这个概率为1/7。一般对于X和Y来说，对应的联合概率记为P(XY)。\n\n- **条件概率**\n\n那么条件概率形如P(X∣Y)，这种格式的。表示为在Y发生的条件下，发生X的概率。假设X代表星期，Y代表天气，则 P(X=3∣Y=sun)如何求？\n\n从表中我们可以得出，P(X=3,Y=sun)=1/7,P(Y)=3/7。\n\nP(X=3∣Y=sun)=1/3=P(X=3,Y=sun)/P(Y)\n\n在条件概率中，有一个重要的特性\n\n- **如果每个事件之间相互独立**\n\n那么则有![image-20210306170501376](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170146.png)\n\n这个式子的意思是给定条件下，所有的X的概率为单独的Y条件下每个X发生的概率乘积，我们通过后面再继续去理解这个式子的具体含义。\n\n**贝叶斯公式**\n\n首先我们给出该公式的表示，![image-20210306170533187](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170147.png),其中ci为类别，W为特征向量。\n\n贝叶斯公式最常用于文本分类，上式左边可以理解为给定一个文本词向量W，那么它属于类别ci的概率是多少。那么式子右边分几部分，P(W∣ci)理解为在给定类别的情况下，该文档的词向量的概率。可以通过条件概率中的重要特性来求解。\n\n假设我们有已分类的文档，\n\n```\na = \"life is short,i like python\"\nb = \"life is too long,i dislike python\"\nc = \"yes,i like python\"\nlabel=[1,0,1]\n```\n\n**词袋法的特征值计算**\n\n若使用词袋法，且以训练集中的文本为词汇表，即将训练集中的文本中出现的单词(不重复)都统计出来作为词典，那么记单词的数目为n，这代表了文本的n个维度。以上三个文本在这8个特征维度上的表示为：\n\n|      | life |  is  |  i   | short | long | like | dislike | too  | python | yes  |\n| :--: | :--: | :--: | :--: | :---: | :--: | :--: | :-----: | :--: | :----: | :--: |\n|  a'  |  1   |  1   |  1   |   1   |  0   |  1   |    0    |  0   |   1    |  0   |\n|  b'  |  1   |  1   |  1   |   0   |  1   |  0   |    1    |  1   |   1    |  0   |\n|  c'  |  0   |  0   |  1   |   0   |  0   |  1   |    0    |  0   |   1    |  1   |\n\n上面a',b'就是两个文档的词向量的表现形式，对于贝叶斯公式，从label中我们可以得出两个类别的概率为：\n\n![image-20210306170650996](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170148.png)\n\n对于一个给定的文档类别，每个单词特征向量的概率是多少呢？\n\n**提供一种TF计算方法**，为类别yk每个单词出现的次数Ni,除以文档类别yk中所有单词出现次数的总数N：\n\nPi=Ni/N\n\n首先求出现总数，对于1类别文档，在a'中，就可得出总数为1+1+1+1+1+1=6，c'中，总共1+1+1+1=4，故在1类别文档中总共有10次。\n\n每个单词出现总数，假设是两个列表，a'+c'就能得出每个单词出现次数，比如P(w=python)=2/10=0.20000000,同样可以得到其它的单词概率。最终结果如下：\n\n```\n# 类别1文档中的词向量概率\np1 = [0.10000000,0.10000000,0.20000000,0.10000000,0,0.20000000,0,0,0.20000000,0.10000000]\n# 类别0文档中的词向量概率\np0 = [0.16666667,0.16666667,0.16666667,0,0.16666667,0,0.16666667,0.16666667,0.16666667,0]\n```\n\n**拉普拉斯平滑系数**\n\n为了避免训练集样本对一些特征的缺失，即某一些特征出现的次数为0，在计算P(X1,X2,X3,…,Xn∣Yi)的时候，各个概率相乘最终结果为零，这样就会影响结果。我们需要对这个概率计算公式做一个平滑处理:\n\n![image-20210306170847401](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170149.png)\n\n其中m为特征词向量的个数，α为平滑系数，当α=1，称为拉普拉斯平滑。\n\n### sklearn.naive_bayes.MultinomialNB\n\n```python\nclass sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n  \"\"\"\n  :param alpha：float，optional（default = 1.0）加法（拉普拉斯/ Lidstone）平滑参数（0为无平滑）\n  \"\"\"\n```\n\n![image-20210107112004526](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170150.png)\n\n### 互联网新闻分类\n\n**读取20类新闻文本的数据细节**\n\n```python\nfrom sklearn.datasets import fetch_20newsgroups\n\nnews = fetch_20newsgroups(subset='all')\n\nprint news.data[0]\n```\n\n上述代码得出该数据共有18846条新闻，但是这些文本数据既没有被设定特征，也没有数字化的亮度。因此，在交给朴素贝叶斯分类器学习之前，要对数据做进一步的处理。\n\n**20类新闻文本数据分割**\n\n```python\nfrom sklearn.cross_validation import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(news.data,news.target,test_size=0.25,random_state=42)\n```\n\n**文本转换为特征向量进行TF特征抽取**\n\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvec = CountVectorizer()\n# 训练数据输入，并转换为特征向量\nX_train = vec.fit_transform(X_train)\n# 测试数据转换\nX_test = vec.transform(X_test)\n```\n\n**朴素贝叶斯分类器对文本数据进行类别预测**\n\n```python\nfrom sklearn.naive_bayes import MultinomialNB\n\n# 使用平滑处理初始化的朴素贝叶斯模型\nmnb = MultinomialNB(alpha=1.0)\n\n# 利用训练数据对模型参数进行估计\nmnb.fit(X_train,y_train)\n\n# 对测试验本进行类别预测。结果存储在变量y_predict中\ny_predict = mnb.predict(X_test)\n```\n\n**性能测试**\n\n- 特点分析\n\n朴素贝叶斯模型被广泛应用于海量互联网文本分类任务。由于其较强的特征条件独立假设，使得模型预测所需要估计的参数规模从幂指数量级想线性量级减少，极大的节约了内存消耗和计算时间。到那时，也正是受这种强假设的限制，模型训练时无法将各个特征之间的联系考量在内，**使得该模型在其他数据特征关联性较强的分类任务上的性能表现不佳**。\n\n## 3.4 分类算法之逻辑回归\n\n逻辑回归（Logistic Regression），简称LR。它的特点是能够是我们的特征输入集合转化为0和1这两类的概率。一般来说，回归不用在分类问题上，因为回归是连续型模型，而且受噪声影响比较大。如果非要应用进入，可以使用逻辑回归。了解过线性回归之后再来看逻辑回归可以更好的理解。\n\n\u003e 优点：计算代价不高，易于理解和实现\n\u003e\n\u003e 缺点：容易欠拟合，分类精度不高\n\u003e\n\u003e 适用数据：数值型和标称型\n\n### 逻辑回归\n\n对于回归问题后面会介绍，Logistic回归本质上是线性回归，只是在特征到结果的映射中加入了一层函数映射，即先把特征线性求和，然后使用函数g(z)将最为假设函数来预测。g(z)可以将连续值映射到0和1上。Logistic回归用来分类0/1问题，也就是预测结果属于0或者1的二值分类问题\n\n映射函数为：\n\n![image-20210107113501457](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170151.png)映射出来的效果如下如：\n\n![image-20210107113512797](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170152.png)\n\n### sklearn.linear_model.LogisticRegression\n\n逻辑回归类\n\n```python\nclass sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n  \"\"\"\n  :param C: float，默认值：1.0\n\n  :param penalty: 特征选择的方式\n\n  :param tol: 公差停止标准\n  \"\"\"\n```\n\n![image-20210107113608904](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170153.png)\n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_digits\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(C=1.0, penalty='l1', tol=0.01)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nLR.fit(X_train,y_train)\nLR.predict(X_test)\nLR.score(X_test,y_test)\n0.96464646464646464\n# c=100.0\n0.96801346801346799\n```\n\n### 属性\n\n**coef_**\n\n决策功能的特征系数\n\n**Cs_**\n\n数组C，即用于交叉验证的正则化参数值的倒数\n\n### 特点分析\n\n线性分类器可以说是最为基本和常用的机器学习模型。尽管其受限于数据特征与分类目标之间的线性假设，我们仍然可以在科学研究与工程实践中把线性分类器的表现性能作为基准。\n\n## 3.5 逻辑回归算法案例分析\n\n### 良／恶性乳腺癌肿瘤预测\n\n原始数据的下载地址为：https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/\n\n**数据预处理**\n\n```python\nimport pandas as pd\nimport numpy as np\n\n# 根据官方数据构建类别\ncolumn_names = ['Sample code number','Clump Thickness ','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','Class'],\n\ndata = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/',names = column_names)\n\n# 将？替换成标准缺失值表示\ndata = data.replace(to_replace='?',value = np.nan)\n\n# 丢弃带有缺失值的数据（只要一个维度有缺失）\ndata = data.dropna(how='any')\n\ndata.shape\n```\n\n处理的缺失值后的样本共有683条，特征包括细胞厚度、细胞大小、形状等九个维度\n\n**准备训练测试数据**\n\n```python\nfrom sklearn.cross_validation import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(data[column_names[1:10]],data[column_names[10]],test_size=0.25,random_state=42)\n\n# 查看训练和测试样本的数量和类别分布\ny_train.value_counts()\n\ny_test.value_counts()\n```\n\n**使用逻辑回归进行良／恶性肿瘤预测任务**\n\n```python\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n\n# 标准化数据，保证每个维度的特征数据方差为1，均值为0。使得预测结果不会被某些维度过大的特征值而主导\nss = StandardScaler()\n\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)\n\n# 初始化 LogisticRegression\n\nlr = LogisticRegression(C=1.0, penalty='l1', tol=0.01)\n\n# 跳用LogisticRegression中的fit函数／模块来训练模型参数\nlr.fit(X_train,y_train)\n\nlr_y_predict = lr.predict(X_test)\n```\n\n**性能分析**\n\n```python\nfrom sklearn.metrics import classification_report\n\n# 利用逻辑斯蒂回归自带的评分函数score获得模型在测试集上的准确定结果\nprint '精确率为：',lr.score(X_test,y_test)\n\nprint classification_report(y_test,lr_y_predict,target_names = ['Benign','Maligant'])\n```\n\n## 3.6 分类器性能评估\n\n在许多实际问题中，衡量分类器任务的成功程度是通过固定的性能指标来获取。一般最常见使用的是准确率，即预测结果正确的百分比。然而有时候，我们关注的是负样本是否被正确诊断出来。例如，关于肿瘤的的判定，需要更加关心多少恶性肿瘤被正确的诊断出来。也就是说，在二类分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵。\n\n在二类问题中，如果将一个正例判为正例，那么就可以认为产生了一个真正例（True Positive，TP）；如果对一个反例正确的判为反例，则认为产生了一个真反例（True Negative，TN）。相应地，两外两种情况则分别称为伪反例（False Negative，FN，也称）和伪正例（False Positive，TP），四种情况如下图：\n\n![image-20210107115219067](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170154.png)\n\n![image-20210306171205933](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170155.png)\n\n在分类中，当某个类别的重要性高于其他类别时，我们就可以利用上述定义出多个逼错误率更好的新指标。第一个指标就是**精确率**（Precision），它等于TP/(TP+FP)，给出的是预测为正例的样本中占真实结果总数的比例。第二个指标是**召回率**（Recall）。它等于TP/(TP+FN)，给出的是预测为正例的真实正例占所有真实正例的比例。\n\n那么除了正确率和精确率这两个指标之外，为了综合考量召回率和精确率，我们计算这两个指标的调和平均数，得到F1指标（F1 measure）:\n\n![image-20210107115233827](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170156.png)\n\n之所以使用调和平均数，是因为它除了具备平均功能外，还会对那些召回率和精确率更加接近的模型给予更高的分数；而这也是我们所希望的，因为那些召回率和精确率差距过大的学习模型，往往没有足够的使用价值。\n\n![image-20210306172033897](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170157.png)\n\n**sklearn.metrics.classification_report**\n\n![image-20210306172051215](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170158.png)\n\nsklearn中metrics中提供了计算四个指标的模块，也就是classification_report。\n\n```python\nclassification_report(y_true, y_pred, labels=None, target_names=None, digits=2)\n  \"\"\"\n  计算分类指标\n  :param y_true:真实目标值\n\n  :param y_pred:分类器返回的估计值\n\n  :param target_names:可选的，计算与目标类别匹配的结果\n\n  :param digits:格式化输出浮点值的位数\n\n  :return :字符串，三个指标值\n\n  \"\"\"\n```\n\n我们通过一个例子来分析一下指标的结果：\n\n```python\nfrom sklearn.metrics import classification_report\ny_true = [0, 1, 2, 2, 2]\ny_pred = [0, 0, 2, 2, 1]\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_true, y_pred, target_names=target_names))\n\n\n             precision    recall  f1-score   support\n\n    class 0       0.50      1.00      0.67         1\n    class 1       0.00      0.00      0.00         1\n    class 2       1.00      0.67      0.80         3\n\navg / total       0.70      0.60      0.61         5\n```\n\n## 3.7 分类算法之决策树\n\n决策树是一种基本的分类方法，当然也可以用于回归。我们一般只讨论用于分类的决策树。决策树模型呈树形结构。在分类问题中，表示基于特征对实例进行分类的过程，它可以认为是if-then规则的集合。在决策树的结构中，每一个实例都被一条路径或者一条规则所覆盖。通常决策树学习包括三个步骤：**特征选择**、决策树的生成和决策树的修剪\n\n\u003e 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，**可以处理逻辑回归等不能解决的非线性特征数据**\n\u003e\n\u003e 缺点：可能产生过度匹配问题\n\u003e\n\u003e 适用数据类型：数值型和标称型\n\n![image-20210307235009516](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170159.png)\n\n![image-20210307234656507](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170200.png)\n\n### 特征选择\n\n特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率，如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的京都影响不大。通常特征选择的准则是信息增益，这是个数学概念。通过一个例子来了解特征选择的过程。\n\n|  ID  | 年龄 | 有工作 | 有自己的房子 | 信贷情况 | 类别 |\n| :--: | :--: | :----: | :----------: | :------: | :--: |\n|  1   | 青年 |   否   |      否      |   一般   |  否  |\n|  2   | 青年 |   否   |      否      |    好    |  否  |\n|  3   | 青年 |   是   |      否      |    好    |  是  |\n|  4   | 青年 |   是   |      是      |   一般   |  是  |\n|  5   | 青年 |   否   |      否      |   一般   |  否  |\n|  6   | 中年 |   否   |      否      |   一般   |  否  |\n|  7   | 中年 |   否   |      否      |    好    |  否  |\n|  8   | 中年 |   是   |      是      |    好    |  是  |\n|  9   | 中年 |   否   |      是      |  非常好  |  是  |\n|  10  | 中年 |   否   |      是      |  非常好  |  是  |\n|  11  | 老年 |   否   |      是      |  非常好  |  是  |\n|  12  | 老年 |   否   |      是      |    好    |  是  |\n|  13  | 老年 |   是   |      否      |    好    |  是  |\n|  14  | 老年 |   是   |      否      |  非常好  |  是  |\n|  15  | 老年 |   否   |      否      |   一般   |  否  |\n\n我们希望通过所给的训练数据学习一个贷款申请的决策树，用以对文莱的贷款申请进行分类，即当新的客户提出贷款申请是，根据申请人的特征利用决策树决定是否批准贷款申请。特征选择其实是决定用那个特征来划分特征空间。下图中分别是按照年龄，还有是否有工作来划分得到不同的子节点\n\n![image-20210107115449745](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170201.png)\n\n![image-20210107115507352](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170202.png)\n\n问题是究竟选择哪个特征更好些呢？那么直观上，如果一个特征具有更好的分类能力，是的各个自己在当前的条件下有最好的分类，那么就更应该选择这个特征。信息增益就能很好的表示这一直观的准则。这样得到的一棵决策树只用了两个特征就进行了判断：\n\n![image-20210107115650711](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170203.png)\n\n通过信息增益生成的决策树结构，更加明显、快速的划分类别。下面介绍scikit-learn中API的使用\n\n### 信息的度量和作用\n\n我们常说信息有用，那么它的作用如何客观、定量地体现出来呢？信息用途的背后是否有理论基础呢？这个问题一直没有很好的回答，直到1948年，香农在他的论文“通信的数学原理”中提到了“信息熵”的概念，才解决了信息的度量问题，并量化出信息的作用。\n\n一条信息的信息量与其不确定性有着直接的关系，比如我们要搞清一件非常不确定的事，就需要大量的信息。相反如果对某件事了解较多，则不需要太多的信息就能把它搞清楚 。所以从这个角度看，可以认为，信息量就等于不确定的多少。那么如何量化信息量的度量呢？2022年举行世界杯，大家很关系谁是冠军。假如我错过了看比赛，赛后我问朋友 ，“谁是冠军”？他不愿意直接告诉我，让我每猜一次给他一块钱，他告诉我是否猜对了，那么我需要掏多少钱才能知道谁是冠军？我可以把球编上号，从1到32，然后提问：冠 军在1-16号吗？依次询问，只需要五次，就可以知道结果。所以谁是世界杯冠军这条消息只值五块钱。当然香农不是用钱，而是用“比特”这个概念来度量信息量。一个比特是 一位二进制数，在计算机中一个字节是8比特。\n\n那么如果说有一天有64支球队进行决赛阶段的比赛，那么“谁是世界杯冠军”的信息量就是6比特，因为要多猜一次，有的同学就会发现，信息量的比特数和所有可能情况的对数函数log有关，(log32=5,log64=6)\n\n另外一方面你也会发现实际上我们不需要猜五次就能才出冠军，因为像西班牙、巴西、德国、意大利这样的球队夺得冠军的可能性比南非、尼日利亚等球队大得多，因此第一次猜测时不需要把32支球队等分成两个组，而可以把少数几支最有可能的球队分成一组，把其他球队分成一组。然后才冠军球队是否在那几支热门队中。这样，也许三次就猜出结果。因此，当每支球队夺冠的可能性不等时，“谁是世界杯冠军”的信息量比5比特少。香农指出，它的准确信息量应该是：\n\nH = -(p1*logp1 + p2*logp2 + … + p32log32)\n\n其中，p1…p32为这三支球队夺冠的概率。**H的专业术语称之为信息熵，单位为比特**，当这32支球队夺冠的几率相同时，对应的信息熵等于5比特，这个可以通过计算得出。有一个特性就是，5比特是公式的最大值。那么信息熵（经验熵）的具体定义可以为如下：\n\n![image-20210107115903126](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170204.png)\n\n### 信息增益\n\n自古以来，信息和消除不确定性是相联系的。所以决策树的过程其实是在寻找某一个特征对整个分类结果的不确定减少的过程。那么这样就有一个概念叫做信息增益（information gain）。\n\n**那么信息增益表示得知特征X的信息而是的类Y的信息的不确定性减少的程度**，所以我们对于选择特征进行分类的时候，当然选择信息增益较大的特征，这样具有较强的分类能力。特征A对训练数据集D的信息增益g(D,A),定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即公式为：\n\ng(D,A)=H(D)−H(D∣A)\n\n**根据信息增益的准则的特征选择方法是：对于训练数据集D，计算其每个特征的信息增益，并比较它们的阿笑，选择信息增益最大的特征**\n\n### 信息增益的计算\n\n![image-20210107120116471](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170205.png)\n\n既然我们有了这两个公式，我们可以根据前面的是否通过贷款申请的例子来通过计算得出我们的决策特征顺序。那么我们首先计算总的经验熵为：\n\n![image-20210107120135360](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170206.png)\n\n然后我们让A1,A2,A3,A4*A*1,*A*2,*A*3,*A*4分别表示年龄、有工作、有自己的房子和信贷情况4个特征，则计算出年龄的信息增益为：\n\n![image-20210107120154953](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170207.png)\n\n同理其他的也可以计算出来，g(D,A2)=0.324,g(D,A3)=0.420,g(D,A4)=0.363，相比较来说其中特征A3（有自己的房子）的信息增益最大，所以我们选择特征A3为最有特征\n\n### sklearn.tree.DecisionTreeClassifier\n\nsklearn.tree.DecisionTreeClassifier是一个能对数据集进行多分类的类\n\n```python\nclass sklearn.tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_split=1e-07, class_weight=None, presort=False)\n  \"\"\"\n  :param max_depth：int或None，可选（默认=无）树的最大深度。如果没有，那么节点将被扩展，直到所有的叶子都是纯类，或者直到所有的叶子都包含少于min_samples_split样本\n\n  :param random_state：random_state是随机数生成器使用的种子\n  \"\"\"\n```\n\n首先我们导入类，以及数据集，还有将数据分成训练数据集和测试数据集两部分\n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\niris = load_iris()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nestimator = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\nestimator.fit(X_train, y_train)\n```\n\n### method\n\n**apply** 返回每个样本被预测的叶子的索引\n\n```python\nestimator.apply(X)\n\narray([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  5,\n        5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n        5,  5, 15,  5,  5,  5,  5,  5,  5, 10,  5,  5,  5,  5,  5, 10,  5,\n        5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5, 16, 16,\n       16, 16, 16, 16,  6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n        8, 16, 16, 16, 16, 16, 16, 14, 16, 16, 11, 16, 16, 16,  8,  8, 16,\n       16, 16, 14, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16])\n```\n\n**decision_path** 返回树中的决策路径\n\n```python\ndp = estimator.decision_path(X_test)\n```\n\n**fit_transform(X,y=None，**fit_params)** 输入数据，然后转换\n\n**predict(X)** 预测输入数据的类型,完整代码\n\n```python\nestimator.predict(X_test)\narray([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 2, 1, 0, 1, 2, 1, 0, 2])\n\nprint y_test\n\narray([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1, 0,\n       0, 2, 0, 0, 1, 1, 0, 2, 1, 0, 2, 2, 1, 0, 1])\n```\n\n**score(X,y,sample_weight=None)** 返回给定测试数据的准确精度\n\n```python\nestimator.score(X_test,y_test)\n\n0.89473684210526316\n```\n\n### 决策树本地保存\n\n**sklearn.tree.export_graphviz()** 该函数能够导出DOT格式\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier()\niris = load_iris()\nclf = clf.fit(iris.data, iris.target)\ntree.export_graphviz(clf,out_file='tree.dot')\n```\n\n那么有了tree.dot文件之后，我们可以通过命令转换为png或者pdf格式，首先得安装graphviz\n\n```\nubuntu:sudo apt-get install graphviz\nMac:brew install graphviz\n```\n\n然后我们运行这个命令\n\n```\n$ dot -Tps tree.dot -o tree.ps\n$ dot -Tpng tree.dot -o tree.png\n```\n\n或者，如果我们安装了Python模块pydotplus，我们可以直接在Python中生成PDF文件，通过pip install pydotplus，然后运行\n\n```python\nimport pydotplus\ndot_data = tree.export_graphviz(clf, out_file=None)\ngraph = pydotplus.graph_from_dot_data(dot_data)\ngraph.write_pdf(\"iris.pdf\")\n```\n\n查看决策树结构图片，这个结果是经过决策树学习的三个步骤之后形成的。当作了解\n\n![image-20210107203456582](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170208.png)\n\n\u003e 扩展：所有各种决策树算法是什么，它们之间有什么不同？哪一个在scikit-learn中实现？\n\u003e\n\u003e ID3 --- 信息增益 最大的准则\n\u003e\n\u003e C4.5 --- 信息增益比 最大的准则\n\u003e\n\u003e CART 回归树: 平方误差 最小 分类树: 基尼系数 最小的准则 在sklearn中可以选择划分的原则\n\n### 决策树优缺点分析\n\n决策树的一些优点是：\n\n- 简单的理解和解释。树木可视化。\n- 需要很少的数据准备。其他技术通常需要数据归一化，需要创建虚拟变量，并删除空值。但请注意，此模块不支持缺少值。\n- 使用树的成本（即，预测数据）在用于训练树的数据点的数量上是对数的。\n\n决策树的缺点包括：\n\n- 决策树学习者可以创建不能很好地推广数据的过于复杂的树。这被称为过拟合。修剪（目前不支持）的机制，设置叶节点所需的最小采样数或设置树的最大深度是避免此问题的必要条件。\n- 决策树可能不稳定，因为数据的小变化可能会导致完全不同的树被生成。通过使用合奏中的决策树来减轻这个问题。\n\n## 3.8 集成方法（分类）之随机森林\n\n在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。利用相同的训练数搭建多个独立的分类模型，然后通过投票的方式，以少数服从多数的原则作出最终的分类决策。例如, 如果你训练了5个树, 其中有4个树的结果是True, 1个数的结果是False, 那么最终结果会是True.\n\n在前面的决策当中我们提到，一个标准的决策树会根据每维特征对预测结果的影响程度进行排序，进而决定不同的特征从上至下构建分裂节点的顺序，如此以来，所有在随机森林中的决策树都会受这一策略影响而构建的完全一致，从而丧失的多样性。所以在随机森林分类器的构建过程中，每一棵决策树都会放弃这一固定的排序算法，转而随机选取特征。\n\n集成学习通过建立几个模型组合的来解决单一预测问题。它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。这些预测最后结合成单预测，因此优于任何一个单分类的做出预测。\n\n定义：在机器学习中，**随机森林**是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。\n\n**优点**：\n\n- 在当前所有算法中，具有极好的准确率\n- 能够有效地运行在大数据集上\n- 能够处理具有高维特征的输入样本，而且不需要降维\n- 能够评估各个特征在分类问题上的重要性\n- 对于缺省值问题也能够获得很好得结果\n\n![image-20210307235205695](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170209.png)\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170210.png)\n\n### 学习算法\n\n根据下列算法而建造每棵树：\n\n- 用N来表示训练用例（样本）的个数，M表示特征数目。\n- 输入特征数目m，用于确定决策树上一个节点的决策结果；其中m应远小于M。\n- 从N个训练用例（样本）中以有放回抽样的方式，取样N次，形成一个训练集（即bootstrap取样），并用未抽到的用例（样本）作预测，评估其误差。\n- 对于每一个节点，随机选择m个特征，决策树上每个节点的决定都是基于这些特征确定的。根据这m个特征，计算其最佳的分裂方式。\n\n### sklearn.ensemble，集成方法模块\n\nsklearn.ensemble提供了准确性更加好的集成方法，里面包含了主要的RandomForestClassifier(随机森林)方法。\n\n![image-20210307235222283](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170210.png)\n\n```python\nclass sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None)\n  \"\"\"\n  :param n_estimators：integer，optional（default = 10） 森林里的树木数量。\n\n  :param criteria：string，可选（default =“gini”）分割特征的测量方法\n\n  :param max_depth：integer或None，可选（默认=无）树的最大深度\n\n  :param bootstrap：boolean，optional（default = True）是否在构建树时使用自举样本。\n\n  \"\"\"\n```\n\n#### 属性\n\n- classes_：shape = [n_classes]的数组或这样的数组的列表，类标签（单输出问题）或类标签数组列表（多输出问题）。\n- feature*importances*：array = [n_features]的数组， 特征重要性（越高，功能越重要）。\n\n#### 方法\n\n- fit（X，y [，sample_weight]） 从训练集（X，Y）构建一棵树林。\n- predict（X） 预测X的类\n- score（X，y [，sample_weight]） 返回给定测试数据和标签的平均精度。\n- decision_path（X） 返回森林中的决策路径\n\n### 泰坦尼克号乘客数据案例\n\n![image-20210307234904569](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170211.png)\n\n这里我们通过决策树和随机森林对这个数据进行一个分类，判断乘客的生还。\n\n```python\nimport pandas as pd\nimport sklearn\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\n\ntitanic = pd.read_csv('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt')\n\n#选取一些特征作为我们划分的依据\nx = titanic[['pclass', 'age', 'sex']]\ny = titanic['survived']\n\n# 填充缺失值\nx['age'].fillna(x['age'].mean(), inplace=True)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\ndt = DictVectorizer(sparse=False)\n\nprint(x_train.to_dict(orient=\"record\"))\n\n# 按行，样本名字为键，列名也为键，[{\"1\":1,\"2\":2,\"3\":3}]\nx_train = dt.fit_transform(x_train.to_dict(orient=\"record\"))\n\nx_test = dt.fit_transform(x_test.to_dict(orient=\"record\"))\n\n# 使用决策树\ndtc = DecisionTreeClassifier()\n\ndtc.fit(x_train, y_train)\n\ndt_predict = dtc.predict(x_test)\n\nprint(dtc.score(x_test, y_test))\n\nprint(classification_report(y_test, dt_predict, target_names=[\"died\", \"survived\"]))\n\n# 使用随机森林\n\nrfc = RandomForestClassifier()\n\nrfc.fit(x_train, y_train)\n\nrfc_y_predict = rfc.predict(x_test)\n\nprint(rfc.score(x_test, y_test))\n\nprint(classification_report(y_test, rfc_y_predict, target_names=[\"died\", \"survived\"]))\n```\n\n### 总结\n\n![第二天总结](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170212.png)\n\n# 4. 回归算法\n\n回归是统计学中最有力的工具之一。机器学习监督学习算法分为分类算法和回归算法两种，其实就是根据类别标签分布类型为离散型、连续性而定义的。回归算法用于连续型分布预测，针对的是数值型的样本，使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签。\n\n回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。那么什么是线性关系和非线性关系？\n\n比如说在房价上，房子的面积和房子的价格有着明显的关系。那么X=房间大小，Y=房价，那么在坐标系中可以看到这些点：\n\n![image-20210109202049907](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170213.png)\n\n那么通过一条直线把这个关系描述出来，叫线性关系。\n\n![image-20210109202116618](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170214.png)\n\n如果是一条曲线，那么叫非线性关系\n\n![image-20210109202124416](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170215.png)\n\n那么回归的目的就是建立一个回归方程（函数）用来预测目标值，回归的求解就是求这个回归方程的回归系数。\n\n## 4.1 回归算法之线性回归\n\n线性回归的定义是：目标值预期是输入变量的线性组合。线性模型形式简单、易于建模，但却蕴含着机器学习中一些重要的基本思想。线性回归，是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。\n\n\u003e 优点：结果易于理解，计算不复杂\n\u003e\n\u003e 缺点：对非线性的数据拟合不好\n\u003e\n\u003e 适用数据类型：数值型和标称型\n\n对于单变量线性回归，例如：前面房价例子中房子的大小预测房子的价格。**f(x) = w1\\*x+w0**，这样通过主要参数w1就可以得出预测的值。\n\n通用公式为：\n\n![image-20210109202317714](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170216.png)\n\n那么对于多变量回归，例如：瓜的好坏程度 **f(x) = w0+0.2\\*色泽+0.5\\*根蒂+0.3\\*敲声**，得出的值来判断一个瓜的好与不好的程度。\n\n通用公式为：\n\n![image-20210109202341953](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170217.png)\n\n线性模型中的向量W值，客观的表达了各属性在预测中的重要性，因此线性模型有很好的解释性。对于这种“多特征预测”也就是（多元线性回归），那么线性回归就是在这个基础上得到这些W的值，然后以这些值来建立模型，预测测试数据。简单的来说就是学得一个线性模型以尽可能准确的预测实值输出标记。\n\n那么如果对于多变量线性回归来说我们可以通过向量的方式来表示W值与特征X值之间的关系：\n\n![image-20210109202404048](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170218.png)\n\n两向量相乘，结果为一个整数是估计值,其中所有特征集合的第一个特征值x0=1,那么我们可以通过通用的向量公式来表示线性模型：\n\n![image-20210109202458163](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170219.png)\n\n一个列向量的转置与特征的乘积，得出我们预测的结果，但是显然我们这个模型得到的结果可定会有误差，如下图所示：\n\n![image-20210109202642215](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170220.png)\n\n### 损失函数\n\n损失函数是一个贯穿整个机器学习重要的一个概念，大部分机器学习算法都会有误差，我们得通过显性的公式来描述这个误差，并且将这个误差优化到最小值。\n\n对于线性回归模型，将模型与数据点之间的距离差之和做为衡量匹配好坏的标准，误差越小,匹配程度越大。我们要找的模型就是需要将f(x)和我们的真实值之间最相似的状态。于是我们就有了误差公式，模型与数据差的平方和最小：\n\n![image-20210109202707788](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170221.png)\n\n上面公式定义了所有的误差和，那么现在需要使这个值最小？那么有两种方法，**一种使用梯度下降算法**，**另一种使正规方程解法（只适用于简单的线性回归）**。\n\n### 梯度下降算法\n\n上面误差公式是一个通式，我们取两个单个变量来求最小值，误差和可以表示为：\n\n![image-20210109202725779](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170222.png)\n\n可以通过调整不同的w1和w0的值，就能使误差不断变化，而当你找到这个公式的最小值时，你就能得到最好的w1,w0 而这对(w1,w0)就是能最好描述你数据关系的模型参数。\n\n怎么找cost(w0+w1x1)的最小? cost(w0+w1x1)的图像其实像一个山谷一样，有一个最低点。找这个最低点的办法就是，先随便找一个点(w1=5, w0=4), 然后 沿着这个碗下降的方向找，最后就能找到山谷的最低点。\n\n![image-20210109202849700](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170223.png)\n\n所以得出![image-20210109202900085](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170224.png)，那么这个过程是按照某一点在w1上的偏导数下降寻找最低点。当然在进行移动的时候也需要考虑，每次移动的速度，也就是α*α*的值,这个值也叫做（学习率），如下式：\n\n![image-20210109202923425](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170225.png)\n\n这样就能求出w0,w1的值，当然你这个过程是不断的进行迭代求出来，通过交叉验证方法即可。\n\n### LinearRegression\n\n### sklearn.linear_model.LinearRegression\n\n```python\nclass LinearRegression(fit_intercept = True，normalize = False，copy_X = True，n_jobs = 1)\n  \"\"\"\n  :param normalize:如果设置为True时，数据进行标准化。请在使用normalize = False的估计器调时用fit之前使用preprocessing.StandardScaler\n\n  :param copy_X:boolean，可选，默认为True，如果为True，则X将被复制\n\n  :param n_jobs：int，可选，默认1。用于计算的CPU核数\n  \"\"\"\n```\n\n实例代码：\n\n```python\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\n```\n\n### 方法\n\n**fit(X,y,sample_weight = None)**\n\n使用X作为训练数据拟合模型，y作为X的类别值。X，y为数组或者矩阵\n\n```python\nreg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n```\n\n**predict(X)**\n\n预测提供的数据对应的结果\n\n```python\nreg.predict([[3,3]])\n\narray([ 3.])\n```\n\n### 属性\n\n**coef_**\n\n表示回归系数w=(w1,w2….)\n\n```python\nreg.coef_\n\narray([ 0.5,  0.5])\n```\n\n**intercept_** 表示w0\n\n### 加入交叉验证\n\n前面我们已经提到了模型的交叉验证，那么我们这个自己去建立数据集，然后通过线性回归的交叉验证得到模型。由于sklearn中另外两种回归岭回归、lasso回归都本省提供了回归CV方法，比如linear_model.Lasso，交叉验证linear_model.LassoCV；linear_model.Ridge，交叉验证linear_model.RidgeCV。所以我们需要通过前面的cross_validation提供的方法进行k-折交叉验证。\n\n```python\nfrom sklearn.datasets.samples_generator import make_regression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\n\nlr = linear_model.LinearRegression()\nX, y = make_regression(n_samples=200, n_features=5000, random_state=0)\nresult = cross_val_score(lr, X, y)\nprint result\n```\n\n## 4.2 线性回归案例分析\n\n### 波士顿房价预测\n\n使用scikit-learn中内置的回归模型对“美国波士顿房价”数据进行预测。对于一些比赛数据，可以从kaggle官网上获取，网址：https://www.kaggle.com/datasets\n\n**1.美国波士顿地区房价数据描述**\n\n```python\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\n\nprint boston.DESCR\n```\n\n**2.波士顿地区房价数据分割**\n\n```python\nfrom sklearn.cross_validation import train_test_split\nimport numpy as np\nX = boston.data\ny = boston.target\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=33,test_size = 0.25)\n```\n\n**3.训练与测试数据标准化处理**\n\n```python\nfrom sklearn.preprocessing import StandardScaler\nss_X = StandardScaler()\nss_y = StandardScaler()\n\nX_train = ss_X.fit_transform(X_train)\nX_test = ss_X.transform(X_test)\ny_train = ss_X.fit_transform(y_train)\nX_train = ss_X.transform(y_test)\n```\n\n**4.使用最简单的线性回归模型LinearRegression和梯度下降估计SGDRegressor对房价进行预测**\n\n```python\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train,y_train)\nlr_y_predict = lr.predict(X_test)\n\nfrom sklearn.linear_model import SGDRegressor\nsgdr = SGDRegressor()\nsgdr.fit(X_train,y_train)\nsgdr_y_predict = sgdr.predict(X_test)\n```\n\n**5.性能评测**\n\n对于不同的类别预测，我们不能苛刻的要求回归预测的数值结果要严格的与真实值相同。一般情况下，我们希望衡量预测值与真实值之间的差距。因此，可以测评函数进行评价。其中最为直观的评价指标均方误差(Mean Squared Error)MSE，因为这也是线性回归模型所要优化的目标。\n\nMSE的计算方法如式：\n\nMSE=1m∑i=1m(yi−y¯)2*M**S**E*=*m*1∑*i*=1*m*(*y**i*−*y*¯)2\n\n**使用MSE评价机制对两种模型的回归性能作出评价**\n\n```python\nfrom sklearn.metrics import mean_squared_error\n\nprint '线性回归模型的均方误差为：',mean_squared_error(ss_y.inverse_transform(y_test),ss_y.inverse_tranform(lr_y_predict))\nprint '梯度下降模型的均方误差为：',mean_squared_error(ss_y.inverse_transform(y_test),ss_y.inverse_tranform(sgdr_y_predict))\n```\n\n通过这一比较发现，使用梯度下降估计参数的方法在性能表现上不及使用解析方法的LinearRegression，但是如果面对训练数据规模十分庞大的任务，随即梯度法不论是在分类还是回归问题上都表现的十分高效，可以在不损失过多性能的前提下，节省大量计算时间。根据Scikit-learn光网的建议，如果数据规模超过10万，推荐使用随机梯度法估计参数模型。\n\n\u003e 注意：线性回归器是最为简单、易用的回归模型。正式因为其对特征与回归目标之间的线性假设，从某种程度上说也局限了其应用范围。特别是，现实生活中的许多实例数据的各种特征与回归目标之间，绝大多数不能保证严格的线性关系。尽管如此，在不清楚特征之间关系的前提下，我们仍然可以使用线性回归模型作为大多数数据分析的基线系统。\n\n完整代码如下：\n\n```python\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_boston\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import mean_squared_error,classification_report\nfrom sklearn.cluster import KMeans\n\n\ndef linearmodel():\n    \"\"\"\n    线性回归对波士顿数据集处理\n    :return: None\n    \"\"\"\n\n    # 1、加载数据集\n\n    ld = load_boston()\n\n    x_train,x_test,y_train,y_test = train_test_split(ld.data,ld.target,test_size=0.25)\n\n    # 2、标准化处理\n\n    # 特征值处理\n    std_x = StandardScaler()\n    x_train = std_x.fit_transform(x_train)\n    x_test = std_x.transform(x_test)\n\n\n    # 目标值进行处理\n\n    std_y  = StandardScaler()\n    y_train = std_y.fit_transform(y_train)\n    y_test = std_y.transform(y_test)\n\n    # 3、估计器流程\n\n    # LinearRegression\n    lr = LinearRegression()\n\n    lr.fit(x_train,y_train)\n\n    # print(lr.coef_)\n\n    y_lr_predict = lr.predict(x_test)\n\n    y_lr_predict = std_y.inverse_transform(y_lr_predict)\n\n    print(\"Lr预测值：\",y_lr_predict)\n\n\n    # SGDRegressor\n    sgd = SGDRegressor()\n\n    sgd.fit(x_train,y_train)\n\n    # print(sgd.coef_)\n\n    y_sgd_predict = sgd.predict(x_test)\n\n    y_sgd_predict = std_y.inverse_transform(y_sgd_predict)\n\n    print(\"SGD预测值：\",y_sgd_predict)\n\n    # 带有正则化的岭回归\n\n    rd = Ridge(alpha=0.01)\n\n    rd.fit(x_train,y_train)\n\n    y_rd_predict = rd.predict(x_test)\n\n    y_rd_predict = std_y.inverse_transform(y_rd_predict)\n\n    print(rd.coef_)\n\n    # 两种模型评估结果\n\n    print(\"lr的均方误差为：\",mean_squared_error(std_y.inverse_transform(y_test),y_lr_predict))\n\n    print(\"SGD的均方误差为：\",mean_squared_error(std_y.inverse_transform(y_test),y_sgd_predict))\n\n    print(\"Ridge的均方误差为：\",mean_squared_error(std_y.inverse_transform(y_test),y_rd_predict))\n\n    return None\n```\n\n## 4.3 欠拟合与过拟合\n\n机器学习中的泛化，泛化即是，模型学习到的概念在它处于学习的过程中时模型没有遇见过的样本时候的表现。在机器学习领域中，当我们讨论一个机器学习模型学习和泛化的好坏时，我们通常使用术语：过拟合和欠拟合。我们知道模型训练和测试的时候有两套数据，训练集和测试集。在对训练数据进行拟合时，需要照顾到每个点，而其中有一些噪点，当某个模型过度的学习训练数据中的细节和噪音，以至于模型在新的数据上表现很差，这样的话模型容易复杂，拟合程度较高，造成过拟合。而相反如果值描绘了一部分数据那么模型复杂度过于简单，欠拟合指的是模型在训练和预测时表现都不好的情况，称为欠拟合。\n\n我们来看一下线性回归中拟合的几种情况图示：\n\n![image-20210109203652639](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170226.png)\n\n![image-20210109203709661](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170227.png)\n\n![image-20210109203735076](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170228.png)\n\n### 解决过拟合的方法\n\n在线性回归中，对于特征集过小的情况，容易造成欠拟合（underfitting），对于特征集过大的情况，容易造成过拟合（overfitting）。针对这两种情况有了更好的解决办法\n\n#### 欠拟合\n\n欠拟合指的是模型在训练和预测时表现都不好的情况，欠拟合通常不被讨论，因为给定一个评估模型表现的指标的情况下，欠拟合很容易被发现。矫正方法是继续学习并且试着更换机器学习算法。\n\n##### 过拟合\n\n对于过拟合，特征集合数目过多，我们需要做的是尽量不让回归系数数量变多，对拟合（损失函数）加以限制。\n\n（1）当然解决过拟合的问题可以减少特征数，显然这只是权宜之计，因为特征意味着信息，放弃特征也就等同于丢弃信息，要知道，特征的获取往往也是艰苦卓绝的。\n\n（2）引入了 **正则化** 概念。\n\n**直观上来看，如果我们想要解决上面回归中的过拟合问题，我们最好就要消除x3\\*x\\*3和x4\\*x\\*4的影响，也就是想让θ3,θ4\\*θ\\*3,\\*θ\\*4都等于0，一个简单的方法就是我们对θ3,θ4\\*θ\\*3,\\*θ\\*4进行惩罚，增加一个很大的系数，这样在优化的过程中就会使这两个参数为零。**\n\n## 4.4 回归算法之岭回归\n\n具有L2正则化的线性最小二乘法。岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。当数据集中存在共线性的时候，岭回归就会有用。\n\n### sklearn.linear_model.Ridge\n\n```python\nclass sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)**\n  \"\"\"\n  :param alpha:float类型，正规化的程度\n  \"\"\"\nfrom sklearn.linear_model import Ridge\nclf = Ridge(alpha=1.0)\nclf.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]))\n```\n\n### 方法\n\n**score(X, y, sample_weight=None)**\n\n```python\nclf.score()\n```\n\n### 属性\n\n**coef_**\n\n```python\nclf.coef_\narray([ 0.34545455,  0.34545455])\n```\n\n**intercept_**\n\n```python\nclf.intercept_\n0.13636...\n```\n\n### 案例\n\n```python\ndef linearmodel():\n    \"\"\"\n    线性回归对波士顿数据集处理\n    :return: None\n    \"\"\"\n\n    # 1、加载数据集\n\n    ld = load_boston()\n\n    x_train,x_test,y_train,y_test = train_test_split(ld.data,ld.target,test_size=0.25)\n\n    # 2、标准化处理\n\n    # 特征值处理\n    std_x = StandardScaler()\n    x_train = std_x.fit_transform(x_train)\n    x_test = std_x.transform(x_test)\n\n\n    # 目标值进行处理\n\n    std_y  = StandardScaler()\n    y_train = std_y.fit_transform(y_train)\n    y_test = std_y.transform(y_test)\n\n    # 3、估计器流程\n\n    # LinearRegression\n    lr = LinearRegression()\n\n    lr.fit(x_train,y_train)\n\n    # print(lr.coef_)\n\n    y_lr_predict = lr.predict(x_test)\n\n    y_lr_predict = std_y.inverse_transform(y_lr_predict)\n\n    print(\"Lr预测值：\",y_lr_predict)\n\n\n    # SGDRegressor\n    sgd = SGDRegressor()\n\n    sgd.fit(x_train,y_train)\n\n    # print(sgd.coef_)\n\n    y_sgd_predict = sgd.predict(x_test)\n\n    y_sgd_predict = std_y.inverse_transform(y_sgd_predict)\n\n    print(\"SGD预测值：\",y_sgd_predict)\n\n    # 带有正则化的岭回归\n\n    rd = Ridge(alpha=0.01)\n\n    rd.fit(x_train,y_train)\n\n    y_rd_predict = rd.predict(x_test)\n\n    y_rd_predict = std_y.inverse_transform(y_rd_predict)\n\n    print(rd.coef_)\n\n    # 两种模型评估结果\n\n    print(\"lr的均方误差为：\",mean_squared_error(std_y.inverse_transform(y_test),y_lr_predict))\n\n    print(\"SGD的均方误差为：\",mean_squared_error(std_y.inverse_transform(y_test),y_sgd_predict))\n\n    print(\"Ridge的均方误差为：\",mean_squared_error(std_y.inverse_transform(y_test),y_rd_predict))\n\n    return None\n```\n\n# 5. 非监督学习\n\n从本节开始，将正式进入到无监督学习（Unsupervised Learning）部分。无监督学习，顾名思义，就是不受监督的学习，一种自由的学习方式。该学习方式不需要先验知识进行指导，而是不断地自我认知，自我巩固，最后进行自我归纳，在机器学习中，无监督学习可以被简单理解为不为训练集提供对应的类别标识（label），其与有监督学习的对比如下： 有监督学习（Supervised Learning）下的训练集：\n\n(x(1),y(1)),(x(2),y2)(x(1),y(1)),(x(2),y2)\n\n无监督学习（Unsupervised Learning）下的训练集：\n\n(x(1)),(x(2)),(x(3))(x(1)),(x(2)),(x(3))\n\n在有监督学习中，我们把对样本进行分类的过程称之为分类（Classification），而在无监督学习中，我们将物体被划分到不同集合的过程称之为聚类（Clustering）\n\n##5.1 非监督学习之k-means\n\nK-means通常被称为劳埃德算法，这在数据聚类中是最经典的，也是相对容易理解的模型。算法执行的过程分为4个阶段。\n\n- 1.首先，随机设K个特征空间内的点作为初始的聚类中心。\n- 2.然后，对于根据每个数据的特征向量，从K个聚类中心中寻找距离最近的一个，并且把该数据标记为这个聚类中心。\n- 3.接着，在所有的数据都被标记过聚类中心之后，根据这些数据新分配的类簇，通过取分配给每个先前质心的所有样本的平均值来创建新的质心重,新对K个聚类中心做计算。\n- 4.最后，计算旧和新质心之间的差异,如果所有的数据点从属的聚类中心与上一次的分配的类簇没有变化，那么迭代就可以停止，否则回到步骤2继续循环。\n\nK均值等于具有小的全对称协方差矩阵的期望最大化算法。\n\n### sklearn.cluster.KMeans\n\n```python\nclass sklearn.cluster.KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto')\n  \"\"\"\n  :param n_clusters:要形成的聚类数以及生成的质心数\n\n  :param init:初始化方法，默认为'k-means ++',以智能方式选择k-均值聚类的初始聚类中心，以加速收敛;random,从初始质心数据中随机选择k个观察值（行\n\n  :param n_init：int，默认值：10使用不同质心种子运行k-means算法的时间。最终结果将是n_init连续运行在惯性方面的最佳输出。\n\n  :param n_jobs：int用于计算的作业数量。这可以通过并行计算每个运行的n_init。如果-1使用所有CPU。如果给出1，则不使用任何并行计算代码，这对调试很有用。对于-1以下的n_jobs，使用（n_cpus + 1 + n_jobs）。因此，对于n_jobs = -2，所有CPU都使用一个。\n\n  :param random_state:随机数种子，默认为全局numpy随机数生成器\n  \"\"\"\nfrom sklearn.cluster import KMeans\nimport numpy as np\nX = np.array([[1, 2], [1, 4], [1, 0],[4, 2], [4, 4], [4, 0]])\nkmeans = KMeans(n_clusters=2, random_state=0)\n```\n\n### 方法\n\n**fit(X,y=None)**\n\n使用X作为训练数据拟合模型\n\n```python\nkmeans.fit(X)\n```\n\n**predict(X)**\n\n预测新的数据所在的类别\n\n```python\nkmeans.predict([[0, 0], [4, 4]])\narray([0, 1], dtype=int32)\n```\n\n### 属性\n\n**cluster\\*centers\\***\n\n集群中心的点坐标\n\n```python\nkmeans.cluster_centers_\narray([[ 1.,  2.],\n       [ 4.,  2.]])\n```\n\n**labels_**\n\n每个点的类别\n\n```python\nkmeans.labels_\n```\n\n### k-means ++\n\n### 手写数字数据上K-Means聚类的演示\n\n```python\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\n\ndef kmeans():\n    \"\"\"\n    手写数字聚类过程\n    :return: None\n    \"\"\"\n    # 加载数据\n\n    ld = load_digits()\n\n    print(ld.target[:20])\n\n\n    # 聚类\n    km = KMeans(n_clusters=810)\n\n    km.fit_transform(ld.data)\n\n    print(km.labels_[:20])\n\n    print(silhouette_score(ld.data,km.labels_))\n\n    return None\n\n\n\nif __name__==\"__main__\":\n    kmeans()\n```\n\n=======  \n![image-20210306160955457](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170111.png)\n\n# 1. Scikit-learn与特征工程\n\n数据决定了机器学习的上限，而算法只是尽可能逼近这个上限”，这句话很好的阐述了数据在机器学习中的重要性。大部分直接拿过来的数据都是特征不明显的、没有经过处理的或者说是存在很多无用的数据，那么需要进行一些特征处理，特征的缩放等等，满足训练数据的要求。\n\n我们将初次接触到Scikit-learn这个机器学习库的使用\n\n![image-20200623201306065](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170112.png)\n\n![image-20200623200615498](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170113.png)\n\n\u003e Scikit-learn\n\u003e\n\u003e - Python语言的机器学习工具\n\u003e - 所有人都适用，可在不同的上下文中重用\n\u003e - 基于NumPy、SciPy和matplotlib构建\n\u003e - 开源、商业可用 - BSD许可\n\u003e - 目前稳定版本0.18\n\n自2007年发布以来，scikit-learn已经成为最给力的Python机器学习库（library）了。scikit-learn支持的机器学习算法包括分类，回归，降维和聚类。还有一些特征提取（extracting features）、数据处理（processing data）和模型评估（evaluating models）的模块。作为Scipy库的扩展，scikit-learn也是建立在Python的NumPy和matplotlib库基础之上。NumPy可以让Python支持大量多维矩阵数据的高效操作，matplotlib提供了可视化工具，SciPy带有许多科学计算的模型。     \n\nscikit-learn文档完善，容易上手，丰富的API，使其在学术界颇受欢迎。开发者用scikit-learn实验不同的算法，只要几行代码就可以搞定。scikit-learn包括许多知名的机器学习算法的实现，包括LIBSVM和LIBLINEAR。还封装了其他的Python库，如自然语言处理的NLTK库。另外，scikit-learn内置了大量数据集，允许开发者集中于算法设计，节省获取和整理数据集的时间。\n\n安装的话参考下面步骤： 创建一个基于Python3的虚拟环境：\n\n```\nmkvirtualenv -p /usr/local/bin/python3.6 ml3\n```\n\n在ubuntu的虚拟环境当中运行以下命令\n\n```\npip3 install Scikit-learn\n```\n\n然后通过导入命令查看是否可以使用：\n\n```python\nimport sklearn\n```\n\n### 数据的特征工程\n\n从数据中抽取出来的对预测结果有用的信息，通过专业的技巧进行数据处理，是的特征能在机器学习算法中发挥更好的作用。优质的特征往往描述了数据的固有结构。 最初的原始特征数据集可能太大，或者信息冗余，因此在机器学习的应用中，一个初始步骤就是选择特征的子集，或构建一套新的特征集，减少功能来促进算法的学习，提高泛化能力和可解释性。\n\n例如：你要查看不同地域女性的穿衣品牌情况，预测不同地域的穿衣品牌。如果其中含有一些男性的数据，是不是要将这些数据给去除掉\n\n![image-20200623201321325](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170114.png)\n\n### 特征工程的意义\n\n- 更好的特征意味着更强的鲁棒性\n- 更好的特征意味着只需用简单模型\n- 更好的特征意味着更好的结果\n\n### 特征工程之特征处理\n\n特征工程中最重要的一个环节就是特征处理，特征处理包含了很多具体的专业技巧\n\n- 特征预处理\n  - 单个特征\n    - 归一化\n    - 标准化\n    - 缺失值\n  - 多个特征\n    - 降维\n      - PCA\n\n### 特征工程之特征抽取与特征选择\n\n如果说特征处理其实就是在对已有的数据进行运算达到我们目标的数据标准。特征抽取则是将任意数据格式（例如文本和图像）转换为机器学习的数字特征。而特征选择是在已有的特征中选择更好的特征。后面会详细介绍特征选择主要区别于降维。\n\n## 1.1 数据的来源与类型\n\n大部分的数据都来自已有的数据库，如果没有的话也可以交给很多爬虫工程师去采集，来提供。也可以来自平时的记录，反正数据无处不在，大都是可用的。\n\n### 数据的类型\n\n按照**机器学习的数据分类**我们可以将数据分成：\n\n- 标称型：标称型目标变量的结果只在有限目标集中取值，如真与假(标称型目标变量主要用于分类)\n- 数值型：数值型目标变量则可以从无限的数值集合中取值，如0.100，42.001等 (数值型目标变量主要用于回归分析)\n\n按照**数据的本身分布特性**\n\n- 离散型\n- 连续型\n\n那么什么是离散型和连续型数据呢？首先连续型数据是有规律的,离散型数据是没有规律的\n\n- 离散变量是指其数值只能用自然数或整数单位计算的则为离散变量.例如，班级人数、进球个数、是否是某个类别等等\n- 连续型数据是指在指定区间内可以是任意一个数值,例如，票房数据、花瓣大小分布数据\n\n## 1.2 数据的特征抽取\n\n现实世界中多数特征都不是连续变量，比如分类、文字、图像等，为了对非连续变量做特征表述，需要对这些特征做数学化表述，因此就用到了特征提取. `sklearn.feature_extraction`提供了特征提取的很多方法\n\n### 分类特征变量提取\n\n我们将城市和环境作为字典数据，来进行特征的提取。\n\n`sklearn.feature_extraction.DictVectorizer(sparse = True)`\n\n将映射列表转换为Numpy数组或scipy.sparse矩阵\n\n- sparse 是否转换为scipy.sparse矩阵表示，默认开启\n\n![image-20210316201130096](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170115.png)\n\n**方法**\n\n**fit_transform(X,y)**\n\n应用并转化映射列表X，y为目标类型\n\n**inverse_transform(X[, dict_type])**\n\n将Numpy数组或scipy.sparse矩阵转换为映射列表\n\n```python\nfrom sklearn.feature_extraction import DictVectorizer\nonehot = DictVectorizer()  # 如果结果不用toarray，请开启sparse=False\n    instances = [{'city': '北京', 'temperature': 100}, {'city': '上海', 'temperature': 60},\n                 {'city': '深圳', 'temperature': 30}]\n    X = onehot.fit_transform(instances).toarray()\n    print(onehot.inverse_transform(X))\n    print(onehot.get_feature_names())\n    print(X)\n```\n\n### 文本特征提取（只限于英文）\n\n文本的特征提取应用于很多方面，比如说文档分类、垃圾邮件分类和新闻分类。那么文本分类是通过词是否存在、以及词的概率（重要性）来表示。\n\n**(1)文档的中词的出现**\n\n数值为1表示词表中的这个词出现，为0表示未出现\n\n**sklearn.feature_extraction.text.CountVectorizer()**\n\n将文本文档的集合转换为计数矩阵（scipy.sparse matrices）\n\n**方法**\n\n**fit_transform(raw_documents,y)**\n\n学习词汇词典并返回词汇文档矩阵\n\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\ncontent = [\"life is short,i like python\",\"life is too long,i dislike python\"]\nvectorizer = CountVectorizer()\nprint(vectorizer.fit_transform(content).toarray())\n```\n\n![image-20210316200811022](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170116.png)\n\n需要toarray()方法转变为numpy的数组形式\n\n\u003e 温馨提示：每个文档中的词，只是整个语料库中所有词，的很小的一部分，这样造成特征向量的稀疏性（很多值为0）为了解决存储和运算速度的问题，使用Python的scipy.sparse矩阵结构\n\n**(2)TF-IDF表示词的重要性**\n\nTfidfVectorizer会根据指定的公式将文档中的词转换为概率表示。（朴素贝叶斯介绍详细的用法）\n\n**class sklearn.feature_extraction.text.TfidfVectorizer()**\n\n**方法**\n\n**fit_transform(raw_documents,y)**\n\n学习词汇和idf，返回术语文档矩阵。\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ncontent = [\"life is short,i like python\",\"life is too long,i dislike python\"]\nvectorizer = TfidfVectorizer(stop_words='english')\nprint(vectorizer.fit_transform(content).toarray())\nprint(vectorizer.vocabulary_)\n```\n\n### 文本特征提取（中文）\n\n![image-20200623201427282](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170117.png)\n\n![image-20210306161347051](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170118.png)\n\n```python\ndef cutword():\n    con1 = jieba.cut(\"今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今。\")\n    con2 = jieba.cut(\"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\")\n    con3 = jieba.cut(\"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\")\n    # 转换成列表\n    content1 = list(con1)\n    content2 = list(con2)\n    content3 = list(con3)\n    # 吧列表转换成字符串\n    c1 = ' '.join(content1)\n    c2 = ' '.join(content2)\n    c3 = ' '.join(content3)\n    return c1, c2, c3\n```\n\n![image-20210316201043435](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170119.png)\n\n## 1.3 数据的特征预处理\n\n![image-20210306161652247](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170120.png)\n\n### 单个特征\n\n**（1）归一化**\n\n归一化首先在特征（维度）非常多的时候，可以防止某一维或某几维对数据影响过大，也是为了把不同来源的数据统一到一个参考区间下，这样比较起来才有意义，其次可以程序可以运行更快。 例如：一个人的身高和体重两个特征，假如体重50kg，身高175cm,由于两个单位不一样，数值大小不一样。如果比较两个人的体型差距时，那么身高的影响结果会比较大，k-临近算法会有这个距离公式。\n\n**min-max方法**\n\n常用的方法是通过对原始数据进行线性变换把数据映射到[0,1]之间，变换的函数为：\n\n![image-20210306161529396](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170121.png)\n\n![image-20210306161734975](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170122.png)\n\n其中min是样本中最小值，max是样本中最大值，注意在**数据流场景下最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景**。\n\n![image-20210306161914681](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170123.png)\n\n`sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)…)`\n\n- min-max自定义处理\n\n这里我们使用相亲约会对象数据在MatchData.txt，这个样本时男士的数据，三个特征，玩游戏所消耗时间的百分比、每年获得的飞行常客里程数、每周消费的冰淇淋公升数。然后有一个 所属类别，被女士评价的三个类别，不喜欢、魅力一般、极具魅力。 首先导入数据进行矩阵转换处理.\n\n![image-20200623201658337](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170124.png)\n\n```python\nimport numpy as np\n\ndef data_matrix(file_name):\n  \"\"\"\n  将文本转化为matrix\n  :param file_name: 文件名\n  :return: 数据矩阵\n  \"\"\"\n  fr = open(file_name)\n  array_lines = fr.readlines()\n  number_lines = len(array_lines)\n  return_mat = zeros((number_lines, 3))\n  # classLabelVector = []\n  index = 0\n  for line in array_lines:\n    line = line.strip()\n    list_line = line.split('\\t')\n    return_mat[index,:] = list_line[0:3]\n    # if(listFromLine[-1].isdigit()):\n    #     classLabelVector.append(int(listFromLine[-1]))\n    # else:\n    #     classLabelVector.append(love_dictionary.get(listFromLine[-1]))\n    # index += 1\n  return return_mat\n```\n\n输出结果为\n\n```python\n[[  4.09200000e+04   8.32697600e+00   9.53952000e-01]\n [  1.44880000e+04   7.15346900e+00   1.67390400e+00]\n [  2.60520000e+04   1.44187100e+00   8.05124000e-01]\n ...,\n [  2.65750000e+04   1.06501020e+01   8.66627000e-01]\n [  4.81110000e+04   9.13452800e+00   7.28045000e-01]\n [  4.37570000e+04   7.88260100e+00   1.33244600e+00]]\n```\n\n我们查看数据集会发现，有的数值大到几万，有的才个位数，同样如果计算两个样本之间的距离时，其中一个影响会特别大。也就是说飞行里程数对于结算结果或者说相亲结果影响较大，**但是统计的人觉得这三个特征同等重要**，所以需要将数据进行这样的处理。\n\n这样每个特征任意的范围将变成[0,1]的区间内的值，或者也可以根据需求处理到[-1,1]之间，我们再定义一个函数，进行这样的转换。\n\n```python\ndef feature_normal(data_set):\n    \"\"\"\n    特征归一化\n    :param data_set:\n    :return:\n    \"\"\"\n    # 每列最小值\n    min_vals = data_set.min(0)\n    # 每列最大值\n    max_vals = data_set.max(0)\n    ranges = max_vals - min_vals\n    norm_data = np.zeros(np.shape(data_set))\n    # 得出行数\n    m = data_set.shape[0]\n    # 矩阵相减\n    norm_data = data_set - np.tile(min_vals, (m,1))\n    # 矩阵相除\n    norm_data = norm_data/np.tile(ranges, (m, 1)))\n    return norm_data\n```\n\n输出结果为\n\n```python\n[[ 0.44832535  0.39805139  0.56233353]\n [ 0.15873259  0.34195467  0.98724416]\n [ 0.28542943  0.06892523  0.47449629]\n ...,\n [ 0.29115949  0.50910294  0.51079493]\n [ 0.52711097  0.43665451  0.4290048 ]\n [ 0.47940793  0.3768091   0.78571804]]\n```\n\n这样得出的结果都非常相近，这样的数据可以直接提供测试验证了。\n\n**注意在特定场景下最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。**\n\n- min-max的scikit-learn处理\n\nscikit-learn.preprocessing中的类MinMaxScaler，将数据矩阵缩放到[0,1]之间\n\n```python\n\u003e\u003e\u003e X_train = np.array([[ 1., -1.,  2.],\n...                     [ 2.,  0.,  0.],\n...                     [ 0.,  1., -1.]])\n...\n\u003e\u003e\u003e min_max_scaler = preprocessing.MinMaxScaler()\n\u003e\u003e\u003e X_train_minmax = min_max_scaler.fit_transform(X_train)\n\u003e\u003e\u003e X_train_minmax\narray([[ 0.5       ,  0.        ,  1.        ],\n       [ 1.        ,  0.5       ,  0.33333333],\n       [ 0.        ,  1.        ,  0.        ]])\n```\n\n**（3）标准化**\n\n特点：通过对原始数据进行变换把数据变换到均值为0,方差为1范围内。\n\n常用的方法是z-score标准化，经过处理后的数据均值为0，标准差为1，处理方法是：\n\n![image-20210306162207644](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170125.png)\n\n![image-20210306162246784](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170126.png)\n\n其中*μ*是样本的均值，*σ*是样本的标准差，它们可以通过现有的样本进行估计，在已有的样本足够多的情况下比较稳定，适合嘈杂的数据场景。\n\nsklearn中提供了StandardScalar类实现列标准化:\n\n```python\nIn [2]: import numpy as np\n\nIn [3]: X_train = np.array([[ 1., -1.,  2.],[ 2.,  0.,  0.],[ 0.,  1., -1.]])\n\nIn [4]: from sklearn.preprocessing import StandardScaler\n\nIn [5]: std = StandardScaler()\n\nIn [6]: X_train_std = std.fit_transform(X_train)\n\nIn [7]: X_train_std\nOut[7]:\narray([[ 0.        , -1.22474487,  1.33630621],\n       [ 1.22474487,  0.        , -0.26726124],\n       [-1.22474487,  1.22474487, -1.06904497]])\n```\n\n![image-20200623201857964](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170127.png)\n\n**（3）缺失值**\n\n由于各种原因，许多现实世界的数据集包含缺少的值，通常编码为空白，NaN或其他占位符。然而，这样的数据集与scikit的分类器不兼容，它们假设数组中的所有值都是数字，并且都具有和保持含义。使用不完整数据集的基本策略是丢弃包含缺失值的整个行和/或列。然而，这是以丢失可能是有价值的数据（即使不完整）的代价。更好的策略是估算缺失值，即从已知部分的数据中推断它们。\n\n(1)填充缺失值 使用sklearn.preprocessing中的Imputer类进行数据的填充(旧API)\n\n新：`from sklearn.impute import SimpleImputer`\n\n```python\nclass Imputer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)\n    \"\"\"\n    用于完成缺失值的补充\n\n    :param param missing_values: integer or \"NaN\", optional (default=\"NaN\")\n        丢失值的占位符，对于编码为np.nan的缺失值，使用字符串值“NaN”\n\n    :param strategy: string, optional (default=\"mean\")\n        插补策略\n        如果是“平均值”，则使用沿轴的平均值替换缺失值\n        如果为“中位数”，则使用沿轴的中位数替换缺失值\n        如果“most_frequent”，则使用沿轴最频繁的值替换缺失\n\n    :param axis: integer, optional (default=0)\n        插补的轴\n        如果axis = 0，则沿列排列\n        如果axis = 1，则沿行排列\n    \"\"\"\n\u003e\u003e\u003e import numpy as np\n\u003e\u003e\u003e from sklearn.impute import SimpleImputer\n\u003e\u003e\u003e imp = Imputer(missing_values='NaN', strategy='mean')\n\u003e\u003e\u003e imp.fit([[1, 2], [np.nan, 3], [7, 6]])\nImputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)\n\u003e\u003e\u003e X = [[np.nan, 2], [6, np.nan], [7, 6]]\n\u003e\u003e\u003e print(imp.transform(X))                          \n[[ 4.          2.        ]\n [ 6.          3.666...]\n [ 7.          6.        ]]\n```\n\n### 多个特征\n\n#### **降维**\n\n高维度数据容易出现的问题:特征之间通常是线性相关的。\n\nPCA（Principal component analysis），主成分分析。特点是保存数据集中对方差影响最大的那些特征，PCA极其容易受到数据中特征范围影响，所以在运用PCA前一定要做特征标准化，这样才能保证每维度特征的重要性等同。\n\n目的：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。\n\n作用：可以削减回归分析或者聚类分析中特征的数量\n\n**sklearn.decomposition.PCA**\n\n![image-20210306163131804](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170128.png)\n\n```python\nclass PCA(sklearn.decomposition.base)\n   \"\"\"\n   主成成分分析\n   :param n_components: int, float, None or string\n   这个参数可以帮我们指定希望PCA降维后的特征维度数目。最常用的做法是直接指定降维到的维度数目，此时n_components是一个大于1的整数。\n   我们也可以用默认值，即不输入n_components，此时n_components=min(样本数，特征数)\n\t:param whiten: bool, optional (default False)\n      判断是否进行白化。所谓白化，就是对降维后的数据的每个特征进行归一化。对于PCA降维本身来说一般不需要白化,如果你PCA降维后有后续的数据处理动作，可以考虑白化，默认值是False，即不进行白化\n\n   :param svd_solver:\n      选择一个合适的SVD算法来降维,一般来说，使用默认值就够了。\n    \"\"\"\n```\n\n通过一个例子来看\n\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\npca = PCA(n_components=2)\npca.fit(X)\nPCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n  svd_solver='auto', tol=0.0, whiten=False)\nprint(pca.explained_variance_ratio_)\n# [ 0.99244...  0.00755...]\n```\n\n## 1.4 数据的特征选择\n\n降维本质上是从一个维度空间映射到另一个维度空间，特征的多少别没有减少，当然在映射的过程中特征值也会相应的变化。举个例子，现在的特征是1000维，我们想要把它降到500维。降维的过程就是找个一个从1000维映射到500维的映射关系。原始数据中的1000个特征，每一个都对应着降维后的500维空间中的一个值。假设原始特征中有个特征的值是9，那么降维后对应的值可能是3。而对于特征选择来说，有很多方法：\n\n- Filter(过滤式):VarianceThreshold\n- Embedded(嵌入式)：正则化、决策树\n- Wrapper(包裹式)\n\n其中过滤式的特征选择后，数据本身不变，而数据的维度减少。而嵌入式的特征选择方法也会改变数据的值，维度也改变。Embedded方式是一种自动学习的特征选择方法，后面讲到具体的方法的时候就能理解了。\n\n特征选择主要有两个**功能**：\n\n（1）减少特征数量，降维，使模型泛化能力更强，减少过拟合\n\n（2）增强特征和特征值之间的理解\n\n**sklearn.feature_selection**\n\n**去掉取值变化小的特征（删除低方差特征）**\n\nVarianceThreshold 是特征选择中的一项基本方法。它会移除所有方差不满足阈值的特征。默认设置下，它将移除所有方差为0的特征，即那些在所有样本中数值完全相同的特征。\n\n![image-20210306162950853](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170129.png)\n\n假设我们要移除那些超过80%的数据都为1或0的特征：\n\n```python\nVarianceThreshold(threshold = 0.0)\n删除所有低方差特征\n\nVariance.fit_transform(X,y)       \nX:numpy array格式的数据[n_samples,n_features]\n返回值：训练集差异低于threshold的特征将被删除。\n默认值是保留所有非零方差特征，即删除所有样本\n中具有相同值的特征。\n```\n\n```python\nfrom sklearn.feature_selection import VarianceThreshold\nX = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\nsel = VarianceThreshold(threshold=(.8 * (1 - .8)))\nsel.fit_transform(X)\narray([[0, 1],\n       [1, 0],\n       [0, 0],\n       [1, 1],\n       [1, 0],\n       [1, 1]])\n```\n\n# 2. sklearn数据集与机器学习组成\n\n### 机器学习组成：模型、策略、优化\n\n《统计机器学习》中指出：机器学习=模型+策略+算法。其实机器学习可以表示为：Learning= Representation+Evalution+Optimization。我们就可以将这样的表示和李航老师的说法对应起来。机器学习主要是由三部分组成，即：表示(模型)、评价(策略)和优化(算法)。\n\n**表示(或者称为：模型)：Representation**\n\n表示主要做的就是建模，故可以称为模型。模型要完成的主要工作是转换：将实际问题转化成为计算机可以理解的问题，就是我们平时说的建模。类似于传统的计算机学科中的算法，数据结构，如何将实际的问题转换成计算机可以表示的方式。这部分可以见“简单易学的机器学习算法”。给定数据，我们怎么去选择对应的问题去解决，选择正确的已有的模型是重要的一步。\n\n**评价(或者称为：策略)：Evalution**\n\n评价的目标是判断已建好的模型的优劣。对于第一步中建好的模型，评价是一个指标，用于表示模型的优劣。这里就会是一些评价的指标以及一些评价函数的设计。在机器学习中会有针对性的评价指标。\n\n- 分类问题\n\n**优化：Optimization**\n\n优化的目标是评价的函数，我们是希望能够找到最好的模型，也就是说评价最高的模型。\n\n### 开发机器学习应用程序的步骤\n\n**（1）收集数据**\n\n我们可以使用很多方法收集样本护具，如：制作网络爬虫从网站上抽取数据、从RSS反馈或者API中得到信息、设备发送过来的实测数据。\n\n**（2）准备输入数据**\n\n得到数据之后，还必须确保数据格式符合要求。\n\n**（3）分析输入数据**\n\n这一步的主要作用是确保数据集中没有垃圾数据。如果是使用信任的数据来源，那么可以直接跳过这个步骤\n\n**（4）训练算法**\n\n机器学习算法从这一步才真正开始学习。如果使用无监督学习算法，由于不存在目标变量值，故而也不需要训练算法，所有与算法相关的内容在第（5）步\n\n**（5）测试算法**\n\n这一步将实际使用第（4）步机器学习得到的知识信息。当然在这也需要评估结果的准确率，然后根据需要重新训练你的算法\n\n**（6）使用算法**\n\n转化为应用程序，执行实际任务。以检验上述步骤是否可以在实际环境中正常工作。如果碰到新的数据问题，同样需要重复执行上述的步骤\n\n## 2.1 scikit-learn数据集\n\n我们将介绍sklearn中的数据集类，模块包括用于加载数据集的实用程序，包括加载和获取流行参考数据集的方法。它还具有一些人工数据生成器。\n\n### sklearn.datasets\n\n**（1）datasets.load_\\*()**\n\n获取小规模数据集，数据包含在datasets里\n\n**（2）datasets.fetch_\\*()**\n\n获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home，表示数据集下载的目录，默认是 ~/scikit_learn_data/，要修改默认目录，可以修改环境变量SCIKIT_LEARN_DATA\n\n![image-20210306164558336](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170130.png)\n\n**（3）datasets.make_\\*()**\n\n本地生成数据集\n\n**load\\**和 fetch\\** 函数返回的数据类型是 datasets.base.Bunch，本质上是一个 dict，它的键值对可用通过对象的属性方式访问。主要包含以下属性：**\n\n- data：特征数据数组，是 n_samples * n_features 的二维 numpy.ndarray 数组\n- target：标签数组，是 n_samples 的一维 numpy.ndarray 数组\n- DESCR：数据描述\n- feature_names：特征名\n- target_names：标签名\n\n**数据集目录可以通过datasets.get_data_home()获取，clear_data_home(data_home=None)删除所有下载数据**\n\n**(4)datasets.get_data_home(data_home=None)**\n\n返回scikit学习数据目录的路径。这个文件夹被一些大的数据集装载器使用，以避免下载数据。默认情况下，数据目录设置为用户主文件夹中名为“scikit_learn_data”的文件夹。或者，可以通过“SCIKIT_LEARN_DATA”环境变量或通过给出显式的文件夹路径以编程方式设置它。'〜'符号扩展到用户主文件夹。如果文件夹不存在，则会自动创建。\n\n**(5)sklearn.datasets.clear_data_home(data_home=None)**\n\n删除存储目录中的数据\n\n### 获取小数据集\n\n**用于分类**\n\n- sklearn.datasets.load_iris\n\n```python\nclass sklearn.datasets.load_iris(return_X_y=False)\n  \"\"\"\n  加载并返回虹膜数据集\n\n  :param return_X_y: 如果为True，则返回而不是Bunch对象，默认为False\n\n  :return: Bunch对象，如果return_X_y为True，那么返回tuple，（data,target）\n  \"\"\"\nIn [12]: from sklearn.datasets import load_iris\n    ...: data = load_iris()\n    ...:\n\nIn [13]: data.target\nOut[13]:\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\nIn [14]: data.feature_names\nOut[14]:\n['sepal length (cm)',\n 'sepal width (cm)',\n 'petal length (cm)',\n 'petal width (cm)']\n\nIn [15]: data.target_names\nOut[15]:\narray(['setosa', 'versicolor', 'virginica'],\n      dtype='|S10')\n\nIn [17]: data.target[[1,10, 100]]\nOut[17]: array([0, 0, 2])\n```\n\n| 名称         | 数量 |\n| :----------- | ---: |\n| 类别         |    3 |\n| 特征         |    4 |\n| 样本数量     |  150 |\n| 每个类别数量 |   50 |\n\n- sklearn.datasets.load_digits\n\n```python\nclass sklearn.datasets.load_digits(n_class=10, return_X_y=False)\n    \"\"\"\n    加载并返回数字数据集\n\n    :param n_class: 整数，介于0和10之间，可选（默认= 10，要返回的类的数量\n\n    :param return_X_y: 如果为True，则返回而不是Bunch对象，默认为False\n\n    :return: Bunch对象，如果return_X_y为True，那么返回tuple，（data,target）\n    \"\"\"\nIn [20]: from sklearn.datasets import load_digits\n\nIn [21]: digits = load_digits()\n\nIn [22]: print(digits.data.shape)\n(1797, 64)\n\nIn [23]: digits.target\nOut[23]: array([0, 1, 2, ..., 8, 9, 8])\n\nIn [24]: digits.target_names\nOut[24]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\nIn [25]: digits.images\nOut[25]:\narray([[[  0.,   0.,   5., ...,   1.,   0.,   0.],\n        [  0.,   0.,  13., ...,  15.,   5.,   0.],\n        [  0.,   3.,  15., ...,  11.,   8.,   0.],\n        ...,\n        [  0.,   4.,  11., ...,  12.,   7.,   0.],\n        [  0.,   2.,  14., ...,  12.,   0.,   0.],\n        [  0.,   0.,   6., ...,   0.,   0.,   0.]],\n\n        [[  0.,   0.,  10., ...,   1.,   0.,   0.],\n        [  0.,   2.,  16., ...,   1.,   0.,   0.],\n        [  0.,   0.,  15., ...,  15.,   0.,   0.],\n        ...,\n        [  0.,   4.,  16., ...,  16.,   6.,   0.],\n        [  0.,   8.,  16., ...,  16.,   8.,   0.],\n        [  0.,   1.,   8., ...,  12.,   1.,   0.]]])\n```\n\n| 名称     | 数量 |\n| :------- | ---: |\n| 类别     |   10 |\n| 特征     |   64 |\n| 样本数量 | 1797 |\n\n**用于回归**\n\n- sklearn.datasets.load_boston\n\n```python\nclass  sklearn.datasets.load_boston(return_X_y=False)\n  \"\"\"\n  加载并返回波士顿房价数据集\n\n  :param return_X_y: 如果为True，则返回而不是Bunch对象，默认为False\n\n  :return: Bunch对象，如果return_X_y为True，那么返回tuple，（data,target）\n  \"\"\"\nIn [34]: from sklearn.datasets import load_boston\n\nIn [35]: boston = load_boston()\n\nIn [36]: boston.data.shape\nOut[36]: (506, 13)\n\nIn [37]: boston.feature_names\nOut[37]:\narray(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n       'TAX', 'PTRATIO', 'B', 'LSTAT'],\n      dtype='|S7')\n\nIn [38]:\n```\n\n| 名称     | 数量 |\n| :------- | ---: |\n| 目标类别 | 5-50 |\n| 特征     |   13 |\n| 样本数量 |  506 |\n\n- sklearn.datasets.load_diabetes\n\n```python\nclass sklearn.datasets.load_diabetes(return_X_y=False)\n  \"\"\"\n  加载和返回糖尿病数据集\n\n  :param return_X_y: 如果为True，则返回而不是Bunch对象，默认为False\n\n  :return: Bunch对象，如果return_X_y为True，那么返回tuple，（data,target）\n  \"\"\"\nIn [13]:  from sklearn.datasets import load_diabetes\n\nIn [14]: diabetes = load_diabetes()\n\nIn [15]: diabetes.data\nOut[15]:\narray([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n         0.01990842, -0.01764613],\n       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n        -0.06832974, -0.09220405],\n       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n         0.00286377, -0.02593034],\n       ...,\n       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n        -0.04687948,  0.01549073],\n       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n         0.04452837, -0.02593034],\n       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n        -0.00421986,  0.00306441]])\n```\n\n| 名称     |   数量 |\n| :------- | -----: |\n| 目标范围 | 25-346 |\n| 特征     |     10 |\n| 样本数量 |    442 |\n\n### 获取大数据集\n\n- sklearn.datasets.fetch_20newsgroups\n\n```python\nclass sklearn.datasets.fetch_20newsgroups(data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True)\n  \"\"\"\n  加载20个新闻组数据集中的文件名和数据\n\n  :param subset: 'train'或者'test','all'，可选，选择要加载的数据集：训练集的“训练”，测试集的“测试”，两者的“全部”，具有洗牌顺序\n\n\n  :param data_home: 可选，默认值：无，指定数据集的下载和缓存文件夹。如果没有，所有scikit学习数据都存储在'〜/ scikit_learn_data'子文件夹中\n\n  :param categories: 无或字符串或Unicode的集合，如果没有（默认），加载所有类别。如果不是无，要加载的类别名称列表（忽略其他类别）\n\n  :param shuffle: 是否对数据进行洗牌\n\n  :param random_state: numpy随机数生成器或种子整数\n\n  :param download_if_missing: 可选，默认为True，如果False，如果数据不在本地可用而不是尝试从源站点下载数据，则引发IOError\n\n  :param remove: 元组\n  \"\"\"\nIn [29]: from sklearn.datasets import fetch_20newsgroups\n\nIn [30]: data_test = fetch_20newsgroups(subset='test',shuffle=True, random_sta\n    ...: te=42)\n\nIn [31]: data_train = fetch_20newsgroups(subset='train',shuffle=True, random_s\n    ...: tate=42)\n```\n\n- sklearn.datasets.fetch_20newsgroups_vectorized\n\n```python\nclass sklearn.datasets.fetch_20newsgroups_vectorized(subset='train', remove=(), data_home=None)\n  \"\"\"\n  加载20个新闻组数据集并将其转换为tf-idf向量，这是一个方便的功能; 使用sklearn.feature_extraction.text.Vectorizer的默认设置完成tf-idf 转换。对于更高级的使用（停止词过滤，n-gram提取等），将fetch_20newsgroup与自定义Vectorizer或CountVectorizer组合在一起\n\n  :param subset: 'train'或者'test','all'，可选，选择要加载的数据集：训练集的“训练”，测试集的“测试”，两者的“全部”，具有洗牌顺序\n\n  :param data_home: 可选，默认值：无，指定数据集的下载和缓存文件夹。如果没有，所有scikit学习数据都存储在'〜/ scikit_learn_data'子文件夹中\n\n  :param remove: 元组\n  \"\"\"\nIn [57]: from sklearn.datasets import fetch_20newsgroups_vectorized\n\nIn [58]: bunch = fetch_20newsgroups_vectorized(subset='all')\n\nIn [59]: from sklearn.utils import shuffle\n\nIn [60]: X, y = shuffle(bunch.data, bunch.target)\n    ...: offset = int(X.shape[0] * 0.8)\n    ...: X_train, y_train = X[:offset], y[:offset]\n    ...: X_test, y_test = X[offset:], y[offset:]\n    ...:\n```\n\n### 获取本地生成数据\n\n生成本地分类数据：\n\n- sklearn.datasets.make_classification\n\n  ```python\n  class make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n  \"\"\"\n  生成用于分类的数据集\n  \n  :param n_samples:int，optional（default = 100)，样本数量\n  \n  :param n_features:int，可选（默认= 20），特征总数\n  \n  :param n_classes:int，可选（default = 2),类（或标签）的分类问题的数量\n  \n  :param random_state:int，RandomState实例或无，可选（默认=无）\n    如果int，random_state是随机数生成器使用的种子; 如果RandomState的实例，random_state是随机数生成器; 如果没有，随机数生成器所使用的RandomState实例np.random\n  \n  :return :X,特征数据集；y,目标分类值\n  \"\"\"\n  ```\n\n```python\nfrom sklearn.datasets.samples_generator import make_classification\nX,y= datasets.make_classification(n_samples=100000, n_features=20,n_informative=2, n_redundant=10,random_state=42)\n```\n\n生成本地回归数据：\n\n- sklearn.datasets.make_regression\n\n```python\nclass make_regression(n_samples=100, n_features=100, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)\n  \"\"\"\n  生成用于回归的数据集\n\n  :param n_samples:int，optional（default = 100)，样本数量\n\n  :param  n_features:int,optional（default = 100)，特征数量\n\n  :param  coef:boolean，optional（default = False），如果为True，则返回底层线性模型的系数\n\n  :param random_state:int，RandomState实例或无，可选（默认=无）\n    如果int，random_state是随机数生成器使用的种子; 如果RandomState的实例，random_state是随机数生成器; 如果没有，随机数生成器所使用的RandomState实例np.random\n\n  :return :X,特征数据集；y,目标值\n  \"\"\"\nfrom sklearn.datasets.samples_generator import make_regression\nX, y = make_regression(n_samples=200, n_features=5000, random_state=42)\n```\n\n## 2.2 模型的选择\n\n算法是核心，数据和计算是基础。这句话很好的说明了机器学习中算法的重要性。那么我们开看下机器学习的几种分类：\n\n- 监督学习\n  - 分类 k-近邻算法、决策树、贝叶斯、逻辑回归(LR)、支持向量机(SVM)\n  - 回归 线性回归、岭回归\n  - 标注 隐马尔可夫模型(HMM)\n- 无监督学习\n  - 聚类 k-means\n\n### 如何选择合适的算法模型\n\n在解决问题的时候，必须考虑下面两个问题：一、使用机器学习算法的目的，想要算法完成何种任务，比如是预测明天下雨的概率是对投票者按照兴趣分组；二、需要分析或者收集的数据时什么\n\n首先考虑使用机器学习算法的目的。如果想要预测目标变量的值，则可以选择监督学习算法，否则可以选择无监督学习算法，确定选择监督学习算法之后，需要进一步确定目标变量类型，如果目标变量是离散型，如是／否、1/2/3，A/B/C/或者红／黑／黄等，则可以选择**分类**算法；如果目标变量是连续的数值，如0.0～100.0、-999～999等，则需要选择**回归**算法\n\n如果不想预测目标变量的值，则可以选择**无监督**算法。进一步分析是否需要将数据划分为离散的组。如果这是唯一的需求，则使用**聚类**算法。\n\n当然在大多数情况下，上面给出的选择办法都能帮助读者选择恰当的机器学习算法，但这也并非已成不变。也有分类算法可以用于回归。\n\n其次考虑的是数据问题，我们应该充分了解数据，对实际数据了解的越充分，越容易创建符合实际需求的应用程序，主要应该了解数据的一下特性：特征值是**离散型变量** 还是 **连续型变量** ，特征值中是否存在缺失的值，何种原因造成缺失值，数据中是够存在异常值，某个特征发生的频率如何，等等。充分了解上面提到的这些数据特性可以缩短选择机器学习算法的时间。\n\n### 监督学习中三类问题的解释\n\n**（1）分类问题** 分类是监督学习的一个核心问题，在监督学习中，当输出变量取有限个离散值时，预测问题变成为分类问题。这时，输入变量可以是离散的，也可以是连续的。监督学习从数据中学习一个分类模型活分类决策函数，称为分类器。分类器对新的输入进行输出的预测，称为分类。最基础的便是二分类问题，即判断是非，从两个类别中选择一个作为预测结果；除此之外还有多酚类的问题，即在多于两个类别中选择一个。\n\n分类问题包括学习和分类两个过程，在学习过程中，根据已知的训练数据集利用有效的学习方法学习一个分类器，在分类过程中，利用学习的分类器对新的输入实例进行分类。图中(X1,Y1),(X2,Y2)…都是训练数据集，学习系统有训练数据学习一个分类器P(Y|X)或Y=f(X);分类系统通过学习到的分类器对于新输入的实例子Xn+1进行分类，即预测术其输出的雷标记Yn+1\n\n![image-20210107105644583](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170131.png)\n\n分类在于根据其特性将数据“分门别类”，所以在许多领域都有广泛的应用。例如，在银行业务中，可以构建一个客户分类模型，按客户按照贷款风险的大小进行分类；在网络安全领域，可以利用日志数据的分类对非法入侵进行检测；在图像处理中，分类可以用来检测图像中是否有人脸出现；在手写识别中，分类可以用于识别手写的数字；在互联网搜索中，网页的分类可以帮助网页的抓取、索引和排序。\n\n即一个分类应用的例子，文本分类。这里的文本可以是新闻报道、网页、电子邮件、学术论文。类别往往是关于文本内容的。例如政治、体育、经济等；也有关于文本特点的，如正面意见、反面意见；还可以根据应用确定，如垃圾邮件、非垃圾邮件等。文本分类是根据文本的特征将其划分到已有的类中。输入的是文本的特征向量，输出的是文本的类别。通常把文本的单词定义出现取值是1，否则是0；也可以是多值的，，表示单词在文本中出现的频率。直观地，如果“股票”“银行““货币”这些词出现很多，这个文本可能属于经济学，如果“网球””比赛“”运动员“这些词频繁出现，这个文本可能属于体育类。\n\n**（2）回归问题**\n\n回归是监督学习的另一个重要问题。回归用于预测输入变量和输出变量之间的关系，特别是当初如变量的值发生变化时，输出变量的值随之发生的变化。回归模型正式表示从输入到输出变量之间映射的函数。回归稳日的学习等价与函数拟合：选择一条函数曲线使其更好的拟合已知数据且很好的预测位置数据\n\n![image-20210107105911365](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170132.png)\n\n回归问题按照输入变量的个数，分为一元回归和多元回归；按照输入变量和输出变量之间关系的类型即模型的类型，分为线性回归和非线性回归。\n\n许多领域的任务都可以形式化为回归问题，比如，回归可以用于商务领域，作为市场趋势预测、产品质量管理、客户满意度调查、偷袭风险分析的工具。\n\n**（3）标注问题**\n\n标注也是一个监督学习问题。可以认为标注问题是分类问题的一个推广，标注问题又是更复杂的结构预测问题的简单形式。标注问题的输入是一个观测序列，输出是一个标记序列或状态序列。标注问题在信息抽取、自然语言处理等领域广泛应用，是这些领域的基本问题。例如，自然语言处理的词性标注就是一个典型的标注，即对一个单词序列预测其相应的词性标记序\n\n![image-20210107110012298](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170133.png)\n\n\u003e 当然我们主要关注的是分类和回归问题，并且标注问题的算法复杂\n\n## 2.3 模型检验-交叉验证\n\n一般在进行模型的测试时，我们会将数据分为训练集和测试集。在给定的样本空间中，拿出大部分样本作为训练集来训练模型，剩余的小部分样本使用刚建立的模型进行预测。\n\n### 训练集与测试集\n\n训练集与测试集的分割可以使用cross_validation中的train_test_split方法，大部分的交叉验证迭代器都内建一个划分数据前进行数据索引打散的选项，train_test_split 方法内部使用的就是交叉验证迭代器。默认不会进行打散，包括设置cv=some_integer（直接）k折叠交叉验证的cross_val_score会返回一个随机的划分。如果数据集具有时间性，千万不要打散数据再划分！\n\n- sklearn.cross_validation.train_test_split\n\n```python\ndef train_test_split(*arrays,**options)\n  \"\"\"\n  :param arrays:允许的输入是列表，数字阵列\n\n  :param test_size:float，int或None（默认为无）,如果浮点数应在0.0和1.0之间，并且表示要包括在测试拆分中的数据集的比例。如果int，表示测试样本的绝对数\n\n  :param train_size:float，int或None（默认为无）,如果浮点数应在0.0到1.0之间，表示数据集包含在列车拆分中的比例。如果int，表示列车样本的绝对数\n\n  :param random_state:int或RandomState,用于随机抽样的伪随机数发生器状态，参数 random_state 默认设置为 None，这意为着每次打散都是不同的。\n  \"\"\"\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nprint iris.data.shape,iris.target.shape\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=42)\nprint X_train.shape,y_train.shape\nprint X_test.shape,y_test.shape\n```\n\n上面的方式也有局限。因为只进行一次测试，并不一定能代表模型的真实准确率。因为，模型的准确率和数据的切分有关系，在数据量不大的情况下，影响尤其突出。所以还需要一个比较好的解决方案。\n\n模型评估中，除了训练数据和测试数据，还会涉及到验证数据。使用训练数据与测试数据进行了交叉验证，只有这样训练出的模型才具有更可靠的准确率，也才能期望模型在新的、未知的数据集上，能有更好的表现。这便是模型的推广能力，也即泛化能力的保证。\n\n### holdout method\n\n评估模型泛化能力的典型方法是holdout交叉验证(holdout cross validation)。holdout方法很简单，我们只需要将原始数据集分割为训练集和测试集，前者用于训练模型，后者用于评估模型的性能。一般来说，Holdout 验证并非一种交叉验证，因为数据并没有交叉使用。 随机从最初的样本中选出部分，形成交叉验证数据，而剩余的就当做训练数据。 一般来说，少于原本样本三分之一的数据被选做验证数据。所以这种方法得到的结果其实并不具有说服性\n\n### k-折交叉验证\n\nK折交叉验证，初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。\n\n![image-20210107110259176](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170134.png)\n\n例如5折交叉验证，全部可用数据集分成五个集合，每次迭代都选其中的1个集合数据作为验证集，另外4个集合作为训练集，经过5组的迭代过程。交叉验证的好处在于，可以保证所有数据都有被训练和验证的机会，也尽最大可能让优化的模型性能表现的更加可信。\n\n使用交叉验证的最简单的方法是在估计器和数据集上使用cross_val_score函数。\n\n- sklearn.cross_validation.cross_val_score\n\n```python\ndef cross_val_score(estimator, X, y=None, \tgroups=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')\n  \"\"\"\n  :param estimator:模型估计器\n\n  :param X:特征变量集合\n\n  :param y:目标变量\n\n  :param cv:int，使用默认的3折交叉验证，整数指定一个（分层）KFold中的折叠数\n\n  :return :预估系数\n  \"\"\"\nfrom sklearn.cross_validation import cross_val_score\ndiabetes = datasets.load_diabetes()\nX = diabetes.data[:150]\ny = diabetes.target[:150]\nlasso = linear_model.Lasso()\nprint(cross_val_score(lasso, X, y))\n```\n\n使用交叉验证方法的目的主要有2个：\n\n- 从有限的学习数据中获取尽可能多的有效信息；\n- 可以在一定程度上避免过拟合问题。\n\n### 超参数搜索-网格搜索\n\n通常情况下，有很多参数是需要手动指定的（如k-近邻算法中的K值），\n\n这种叫超参数。但是手动过程繁杂，所以需要对模型预设几种超参数组\n\n合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建\n\n立模型。\n\n![image-20210307234523125](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170135.png)\n\n**sklearn.model_selection.GridSearchCV**\n\n![image-20210307234552494](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170136.png)\n\n## 2.4 estimator的工作流程\n\n![image-20210306164823141](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170137.png)\n\n![image-20210306164928862](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170138.png)\n\n![image-20210306164949155](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170139.png)\n\n在sklearn中，估计器(estimator)是一个重要的角色，分类器和回归器都属于estimator。在估计器中有有两个重要的方法是fit和transform。\n\n- fit方法用于从训练集中学习模型参数\n- transform用学习到的参数转换数据\n\n![image-20210107110717711](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170140.png)\n\n# 3. Scikit-learn的分类器算法\n\n## 3.1 分类算法之k-近邻\n\n定义：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。\n\n来源：KNN算法最早是由Cover和Hart提出的一种分类算法。\n\nk-近邻算法采用测量不同特征值之间的距离来进行分类\n\n\u003e 优点：精度高、对异常值不敏感、无数据输入假定\n\u003e\n\u003e 缺点：计算复杂度高、空间复杂度高，必须指定K值，K值选择不当则分类精度不能保证\n\u003e\n\u003e 使用数据范围：数值型和标称型\n\n加快搜索速度——基于算法的改进KDTree,API接口里面有实现\n\n### 一个例子弄懂k-近邻\n\n电影可以按照题材分类，每个题材又是如何定义的呢？那么假如两种类型的电影，动作片和爱情片。动作片有哪些公共的特征？那么爱情片又存在哪些明显的差别呢？我们发现动作片中打斗镜头的次数较多，而爱情片中接吻镜头相对更多。当然动作片中也有一些接吻镜头，爱情片中也会有一些打斗镜头。所以不能单纯通过是否存在打斗镜头或者接吻镜头来判断影片的类别。那么现在我们有6部影片已经明确了类别，也有打斗镜头和接吻镜头的次数，还有一部电影类型未知。\n\n|         电影名称          | 打斗镜头 | 接吻镜头 | 电影类型 |\n| :-----------------------: | :------: | :------: | :------: |\n|      California Man       |    3     |   104    |  爱情片  |\n| He's not Really into dues |    2     |   100    |  爱情片  |\n|      Beautiful Woman      |    1     |    81    |  爱情片  |\n|      Kevin Longblade      |   101    |    10    |  动作片  |\n|     Robo Slayer 3000      |    99    |    5     |  动作片  |\n|         Amped II          |    98    |    2     |  动作片  |\n|             ?             |    18    |    90    |   未知   |\n\n那么我们使用K-近邻算法来分类爱情片和动作片：存在一个样本数据集合，也叫训练样本集，样本个数M个，知道每一个数据特征与类别对应关系，然后存在未知类型数据集合1个，那么我们要选择一个测试样本数据中与训练样本中M个的距离，排序过后选出最近的K个，这个取值一般不大于20个。选择K个最相近数据中次数最多的分类。那么我们根据这个原则去判断未知电影的分类\n\n|         电影名称          | 与未知电影的距离 |\n| :-----------------------: | :--------------: |\n|      California Man       |       20.5       |\n| He's not Really into dues |       18.7       |\n|      Beautiful Woman      |       19.2       |\n|      Kevin Longblade      |      115.3       |\n|     Robo Slayer 3000      |      117.4       |\n|         Amped II          |      118.9       |\n\n我们假设K为3，那么排名前三个电影的类型都是爱情片，所以我们判定这个未知电影也是一个爱情片。那么计算距离是怎样计算的呢？\n\n![image-20210306165428737](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170141.png)\n\n### sklearn.neighbors\n\nsklearn.neighbors提供监督的基于邻居的学习方法的功能，sklearn.neighbors.KNeighborsClassifier是一个最近邻居分类器。那么KNeighborsClassifier是一个类，我们看一下实例化时候的参数\n\n![image-20210306165606032](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170142.png)\n\n```python\nclass sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)**\n  \"\"\"\n  :param n_neighbors：int，可选（默认= 5），k_neighbors查询默认使用的邻居数\n\n  :param algorithm：{'auto'，'ball_tree'，'kd_tree'，'brute'}，可选用于计算最近邻居的算法：'ball_tree'将会使用 BallTree，'kd_tree'将使用 KDTree，“野兽”将使用强力搜索。'auto'将尝试根据传递给fit方法的值来决定最合适的算法。\n\n  :param n_jobs：int，可选（默认= 1),用于邻居搜索的并行作业数。如果-1，则将作业数设置为CPU内核数。不影响fit方法。\n\n  \"\"\"\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\n\nneigh = KNeighborsClassifier(n_neighbors=3)\n```\n\n### Method\n\n**fit(X, y)**\n\n使用X作为训练数据拟合模型，y作为X的类别值。X，y为数组或者矩阵\n\n```python\nX = np.array([[1,1],[1,1.1],[0,0],[0,0.1]])\ny = np.array([1,1,0,0])\nneigh.fit(X,y)\n```\n\n**kneighbors(X=None, n_neighbors=None, return_distance=True)**\n\n找到指定点集X的n_neighbors个邻居，return_distance为False的话，不返回距离\n\n```python\nneigh.kneighbors(np.array([[1.1,1.1]]),return_distance= False)\n\nneigh.kneighbors(np.array([[1.1,1.1]]),return_distance= False,an_neighbors=2)\n```\n\n**predict(X)**\n\n预测提供的数据的类标签\n\n```python\nneigh.predict(np.array([[0.1,0.1],[1.1,1.1]]))\n```\n\n**predict_proba(X)**\n\n返回测试数据X属于某一类别的概率估计\n\n```python\nneigh.predict_proba(np.array([[1.1,1.1]]))\n```\n\n## 3.2 k-近邻算法案例分析\n\n本案例使用最著名的”鸢尾“数据集，该数据集曾经被Fisher用在经典论文中，目前作为教科书般的数据样本预存在Scikit-learn的工具包中。\n\n![image-20210306170053370](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170143.png)\n\n![image-20210306170125343](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170144.png)\n\n**读入Iris数据集细节资料**\n\n```python\nfrom sklearn.datasets import load_iris\n# 使用加载器读取数据并且存入变量iris\niris = load_iris()\n\n# 查验数据规模\niris.data.shape\n\n# 查看数据说明（这是一个好习惯）\nprint iris.DESCR\n```\n\n通过上述代码对数据的查验以及数据本身的描述，我们了解到Iris数据集共有150朵鸢尾数据样本，并且均匀分布在3个不同的亚种；每个数据样本有总共4个不同的关于花瓣、花萼的形状特征所描述。由于没有制定的测试集合，因此按照惯例，我们需要对数据进行随即分割，25%的样本用于测试，其余75%的样本用于模型的训练。\n\n由于不清楚数据集的排列是否随机，可能会有按照类别去进行依次排列，这样训练样本的不均衡的，所以我们需要分割数据，已经默认有随机采样的功能。\n\n**对Iris数据集进行分割**\n\n```python\nfrom sklearn.cross_validation import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(iris.data,iris.target,test_size=0.25,random_state=42)\n```\n\n**对特征数据进行标准化**\n\n```python\nfrom sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.fit_transform(X_test)\n```\n\nK近邻算法是非常直观的机器学习模型，我们可以发现K近邻算法没有参数训练过程，也就是说，我们没有通过任何学习算法分析训练数据，而只是根据测试样本训练数据的分布直接作出分类决策。因此，K近邻属于无参数模型中非常简单一种。\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\n\ndef knniris():\n    \"\"\"\n    鸢尾花分类\n    :return: None\n    \"\"\"\n\n    # 数据集获取和分割\n    lr = load_iris()\n\n    x_train, x_test, y_train, y_test = train_test_split(lr.data, lr.target, test_size=0.25)\n\n    # 进行标准化\n\n    std = StandardScaler()\n\n    x_train = std.fit_transform(x_train)\n    x_test = std.transform(x_test)\n\n    # estimator流程\n    knn = KNeighborsClassifier()\n\n    # # 得出模型\n    # knn.fit(x_train,y_train)\n    #\n    # # 进行预测或者得出精度\n    # y_predict = knn.predict(x_test)\n    #\n    # # score = knn.score(x_test,y_test)\n\n    # 通过网格搜索,n_neighbors为参数列表\n    param = {\"n_neighbors\": [3, 5, 7]}\n\n    gs = GridSearchCV(knn, param_grid=param, cv=10)\n\n    # 建立模型\n    gs.fit(x_train,y_train)\n\n    # print(gs)\n\n    # 预测数据\n\n    print(gs.score(x_test,y_test))\n\n    # 分类模型的精确率和召回率\n\n    # print(\"每个类别的精确率与召回率：\",classification_report(y_test, y_predict,target_names=lr.target_names))\n\n    return None\n\nif __name__ == \"__main__\":\n    knniris()\n```\n\n## 3.3 朴素贝叶斯\n\n朴素贝叶斯（Naive Bayes）是一个非常简单，但是实用性很强的分类模型。朴素贝叶斯分类器的构造基础是贝叶斯理论。\n\n![image-20210307234131617](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170145.png)\n\n### 概率论基础\n\n概率定义为一件事情发生的可能性。事情发生的概率可以 通过观测数据中的事件发生次数来计算，事件发生的概率等于改事件发生次数除以所有事件发生的总次数。举一些例子：\n\n- 扔出一个硬币，结果头像朝上\n- 某天是晴天\n- 某个单词在未知文档中出现\n\n我们将事件的概率记作P(X)，那么假设这一事件为X属于样本空间中的一个类别，那么0≤*P*(*X*)≤1。\n\n**联合概率与条件概率**\n\n- **联合概率**\n\n是指两件事情同时发生的概率。那么我们假设样本空间有一些天气数据：\n\n| 编号 | 星期几 | 天气 |\n| :--: | :----: | :--: |\n|  1   |   2    | 晴天 |\n|  2   |   1    | 下雨 |\n|  3   |   3    | 晴天 |\n|  4   |   4    | 晴天 |\n|  5   |   1    | 下雨 |\n|  6   |   2    | 下雪 |\n|  7   |   3    | 下雪 |\n\n那么天气被分成了三类，那么P(X=sun)=3/7，假如说天气=下雪且星期几=2？这个概率怎么求？这个概率应该等于两件事情为真的次数除以所有事件发生 的总次数。我们可以看到只有一个样本满足天气=下雪且星期几=2，所以这个概率为1/7。一般对于X和Y来说，对应的联合概率记为P(XY)。\n\n- **条件概率**\n\n那么条件概率形如P(X∣Y)，这种格式的。表示为在Y发生的条件下，发生X的概率。假设X代表星期，Y代表天气，则 P(X=3∣Y=sun)如何求？\n\n从表中我们可以得出，P(X=3,Y=sun)=1/7,P(Y)=3/7。\n\nP(X=3∣Y=sun)=1/3=P(X=3,Y=sun)/P(Y)\n\n在条件概率中，有一个重要的特性\n\n- **如果每个事件之间相互独立**\n\n那么则有![image-20210306170501376](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170146.png)\n\n这个式子的意思是给定条件下，所有的X的概率为单独的Y条件下每个X发生的概率乘积，我们通过后面再继续去理解这个式子的具体含义。\n\n**贝叶斯公式**\n\n首先我们给出该公式的表示，![image-20210306170533187](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170147.png),其中ci为类别，W为特征向量。\n\n贝叶斯公式最常用于文本分类，上式左边可以理解为给定一个文本词向量W，那么它属于类别ci的概率是多少。那么式子右边分几部分，P(W∣ci)理解为在给定类别的情况下，该文档的词向量的概率。可以通过条件概率中的重要特性来求解。\n\n假设我们有已分类的文档，\n\n```\na = \"life is short,i like python\"\nb = \"life is too long,i dislike python\"\nc = \"yes,i like python\"\nlabel=[1,0,1]\n```\n\n**词袋法的特征值计算**\n\n若使用词袋法，且以训练集中的文本为词汇表，即将训练集中的文本中出现的单词(不重复)都统计出来作为词典，那么记单词的数目为n，这代表了文本的n个维度。以上三个文本在这8个特征维度上的表示为：\n\n|      | life |  is  |  i   | short | long | like | dislike | too  | python | yes  |\n| :--: | :--: | :--: | :--: | :---: | :--: | :--: | :-----: | :--: | :----: | :--: |\n|  a'  |  1   |  1   |  1   |   1   |  0   |  1   |    0    |  0   |   1    |  0   |\n|  b'  |  1   |  1   |  1   |   0   |  1   |  0   |    1    |  1   |   1    |  0   |\n|  c'  |  0   |  0   |  1   |   0   |  0   |  1   |    0    |  0   |   1    |  1   |\n\n上面a',b'就是两个文档的词向量的表现形式，对于贝叶斯公式，从label中我们可以得出两个类别的概率为：\n\n![image-20210306170650996](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170148.png)\n\n对于一个给定的文档类别，每个单词特征向量的概率是多少呢？\n\n**提供一种TF计算方法**，为类别yk每个单词出现的次数Ni,除以文档类别yk中所有单词出现次数的总数N：\n\nPi=Ni/N\n\n首先求出现总数，对于1类别文档，在a'中，就可得出总数为1+1+1+1+1+1=6，c'中，总共1+1+1+1=4，故在1类别文档中总共有10次。\n\n每个单词出现总数，假设是两个列表，a'+c'就能得出每个单词出现次数，比如P(w=python)=2/10=0.20000000,同样可以得到其它的单词概率。最终结果如下：\n\n```\n# 类别1文档中的词向量概率\np1 = [0.10000000,0.10000000,0.20000000,0.10000000,0,0.20000000,0,0,0.20000000,0.10000000]\n# 类别0文档中的词向量概率\np0 = [0.16666667,0.16666667,0.16666667,0,0.16666667,0,0.16666667,0.16666667,0.16666667,0]\n```\n\n**拉普拉斯平滑系数**\n\n为了避免训练集样本对一些特征的缺失，即某一些特征出现的次数为0，在计算P(X1,X2,X3,…,Xn∣Yi)的时候，各个概率相乘最终结果为零，这样就会影响结果。我们需要对这个概率计算公式做一个平滑处理:\n\n![image-20210306170847401](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170149.png)\n\n其中m为特征词向量的个数，α为平滑系数，当α=1，称为拉普拉斯平滑。\n\n### sklearn.naive_bayes.MultinomialNB\n\n```python\nclass sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n  \"\"\"\n  :param alpha：float，optional（default = 1.0）加法（拉普拉斯/ Lidstone）平滑参数（0为无平滑）\n  \"\"\"\n```\n\n![image-20210107112004526](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170150.png)\n\n### 互联网新闻分类\n\n**读取20类新闻文本的数据细节**\n\n```python\nfrom sklearn.datasets import fetch_20newsgroups\n\nnews = fetch_20newsgroups(subset='all')\n\nprint news.data[0]\n```\n\n上述代码得出该数据共有18846条新闻，但是这些文本数据既没有被设定特征，也没有数字化的亮度。因此，在交给朴素贝叶斯分类器学习之前，要对数据做进一步的处理。\n\n**20类新闻文本数据分割**\n\n```python\nfrom sklearn.cross_validation import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(news.data,news.target,test_size=0.25,random_state=42)\n```\n\n**文本转换为特征向量进行TF特征抽取**\n\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvec = CountVectorizer()\n# 训练数据输入，并转换为特征向量\nX_train = vec.fit_transform(X_train)\n# 测试数据转换\nX_test = vec.transform(X_test)\n```\n\n**朴素贝叶斯分类器对文本数据进行类别预测**\n\n```python\nfrom sklearn.naive_bayes import MultinomialNB\n\n# 使用平滑处理初始化的朴素贝叶斯模型\nmnb = MultinomialNB(alpha=1.0)\n\n# 利用训练数据对模型参数进行估计\nmnb.fit(X_train,y_train)\n\n# 对测试验本进行类别预测。结果存储在变量y_predict中\ny_predict = mnb.predict(X_test)\n```\n\n**性能测试**\n\n- 特点分析\n\n朴素贝叶斯模型被广泛应用于海量互联网文本分类任务。由于其较强的特征条件独立假设，使得模型预测所需要估计的参数规模从幂指数量级想线性量级减少，极大的节约了内存消耗和计算时间。到那时，也正是受这种强假设的限制，模型训练时无法将各个特征之间的联系考量在内，**使得该模型在其他数据特征关联性较强的分类任务上的性能表现不佳**。\n\n## 3.4 分类算法之逻辑回归\n\n逻辑回归（Logistic Regression），简称LR。它的特点是能够是我们的特征输入集合转化为0和1这两类的概率。一般来说，回归不用在分类问题上，因为回归是连续型模型，而且受噪声影响比较大。如果非要应用进入，可以使用逻辑回归。了解过线性回归之后再来看逻辑回归可以更好的理解。\n\n\u003e 优点：计算代价不高，易于理解和实现\n\u003e\n\u003e 缺点：容易欠拟合，分类精度不高\n\u003e\n\u003e 适用数据：数值型和标称型\n\n### 逻辑回归\n\n对于回归问题后面会介绍，Logistic回归本质上是线性回归，只是在特征到结果的映射中加入了一层函数映射，即先把特征线性求和，然后使用函数g(z)将最为假设函数来预测。g(z)可以将连续值映射到0和1上。Logistic回归用来分类0/1问题，也就是预测结果属于0或者1的二值分类问题\n\n映射函数为：\n\n![image-20210107113501457](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170151.png)映射出来的效果如下如：\n\n![image-20210107113512797](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170152.png)\n\n### sklearn.linear_model.LogisticRegression\n\n逻辑回归类\n\n```python\nclass sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n  \"\"\"\n  :param C: float，默认值：1.0\n\n  :param penalty: 特征选择的方式\n\n  :param tol: 公差停止标准\n  \"\"\"\n```\n\n![image-20210107113608904](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170153.png)\n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_digits\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(C=1.0, penalty='l1', tol=0.01)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nLR.fit(X_train,y_train)\nLR.predict(X_test)\nLR.score(X_test,y_test)\n0.96464646464646464\n# c=100.0\n0.96801346801346799\n```\n\n### 属性\n\n**coef_**\n\n决策功能的特征系数\n\n**Cs_**\n\n数组C，即用于交叉验证的正则化参数值的倒数\n\n### 特点分析\n\n线性分类器可以说是最为基本和常用的机器学习模型。尽管其受限于数据特征与分类目标之间的线性假设，我们仍然可以在科学研究与工程实践中把线性分类器的表现性能作为基准。\n\n## 3.5 逻辑回归算法案例分析\n\n### 良／恶性乳腺癌肿瘤预测\n\n原始数据的下载地址为：https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/\n\n**数据预处理**\n\n```python\nimport pandas as pd\nimport numpy as np\n\n# 根据官方数据构建类别\ncolumn_names = ['Sample code number','Clump Thickness ','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','Class'],\n\ndata = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/',names = column_names)\n\n# 将？替换成标准缺失值表示\ndata = data.replace(to_replace='?',value = np.nan)\n\n# 丢弃带有缺失值的数据（只要一个维度有缺失）\ndata = data.dropna(how='any')\n\ndata.shape\n```\n\n处理的缺失值后的样本共有683条，特征包括细胞厚度、细胞大小、形状等九个维度\n\n**准备训练测试数据**\n\n```python\nfrom sklearn.cross_validation import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(data[column_names[1:10]],data[column_names[10]],test_size=0.25,random_state=42)\n\n# 查看训练和测试样本的数量和类别分布\ny_train.value_counts()\n\ny_test.value_counts()\n```\n\n**使用逻辑回归进行良／恶性肿瘤预测任务**\n\n```python\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n\n# 标准化数据，保证每个维度的特征数据方差为1，均值为0。使得预测结果不会被某些维度过大的特征值而主导\nss = StandardScaler()\n\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)\n\n# 初始化 LogisticRegression\n\nlr = LogisticRegression(C=1.0, penalty='l1', tol=0.01)\n\n# 跳用LogisticRegression中的fit函数／模块来训练模型参数\nlr.fit(X_train,y_train)\n\nlr_y_predict = lr.predict(X_test)\n```\n\n**性能分析**\n\n```python\nfrom sklearn.metrics import classification_report\n\n# 利用逻辑斯蒂回归自带的评分函数score获得模型在测试集上的准确定结果\nprint '精确率为：',lr.score(X_test,y_test)\n\nprint classification_report(y_test,lr_y_predict,target_names = ['Benign','Maligant'])\n```\n\n## 3.6 分类器性能评估\n\n在许多实际问题中，衡量分类器任务的成功程度是通过固定的性能指标来获取。一般最常见使用的是准确率，即预测结果正确的百分比。然而有时候，我们关注的是负样本是否被正确诊断出来。例如，关于肿瘤的的判定，需要更加关心多少恶性肿瘤被正确的诊断出来。也就是说，在二类分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵。\n\n在二类问题中，如果将一个正例判为正例，那么就可以认为产生了一个真正例（True Positive，TP）；如果对一个反例正确的判为反例，则认为产生了一个真反例（True Negative，TN）。相应地，两外两种情况则分别称为伪反例（False Negative，FN，也称）和伪正例（False Positive，TP），四种情况如下图：\n\n![image-20210107115219067](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170154.png)\n\n![image-20210306171205933](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170155.png)\n\n在分类中，当某个类别的重要性高于其他类别时，我们就可以利用上述定义出多个逼错误率更好的新指标。第一个指标就是**精确率**（Precision），它等于TP/(TP+FP)，给出的是预测为正例的样本中占真实结果总数的比例。第二个指标是**召回率**（Recall）。它等于TP/(TP+FN)，给出的是预测为正例的真实正例占所有真实正例的比例。\n\n那么除了正确率和精确率这两个指标之外，为了综合考量召回率和精确率，我们计算这两个指标的调和平均数，得到F1指标（F1 measure）:\n\n![image-20210107115233827](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170156.png)\n\n之所以使用调和平均数，是因为它除了具备平均功能外，还会对那些召回率和精确率更加接近的模型给予更高的分数；而这也是我们所希望的，因为那些召回率和精确率差距过大的学习模型，往往没有足够的使用价值。\n\n![image-20210306172033897](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170157.png)\n\n**sklearn.metrics.classification_report**\n\n![image-20210306172051215](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170158.png)\n\nsklearn中metrics中提供了计算四个指标的模块，也就是classification_report。\n\n```python\nclassification_report(y_true, y_pred, labels=None, target_names=None, digits=2)\n  \"\"\"\n  计算分类指标\n  :param y_true:真实目标值\n\n  :param y_pred:分类器返回的估计值\n\n  :param target_names:可选的，计算与目标类别匹配的结果\n\n  :param digits:格式化输出浮点值的位数\n\n  :return :字符串，三个指标值\n\n  \"\"\"\n```\n\n我们通过一个例子来分析一下指标的结果：\n\n```python\nfrom sklearn.metrics import classification_report\ny_true = [0, 1, 2, 2, 2]\ny_pred = [0, 0, 2, 2, 1]\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_true, y_pred, target_names=target_names))\n\n\n             precision    recall  f1-score   support\n\n    class 0       0.50      1.00      0.67         1\n    class 1       0.00      0.00      0.00         1\n    class 2       1.00      0.67      0.80         3\n\navg / total       0.70      0.60      0.61         5\n```\n\n## 3.7 分类算法之决策树\n\n决策树是一种基本的分类方法，当然也可以用于回归。我们一般只讨论用于分类的决策树。决策树模型呈树形结构。在分类问题中，表示基于特征对实例进行分类的过程，它可以认为是if-then规则的集合。在决策树的结构中，每一个实例都被一条路径或者一条规则所覆盖。通常决策树学习包括三个步骤：**特征选择**、决策树的生成和决策树的修剪\n\n\u003e 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，**可以处理逻辑回归等不能解决的非线性特征数据**\n\u003e\n\u003e 缺点：可能产生过度匹配问题\n\u003e\n\u003e 适用数据类型：数值型和标称型\n\n![image-20210307235009516](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170159.png)\n\n![image-20210307234656507](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170200.png)\n\n### 特征选择\n\n特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率，如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的京都影响不大。通常特征选择的准则是信息增益，这是个数学概念。通过一个例子来了解特征选择的过程。\n\n|  ID  | 年龄 | 有工作 | 有自己的房子 | 信贷情况 | 类别 |\n| :--: | :--: | :----: | :----------: | :------: | :--: |\n|  1   | 青年 |   否   |      否      |   一般   |  否  |\n|  2   | 青年 |   否   |      否      |    好    |  否  |\n|  3   | 青年 |   是   |      否      |    好    |  是  |\n|  4   | 青年 |   是   |      是      |   一般   |  是  |\n|  5   | 青年 |   否   |      否      |   一般   |  否  |\n|  6   | 中年 |   否   |      否      |   一般   |  否  |\n|  7   | 中年 |   否   |      否      |    好    |  否  |\n|  8   | 中年 |   是   |      是      |    好    |  是  |\n|  9   | 中年 |   否   |      是      |  非常好  |  是  |\n|  10  | 中年 |   否   |      是      |  非常好  |  是  |\n|  11  | 老年 |   否   |      是      |  非常好  |  是  |\n|  12  | 老年 |   否   |      是      |    好    |  是  |\n|  13  | 老年 |   是   |      否      |    好    |  是  |\n|  14  | 老年 |   是   |      否      |  非常好  |  是  |\n|  15  | 老年 |   否   |      否      |   一般   |  否  |\n\n我们希望通过所给的训练数据学习一个贷款申请的决策树，用以对文莱的贷款申请进行分类，即当新的客户提出贷款申请是，根据申请人的特征利用决策树决定是否批准贷款申请。特征选择其实是决定用那个特征来划分特征空间。下图中分别是按照年龄，还有是否有工作来划分得到不同的子节点\n\n![image-20210107115449745](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170201.png)\n\n![image-20210107115507352](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170202.png)\n\n问题是究竟选择哪个特征更好些呢？那么直观上，如果一个特征具有更好的分类能力，是的各个自己在当前的条件下有最好的分类，那么就更应该选择这个特征。信息增益就能很好的表示这一直观的准则。这样得到的一棵决策树只用了两个特征就进行了判断：\n\n![image-20210107115650711](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170203.png)\n\n通过信息增益生成的决策树结构，更加明显、快速的划分类别。下面介绍scikit-learn中API的使用\n\n### 信息的度量和作用\n\n我们常说信息有用，那么它的作用如何客观、定量地体现出来呢？信息用途的背后是否有理论基础呢？这个问题一直没有很好的回答，直到1948年，香农在他的论文“通信的数学原理”中提到了“信息熵”的概念，才解决了信息的度量问题，并量化出信息的作用。\n\n一条信息的信息量与其不确定性有着直接的关系，比如我们要搞清一件非常不确定的事，就需要大量的信息。相反如果对某件事了解较多，则不需要太多的信息就能把它搞清楚 。所以从这个角度看，可以认为，信息量就等于不确定的多少。那么如何量化信息量的度量呢？2022年举行世界杯，大家很关系谁是冠军。假如我错过了看比赛，赛后我问朋友 ，“谁是冠军”？他不愿意直接告诉我，让我每猜一次给他一块钱，他告诉我是否猜对了，那么我需要掏多少钱才能知道谁是冠军？我可以把球编上号，从1到32，然后提问：冠 军在1-16号吗？依次询问，只需要五次，就可以知道结果。所以谁是世界杯冠军这条消息只值五块钱。当然香农不是用钱，而是用“比特”这个概念来度量信息量。一个比特是 一位二进制数，在计算机中一个字节是8比特。\n\n那么如果说有一天有64支球队进行决赛阶段的比赛，那么“谁是世界杯冠军”的信息量就是6比特，因为要多猜一次，有的同学就会发现，信息量的比特数和所有可能情况的对数函数log有关，(log32=5,log64=6)\n\n另外一方面你也会发现实际上我们不需要猜五次就能才出冠军，因为像西班牙、巴西、德国、意大利这样的球队夺得冠军的可能性比南非、尼日利亚等球队大得多，因此第一次猜测时不需要把32支球队等分成两个组，而可以把少数几支最有可能的球队分成一组，把其他球队分成一组。然后才冠军球队是否在那几支热门队中。这样，也许三次就猜出结果。因此，当每支球队夺冠的可能性不等时，“谁是世界杯冠军”的信息量比5比特少。香农指出，它的准确信息量应该是：\n\nH = -(p1*logp1 + p2*logp2 + … + p32log32)\n\n其中，p1…p32为这三支球队夺冠的概率。**H的专业术语称之为信息熵，单位为比特**，当这32支球队夺冠的几率相同时，对应的信息熵等于5比特，这个可以通过计算得出。有一个特性就是，5比特是公式的最大值。那么信息熵（经验熵）的具体定义可以为如下：\n\n![image-20210107115903126](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170204.png)\n\n### 信息增益\n\n自古以来，信息和消除不确定性是相联系的。所以决策树的过程其实是在寻找某一个特征对整个分类结果的不确定减少的过程。那么这样就有一个概念叫做信息增益（information gain）。\n\n**那么信息增益表示得知特征X的信息而是的类Y的信息的不确定性减少的程度**，所以我们对于选择特征进行分类的时候，当然选择信息增益较大的特征，这样具有较强的分类能力。特征A对训练数据集D的信息增益g(D,A),定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即公式为：\n\ng(D,A)=H(D)−H(D∣A)\n\n**根据信息增益的准则的特征选择方法是：对于训练数据集D，计算其每个特征的信息增益，并比较它们的阿笑，选择信息增益最大的特征**\n\n### 信息增益的计算\n\n![image-20210107120116471](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170205.png)\n\n既然我们有了这两个公式，我们可以根据前面的是否通过贷款申请的例子来通过计算得出我们的决策特征顺序。那么我们首先计算总的经验熵为：\n\n![image-20210107120135360](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170206.png)\n\n然后我们让A1,A2,A3,A4*A*1,*A*2,*A*3,*A*4分别表示年龄、有工作、有自己的房子和信贷情况4个特征，则计算出年龄的信息增益为：\n\n![image-20210107120154953](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170207.png)\n\n同理其他的也可以计算出来，g(D,A2)=0.324,g(D,A3)=0.420,g(D,A4)=0.363，相比较来说其中特征A3（有自己的房子）的信息增益最大，所以我们选择特征A3为最有特征\n\n### sklearn.tree.DecisionTreeClassifier\n\nsklearn.tree.DecisionTreeClassifier是一个能对数据集进行多分类的类\n\n```python\nclass sklearn.tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_split=1e-07, class_weight=None, presort=False)\n  \"\"\"\n  :param max_depth：int或None，可选（默认=无）树的最大深度。如果没有，那么节点将被扩展，直到所有的叶子都是纯类，或者直到所有的叶子都包含少于min_samples_split样本\n\n  :param random_state：random_state是随机数生成器使用的种子\n  \"\"\"\n```\n\n首先我们导入类，以及数据集，还有将数据分成训练数据集和测试数据集两部分\n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\niris = load_iris()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nestimator = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\nestimator.fit(X_train, y_train)\n```\n\n### method\n\n**apply** 返回每个样本被预测的叶子的索引\n\n```python\nestimator.apply(X)\n\narray([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  5,\n        5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n        5,  5, 15,  5,  5,  5,  5,  5,  5, 10,  5,  5,  5,  5,  5, 10,  5,\n        5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5, 16, 16,\n       16, 16, 16, 16,  6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n        8, 16, 16, 16, 16, 16, 16, 14, 16, 16, 11, 16, 16, 16,  8,  8, 16,\n       16, 16, 14, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16])\n```\n\n**decision_path** 返回树中的决策路径\n\n```python\ndp = estimator.decision_path(X_test)\n```\n\n**fit_transform(X,y=None，**fit_params)** 输入数据，然后转换\n\n**predict(X)** 预测输入数据的类型,完整代码\n\n```python\nestimator.predict(X_test)\narray([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 2, 1, 0, 1, 2, 1, 0, 2])\n\nprint y_test\n\narray([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1, 0,\n       0, 2, 0, 0, 1, 1, 0, 2, 1, 0, 2, 2, 1, 0, 1])\n```\n\n**score(X,y,sample_weight=None)** 返回给定测试数据的准确精度\n\n```python\nestimator.score(X_test,y_test)\n\n0.89473684210526316\n```\n\n### 决策树本地保存\n\n**sklearn.tree.export_graphviz()** 该函数能够导出DOT格式\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier()\niris = load_iris()\nclf = clf.fit(iris.data, iris.target)\ntree.export_graphviz(clf,out_file='tree.dot')\n```\n\n那么有了tree.dot文件之后，我们可以通过命令转换为png或者pdf格式，首先得安装graphviz\n\n```\nubuntu:sudo apt-get install graphviz\nMac:brew install graphviz\n```\n\n然后我们运行这个命令\n\n```\n$ dot -Tps tree.dot -o tree.ps\n$ dot -Tpng tree.dot -o tree.png\n```\n\n或者，如果我们安装了Python模块pydotplus，我们可以直接在Python中生成PDF文件，通过pip install pydotplus，然后运行\n\n```python\nimport pydotplus\ndot_data = tree.export_graphviz(clf, out_file=None)\ngraph = pydotplus.graph_from_dot_data(dot_data)\ngraph.write_pdf(\"iris.pdf\")\n```\n\n查看决策树结构图片，这个结果是经过决策树学习的三个步骤之后形成的。当作了解\n\n![image-20210107203456582](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170208.png)\n\n\u003e 扩展：所有各种决策树算法是什么，它们之间有什么不同？哪一个在scikit-learn中实现？\n\u003e\n\u003e ID3 --- 信息增益 最大的准则\n\u003e\n\u003e C4.5 --- 信息增益比 最大的准则\n\u003e\n\u003e CART 回归树: 平方误差 最小 分类树: 基尼系数 最小的准则 在sklearn中可以选择划分的原则\n\n### 决策树优缺点分析\n\n决策树的一些优点是：\n\n- 简单的理解和解释。树木可视化。\n- 需要很少的数据准备。其他技术通常需要数据归一化，需要创建虚拟变量，并删除空值。但请注意，此模块不支持缺少值。\n- 使用树的成本（即，预测数据）在用于训练树的数据点的数量上是对数的。\n\n决策树的缺点包括：\n\n- 决策树学习者可以创建不能很好地推广数据的过于复杂的树。这被称为过拟合。修剪（目前不支持）的机制，设置叶节点所需的最小采样数或设置树的最大深度是避免此问题的必要条件。\n- 决策树可能不稳定，因为数据的小变化可能会导致完全不同的树被生成。通过使用合奏中的决策树来减轻这个问题。\n\n## 3.8 集成方法（分类）之随机森林\n\n在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。利用相同的训练数搭建多个独立的分类模型，然后通过投票的方式，以少数服从多数的原则作出最终的分类决策。例如, 如果你训练了5个树, 其中有4个树的结果是True, 1个数的结果是False, 那么最终结果会是True.\n\n在前面的决策当中我们提到，一个标准的决策树会根据每维特征对预测结果的影响程度进行排序，进而决定不同的特征从上至下构建分裂节点的顺序，如此以来，所有在随机森林中的决策树都会受这一策略影响而构建的完全一致，从而丧失的多样性。所以在随机森林分类器的构建过程中，每一棵决策树都会放弃这一固定的排序算法，转而随机选取特征。\n\n集成学习通过建立几个模型组合的来解决单一预测问题。它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。这些预测最后结合成单预测，因此优于任何一个单分类的做出预测。\n\n定义：在机器学习中，**随机森林**是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。\n\n**优点**：\n\n- 在当前所有算法中，具有极好的准确率\n- 能够有效地运行在大数据集上\n- 能够处理具有高维特征的输入样本，而且不需要降维\n- 能够评估各个特征在分类问题上的重要性\n- 对于缺省值问题也能够获得很好得结果\n\n![image-20210307235205695](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170209.png)\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170210.png)\n\n### 学习算法\n\n根据下列算法而建造每棵树：\n\n- 用N来表示训练用例（样本）的个数，M表示特征数目。\n- 输入特征数目m，用于确定决策树上一个节点的决策结果；其中m应远小于M。\n- 从N个训练用例（样本）中以有放回抽样的方式，取样N次，形成一个训练集（即bootstrap取样），并用未抽到的用例（样本）作预测，评估其误差。\n- 对于每一个节点，随机选择m个特征，决策树上每个节点的决定都是基于这些特征确定的。根据这m个特征，计算其最佳的分裂方式。\n\n### sklearn.ensemble，集成方法模块\n\nsklearn.ensemble提供了准确性更加好的集成方法，里面包含了主要的RandomForestClassifier(随机森林)方法。\n\n![image-20210307235222283](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170210.png)\n\n```python\nclass sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None)\n  \"\"\"\n  :param n_estimators：integer，optional（default = 10） 森林里的树木数量。\n\n  :param criteria：string，可选（default =“gini”）分割特征的测量方法\n\n  :param max_depth：integer或None，可选（默认=无）树的最大深度\n\n  :param bootstrap：boolean，optional（default = True）是否在构建树时使用自举样本。\n\n  \"\"\"\n```\n\n#### 属性\n\n- classes_：shape = [n_classes]的数组或这样的数组的列表，类标签（单输出问题）或类标签数组列表（多输出问题）。\n- feature*importances*：array = [n_features]的数组， 特征重要性（越高，功能越重要）。\n\n#### 方法\n\n- fit（X，y [，sample_weight]） 从训练集（X，Y）构建一棵树林。\n- predict（X） 预测X的类\n- score（X，y [，sample_weight]） 返回给定测试数据和标签的平均精度。\n- decision_path（X） 返回森林中的决策路径\n\n### 泰坦尼克号乘客数据案例\n\n![image-20210307234904569](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170211.png)\n\n这里我们通过决策树和随机森林对这个数据进行一个分类，判断乘客的生还。\n\n```python\nimport pandas as pd\nimport sklearn\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\n\ntitanic = pd.read_csv('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt')\n\n#选取一些特征作为我们划分的依据\nx = titanic[['pclass', 'age', 'sex']]\ny = titanic['survived']\n\n# 填充缺失值\nx['age'].fillna(x['age'].mean(), inplace=True)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\ndt = DictVectorizer(sparse=False)\n\nprint(x_train.to_dict(orient=\"record\"))\n\n# 按行，样本名字为键，列名也为键，[{\"1\":1,\"2\":2,\"3\":3}]\nx_train = dt.fit_transform(x_train.to_dict(orient=\"record\"))\n\nx_test = dt.fit_transform(x_test.to_dict(orient=\"record\"))\n\n# 使用决策树\ndtc = DecisionTreeClassifier()\n\ndtc.fit(x_train, y_train)\n\ndt_predict = dtc.predict(x_test)\n\nprint(dtc.score(x_test, y_test))\n\nprint(classification_report(y_test, dt_predict, target_names=[\"died\", \"survived\"]))\n\n# 使用随机森林\n\nrfc = RandomForestClassifier()\n\nrfc.fit(x_train, y_train)\n\nrfc_y_predict = rfc.predict(x_test)\n\nprint(rfc.score(x_test, y_test))\n\nprint(classification_report(y_test, rfc_y_predict, target_names=[\"died\", \"survived\"]))\n```\n\n### 总结\n\n![第二天总结](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170212.png)\n\n# 4. 回归算法\n\n回归是统计学中最有力的工具之一。机器学习监督学习算法分为分类算法和回归算法两种，其实就是根据类别标签分布类型为离散型、连续性而定义的。回归算法用于连续型分布预测，针对的是数值型的样本，使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签。\n\n回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。那么什么是线性关系和非线性关系？\n\n比如说在房价上，房子的面积和房子的价格有着明显的关系。那么X=房间大小，Y=房价，那么在坐标系中可以看到这些点：\n\n![image-20210109202049907](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170213.png)\n\n那么通过一条直线把这个关系描述出来，叫线性关系。\n\n![image-20210109202116618](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170214.png)\n\n如果是一条曲线，那么叫非线性关系\n\n![image-20210109202124416](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170215.png)\n\n那么回归的目的就是建立一个回归方程（函数）用来预测目标值，回归的求解就是求这个回归方程的回归系数。\n\n## 4.1 回归算法之线性回归\n\n线性回归的定义是：目标值预期是输入变量的线性组合。线性模型形式简单、易于建模，但却蕴含着机器学习中一些重要的基本思想。线性回归，是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。\n\n\u003e 优点：结果易于理解，计算不复杂\n\u003e\n\u003e 缺点：对非线性的数据拟合不好\n\u003e\n\u003e 适用数据类型：数值型和标称型\n\n对于单变量线性回归，例如：前面房价例子中房子的大小预测房子的价格。**f(x) = w1\\*x+w0**，这样通过主要参数w1就可以得出预测的值。\n\n通用公式为：\n\n![image-20210109202317714](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170216.png)\n\n那么对于多变量回归，例如：瓜的好坏程度 **f(x) = w0+0.2\\*色泽+0.5\\*根蒂+0.3\\*敲声**，得出的值来判断一个瓜的好与不好的程度。\n\n通用公式为：\n\n![image-20210109202341953](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170217.png)\n\n线性模型中的向量W值，客观的表达了各属性在预测中的重要性，因此线性模型有很好的解释性。对于这种“多特征预测”也就是（多元线性回归），那么线性回归就是在这个基础上得到这些W的值，然后以这些值来建立模型，预测测试数据。简单的来说就是学得一个线性模型以尽可能准确的预测实值输出标记。\n\n那么如果对于多变量线性回归来说我们可以通过向量的方式来表示W值与特征X值之间的关系：\n\n![image-20210109202404048](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170218.png)\n\n两向量相乘，结果为一个整数是估计值,其中所有特征集合的第一个特征值x0=1,那么我们可以通过通用的向量公式来表示线性模型：\n\n![image-20210109202458163](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170219.png)\n\n一个列向量的转置与特征的乘积，得出我们预测的结果，但是显然我们这个模型得到的结果可定会有误差，如下图所示：\n\n![image-20210109202642215](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170220.png)\n\n### 损失函数\n\n损失函数是一个贯穿整个机器学习重要的一个概念，大部分机器学习算法都会有误差，我们得通过显性的公式来描述这个误差，并且将这个误差优化到最小值。\n\n对于线性回归模型，将模型与数据点之间的距离差之和做为衡量匹配好坏的标准，误差越小,匹配程度越大。我们要找的模型就是需要将f(x)和我们的真实值之间最相似的状态。于是我们就有了误差公式，模型与数据差的平方和最小：\n\n![image-20210109202707788](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170221.png)\n\n上面公式定义了所有的误差和，那么现在需要使这个值最小？那么有两种方法，**一种使用梯度下降算法**，**另一种使正规方程解法（只适用于简单的线性回归）**。\n\n### 梯度下降算法\n\n上面误差公式是一个通式，我们取两个单个变量来求最小值，误差和可以表示为：\n\n![image-20210109202725779](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170222.png)\n\n可以通过调整不同的w1和w0的值，就能使误差不断变化，而当你找到这个公式的最小值时，你就能得到最好的w1,w0 而这对(w1,w0)就是能最好描述你数据关系的模型参数。\n\n怎么找cost(w0+w1x1)的最小? cost(w0+w1x1)的图像其实像一个山谷一样，有一个最低点。找这个最低点的办法就是，先随便找一个点(w1=5, w0=4), 然后 沿着这个碗下降的方向找，最后就能找到山谷的最低点。\n\n![image-20210109202849700](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170223.png)\n\n所以得出![image-20210109202900085](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170224.png)，那么这个过程是按照某一点在w1上的偏导数下降寻找最低点。当然在进行移动的时候也需要考虑，每次移动的速度，也就是α*α*的值,这个值也叫做（学习率），如下式：\n\n![image-20210109202923425](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170225.png)\n\n这样就能求出w0,w1的值，当然你这个过程是不断的进行迭代求出来，通过交叉验证方法即可。\n\n### LinearRegression\n\n### sklearn.linear_model.LinearRegression\n\n```python\nclass LinearRegression(fit_intercept = True，normalize = False，copy_X = True，n_jobs = 1)\n  \"\"\"\n  :param normalize:如果设置为True时，数据进行标准化。请在使用normalize = False的估计器调时用fit之前使用preprocessing.StandardScaler\n\n  :param copy_X:boolean，可选，默认为True，如果为True，则X将被复制\n\n  :param n_jobs：int，可选，默认1。用于计算的CPU核数\n  \"\"\"\n```\n\n实例代码：\n\n```python\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\n```\n\n### 方法\n\n**fit(X,y,sample_weight = None)**\n\n使用X作为训练数据拟合模型，y作为X的类别值。X，y为数组或者矩阵\n\n```python\nreg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n```\n\n**predict(X)**\n\n预测提供的数据对应的结果\n\n```python\nreg.predict([[3,3]])\n\narray([ 3.])\n```\n\n### 属性\n\n**coef_**\n\n表示回归系数w=(w1,w2….)\n\n```python\nreg.coef_\n\narray([ 0.5,  0.5])\n```\n\n**intercept_** 表示w0\n\n### 加入交叉验证\n\n前面我们已经提到了模型的交叉验证，那么我们这个自己去建立数据集，然后通过线性回归的交叉验证得到模型。由于sklearn中另外两种回归岭回归、lasso回归都本省提供了回归CV方法，比如linear_model.Lasso，交叉验证linear_model.LassoCV；linear_model.Ridge，交叉验证linear_model.RidgeCV。所以我们需要通过前面的cross_validation提供的方法进行k-折交叉验证。\n\n```python\nfrom sklearn.datasets.samples_generator import make_regression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\n\nlr = linear_model.LinearRegression()\nX, y = make_regression(n_samples=200, n_features=5000, random_state=0)\nresult = cross_val_score(lr, X, y)\nprint result\n```\n\n## 4.2 线性回归案例分析\n\n### 波士顿房价预测\n\n使用scikit-learn中内置的回归模型对“美国波士顿房价”数据进行预测。对于一些比赛数据，可以从kaggle官网上获取，网址：https://www.kaggle.com/datasets\n\n**1.美国波士顿地区房价数据描述**\n\n```python\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\n\nprint boston.DESCR\n```\n\n**2.波士顿地区房价数据分割**\n\n```python\nfrom sklearn.cross_validation import train_test_split\nimport numpy as np\nX = boston.data\ny = boston.target\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=33,test_size = 0.25)\n```\n\n**3.训练与测试数据标准化处理**\n\n```python\nfrom sklearn.preprocessing import StandardScaler\nss_X = StandardScaler()\nss_y = StandardScaler()\n\nX_train = ss_X.fit_transform(X_train)\nX_test = ss_X.transform(X_test)\ny_train = ss_X.fit_transform(y_train)\nX_train = ss_X.transform(y_test)\n```\n\n**4.使用最简单的线性回归模型LinearRegression和梯度下降估计SGDRegressor对房价进行预测**\n\n```python\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train,y_train)\nlr_y_predict = lr.predict(X_test)\n\nfrom sklearn.linear_model import SGDRegressor\nsgdr = SGDRegressor()\nsgdr.fit(X_train,y_train)\nsgdr_y_predict = sgdr.predict(X_test)\n```\n\n**5.性能评测**\n\n对于不同的类别预测，我们不能苛刻的要求回归预测的数值结果要严格的与真实值相同。一般情况下，我们希望衡量预测值与真实值之间的差距。因此，可以测评函数进行评价。其中最为直观的评价指标均方误差(Mean Squared Error)MSE，因为这也是线性回归模型所要优化的目标。\n\nMSE的计算方法如式：\n\nMSE=1m∑i=1m(yi−y¯)2*M**S**E*=*m*1∑*i*=1*m*(*y**i*−*y*¯)2\n\n**使用MSE评价机制对两种模型的回归性能作出评价**\n\n```python\nfrom sklearn.metrics import mean_squared_error\n\nprint '线性回归模型的均方误差为：',mean_squared_error(ss_y.inverse_transform(y_test),ss_y.inverse_tranform(lr_y_predict))\nprint '梯度下降模型的均方误差为：',mean_squared_error(ss_y.inverse_transform(y_test),ss_y.inverse_tranform(sgdr_y_predict))\n```\n\n通过这一比较发现，使用梯度下降估计参数的方法在性能表现上不及使用解析方法的LinearRegression，但是如果面对训练数据规模十分庞大的任务，随即梯度法不论是在分类还是回归问题上都表现的十分高效，可以在不损失过多性能的前提下，节省大量计算时间。根据Scikit-learn光网的建议，如果数据规模超过10万，推荐使用随机梯度法估计参数模型。\n\n\u003e 注意：线性回归器是最为简单、易用的回归模型。正式因为其对特征与回归目标之间的线性假设，从某种程度上说也局限了其应用范围。特别是，现实生活中的许多实例数据的各种特征与回归目标之间，绝大多数不能保证严格的线性关系。尽管如此，在不清楚特征之间关系的前提下，我们仍然可以使用线性回归模型作为大多数数据分析的基线系统。\n\n完整代码如下：\n\n```python\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_boston\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import mean_squared_error,classification_report\nfrom sklearn.cluster import KMeans\n\n\ndef linearmodel():\n    \"\"\"\n    线性回归对波士顿数据集处理\n    :return: None\n    \"\"\"\n\n    # 1、加载数据集\n\n    ld = load_boston()\n\n    x_train,x_test,y_train,y_test = train_test_split(ld.data,ld.target,test_size=0.25)\n\n    # 2、标准化处理\n\n    # 特征值处理\n    std_x = StandardScaler()\n    x_train = std_x.fit_transform(x_train)\n    x_test = std_x.transform(x_test)\n\n\n    # 目标值进行处理\n\n    std_y  = StandardScaler()\n    y_train = std_y.fit_transform(y_train)\n    y_test = std_y.transform(y_test)\n\n    # 3、估计器流程\n\n    # LinearRegression\n    lr = LinearRegression()\n\n    lr.fit(x_train,y_train)\n\n    # print(lr.coef_)\n\n    y_lr_predict = lr.predict(x_test)\n\n    y_lr_predict = std_y.inverse_transform(y_lr_predict)\n\n    print(\"Lr预测值：\",y_lr_predict)\n\n\n    # SGDRegressor\n    sgd = SGDRegressor()\n\n    sgd.fit(x_train,y_train)\n\n    # print(sgd.coef_)\n\n    y_sgd_predict = sgd.predict(x_test)\n\n    y_sgd_predict = std_y.inverse_transform(y_sgd_predict)\n\n    print(\"SGD预测值：\",y_sgd_predict)\n\n    # 带有正则化的岭回归\n\n    rd = Ridge(alpha=0.01)\n\n    rd.fit(x_train,y_train)\n\n    y_rd_predict = rd.predict(x_test)\n\n    y_rd_predict = std_y.inverse_transform(y_rd_predict)\n\n    print(rd.coef_)\n\n    # 两种模型评估结果\n\n    print(\"lr的均方误差为：\",mean_squared_error(std_y.inverse_transform(y_test),y_lr_predict))\n\n    print(\"SGD的均方误差为：\",mean_squared_error(std_y.inverse_transform(y_test),y_sgd_predict))\n\n    print(\"Ridge的均方误差为：\",mean_squared_error(std_y.inverse_transform(y_test),y_rd_predict))\n\n    return None\n```\n\n## 4.3 欠拟合与过拟合\n\n机器学习中的泛化，泛化即是，模型学习到的概念在它处于学习的过程中时模型没有遇见过的样本时候的表现。在机器学习领域中，当我们讨论一个机器学习模型学习和泛化的好坏时，我们通常使用术语：过拟合和欠拟合。我们知道模型训练和测试的时候有两套数据，训练集和测试集。在对训练数据进行拟合时，需要照顾到每个点，而其中有一些噪点，当某个模型过度的学习训练数据中的细节和噪音，以至于模型在新的数据上表现很差，这样的话模型容易复杂，拟合程度较高，造成过拟合。而相反如果值描绘了一部分数据那么模型复杂度过于简单，欠拟合指的是模型在训练和预测时表现都不好的情况，称为欠拟合。\n\n我们来看一下线性回归中拟合的几种情况图示：\n\n![image-20210109203652639](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170226.png)\n\n![image-20210109203709661](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170227.png)\n\n![image-20210109203735076](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/python/ML/20210405170228.png)\n\n### 解决过拟合的方法\n\n在线性回归中，对于特征集过小的情况，容易造成欠拟合（underfitting），对于特征集过大的情况，容易造成过拟合（overfitting）。针对这两种情况有了更好的解决办法\n\n#### 欠拟合\n\n欠拟合指的是模型在训练和预测时表现都不好的情况，欠拟合通常不被讨论，因为给定一个评估模型表现的指标的情况下，欠拟合很容易被发现。矫正方法是继续学习并且试着更换机器学习算法。\n\n##### 过拟合\n\n对于过拟合，特征集合数目过多，我们需要做的是尽量不让回归系数数量变多，对拟合（损失函数）加以限制。\n\n（1）当然解决过拟合的问题可以减少特征数，显然这只是权宜之计，因为特征意味着信息，放弃特征也就等同于丢弃信息，要知道，特征的获取往往也是艰苦卓绝的。\n\n（2）引入了 **正则化** 概念。\n\n**直观上来看，如果我们想要解决上面回归中的过拟合问题，我们最好就要消除x3\\*x\\*3和x4\\*x\\*4的影响，也就是想让θ3,θ4\\*θ\\*3,\\*θ\\*4都等于0，一个简单的方法就是我们对θ3,θ4\\*θ\\*3,\\*θ\\*4进行惩罚，增加一个很大的系数，这样在优化的过程中就会使这两个参数为零。**\n\n##4.4 回归算法之岭回归\n\n具有L2正则化的线性最小二乘法。岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。当数据集中存在共线性的时候，岭回归就会有用。\n\n### sklearn.linear_model.Ridge\n\n```python\nclass sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)**\n  \"\"\"\n  :param alpha:float类型，正规化的程度\n  \"\"\"\nfrom sklearn.linear_model import Ridge\nclf = Ridge(alpha=1.0)\nclf.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]))\n```\n\n### 方法\n\n**score(X, y, sample_weight=None)**\n\n```python\nclf.score()\n```\n\n### 属性\n\n**coef_**\n\n```python\nclf.coef_\narray([ 0.34545455,  0.34545455])\n```\n\n**intercept_**\n\n```python\nclf.intercept_\n0.13636...\n```\n\n### 案例\n\n```python\ndef linearmodel():\n    \"\"\"\n    线性回归对波士顿数据集处理\n    :return: None\n    \"\"\"\n\n    # 1、加载数据集\n\n    ld = load_boston()\n\n    x_train,x_test,y_train,y_test = train_test_split(ld.data,ld.target,test_size=0.25)\n\n    # 2、标准化处理\n\n    # 特征值处理\n    std_x = StandardScaler()\n    x_train = std_x.fit_transform(x_train)\n    x_test = std_x.transform(x_test)\n\n\n    # 目标值进行处理\n\n    std_y  = StandardScaler()\n    y_train = std_y.fit_transform(y_train)\n    y_test = std_y.transform(y_test)\n\n    # 3、估计器流程\n\n    # LinearRegression\n    lr = LinearRegression()\n\n    lr.fit(x_train,y_train)\n\n    # print(lr.coef_)\n\n    y_lr_predict = lr.predict(x_test)\n\n    y_lr_predict = std_y.inverse_transform(y_lr_predict)\n\n    print(\"Lr预测值：\",y_lr_predict)\n\n\n    # SGDRegressor\n    sgd = SGDRegressor()\n\n    sgd.fit(x_train,y_train)\n\n    # print(sgd.coef_)\n\n    y_sgd_predict = sgd.predict(x_test)\n\n    y_sgd_predict = std_y.inverse_transform(y_sgd_predict)\n\n    print(\"SGD预测值：\",y_sgd_predict)\n\n    # 带有正则化的岭回归\n\n    rd = Ridge(alpha=0.01)\n\n    rd.fit(x_train,y_train)\n\n    y_rd_predict = rd.predict(x_test)\n\n    y_rd_predict = std_y.inverse_transform(y_rd_predict)\n\n    print(rd.coef_)\n\n    # 两种模型评估结果\n\n    print(\"lr的均方误差为：\",mean_squared_error(std_y.inverse_transform(y_test),y_lr_predict))\n\n    print(\"SGD的均方误差为：\",mean_squared_error(std_y.inverse_transform(y_test),y_sgd_predict))\n\n    print(\"Ridge的均方误差为：\",mean_squared_error(std_y.inverse_transform(y_test),y_rd_predict))\n\n    return None\n```\n\n# 5. 非监督学习\n\n从本节开始，将正式进入到无监督学习（Unsupervised Learning）部分。无监督学习，顾名思义，就是不受监督的学习，一种自由的学习方式。该学习方式不需要先验知识进行指导，而是不断地自我认知，自我巩固，最后进行自我归纳，在机器学习中，无监督学习可以被简单理解为不为训练集提供对应的类别标识（label），其与有监督学习的对比如下： 有监督学习（Supervised Learning）下的训练集：\n\n(x(1),y(1)),(x(2),y2)(x(1),y(1)),(x(2),y2)\n\n无监督学习（Unsupervised Learning）下的训练集：\n\n(x(1)),(x(2)),(x(3))(x(1)),(x(2)),(x(3))\n\n在有监督学习中，我们把对样本进行分类的过程称之为分类（Classification），而在无监督学习中，我们将物体被划分到不同集合的过程称之为聚类（Clustering）\n\n##5.1 非监督学习之k-means\n\nK-means通常被称为劳埃德算法，这在数据聚类中是最经典的，也是相对容易理解的模型。算法执行的过程分为4个阶段。\n\n- 1.首先，随机设K个特征空间内的点作为初始的聚类中心。\n- 2.然后，对于根据每个数据的特征向量，从K个聚类中心中寻找距离最近的一个，并且把该数据标记为这个聚类中心。\n- 3.接着，在所有的数据都被标记过聚类中心之后，根据这些数据新分配的类簇，通过取分配给每个先前质心的所有样本的平均值来创建新的质心重,新对K个聚类中心做计算。\n- 4.最后，计算旧和新质心之间的差异,如果所有的数据点从属的聚类中心与上一次的分配的类簇没有变化，那么迭代就可以停止，否则回到步骤2继续循环。\n\nK均值等于具有小的全对称协方差矩阵的期望最大化算法。\n\n### sklearn.cluster.KMeans\n\n```python\nclass sklearn.cluster.KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto')\n  \"\"\"\n  :param n_clusters:要形成的聚类数以及生成的质心数\n\n  :param init:初始化方法，默认为'k-means ++',以智能方式选择k-均值聚类的初始聚类中心，以加速收敛;random,从初始质心数据中随机选择k个观察值（行\n\n  :param n_init：int，默认值：10使用不同质心种子运行k-means算法的时间。最终结果将是n_init连续运行在惯性方面的最佳输出。\n\n  :param n_jobs：int用于计算的作业数量。这可以通过并行计算每个运行的n_init。如果-1使用所有CPU。如果给出1，则不使用任何并行计算代码，这对调试很有用。对于-1以下的n_jobs，使用（n_cpus + 1 + n_jobs）。因此，对于n_jobs = -2，所有CPU都使用一个。\n\n  :param random_state:随机数种子，默认为全局numpy随机数生成器\n  \"\"\"\nfrom sklearn.cluster import KMeans\nimport numpy as np\nX = np.array([[1, 2], [1, 4], [1, 0],[4, 2], [4, 4], [4, 0]])\nkmeans = KMeans(n_clusters=2, random_state=0)\n```\n\n### 方法\n\n**fit(X,y=None)**\n\n使用X作为训练数据拟合模型\n\n```python\nkmeans.fit(X)\n```\n\n**predict(X)**\n\n预测新的数据所在的类别\n\n```python\nkmeans.predict([[0, 0], [4, 4]])\narray([0, 1], dtype=int32)\n```\n\n### 属性\n\n**cluster\\*centers\\***\n\n集群中心的点坐标\n\n```python\nkmeans.cluster_centers_\narray([[ 1.,  2.],\n       [ 4.,  2.]])\n```\n\n**labels_**\n\n每个点的类别\n\n```python\nkmeans.labels_\n```\n\n### k-means ++\n\n### 手写数字数据上K-Means聚类的演示\n\n```python\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\n\ndef kmeans():\n    \"\"\"\n    手写数字聚类过程\n    :return: None\n    \"\"\"\n    # 加载数据\n\n    ld = load_digits()\n\n    print(ld.target[:20])\n\n\n    # 聚类\n    km = KMeans(n_clusters=810)\n\n    km.fit_transform(ld.data)\n\n    print(km.labels_[:20])\n\n    print(silhouette_score(ld.data,km.labels_))\n\n    return None\n\n\n\nif __name__==\"__main__\":\n    kmeans()\n```\n\n\u003e\u003e\u003e\u003e\u003e\u003e\u003e 6fcc8243aa27f35e8c359944fe365cdac20d964f:Notes/Python/ML.md\n","lastmodified":"2023-05-09T16:33:58.30736679Z","tags":[]},"/%E7%99%BD%E6%9D%BF-2023-05-02":{"title":"白板 2023-05-02","content":"==⚠  Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠==\n\n\n# Text Elements\n%%\n# Drawing\n```compressed-json\nN4KAkARALgngDgUwgLgAQQQDwMYEMA2AlgCYBOuA7hADTgQBuCpAzoQPYB2KqATL\n\nZMzYBXUtiRoIACyhQ4zZAHoFAc0JRJQgEYA6bGwC2CgF7N6hbEcK4OCtptbErHAL\n\nRY8RMpWdx8Q1TdIEfARcZgRmBShcZQUARm0ADm0eADYaOiCEfQQOKGZuAG1wMFAw\n\nMogSbggANQArAHkATgAlAGkAFQBJAC1OuA4ADQAxBAAzZQBNAHV2gEV0sshYRCrA\n\n7CiOZWCF8sxuZ1iAVniUgGYeBNieQ/5ymH2ea+0Ui8aABheb4sgKEnVuWJvHjaU7\n\nHQ6fW6QSQIQjKaTcADshxBKQRsQSSMhEGsW3EqDeWOYUFIbAA1ggAMJsfBsUhVAD\n\nEbyZzJ2kE0uGwpOUJKEHGIVJpdIkxOszDguECuVZEFGhHw+AAyrBthJJByNIFpUS\n\nSeSpn9JADCcSyQglTAVehBB5pTy4Rxwvk0LEsWxxdg1PcnUysdzhHBOsRHagCgBd\n\nLE8vn25hB0qLaDwPGnb4AX0JCAQxABABYUuizjms1jGCx2Fw0Kdnd8GExWJwAHKc\n\nMTcLMIlJvWKNNtYwjMAAimSgGe4owIYSxmmEfIAosFsrkgxwhPKsUI4MRcIPM06U\n\nglTkDYi2Eu8EliiBxSdxF8uqzTOUO0CP8GFisnwGG6Lg4HAlRu8bHoNC2RVEQcJQ\n\nDsDCEAgFAAEIclyEb8tStIMqMqFoeB2AiJKUCdIO+hKjqlJIUK6CMsyTIYVhOQ4X\n\nhsGcr6vKIYKVQihwYoStRlGkNhuFZEMcqKsqeJSOqIhILcECYdx1G8fhxq6vqhrF\n\nJJVG5LJBEmmaFoQFalQSVJPF4c0wh2g6SnlAZMl4fUboegC3rKZZal4UMnBQEMuD\n\n6HKnqoJWFmqTRfFuQqhBGHiPAEo5AWye0WBQAAgiBZboMEoxgfp0V4T+pAJdxbAU\n\nNCuBbqgV74Bl0nOVkU58vFeUFSExXYnV4HMNgJLygM3CnI0fDKa17X4BMXWNMiR4\n\nJFmpyYspRhsAY3CxpA9AEEIeKxC+5WGVkxmMVGQYQAh4HciQIVhZeS5lY5xEAAoE\n\nAgMh/qmUUVVAGnkjZUClgu50SXAgRmMIzAAOKkMdoV4qV4CPZAsrBDGr7JkAA===\n\n\n```\n%%","lastmodified":"2023-05-09T16:33:56.555324015Z","tags":["excalidraw"]},"/%E7%A7%BB%E5%8A%A8Web%E4%B8%8E%E5%93%8D%E5%BA%94%E5%BC%8F":{"title":"基本知识","content":"\n# 基本知识\n\n## 视口\n\n**视口（viewport）**就是浏览器显示页面内容的屏幕区域。最先由 Apple 引入，用于解决移动端页面的显示问题，通过一个叫 `\u003cmeta\u003e` 的 DOM 标签，允许我们可以定义视口的各种行为，比如宽度，高度，初始缩放比例等，视口可以分为**布局视口**、**视觉视口**和**理想视口**。\n\n### **布局视口** **layout viewport**\n\n一般移动设备的浏览器都默认设置了一个布局视口，用于解决早期的PC端页面在手机上显示的问题。\n\n`layout viewport` 是一个固定的值，由浏览器厂商设定，\n\n- IOS 和 Android 基本都是 980px\n- 黑莓（BlackBerry）和 IE10 是 1024px\n- 在PC端上，布局视口等于浏览器窗口的宽度\n\n所以PC上的网页大多都能在手机上呈现，只不过元素看上去很小，一般默认可以通过手动缩放网页。\n\n![image-20210806171825292](image-20210806171825292.png)\n\n而在移动端上，由于要使为PC端浏览器设计的网站能够完全显示在移动端的小屏幕里，此时的布局视口会远大于移动设备的屏幕，就会出现滚动条。\n\n```javascript\nvar layoutViewportWidth = document.documentElement.clientWidth\nvar layoutViewportHeight = document.documentElement.clientHeight\n```\n\n### **视觉视口** **visual viewport**\n\n- 字面意思，它是用户正在看到的网站的区域。**注意：是网站的区域。**\n- 我们可以通过缩放去操作**视觉视口**，但不会影响布局视口，布局视口仍保持原来的宽度。\n\n![image-20210806171911621](image-20210806171911621.png)\n\n用户正在看到的网页的区域。用户可以通过缩放来查看网站的内容。如果用户缩小网站，我们看到的网站区域将变大，此时视觉视口也变大了，同理，用户放大网站，我们能看到的网站区域将缩小，此时视觉视口也变小了。不管用户如何缩放，都不会影响到布局视口的宽度。\n\n```js\nvar visualViewportWidth = window.innerWidth\nvar visualViewportHeight = window.innerHeight\n```\n\n### **理想视口** **ideal viewport**\n\n现在已经有两个viewport了,但浏览器觉得还不够，因为现在越来越多的网站都会为移动设备进行单独的设计，所以必须还要有一个能完美适配移动设备的viewport。\n\n所谓的完美适配指的是:\n\n- 不需要用户缩放和横向滚动条就能正常的查看网站的所有内容\n- 显示的文字的大小是合适，比如一段14px大小的文字，不会因为在一个高密度像素的屏幕里显示得太小而无法看清，理想的情况是这段14px的文字无论是在何种密度屏幕，何种分辨率下，显示出来的大小都是差不多的,当然，不只是文字，其他元素像图片什么的也是这个道理。\n\n这个viewport叫做 **ideal viewport(理想视口)**，`window.screen.width`\n\n我们可以通过 `meta` 设置将布局视口转换为理想视口，\n\n```html\n\u003cmeta name=\"viewport\" content=\"width=device-width\"\u003e\n```\n\n### **meta视口标签**\n\n`\u003cmeta name=\"viewport\" content=\"width=device-width, user-scalable=no,  \ninitial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0\"\u003e`\n\n| **属性**      | 解释                                                         |\n| ------------- | ------------------------------------------------------------ |\n| width         | 正整数\\|device-width，视口宽度，单位是 CSS 像素，如果等于 device-width，则为理想视口的宽度 |\n| height        | 正整数\\|device-height，视口宽度，单位是 CSS 像素，如果等于 device-height，则为理想视口的宽度 |\n| initial-scale | 0-10，初始缩放比例，允许小数点                               |\n| maximum-scale | 0-10，最大缩放比例，必须大于等于 minimum-scale               |\n| minimum-scale | 0-10，最小缩放比例，必须小于等于 maximum-scale               |\n| user-scalable | yes/no，是否允许用户缩放页面，默认是 yes                     |\n\n## 常用单位\n\n### 物理像素\n\n设备屏幕实际拥有的像素点，屏幕的基本单元，是有实体的。比如iPhone 6的屏幕在宽度方向有750个像素点，高度方向有1334个像素点，所有iPhone 6 总共有750*1334个像素点。同一个设备的物理像素是固定的，这是厂商在出厂时就设置好了的。\n\n屏幕普遍采用RGB色域(红、绿、蓝三个子像素构成),而印刷行业普遍使用CMYK色域(青、品红、黄和黑)。\n\n### 逻辑像素\n\n也叫“设备独立像素”（Device Independent Pixel，DIP），可以理解为反映在CSS/JS程序里面的像素点，**也就是说css像素是逻辑像素的一种**。除了css像素是逻辑像素以外，我们平时描述一张图片宽高时一般用 `200px * 100px`，这里的`px`也是逻辑像素。\n\n### 设备像素比（Device Pixel Ratio，DPR）\n\n在很久以前，CSS里写个1`px`，屏幕就给你渲染成1个实际的像素点，即DPR=1。\n\n后来苹果公司为其产品mac、iPhone以及iPad的屏幕配置了Retina高清屏，也就是说这种屏幕拥有的物理像素点数比非高清屏多4倍甚至更多。如果还按照DPR=1进行展示，那么同一张图片在高清屏上面显示的区域面积会是非高清屏的1/4，这样的话由于图片在屏幕上的展示面积大大缩小，也会导致出现“看不清”的问题。\n\n因此苹果使用4个乃至更多物理像素来渲染1个逻辑像素，这样一来，同样的CSS代码设置的尺寸，在Retina和非Retina屏幕上看起来大小是一样的，但在Retina屏幕上要精细得多。\n\n\u003cimg src=\"../../pics/image-20210806174558008.png\" alt=\"image-20210806174558008\" style=\"zoom:50%;\" /\u003e\n\n![image-20210806175307321](image-20210806175307321.png)\n\n在Retian屏上，DPR不再是1，而是大于1，比如2（iPhone 5 6 7 8）或3（iPhone 6 Plus等一系列Plus）或者为非整数（一些Android机），说不定还会涨。\n\n![image-20210806174807000](image-20210806174807000.png)\n\n**举个例子**：iPhone 6的物理像素上面已经说了，是750 * 1334，那它的逻辑像素呢？我们只需在iPhone 6的Safari里打印一下`screen.width`和`screen.height`就知道了，结果是 375 * 667，这就是它的逻辑像素，据此很容易计算出DRP为2。当然，我们还可以直接通过`window.devicePixelRatio`这个值来获取DRP，打印结果是2，符合我们的预期。\n\nPS:在苹果的带动下，Retina技术在移动设备上已经成了标配，所以前端攻城狮必须直面如下事实：\n\n```\n1. 你想画个1px的下边框，但屏幕硬是塞给你一条宽度为2—3个物理像素的线。\n2. 你没法像安卓或iOS的同事那样直接操纵物理像素点。\n```\n\n这就是初级前端面试必考题之“1px边框问题”的由来。\n\n### EM\n\nEM 相对于元素自身的 `font-size`，\n\n```css\np {\n  font-size: 16px;\n  padding: 1em; /* 1em = 16px */\n}\n```\n\n如果元素没有显式地设置 `font-size`，那么 `1em` 等于多少呢？\n\n这个问题其实跟咱说的 `em` 没啥关系，这里跟 `font-size` 的计算规则相关，回顾一下。如果元素没有设置 `font-size`，会继承父元素的 `font-size`，如果父元素也没有，会沿着 DOM 树一直向上查找，**直到根元素 `html`**，根元素的默认字体大小为 16px。\n\n### REM\n\n**REM = Root EM**，顾名思义就是相对于根元素的 EM。所以它的计算规则比较简单，\n\n1 `rem` 就等于根元素 `html` 的字体大小，\n\n```css\nhtml {\n  font-size: 14px;\n}\n\np {\n  font-size: 1rem; /* 1rem = 14px */\n}\n```\n\n所以，如果我们改变根元素的字体大小，页面上所有使用 `rem` 的元素都会被重绘。\n\n**EM 和 REM 都是相对单位**，我们在做响应式布局的时候应该如何选择呢？\n\n根据两者的特性，\n\n- EM 更适合模块化的页面元素，比如 Web Components\n- REM 则更加方便，只需要设置 `html` 的字体大小，所以 REM 的使用更加广泛一些\n\n实际开发中，设计图的单位是 CSS 像素，我们可以借助一些工具将 px 自动转换为 rem，下面是一个用 `PostCSS` 插件在基于 Webpack 构建的项目中自动转换的例子:\n\n```javascript\nvar px2rem = require('postcss-px2rem');\n\nmodule.exports = {\n  module: {\n    loaders: [\n      {\n        test: /\\.css$/,\n        loader: \"style-loader!css-loader!postcss-loader\"\n      }\n    ]\n  },\n  postcss: function() {\n    return [px2rem({remUnit: 75})];\n  }\n}\n```\n\n### vw，vh，百分比\n\n浏览器对于 `vw` 和 `vh` 的支持相对较晚，在 Android 4.4 以下的浏览器中可能没办法使用，下面是来自 Can I use 完整的兼容性统计数据，\n\n![image-20210503152608975](../../../../../Mobile Documents/com~apple~CloudDocs/learn-articles/pics/image-20210503152608975.png)\n\n新生特性往往逃不过兼容性的大坑，但是这并不妨碍我们了解它。\n\n响应式设计里，`vw` 和 `vh` 常被用于布局，因为它们是相对于**理想视口**:\n\n- **vw**，viewport width，视口宽度，所以 1vw = 1% 视口宽度\n- **vh**，viewport height，视口高度，所以 1vh = 1% 视口高度\n\n以 iPhoneX 为例，vw 和 CSS 像素的换算如下，\n\n```html\n\u003c!-- 假设我们设置视口为完美视口，这时视口宽度就等于设备宽度，CSS 像素为 375px --\u003e\n\u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n\n\u003cstyle\u003e\n  p {\n    width: 10vw; /* 10vw = 1% * 10 * 375px = 37.5px */\n  }\n\u003c/style\u003e\n```\n\n我们说百分比也可以用来设置元素的宽高，它和 `vw`，`vh` 的区别是什么？\n\n这里只需要记住一点，**百分比是相对于父元素的宽度和高度计算的。**\n\n## 二倍图\n\n- 对于一张 50px * 50px 的图片,在手机 Retina 屏中打开，按照刚才的物理像素比会放大倍数，这样会造成图片模糊\n- 在标准的viewport设置中，使用倍图来提高图片质量，解决在高清设备中的模糊问题\n- 通常使用二倍图， 因为iPhone 6\\7\\8 的影响,但是现在还存在3倍图4倍图的情况，这个看实际开发公司需求\n- 背景图片也注意缩放问题\n\n```css\n /* 在 iphone8 下面 */\nimg{\n /*原始图片100*100px*/\n width: 50px;\n height: 50px;\n} \n.box{\n /*原始图片100*100px*/\n background-size: 50px 50px;\n}\n```\n\n## 媒体查询\n\n媒体查询（Media Query）是 CSS3 规范中的一部分，媒体查询提供了简单的判断方法，允许开发者根据不同的设备特征应用不同的样式。响应式布局中，常用的设备特征有，\n\n- **min-width**，数值，视口宽度大于 `min-width` 时应用样式\n- **max-width**，数值，视口宽度小于 `max-width` 时应用样式\n- **orientation**，`portrait` | `landscape`，当前设备的方向\n\n选择 `min-width` 和 `max-width` 取值的过程，称为**设备断点选择**，它可能取决于产品设计本身，下面是 百度 Web 生态团队 总结的一套比较具有代表性的设备断点，\n\n```css\n/* 很小的设备（手机等，小于 600px） */\n@media only screen and (max-width: 600px) { }\n\n/* 比较小的设备（竖屏的平板，屏幕较大的手机等, 大于 600px） */\n@media only screen and (min-width: 600px) { }\n\n/* 中型大小设备（横屏的平板, 大于 768px） */\n@media only screen and (min-width: 768px) { }\n\n/* 大型设备（电脑, 大于 992px） */\n@media only screen and (min-width: 992px) { }\n\n/* 超大型设备（大尺寸电脑屏幕, 大于 1200px） */\n@media only screen and (min-width: 1200px) { }\n```\n","lastmodified":"2023-05-09T16:33:58.30736679Z","tags":[]},"/%E7%BB%84%E4%BB%B6%E5%BA%93%E8%AE%BE%E8%AE%A1":{"title":"组件库设计","content":"\n## 1.前端组件库的设计原则\n\n### 1.1 细粒度的考量\n\n我们在学习设计模式的时候会遇到很多种设计原则,其中一个设计原则就是**单一职责原则**,在组件库的开发中同样适用,我们原则上一个组件只专注一件事情,单一职责的组件的好处很明显,由于职责单一就可以最大可能性地复用组件,但是这也带来一个问题,过度单一职责的组件也可能会导致过度抽象,造成组件库的碎片化。\n\n举个例子，一个自动完成组件(AutoComplete),他其实是由 Input 组件和 Select 组件组合而成的,因此我们完全可以复用之前的相关组件,就比如 Antd 的 AutoComplete 组件中就复用了 Select 组件,同时 Calendar、 Form 等等一系列组件都复用了 Select 组件,那么 Select 的细粒度就是合适的,因为 Select 保持的这种细粒度很容易被复用.\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182538.png)\n\n那么还有一个例子,一个徽章数组件(Badge),它的右上角会有红点提示,可能是数字也可能是 icon,他的职责当然也很单一，这个红点提示也理所当然也可以被单独抽象为一个独立组件,但是我们通常不会将他作为独立组件,因为在其他场景中这个组件是无法被复用的，因为没有类似的场景再需要小红点这个小组件了，所以作为独立组件就属于细粒度过小,因此我们往往将它作为 Badge 的内部组件,比如在 Antd 中它以 ScrollNumber 的名称作为 Badge 的内部组件存在。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182534.png)\n\n所以，所谓的单一职责组件要建立在可复用的基础上，对于不可复用的单一职责组件我们仅仅作为独立组件的内部组件即可。\n\n### 1.2 通用性考量\n\n我们要设计的本身就是通用组件库,不同于我们常见的业务组件,通用组件是与业务解耦但是又服务于业务开发的,那么问题来了,如何保证组件的通用性,通用性高一定是好事吗?\n\n比如我们设计一个选择器(Select)组件,通常我们会设计成这样\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182531.png)\n\n这是一个我们最常见也最常用的选择器,但是问题是其通用性大打折扣\n\n当我们有一个需求是长这样的时候,我们之前的选择器组件就不符合要求了,因为这个 Select 组件的最下部需要有一个可拓展的条目的按钮\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182525.png)\n\n这个时候我们难道要重新修改之前的选择器组件,甚至再造一个符合要求的选择器组件吗?一旦有这种情况发生,那么只能说明之前的选择器组件通用性不够,需要我们重新设计.\n\nAntd 的 Select 组件预留了`dropdownRender`来进行自定义渲染,其依赖的 `rc-select`组件中的代码如下\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182524.png)\n\n\u003e Antd 依赖了大量以`rc-`开头的底层组件,这些组件被 react-component 团队(同时也就是 Antd 团队)维护,其主要实现组件的底层逻辑,Antd 则是在此基础上添加 Ant Design 设计语言而实现的\n\n当然类似的设计还有很多,通用性设计其实是一定意义上放弃对 DOM 的掌控,而将 DOM 结构的决定权转移给开发者,`dropdownRender`其实就是放弃对 Select 下拉菜单中条目的掌控,Antd 的 Select 组件其实还有一个没有在文档中体现的方法`getInputElement`应该是对 Input 组件的自定义方法,Antd 整个 Select 的组件设计非常复杂,基本将所有的 DOM 结构控制权全部暴露给了开发者,其本身只负责底层逻辑和最基本的 DOM 结构.\n\n这是 Antd 所依赖的 re-select 最终 jsx 的结构,其 DOM 结构很简单,但是暴露了大量自定义渲染的接口给开发者.\n\n```jsx\nreturn (\n      \u003cSelectTrigger\n        onPopupFocus={this.onPopupFocus}\n        onMouseEnter={this.props.onMouseEnter}\n        onMouseLeave={this.props.onMouseLeave}\n        dropdownAlign={props.dropdownAlign}\n        dropdownClassName={props.dropdownClassName}\n        dropdownMatchSelectWidth={props.dropdownMatchSelectWidth}\n        defaultActiveFirstOption={props.defaultActiveFirstOption}\n        dropdownMenuStyle={props.dropdownMenuStyle}\n        transitionName={props.transitionName}\n        animation={props.animation}\n        prefixCls={props.prefixCls}\n        dropdownStyle={props.dropdownStyle}\n        combobox={props.combobox}\n        showSearch={props.showSearch}\n        options={options}\n        multiple={multiple}\n        disabled={disabled}\n        visible={realOpen}\n        inputValue={state.inputValue}\n        value={state.value}\n        backfillValue={state.backfillValue}\n        firstActiveValue={props.firstActiveValue}\n        onDropdownVisibleChange={this.onDropdownVisibleChange}\n        getPopupContainer={props.getPopupContainer}\n        onMenuSelect={this.onMenuSelect}\n        onMenuDeselect={this.onMenuDeselect}\n        onPopupScroll={props.onPopupScroll}\n        showAction={props.showAction}\n        ref={this.saveSelectTriggerRef}\n        menuItemSelectedIcon={props.menuItemSelectedIcon}\n        dropdownRender={props.dropdownRender}\n        ariaId={this.ariaId}\n      \u003e\n        \u003cdiv\n          id={props.id}\n          style={props.style}\n          ref={this.saveRootRef}\n          onBlur={this.onOuterBlur}\n          onFocus={this.onOuterFocus}\n          className={classnames(rootCls)}\n          onMouseDown={this.markMouseDown}\n          onMouseUp={this.markMouseLeave}\n          onMouseOut={this.markMouseLeave}\n        \u003e\n          \u003cdiv\n            ref={this.saveSelectionRef}\n            key=\"selection\"\n            className={`${prefixCls}-selection\n            ${prefixCls}-selection--${multiple ? 'multiple' : 'single'}`}\n            role=\"combobox\"\n            aria-autocomplete=\"list\"\n            aria-haspopup=\"true\"\n            aria-controls={this.ariaId}\n            aria-expanded={realOpen}\n            {...extraSelectionProps}\n          \u003e\n            {ctrlNode}\n            {this.renderClear()}\n            {this.renderArrow(!!multiple)}\n          div\u003e\n        div\u003e\n      SelectTrigger\u003e\n    );\n```\n\n那么这么多需要自定义的地方,这个 Select 组件岂不是很难用?因为好像所有地方都需要开发者自定义,通用性设计在将 DOM 结构决定权交给开发者的同时也保留了默认结构,在开发者没有显示自定义的时候默认使用默认渲染结构,其实 Select 的基本使用很方便,如下:\n\n```jsx\n\u003cselect defaultvalue=\"lucy\" style={{ width: 120 }} disabled\u003e\n  \u003cOption value=\"lucy\"\u003eLucyOption\u003e\n\u003c/Select\u003e\n```\n\n\u003e 组件的形态(DOM 结构)永远是千变万化的,但是其行为(逻辑)是固定的,因此通用组件的秘诀之一就是将 DOM 结构的控制权交给开发者,组件只负责行为和最基本的 DOM 结构\n\n---\n\n## 2 技术选型\n\n### 2.1 css 解决方案\n\n由于 CSS 本身的众多缺陷，如书写繁琐（不支持嵌套）、样式易冲突（没有作用域概念）、缺少变量（不便于一键换主题）等不一而足。为了解决这些问题，社区里的解决方案也是出了一茬又一茬，从最早的 CSS prepocessor（SASS、LESS、Stylus）到后来的后起之秀 PostCSS，再到 CSS Modules、Styled-Components 等。\n\nAntd 选择了 less 作为 css 的预处理方案,Bootstrap 选择了 Scss,这两种方案孰优孰劣已经争论了很多年了:\n\nSCSS 和 LESS 相比有什么优势？\n\n但是不管是哪种方案都有一个很烦人的点,就是需要额外引入 css,比如 Antd 需要这样显示引入:\n\n```\nimport Button from 'antd/lib/button';\nimport 'antd/lib/button/style';\n```\n\n为了解决这种尴尬的情况,Antd 用 Babel 插件将这种情况 Hack 掉了\n\n而`material-ui`并不存在这种情况,他不需要显示引入 css,这个最流行的 React 前端组件库里面只有 js 和 ts 两种代码,并不存在 css 相关的代码,为什么呢?\n\n![image-20200714180847810](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182516.png)\n\n他们用 `jss` 作为 css-in-js 的解决方案,jsx 的引入已经将 js 和 html 耦合,css-in-js 将 css 也耦合进去,此时组件便不需要显示引入 css,而是直接引用 js 即可.\n\n这不是退化到史前前端那种写内联样式的时代了吗?\n\n并不是,史前前端的内联样式是整个项目耦合的状态,当然要被抛弃到历史的垃圾堆中,后来的样式和逻辑分离,实际上是以页面为维度将 js css html 解耦的过程,如今的时代是组件化的时代了,jsx 已经将 js 和 html 框定到一个组件中,css 依然处于分离状态,这就导致了每次引用组件却还需要显示引入 css,css-in-js 正式彻底组件化的解决方案.\n\n当然,我个人目前在用 styled-components,其优点引用如下:\n\n1. 首先，styled-components 所有语法都是标准 css 语法，同时支持 scss 嵌套等常用语法，覆盖了所有 css 场景。\n2. 在样式复写场景下，styled-components 支持在任何地方注入全局 css，就像写普通 css 一样\n3. styled-components 支持自定义 className，两种方式，一种是用 babel 插件, 另一种方式是使用 styled.div.withConfig({ componentId: \"prefix-button-container\" }) 相当于添加 className=\"prefix-button-container\"\n4. className 语义化更轻松，这也是 class 起名的初衷\n5. 更适合组件库使用，直接引用 import \"module\" 即可，否则你有三条路可以走：像 antd 一样，单独引用 css，你需要给 node_modules 添加 css-loader；组件内部直接 import css 文件，如果任何业务项目没有 css-loader 就会报错；组件使用 scss 引用，所有业务项目都要配置一份 scss-loader 给 node_modules；这三种对组件库来说，都没有直接引用来的友好\n6. 当你写一套组件库，需要单独发包，又有统一样式的配置文件需求，如果这个配置文件是 js 的，所有组件直接引用，对外完全不用关注。否则，如果是 scss 配置文件，摆在面前还是三条路：每个组件单独引用 scss 文件，需要每个业务项目给 node_modules 添加 scss-loader（如果业务用了 less，还要装一份 scss 是不）；或者业务方只要使用了你的组件库，就要在入口文件引用你的 scss 文件，比如你的组件叫 button，scss 可能叫 common-css，别人听都没听过，还要查文档；或者业务方在 webpack 配置中单独引用你的 common-css，这也不科学，如果用了 3 个组件库，天天改 webpack 配置也很不方便。\n7. 当 css 设置了一半样式，另一半真的需要 js 动态传入，你不得不 css + css-in-js 混合使用，项目久了，维护的时候发现某些 css-in-js 不变了，可以固化在 css 里，css 里固定的值又因为新去求变得可变了，你又得拿出来放在 css-in-js 里，实践过就知道有多么烦心。\n\n### 2.2 js 解决方案\n\n选 Typescript ,因为巨硬大法好…\n\n---\n\n## 3. 如何快速启动一个组件库项目\n\n组件的具体实现部分当然是组件库的核心,但是在现代前端库中其他部分也必不可少,我们需要一堆工具来辅助我们开发,例如编译工具、代码检测工具、打包工具等等。\n\n### 3.1 打包工具(rollup vs webpack)\n\n市面上打包工具数不胜数,最火爆的当然是需要配置工程师专门配置的 webpack,但是在类库开发领域它有一个强大的对手就是 rollup。\n\n现代市面上主流的库基本都选择了 rollup 作为打包工具，包括 Angular React 和 Vue, 作为基础类库的打包工具 rollup 的优势如下:\n\n- Tree Shaking: 自动移除未使用的代码, 输出更小的文件\n- Scope Hoisting: 所有模块构建在一个函数内, 执行效率更高\n- Config 文件支持通过 ESM 模块格式书写\n  可以一次输出多种格式:\n- 模块规范: IIFE, AMD, CJS, UMD, ESM Development 与 production 版本: .js, .min.js\n\n虽然上面部分功能已经被 webpack 实现了,但是 rollup 明显引入得更早,而 Scope Hoisting 更是杀手锏,由于 webpack 不得不在打包代码中构建模块系统来适应 app 开发(模块系统对于单一类库用处很小),Scope Hoisting 将模块构建在一个函数内的做法更适合类库的打包.\n\n#### 3.2 代码检测\n\n由于 JavaScript 各种诡异的特性和大型前端项目的出现,代码检测工具已经是前端开发者的标配了,Douglas Crockford 最早于 2002 创造出了 JSLint,但是其无法拓展,具有极强的 Douglas Crockford 个人色彩,Anton Kovalyov 由于无法忍受 JSLint 无法拓展的行为在 2011 年发布了可拓展的 JSHint,一时之间 JSHint 成为了前端代码检测的流行解决方案.\n\n随后的 2013 年,Nicholas C. Zakas 鉴于 JSHint 拓展的灵活度不够的问题开发了全新的基于 AST 的 Lint 工具 ESLint,并随着 ES6 的流行统治了前端界,ESLint 基于 Esprima 进行 JavaScript 解析的特性极易拓展,JSHint 在很长一段时间无法支持 ES6 语法导致被 ESLint 超越.\n\n但是在 Typescript 领域 ESLint 却处于弱势地位,TSLint 的出现要比 ESLint 正式支持 Typescript 早很多,_目前 TSLint 似乎是 TS 的事实上的代码检测工具_.\n\n\u003e 注: 文章成文较早,我也没想到前阵子 TS 官方钦点了 ESLint,TSLint 失宠了,面向未来的官方标配的代码检测工具肯定是 ESLint 了,但是 TSLint 目前依然被大量使用,现在仍然可以放心使用\n\n代码检测工具是一方面,代码检测风格也需要我们做选择,市面上最流行的代码检测风格应该是 Airbnb 出品的`eslint-config-airbnb`,其最大的特点就是极其严格,没有给开发者任何选择的余地,当然在大型前端项目的开发中这种严格的代码风格是有利于协作的,但是作为一个类库的代码检测工具而言并不适合,所以我们选择了`eslint-config-standard`这种相对更为宽松的代码检测风格.\n\n#### 3.3 commit 规范\n\n以下两种 commit 哪个更严谨且易于维护?\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182508.jpeg)\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182506.png)\n\n最开始使用 commit 的时候我也经常犯下图的错误,直到看到很多明星类库的 commit 才意识到自己的错误,写好 commit message 不仅有助于他人 review, 还可以有效的输出 CHANGELOG, 对项目的管理实际至关重要.\n\n目前流行的方案是 Angular 团队的规范,其关于 head 的大致规范如下:\n\n- type: commit 的类型\n- feat: 新特性\n- fix: 修改问题\n- refactor: 代码重构\n- docs: 文档修改\n- style: 代码格式修改, 注意不是 css 修改\n- test: 测试用例修改\n- chore: 其他修改, 比如构建流程, 依赖管理.\n- scope: commit 影响的范围, 比如: route, component, utils, build…\n- subject: commit 的概述, 建议符合 50/72 formatting\n- body: commit 具体修改内容, 可以分为多行, 建议符合 50/72 formatting\n- footer: 一些备注, 通常是 BREAKING CHANGE 或修复的 bug 的链接.\n\n当然规范人们不一定会遵守,我最初知道此类规范的时候也并没有严格遵循,因为人总会偷懒,直到用`commitizen`将此规范集成到工具流中,每个 commit 就不得不遵循规范了.\n\n我具体参考了这篇文章: 优雅的提交你的 Git Commit Message\n\n#### 3.4 测试工具\n\n业务开发中由于前端需求变动频繁的特性,导致前端对测试的要求并没有后端那么高,后端业务逻辑一旦定型变动很少,比较适合测试.\n\n但是基础类库作为被反复依赖的模块和较为稳定的需求是必须做测试的,前端测试库也可谓是种类繁多了,经过比对之后我还是选择了目前最流行也是被三大框架同时选择了的 Jest 作为测试工具,其优点很明显:\n\n1. 开箱即用,内置断言、测试覆盖率工具,如果你用 MoCha 那可得自己手动配置 n 多了\n2. 快照功能,Jest 可以利用其特有的快照测试功能，通过比对 UI 代码生成的快照文件\n3. 速度优势,Jest 的测试用例是并行执行的，而且只执行发生改变的文件所对应的测试，提升了测试速度\n\n#### 3.5 其它\n\n当然以上是主要工具的选择,还有一些比如:\n\n- 代码美化工具 prettier,解放人肉美化,同时利于不同人协作的风格一致\n- 持续集成工具 travis-ci,解放人肉测试 lint,利于保证每次 push 的可靠程度\n\n#### 3.6 快速启动脚手架\n\n那么以上这么多配置难道要我们每次都自己写吗?组件的具体实现才是组件库的核心,我们为什么要花这么多时间在配置上面?\n\n我们在建立 APP 项目时通常会用到框架官方提供的脚手架,比如 React 的 create-react-app,Angular 的 Angular-Cli 等等,那么能不能有一个专门用于组件开发的快速启动的脚手架呢?\n\n有的,我最近开发了一款快速启动组件库开发的命令行工具--create-component\n\n\u003e https://github.com/xiaomuzhu/create-component\n\n利用\n\n```\n1create-component init\n```\n\n来快速启动项目,我们提供了丰富的可选配置,只要你做好技术选型后,根据提示去选择配置即可,create-component 会自动根据配置生成脚手架,其灵感就来源于 vue-cli 和 Angular-cli.\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182456.jpeg)\n\n---\n\n## 4. 如何设计一个轮播图组件 notion\n\n说了很多理论,那么实战如何呢?设计一个通用组件试试吧!\n\n### 4.1 轮播图基本原理\n\n轮播图(Carousel),在 Antd 中被称为走马灯,可能是前端开发者最常见的组件之一了,不管是在 PC 端还是在移动端我们总能见到他的身影.\n\n![image-20200714181317645](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182453.png)\n\n那么我们通常是如何使用轮播图的呢?Antd 的代码如下\n\n```html\n1\n\u003cCarousel\u003e\n  2\n  \u003cdiv\u003e\n    \u003ch3\u003e\n      1h3\u003ediv\u003e 3\n      \u003cdiv\u003e\n        \u003ch3\u003e\n          2h3\u003ediv\u003e 4\n          \u003cdiv\u003e\n            \u003ch3\u003e\n              3h3\u003ediv\u003e 5\n              \u003cdiv\u003e\u003ch3\u003e4h3\u003ediv\u003e 6 Carousel\u003e\u003c/h3\u003e\u003c/div\u003e\n            \u003c/h3\u003e\n          \u003c/div\u003e\n        \u003c/h3\u003e\n      \u003c/div\u003e\n    \u003c/h3\u003e\n  \u003c/div\u003e\u003c/Carousel\n\u003e\n```\n\n问题是我们在`Carousel`中放入了四组`div`为什么一次只显示一组呢?\n\n![image-20200714181338491](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182451.png)\n\n图中被红框圈住的为可视区域,可视区域的位置是固定的,我们只需要移动后面`div`的位置就可以做到 1 2 3 4 四个子组件轮播的效果,那么子组件 2 目前在可视区域是可以被看到的,1 3 4 应该被隐藏,这就需要我们设置 overflow 属性为 hidden 来隐藏非可视区域的子组件.\n\n\u003e 复制查看动图: https://images2015.cnblogs.com/blog/979044/201707/979044-20170710105934040-1007626405.gif\n\n因此就比较明显了,我们设计一个可视窗口组件`Frame`,然后将四个 `div`共同放入幻灯片组合组件`SlideList`中,并用`SlideItem`分别将 `div`包裹起来,实际代码应该是这样的:\n\n```html\n1\n\u003cframe\u003e\n  2\n  \u003cSlideList\u003e\n    3\n    \u003cSlideItem\u003e\n      4\n      \u003cdiv\u003e\n        \u003ch3\u003e\n          1h3\u003ediv\u003e 5 SlideItem\u003e 6\n          \u003cSlideItem\u003e\n            7\n            \u003cdiv\u003e\n              \u003ch3\u003e\n                2h3\u003ediv\u003e 8 SlideItem\u003e 9\n                \u003cSlideItem\u003e\n                  10\n                  \u003cdiv\u003e\n                    \u003ch3\u003e\n                      3h3\u003ediv\u003e 11 SlideItem\u003e 12\n                      \u003cSlideItem\u003e\n                        13\n                        \u003cdiv\u003e\n                          \u003ch3\u003e\n                            4h3\u003ediv\u003e 14 SlideItem\u003e 15 SlideList\u003e 16 Frame\u003e\n                          \u003c/h3\u003e\n                        \u003c/div\u003e\u003c/SlideItem\n                      \u003e\n                    \u003c/h3\u003e\n                  \u003c/div\u003e\u003c/SlideItem\n                \u003e\n              \u003c/h3\u003e\n            \u003c/div\u003e\u003c/SlideItem\n          \u003e\n        \u003c/h3\u003e\n      \u003c/div\u003e\u003c/SlideItem\n    \u003e\u003c/SlideList\n  \u003e\u003c/frame\n\u003e\n```\n\n我们不断利用`translateX`来改变`SlideList`的位置来达到轮播效果,如下图所示,每次轮播的触发都是通过改变`transform: translateX()`来操作的\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182448.jpeg)\n\n#### 4.2 轮播图基础实现\n\n搞清楚基本原理那么实现起来相对容易了,我们以移动端的实现为例,来实现一个基础的移动端轮播图.\n\n首先我们要确定**可视窗口**的宽度,因为我们需要这个宽度来计算出`SlideList`的长度(`SlideList`的长度通常是可视窗口的倍数,比如要放三张图片,那么`SlideList`应该为可视窗口的至少 3 倍),不然我们无法通过`translateX`来移动它.\n\n我们通过`getBoundingClientRect`来获取可视区域真实的长度,`SlideList`的长度那么为:\n\n`slideListWidth = (len + 2) * width`(len 为传入子组件的数量,width 为可视区域宽度)\n\n至于为什么要`+2`后面会提到.\n\n```typescript\n 1  /**\n 2   * 设置轮播区域尺寸\n 3   * @param x\n 4   */\n 5  private setSize(x?: number) {\n 6    const { width } = this.frameRef.current!.getBoundingClientRect()\n 7    const len = React.Children.count(this.props.children)\n 8    const total = len + 2\n 9\n10    this.setState({\n11      slideItemWidth: width,\n12      slideListWidth: total * width,\n13      total,\n14      translateX: -width * this.state.currentIndex,\n15      startPositionX: x !== undefined ? x : 0,\n16    })\n17  }\n```\n\n获取到了总长度之后如何实现轮播呢?我们需要根据用户反馈来触发轮播,在移动端通常是通过手指滑动来触发轮播,这就需要三个事件`onTouchStart` ` onTouchMove``onTouchEnd `.\n\n![image-20200714181546085](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182445.png)\n\n`onTouchStart`顾名思义是在手指触摸到屏幕时触发的事件,在这个事件里我们只需要记录下手指触摸屏幕的横轴坐标 x 即可,因为我们会通过其横向滑动的距离大小来判断是否触发轮播\n\n```typescript\n 1  /**\n 2   * 处理触摸起始时的事件\n 3   *\n 4   * @private\n 5   * @param {React.TouchEvent} e\n 6   * @memberof Carousel\n 7   */\n 8  private onTouchStart(e: React.TouchEvent) {\n 9    clearInterval(this.autoPlayTimer)\n10    // 获取起始的横轴坐标\n11    const { x } = getPosition(e)\n12    this.setSize(x)\n13    this.setState({\n14      startPositionX: x,\n15    })\n16  }\n```\n\n`onTouchMove`顾名思义是处于滑动状态下的事件,此事件在`onTouchStart`触发后,`onTouchEnd`触发前,在这个事件中我们主要做两件事,一件事是判断滑动方向,因为用户可能向左或者向右滑动,另一件事是让轮播图跟随手指移动,这是必要的用户反馈.\n\n```typescript\n 1 /**\n 2   * 当触摸滑动时处理事件\n 3   *\n 4   * @private\n 5   * @param {React.TouchEvent} e\n 6   * @memberof Carousel\n 7   */\n 8  private onTouchMove(e: React.TouchEvent) {\n 9    const { slideItemWidth, currentIndex, startPositionX } = this.state\n10    const { x } = getPosition(e)\n11\n12    const deltaX = x - startPositionX\n13    // 判断滑动方向\n14    const direction = deltaX \u003e 0 ? 'right' : 'left'\n15\n16    this.setState({\n17      direction,\n18      moveDeltaX: deltaX,\n19      // 改变translateX来达到轮播组件跟随手指移动的效果\n20      translateX: -(slideItemWidth * currentIndex) + deltaX,\n21    })\n22  }\n```\n\n`onTouchEnd`顾名思义是滑动完毕时触发的事件,在此事件中我们主要做一个件事情,就是判断是否触发轮播,我们会设置一个阈值`threshold`,当滑动距离超过这个阈值时才会触发轮播,毕竟没有阈值的话用户稍微触碰轮播图就造成轮播,误操作会造成很差的用户体验.\n\n```typescript\n 1  /**\n 2   * 滑动结束处理的事件\n 3   *\n 4   * @private\n 5   * @memberof Carousel\n 6   */\n 7  private onTouchEnd() {\n 8    this.autoPlay()\n 9    const { moveDeltaX, slideItemWidth, direction } = this.state\n10    const threshold = slideItemWidth * THRESHOLD_PERCENTAGE\n11    // 判断是否轮播\n12    const moveToNext = Math.abs(moveDeltaX) \u003e threshold\n13\n14    if (moveToNext) {\n15        // 如果轮播触发那么进行轮播操作\n16      this.handleSwipe(direction!)\n17    } else {\n18        // 轮播不触发,那么轮播图回到原位\n19      this.handleMisoperation()\n20    }\n21  }\n```\n\n#### 4.3 轮播图的动画效果\n\n我们常见的轮播图肯定不是生硬的切换,一般在轮播中会有一个渐变或者缓动的动画,这就需要我们加入动画效果.\n\n我们制作动画通常有两个选择,一个是用 css3 自带的动画效果,另一个是用浏览器提供的 requestAnimationFrame API\n\n孰优孰劣?css3 简单易用上手快,兼容性好,`requestAnimationFrame` 灵活性更高,能实现 css3 实现不了的动画,比如众多缓动动画 css3 都束手无策,因此我们毫无疑问地选择了`requestAnimationFrame`.\n\n\u003e 双方对比请看张鑫旭大神的 CSS3 动画那么强，requestAnimationFrame 还有毛线用？\n\n想用`requestAnimationFrame`实现缓动效果就需要特定的缓动函数,下面就是典型的缓动函数\n\n```typescript\n1type tweenFunction = (t: number, b: number, _c: number, d: number) =\u003e number\n2const easeInOutQuad: tweenFunction = (t, b, _c, d) =\u003e {\n3    const c = _c - b;\n4    if ((t /= d / 2) \u003c 1) {\n5      return c / 2 * t * t + b;\n6    } else {\n7      return -c / 2 * ((--t) * (t - 2) - 1) + b;\n8    }\n9}\n```\n\n缓动函数接收四个参数,分别是:\n\n- t: 时间\n- b:初始位置\n- \\_c:结束的位置\n- d:速度\n\n通过这个函数我们能算出每一帧轮播图所在的位置, 如下:\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182438.jpeg)\n\n在获取每一帧对应的位置后,我们需要用`requestAnimationFrame`不断递归调用依次移动位置,我们不断调用`animation`函数是其触发函数体内的`this.setState({ translateX: tweenQueue[0], })`来达到移动轮播图位置的目的,此时将这数组内的 30 个位置依次快速执行就是一个缓动动画效果.\n\n```typescript\n 1  /**\n 2   * 递归调用,根据轨迹运动\n 3   *\n 4   * @private\n 5   * @param {number[]} tweenQueue\n 6   * @param {number} newIndex\n 7   * @memberof Carousel\n 8   */\n 9  private animation(tweenQueue: number[], newIndex: number) {\n10    if (tweenQueue.length \u003c 1) {\n11      this.handleOperationEnd(newIndex)\n12      return\n13    }\n14    this.setState({\n15      translateX: tweenQueue[0],\n16    })\n17    tweenQueue.shift()\n18    this.rafId = requestAnimationFrame(() =\u003e this.animation(tweenQueue, newIndex))\n19  }\n```\n\n但是我们发现了一个问题,当我们移动轮播图到最后的时候,动画出现了问题,_当我们向左滑动最后一个轮播图`div4`时,这种情况下应该是图片向左滑动,然后第一张轮播图`div1`进入可视区域,但是反常的是图片快速向右滑动`div1`出现在可是区域…_\n\n因为我们此时将位置 4 设置为了位置 1,这样才能达到不断循环的目的,但是也造成了这个副作用,图片行为与用户行为产生了相悖的情况(用户向左划动,图片向右走).\n\n目前业界的普遍做法是将图片首尾相连,例如图片 1 前面连接一个图片 4,图片 4 后跟着一个图片 1,这就是为什么之前计算长度时要`+2`\n\n`slideListWidth = (len + 2) * width`(len 为传入子组件的数量,width 为可视区域宽度)\n\n当我们移动图片 4 时就不会出现上述向左滑图片却向右滑的情况,因为真实情况是:\n\n`图片4 -- 滑动为 -\u003e 伪图片1` 也就是位置 5 变成了位置 6\n\n当动画结束之后,我们迅速把`伪图片1`的位置设置为`真图片1`,这其实是个障眼法,也就是说动画执行过程中实际上是`图片4`到`伪图片1`的过程,当结束后我们偷偷把`伪图片1`换成`真图片1`,因为两个图一模一样,所以这个转换的过程用户根本看不出来…\n\n如此一来我们就可以实现无缝切换的轮播图了\n\n#### 4.4 改进方向\n\n我们实现了轮播图的基本功能,但是其通用性依然存在缺陷:\n\n1. 提示点的自定义: 我的实现是一个小点,而 antd 是用的条,这个地方完全可以将 dom 结构的决定权交给开发者.\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182059.png)\n\n1. 方向的自定义: 本轮播图只有水平方向的实现,其实也可以有纵向轮播\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182106.png)\n\n1. 多张轮播:除了单张轮播也可以多张轮播\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182431.png)\n\n以上都是可以对轮播图进行拓展的方向,相关的还有性能优化方面\n\n我们的具体代码中有一个相关实现,我们的轮播图其实是有自动轮播功能的,但是很多时候页面并不在用户的可视页面中,我们可以根据是否页面被隐藏来取消定时器终止自动播放.\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/frontendcomponent/20210410182425.png)\n\ngithub 项目地址\n\n\u003e https://github.com/xiaomuzhu/rc-carousel\n","lastmodified":"2023-05-09T16:33:58.30736679Z","tags":[]},"/%E7%BD%91%E7%BB%9C":{"title":"综合","content":"\n# 综合\n\n## 1.当你在浏览器中输入 Google.com 并且按下回车之后发生了什么？\n\n1. 首先会对 URL 进行解析，分析所需要使用的传输协议和请求的资源的路径。如果输入的 URL 中的协议或者主机名不**合法**，将会把地址栏中输入的内容传递给搜索引擎。如果没有问题，浏览器会检查 URL 中是否出现了非法字符，如果存在非法字符，则对非法字符进行转义后再进行下一过程。\n2. 浏览器会判断所请求的资源是否在**缓存**里，如果请求的资源在缓存里并且没有失效，那么就直接使用，否则向服务器发起新的请求。\n3. 下一步我们首先需要获取的是输入的 URL 中的域名的 **IP 地址**，首先会判断本地是否有该域名的 IP 地址的**缓存**，如果有则使用，如果没有则向本地 **DNS** 服务器发起请求。本地 DNS 服务器也会先检查是否存在缓存，如果没有就会先向根域名服务器发起请求，获得负责的顶级域名服务器的地址后，再向顶级域名服务器请求，然后获得负责的权威域名服务器的地址后，再向权威域名服务器发起请求，最终获得域名的 IP 地址后，本地 DNS 服务器再将这个 IP 地址返回给请求的用户。用户向本地 DNS 服务器发起请求属于递归请求，本地 DNS 服务器向各级域名服务器发起请求属于迭代请求。\n4. 当浏览器得到 IP 地址后，数据传输还需要知道目的主机 MAC 地址，因为应用层下发数据给传输层，TCP 协议会指定源端口号和目的端口号，然后下发给网络层。网络层会将本机地址作为源地址，获取的 IP 地址作为目的地址。然后将下发给数据链路层，数据链路层的发送需要加入通信双方的 MAC 地址，我们本机的 MAC 地址作为源 MAC 地址，目的 MAC 地址需要分情况处理，通过将 IP 地址与我们本机的子网掩码相与，我们可以判断我们是否与请求主机在同一个子网里，如果在同一个子网里，我们可以使用 **APR** 协议获取到目的主机的 MAC 地址，如果我们不在一个子网里，那么我们的请求应该转发给我们的网关，由它代为转发，此时同样可以通过 ARP 协议来获取网关的 MAC 地址，此时目的主机的 MAC 地址应该为网关的地址。\n5. 下面是 TCP 建立连接的三次握手的过程，首先客户端向服务器发送一个 SYN 连接请求报文段和一个随机序号，服务端接收到请求后向服务器端发送一个 SYN ACK报文段，确认连接请求，并且也向客户端发送一个随机序号。客户端接收服务器的确认应答后，进入连接建立的状态，同时向服务器也发送一个 ACK 确认报文段，服务器端接收到确认后，也进入连接建立状态，此时双方的连接就建立起来了。\n6. 如果使用的是 **HTTPS** 协议，在通信前还存在 TLS 的一个四次握手的过程。首先由客户端向服务器端发送使用的协议的版本号、一个随机数和可以使用的加密方法。服务器端收到后，确认加密的方法，也向客户端发送一个随机数和自己的数字证书。客户端收到后，首先检查数字证书是否有效，如果有效，则再生成一个随机数，并使用证书中的公钥对随机数加密，然后发送给服务器端，并且还会提供一个前面所有内容的 hash 值供服务器端检验。服务器端接收后，使用自己的私钥对数据解密，同时向客户端发送一个前面所有内容的 hash 值供客户端检验。这个时候双方都有了三个随机数，按照之前所约定的加密方法，使用这三个随机数生成一把秘钥，以后双方通信前，就使用这个秘钥对数据进行加密后再传输。\n7. 当页面请求发送到服务器端后，服务器端会返回一个 html 文件作为响应，浏览器接收到响应后，开始对 html 文件进行解析，开始页面的渲染过程。\n8. 浏览器首先会根据 html 文件构建 DOM 树，根据解析到的 css 文件构建 CSSOM 树，如果遇到 script 标签，则判端是否含有 defer 或者 async 属性，要不然 script 的加载和执行会造成页面的渲染的阻塞。当 DOM 树和 CSSOM 树建立好后，根据它们来构建渲染树。渲染树构建好后，会根据渲染树来进行布局。布局完成后，最后使用浏览器的 UI 接口对页面进行绘制。这个时候整个页面就显示出来了。\n9. 最后一步是 TCP 断开连接的四次挥手过程。\n\n## 2.服务端挂了，客户端的 TCP 连接还在吗？\n\n[写的很好](https://blog.csdn.net/bjweimengshu/article/details/126844319)\n\n1. 服务端挂掉 指的是「**服务端进程崩溃**」，服务端的进程在发生崩溃的时候，内核会发送 FIN 报文，与客户端进行四次挥手。\n2. 服务端挂掉 指的是「**服务端主机宕机**」，那么是不会发生四次挥手的，具体后续会发生什么还要看客户端会不会发送数据。\n    - 如果客户端会发送数据，由于服务端已经不存在，客户端的数据报文会超时重传，当重传总间隔时长达到一定阈值（内核会根据 tcp_retries2 设置的值计算出一个阈值）后，会断开 TCP 连接；\n    - 如果客户端一直不会发送数据，再看客户端有没有开启 **TCP keepalive** 机制？\n        - 如果有开启，客户端在一段时间没有进行数据交互时，会触发 TCP keepalive机制，探测对方是否存在，如果探测到对方已经消亡，则会断开自身的 TCP 连接；\n        - 如果没有开启，客户端的 TCP 连接会一直存在，并且一直保持在ESTABLISHED 状态。\n\n# HTTP\n\n## HTTP 常见的状态码，有哪些？\n\n### *1xx*\n\n`1xx` 类状态码属于**提示信息**，是协议处理中的一种中间状态，实际用到的比较少。\n\n### *2xx*\n\n`2xx` 类状态码表示服务器**成功**处理了客户端的请求，也是我们最愿意看到的状态。  \n「**200 OK**」是最常见的成功状态码，表示一切正常。如果是非 `HEAD` 请求，服务器返回的响应头都会有 body 数据。  \n「**204 No Content**」也是常见的成功状态码，与 200 OK 基本相同，但响应头没有 body 数据。  \n「**206 Partial Content**」是应用于 HTTP 分块下载或断电续传，表示响应返回的 body 数据并不是资源的全部，而是其中的一部分，也是服务器处理成功的状态。\n\n### *3xx*\n\n`3xx` 类状态码表示客户端请求的资源发送了变动，需要客户端用新的 URL 重新发送请求获取资源，也就是**重定向**。  \n「**301 Moved Permanently**」表示永久重定向，说明请求的资源已经不存在了，需改用新的 URL 再次访问。  \n「**302 Moved Permanently**」表示临时重定向，说明请求的资源还在，但暂时需要用另一个 URL 来访问。  \n301 和 302 都会在响应头里使用字段 `Location`，指明后续要跳转的URL，浏览器会自动重定向新的URL。  \n「**304 Not Modified**」不具有跳转的含义，表示资源未修改，重定向已存在的缓冲文件，也称缓存重定向，用于缓存控制。\n\n### *4xx*\n\n`4xx` 类状态码表示客户端发送的**报文有误**，服务器无法处理，也就是错误码的含义。  \n「**400 Bad Request**」表示客户端请求的报文有错误，但只是个笼统的错误。  \n「**403 Forbidden**」表示服务器禁止访问资源，并不是客户端的请求出错。  \n「**404 Not Found**」表示请求的资源在服务器上不存在或未找到，所以无法提供给客户端。\n\n### *5xx*\n\n`5xx` 类状态码表示客户端请求报文正确，但是**服务器处理时内部发生了错误**，属于服务器端的错误码。  \n「**500 Internal Server Error**」与 400 类型，是个笼统通用的错误码，服务器发生了什么错误，我们并不知道。  \n「**501 Not Implemented**」表示客户端请求的功能还不支持，类似“即将开业，敬请期待”的意思。  \n「**502 Bad Gateway**」通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，访问后端服务器发生了错误。  \n「**503 Service Unavailable**」表示服务器当前很忙，暂时无法响应服务器，类似“网络服务正忙，请稍后重试”的意思。\n\n## http 常见字段有哪些？\n\n*Host*  \n客户端发送请求时，用来指定服务器的域名。\n\n```\nHost: www.A.com\n```\n\n有了 `Host` 字段，就可以将请求发往「同一台」服务器上的不同网站。  \n*Content-Length 字段*  \n服务器在返回数据时，会有 `Content-Length` 字段，表明本次回应的数据长度。\n\n```\nContent-Length: 1000\n```\n\n如上面则是告诉浏览器，本次服务器回应的数据长度是 1000 个字节，后面的字节就属于下一个回应了。  \n*Connection 字段*  \n`Connection` 字段最常用于客户端要求服务器使用 TCP 持久连接，以便其他请求复用。  \nHTTP/1.1 版本的默认连接都是持久连接，但为了兼容老版本的 HTTP，需要指定 `Connection` 首部字段的值为 `Keep-Alive`。\n\n```\nConnection: keep-alive\n```\n\n一个可以复用的 TCP 连接就建立了，直到客户端或服务器主动关闭连接。但是，这不是标准字段。  \n*Content-Type 字段*  \n`Content-Type` 字段用于服务器回应时，告诉客户端，本次数据是什么格式。\n\n```\nContent-Type: text/html; charset=utf-8\n```\n\n上面的类型表明，发送的是网页，而且编码是UTF-8。  \n客户端请求的时候，可以使用 `Accept` 字段声明自己可以接受哪些数据格式。\n\n```\nAccept: */*\n```\n\n上面代码中，客户端声明自己可以接受任何格式的数据。  \n*Content-Encoding 字段*  \n`Content-Encoding` 字段说明数据的压缩方法。表示服务器返回的数据使用了什么压缩格式\n\n```\nContent-Encoding: gzip\n```\n\n上面表示服务器返回的数据采用了 gzip 方式压缩，告知客户端需要用此方式解压。  \n客户端在请求时，用 `Accept-Encoding` 字段说明自己可以接受哪些压缩方法。\n\n```\nAccept-Encoding: gzip, deflate\n```\n\n## GET 与 POST\n\n### 说一下 GET 和 POST 的区别？\n\n`Get` 方法的含义是请求**从服务器获取资源**，这个资源可以是静态的文本、页面、图片视频等。  \n比如，你打开我的文章，浏览器就会发送 GET 请求给服务器，服务器就会返回文章的所有文字及资源。  \n而`POST` 方法则是相反操作，它向 `URI` 指定的资源提交数据，数据就放在报文的 body 里。  \n比如，你在我文章底部，敲入了留言后点击「提交」（**暗示你们留言**），浏览器就会执行一次 POST 请求，把你的留言文字放进了报文 body 里，然后拼接好 POST 请求头，通过 TCP 协议发送给服务器。\n\n### GET 和 POST 方法都是安全和幂等的吗？\n\n先说明下安全和幂等的概念：\n\n- 在 HTTP 协议里，所谓的「安全」是指请求方法不会「破坏」服务器上的资源。\n- 所谓的「幂等」，意思是多次执行相同的操作，结果都是「相同」的。  \n那么很明显 **GET 方法就是安全且幂等的**，因为它是「只读」操作，无论操作多少次，服务器上的数据都是安全的，且每次的结果都是相同的。  \n**POST** 因为是「新增或提交数据」的操作，会修改服务器上的资源，所以是**不安全**的，且多次提交数据就会创建多个资源，所以**不是幂等**的。\n\n## HTTP/1.1的优点缺点\n\nHTTP 最凸出的优点是「简单、灵活和易于扩展、应用广泛和跨平台」。  \n*1. 简单*  \nHTTP 基本的报文格式就是 `header + body`，头部信息也是 `key-value` 简单文本的形式，**易于理解**，降低了学习和使用的门槛。  \n*2. 灵活和易于扩展*  \nHTTP协议里的各类请求方法、URI/URL、状态码、头字段等每个组成要求都没有被固定死，都允许开发人员**自定义和扩充**。  \n同时 HTTP 由于是工作在应用层（ `OSI` 第七层），则它**下层可以随意变化**。  \nHTTPS 也就是在 HTTP 与 TCP 层之间增加了 SSL/TLS 安全传输层，HTTP/3 甚至把 TCPP 层换成了基于 UDP 的 QUIC。  \n*3. 应用广泛和跨平台*  \n互联网发展至今，HTTP 的应用范围非常的广泛，从台式机的浏览器到手机上的各种 APP，从看新闻、刷贴吧到购物、理财、吃鸡，HTTP 的应用**片地开花**，同时天然具有**跨平台**的优越性。\n\n**缺点**  \nHTTP 协议里有优缺点一体的**双刃剑**，分别是「无状态、明文传输」，同时还有一大缺点「不安全」。  \n*1. 无状态双刃剑*  \n无状态的**好处**，因为服务器不会去记忆 HTTP 的状态，所以不需要额外的资源来记录状态信息，这能减轻服务器的负担，能够把更多的 CPU 和内存用来对外提供服务。  \n无状态的**坏处**，既然服务器没有记忆能力，它在完成有关联性的操作时会非常麻烦。  \n例如登录-\u003e添加购物车-\u003e下单-\u003e结算-\u003e支付，这系列操作都要知道用户的身份才行。但服务器不知道这些请求是有关联的，每次都要问一遍身份信息。  \n这样每操作一次，都要验证信息，这样的购物体验还能愉快吗？别问，问就是**酸爽**！  \n对于无状态的问题，解法方案有很多种，其中比较简单的方式用 **Cookie** 技术。  \n`Cookie` 通过在请求和响应报文中写入 Cookie 信息来控制客户端的状态。  \n相当于，**在客户端第一次请求后，服务器会下发一个装有客户信息的「小贴纸」，后续客户端请求服务器的时候，带上「小贴纸」，服务器就能认得了了**，  \n*2. 明文传输双刃剑*  \n明文意味着在传输过程中的信息，是可方便阅读的，通过浏览器的 F12 控制台或 Wireshark 抓包都可以直接肉眼查看，为我们调试工作带了极大的便利性。  \n但是这正是这样，HTTP 的所有信息都暴露在了光天化日下，相当于**信息裸奔**。在传输的漫长的过程中，信息的内容都毫无隐私可言，很容易就能被窃取，如果里面有你的账号密码信息，那**你号没了**。  \n*3. 不安全*  \nHTTP 比较严重的缺点就是不安全：\n\n- 通信使用明文（不加密），内容可能会被窃听。比如，**账号信息容易泄漏，那你号没了。**\n- 不验证通信方的身份，因此有可能遭遇伪装。比如，**访问假的淘宝、拼多多，那你钱没了。**\n- 无法证明报文的完整性，所以有可能已遭篡改。比如，**网页上植入垃圾广告，视觉污染，眼没了。**  \nHTTP 的安全问题，可以用 HTTPS 的方式解决，也就是通过引入 SSL/TLS 层，使得在安全上达到了极致。\n\n## HTTP/1.1的性能\n\nHTTP 协议是基于 **TCP/IP**，并且使用了「**请求 - 应答**」的通信模式，所以性能的关键就在这**两点**里。  \n*1. 长连接*  \n早期 HTTP/1.0 性能上的一个很大的问题，那就是每发起一个请求，都要新建一次 TCP 连接（三次握手），而且是串行请求，做了无畏的 TCP 连接建立和断开，增加了通信开销。  \n为了解决上述 TCP 连接问题，HTTP/1.1 提出了**长连接**的通信方式，也叫持久连接。这种方式的好处在于减少了 TCP 连接的重复建立和断开所造成的额外开销，减轻了服务器端的负载。  \n持久连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。  \n![短连接与长连接](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/http/20210405161504.webp)  \n*2. 管道网络传输*  \nHTTP/1.1 采用了长连接的方式，这使得管道（pipeline）网络传输成为了可能。  \n即可在同一个 TCP 连接里面，客户端可以发起多个请求，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以**减少整体的响应时间。**  \n举例来说，客户端需要请求两个资源。以前的做法是，在同一个TCP连接里面，先发送 A 请求，然后等待服务器做出回应，收到后再发出 B 请求。管道机制则是允许浏览器同时发出 A 请求和 B 请求。  \n![管道网络传输](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/http/20210405161505.webp)  \n但是服务器还是按照**顺序**，先回应 A 请求，完成后再回应 B 请求。要是前面的回应特别慢，后面就会有许多请求排队等着。这称为「队头堵塞」。  \n*3. 队头阻塞*  \n「请求 - 应答」的模式加剧了 HTTP 的性能问题。  \n因为当顺序发送的请求序列中的一个请求因为某种原因被阻塞时，在后面排队的所有请求也一同被阻塞了，会招致客户端一直请求不到数据，这也就是「**队头阻塞**」。**好比上班的路上塞车**。\n\n![队头阻塞](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/http/20210405161506.webp)队头阻塞  \n总之 HTTP/1.1 的性能一般般，后续的 HTTP/2 和 HTTP/3 就是在优化 HTTP 的性能。\n\n## HTTPS特点\n\n- **混合加密**的方式实现信息的**机密性**，解决了窃听的风险。\n- **摘要算法**的方式来实现**完整性**，它能够为数据生成独一无二的「指纹」，指纹用于校验数据的完整性，解决了篡改的风险。\n- 将服务器公钥放入到**数字证书**中，解决了冒充的风险。  \n*1. 混合加密*  \n通过**混合加密**的方式可以保证信息的**机密性**，解决了窃听的风险。  \n![混合加密](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/http/20210405161503.webp)  \nHTTPS 采用的是**对称加密**和**非对称加密**结合的「混合加密」方式：\n- 在通信建立前采用**非对称加密**的方式交换「会话秘钥」，后续就不再使用非对称加密。\n- 在通信过程中全部使用**对称加密**的「会话秘钥」的方式加密明文数据。  \n采用「混合加密」的方式的原因：\n- **对称加密**只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。\n- **非对称加密**使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。  \n*2. 摘要算法*  \n**摘要算法**用来实现**完整性**，能够为数据生成独一无二的「指纹」，用于校验数据的完整性，解决了篡改的风险。  \n![校验完整性](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/http/20210405161508.webp)  \n客户端在发送明文之前会通过摘要算法算出明文的「指纹」，发送的时候把「指纹 + 明文」一同加密成密文后，发送给服务器，服务器解密后，用相同的摘要算法算出发送过来的明文，通过比较客户端携带的「指纹」和当前算出的「指纹」做比较，若「指纹」相同，说明数据是完整的。  \n*3. 数字证书*  \n客户端先向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用自己的私钥解密。  \n这就存在些问题，如何保证公钥不被篡改和信任度？  \n所以这里就需要借助第三方权威机构 `CA` （数字证书认证机构），将**服务器公钥放在数字证书**（由数字证书认证机构颁发）中，只要证书是可信的，公钥就是可信的。  \n![数子证书工作流程](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/http/20210405161509.webp)  \n通过数字证书的方式保证服务器公钥的身份，解决冒充的风险。\n\n## HTTPS建立连接过程\n\nSSL/TLS 协议基本流程：\n\n- 客户端向服务器索要并验证服务器的公钥。\n- 双方协商生产「会话秘钥」。\n- 双方采用「会话秘钥」进行加密通信。  \n前两步也就是 SSL/TLS 的建立过程，也就是握手阶段。  \nSSL/TLS 的「握手阶段」涉及**四次**通信，可见下图：  \n![HTTPS 连接建立过程](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/http/20210405161510.webp)  \nSSL/TLS 协议建立的详细流程：  \n*1. ClientHello*  \n首先，由客户端向服务器发起加密通信请求，也就是 `ClientHello` 请求。  \n在这一步，客户端主要向服务器发送以下信息：  \n（1）客户端支持的 SSL/TLS 协议版本，如 TLS 1.2 版本。  \n（2）客户端生产的随机数（`Client Random`），后面用于生产「会话秘钥」。  \n（3）客户端支持的密码套件列表，如 RSA 加密算法。  \n*2. SeverHello*  \n服务器收到客户端请求后，向客户端发出响应，也就是 `SeverHello`。服务器回应的内容有如下内容：  \n（1）确认 SSL/ TLS 协议版本，如果浏览器不支持，则关闭加密通信。  \n（2）服务器生产的随机数（`Server Random`），后面用于生产「会话秘钥」。  \n（3）确认的密码套件列表，如 RSA 加密算法。  \n（4）服务器的数字证书。  \n*3.客户端回应*  \n客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。  \n如果证书没有问题，客户端会从数字证书中取出服务器的公钥，然后使用它加密报文，向服务器发送如下信息：  \n（1）一个随机数（`pre-master key`）。该随机数会被服务器公钥加密。  \n（2）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。  \n（3）客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供服务端校验。  \n上面第一项的随机数是整个握手阶段的第三个随机数，这样服务器和客户端就同时有三个随机数，接着就用双方协商的加密算法，**各自生成**本次通信的「会话秘钥」。  \n*4. 服务器的最后回应*  \n服务器收到客户端的第三个随机数（`pre-master key`）之后，通过协商的加密算法，计算出本次通信的「会话秘钥」。然后，向客户端发生最后的信息：  \n（1）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。  \n（2）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供客户端校验。  \n至此，整个 SSL/TLS 的握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的 HTTP 协议，只不过用「会话秘钥」加密内容。\n\n## HTTP/1.1 相比 HTTP/1.0 的提高\n\nHTTP/1.1 相比 HTTP/1.0 性能上的改进：\n\n- 使用 TCP 长连接的方式改善了 HTTP/1.0 短连接造成的性能开销。\n- 支持 管道（pipeline）网络传输，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。  \n但 HTTP/1.1 还是有性能瓶颈：\n- 请求 / 响应头部（Header）未经压缩就发送，首部信息越多延迟越大。只能压缩 `Body` 的部分；\n- 发送冗长的首部。每次互相发送相同的首部造成的浪费较多；\n- 服务器是按请求的顺序响应的，如果服务器响应慢，会招致客户端一直请求不到数据，也就是队头阻塞；\n- 没有请求优先级控制；\n- 请求只能从客户端开始，服务器只能被动响应。\n\n## HTTP/2\n\nHTTP/2 协议是基于 HTTPS 的，所以 HTTP/2 的安全性也是有保障的。  \n那 HTTP/2 相比 HTTP/1.1 性能上的改进：  \n*1. 头部压缩*  \nHTTP/2 会**压缩头**（Header）如果你同时发出多个请求，他们的头是一样的或是相似的，那么，协议会帮你**消除重复的分**。  \n这就是所谓的 `HPACK` 算法：在客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就**提高速度**了。  \n*2. 二进制格式*  \nHTTP/2 不再像 HTTP/1.1 里的纯文本形式的报文，而是全面采用了**二进制格式。**  \n头信息和数据体都是二进制，并且统称为帧（frame）：**头信息帧和数据帧**。  \n![报文区别](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/http/20210405161511.webp)报文区别  \n这样虽然对人不友好，但是对计算机非常友好，因为计算机只懂二进制，那么收到报文后，无需再将明文的报文转成二进制，而是直接解析二进制报文，这**增加了数据传输的效率**。  \n*3. 数据流*  \nHTTP/2 的数据包不是按顺序发送的，同一个连接里面连续的数据包，可能属于不同的回应。因此，必须要对数据包做标记，指出它属于哪个回应。  \n每个请求或回应的所有数据包，称为一个数据流（`Stream`）。  \n每个数据流都标记着一个独一无二的编号，其中规定**客户端发出的数据流编号为奇数， 服务器发出的数据流编号为偶数**\n\n\u003e客户端还可以**指定数据流的优先级**。优先级高的请求，服务器就先响应该请求。\n\n![HTT/1 ~ HTTP/2](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/http/20210405161512.webp)  \n*4. 多路复用*  \nHTTP/2 是可以在**一个连接中并发多个请求或回应，而不用按照顺序一一对应**。  \n移除了 HTTP/1.1 中的串行请求，不需要排队等待，也就不会再出现「队头阻塞」问题，**降低了延迟，大幅度提高了连接的利用率**。  \n举例来说，在一个 TCP 连接里，服务器收到了客户端 A 和 B 的两个请求，如果发现 A 处理过程非常耗时，于是就回应 A 请求已经处理好的部分，接着回应 B 请求，完成后，再回应 A 请求剩下的部分。\n\n![多路复用](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/http/20210405161513.webp)多路复用  \n*5. 服务器推送*  \nHTTP/2 还在一定程度上改善了传统的「请求 - 应答」工作模式，服务不再是被动地响应，也可以**主动**向客户端发送消息。  \n举例来说，在浏览器刚请求 HTML 的时候，就提前把可能会用到的 JS、CSS 文件等静态资源主动发给客户端，**减少延时的等待**，也就是服务器推送（Server Push，也叫 Cache Push）。\n\n## HTTP/2 有哪些缺陷？HTTP/3 做了哪些优化？\n\nHTTP/2 主要的问题在于：多个 HTTP 请求在复用一个 TCP 连接，下层的 TCP 协议是不知道有多少个 HTTP 请求的。  \n所以一旦发生了丢包现象，就会触发 TCP 的重传机制，这样在一个 TCP 连接中的**所有的 HTTP 请求都必须等待这个丢了的包被重传回来**。\n\n- HTTP/1.1 中的管道（ pipeline）传输中如果有一个请求阻塞了，那么队列后请求也统统被阻塞住了\n- HTTP/2 多请求复用一个TCP连接，一旦发生丢包，就会阻塞住所有的 HTTP 请求。  \n这都是基于 TCP 传输层的问题，所以 **HTTP/3 把 HTTP 下层的 TCP 协议改成了 UDP！**  \n![HTTP/1 ~ HTTP/3](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/http/20210405161514.webp)  \nUDP 发生是不管顺序，也不管丢包的，所以不会出现 HTTP/1.1 的队头阻塞 和 HTTP/2 的一个丢包全部重传问题。  \n大家都知道 UDP 是不可靠传输的，但基于 UDP 的 **QUIC 协议** 可以实现类似 TCP 的可靠性传输。\n- QUIC 有自己的一套机制可以保证传输的可靠性的。当某个流发生丢包时，只会阻塞这个流，**其他流不会受到影响**。\n- TL3 升级成了最新的 `1.3` 版本，头部压缩算法也升级成了 `QPack`。\n- HTTPS 要建立一个连接，要花费 6 次交互，先是建立三次握手，然后是 `TLS/1.3` 的三次握手。QUIC 直接把以往的 TCP 和 `TLS/1.3` 的 6 次交互**合并成了 3 次，减少了交互次数**。  \n![TCP HTTPS（TLS/1.3） 和 QUIC HTTPS](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/http/20210405161515.webp)  \n所以， QUIC 是一个在 UDP 之上的**伪** TCP + TLS + HTTP/2 的多路复用的协议。  \nQUIC 是新协议，对于很多网络设备，根本不知道什么是 QUIC，只会当做 UDP，这样会出现新的问题。所以 HTTP/3 现在普及的进度非常的缓慢，不知道未来 UDP 是否能够逆袭 TCP。\n","lastmodified":"2023-05-09T16:33:58.30736679Z","tags":[]},"/%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%AA%92%E4%BD%93":{"title":"网络与媒体","content":"\n[[趣谈网络协议]]  \n[[CDN]]  \n[[HTTP]]  \n[[Image]]  \n[[RegExp]]  \n[[SSH]]\n","lastmodified":"2023-05-09T16:33:58.30736679Z","tags":[]},"/%E8%B6%A3%E8%B0%88%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE":{"title":"趣谈网络协议","content":"\n# 趣谈网络协议\n\n# 00 我是如何创作“趣谈网络协议”专栏的\n\n我用将近半年的时间在“极客时间”写了一个专栏“趣谈网络协议”。对于我自己来讲，这真的是个非常特殊而又难忘的经历。\n\n很多人都很好奇，这个专栏究竟是怎么一步步创作出来的，每一篇文章是怎么写出来的？自己录音频又是什么样的感受？写完整个专栏之后，我终于有时间回顾、整理一下这半年的所感所想。对我来说，这是一次难得的体验，也是一次与“极客时间”的深度沟通。\n\n## 专栏是写给谁的？\n\n和极客时间的编辑谈妥主题之后，他们首先要求我基于约定的主题，写一个36节至50节的大纲，之后会以每周三篇的频率，文字加音频的方式发布。每篇文章的体量要求在3000字左右，录成音频大约就是10分钟。\n\n我本来觉得写这么一个专栏根本就不是个事儿。毕竟咱也是在IT圈摸爬滚打了许多年的“老司机”，干货积累得也不少。只要是熟悉的领域，不用准备，聊个把小时都没啥问题。况且我原来还写过书、写过博客、写过公众号。所以，我对自己文字方面的能力很有自信。\n\n至于语言方面，咱常年出入各大技术论坛，什么场子没趟过。一个两天的线下培训，咱都能扛过来。每篇10分钟，总共36篇，那不才是6个小时嘛，肯定没问题。\n\n但是，写了之后我发现，**自己会是一回事儿，能讲给别人是另一回事儿**，而能讲给“看不见的陌生人”听，是这世上最难的事儿。\n\n我知道，很多技术人都有这样一个“毛病”，就是觉得掌握技术本身是最重要的，其他什么产品、市场、销售，都没技术含量。这种思维导致很多技术比较牛的人会以自我为中心，仅站在自己的角度思考问题。所以，**常常是自己讲得很爽，完全不管听的人是不是真的接受了**。写专栏的时候，这绝对是个大忌。\n\n除此之外，这种思维对职业发展的影响也是很大的。单打独斗，一个人搞定一个软件的时代已经过去了。学会和别人合作，才是现代社会的生存法则，而良好的合作源于沟通。\n\n但沟通不易，高质量的沟通更难。面对的人越多，沟通的难度就越大。因为每个人的背景、知识、基础都不同，想听的内容肯定更是千差万别。况且不是每个人都能准确地表达出自己的需求，加之需求的表达、转述都会因表达方式和传递媒介而发生变形，这样一来，接收信息的一方自然很难把握真实的需求。\n\n写专栏的时候，“极客时间”的编辑不断地告诉我，我的受众只有一个人，就是“你”。我心想，这个简单啊，因为面对的人最少嘛！可是，事实上证明，我又“错”了。\n\n这个抽象的“你”，看起来只有一个，其实却是看不到、摸不着的许许多多的人。所以，这个其实是最难的。协议专栏上线10天，就有10000多人订阅，而订阅专栏的用户里，只有少数人会留言。所以，对于很多读者的真实情况，我都无从得知，你可能每天都听但是没有留言的习惯，也可能买了之后觉得我讲得不好，骂一句“这钱白花了”，然后再也不听。\n\n所以，**如何把控内容，写给广大未知受众，是我写这个专栏面临的最大挑战**。而这里面，文章的深度、广度，音频的语调、语气，每一个细节都非常重要。\n\n## 专栏文章是怎么写的？\n\n经过大纲和前几篇文稿的打磨，我对“极客时间”和专栏创作也有了更深的了解。我私下和很多人交流过一个问题，那就是，咱们平时聊一个话题的时候，有很多话可以说。但是真正去写一篇文章的时候，好像又没有什么可讲的，尤其是那些看起来很基础的内容。\n\n我在写专栏的过程中，仔细思考过这样一个问题：很多人对某一领域或者行业研究得很深入，也有自己长期的实践，但是**有多少人可以从感性认识上升到理性认知的高度呢？**\n\n现在技术变化这么快，我们每个人的精力都是有限的，不少人学习新知识的方式就是看看书，看看博客、技术文章，或者听同事讲一下，了解个大概就觉得可以直接上手去做了。我也是这样的。可是一旦到写专栏的时候，**基础掌握不扎实的问题一下子全都“暴露”出来了。**\n\n落到文字上的东西一定要是严谨的。所以，在写到很多细节的时候，我查了大量的资料，找到权威的书籍、官方文档、RFC里面的具体描述，有时候我甚至要做个实验，或者打开代码再看一下，才放心下笔。\n\n尽管我对自己写文章有很多“完美倾向”的要求，但是这其实依旧是站在我自己的角度去看的。读者究竟想要看什么内容呢？\n\n太深入了，看不懂；太浅显了，也不行。太长了，负担太重；太短了，没有干货；同时，每篇文字还要自成一体，所有文章要是一个完整的知识体系。我发现，原来我不仅是对知识的了解没那么全面、具体，对用户阅读和倾听场景也没有过多的考虑。\n\n除了写文字，专栏还要录音频，所以为了方便“听”，文章内不能放大量代码、实验。如果很多人在通勤路上听，而我把一张图片讲得天花乱坠，听的人却根本看不到，那肯定是不行的，所以写文章的时候，我还要把故事性、画面感都考虑进去，尽量详尽而不啰嗦。\n\n把这些限制条件加起来之后，我发现，写专栏这件事儿，真的太不容易了。每篇文章看起来内容不多，但是都是费了很多心思的，这也是为什么很多老师说，写完专栏就像是过了火焰山。\n\n## 专栏音频是怎么录的？\n\n说完写文章，我来说说录音频。我平时听播音员说话，感觉非常轻松，所以当时我毫不犹豫地就说，“我要自己录”。但是在录开篇词的时候，我就觉得这完全不是我想的那么回事啊！\n\n专栏的文章在录音的时候一定会有个“音频稿”，我一开始很不理解，我对着发布的稿件直接讲就好了啊，为什么还要特意准备一个供录音频的稿件啊？\n\n我在没有音频稿的情况下，自己试着“发挥”了几次，结果，我发现我的嘴会“吃”字，会反复讲一个内容而且表达不清，但是自己却经常毫无察觉，还会自己讲着讲着就收不住等等。\n\n咱们平时说话的时候，会有很多口头语和重复的词语。面对面交流的时候，我们为什么没有注意这个问题呢？因为我们会更注重对方的表情、手势，但是一旦录成音频，这些“啰嗦”的地方就特别明显。\n\n而有了音频稿之后，整个过程就严谨很多。如果哪句话说错了，看着稿件再说一遍就好了。而且，你会发现录音的时间大大缩短了，原来需要用十分钟，现在五分钟就可以很精炼地讲出来了。\n\n有了稿子，那我是不是对着念就好了？这不是很容易吗？不，我又遇到了新的难题。\n\n录音频的时候，我常常一个人关在密闭的房间里，对着显示器“读”，这和公共演讲肯定是不一样的。加上因为有写好的音频稿，我常常感觉束手束脚，找不到演讲那种有激情的感觉，很容易就变成了念课文。\n\n为了同时满足自然和严谨，一方面我会先熟记“台词”；另一方面，每次录的时候，我都假想对面有个人，我在对着他缓缓地讲出来。讲到某些地方，我还会假想他对这个知识点是不是有疑问，这样就更加有互动感。\n\n录音频这件事对我的改变非常大。我说话、演讲的时候变得更加严谨了。我会下意识地不去重复已经说过的话。一旦想重复，也闭嘴不发音，等想好了下一句再说。后面，我的录音也越来越顺利，一开始要录五六遍才能成功，后面基本一遍就过了。\n\n创作专栏的过程还有许多事情，都是我很难得的记忆。我很佩服“极客时间”的编辑做专栏时的专业和认真。我也很庆幸，我没有固执地按照自己认为正确的方向和方式来做，而是尊重了他们的专业。很显然，**他们没有我懂技术，但是他们比我更懂“你”。**\n\n专栏结束后，我回看这半年的准备和努力，我发现，**无论对自己的领域多么熟悉，写这个专栏都让我又上升了一个新高度**。\n\n我知道很多技术人都喜欢分享，而写文章又是最容易实现的方式。写文章的时候，可以检验你对基础知识的掌握是否扎实，是不是有换位思考能力，能不能从感性认识上升到理性认知。\n\n除此之外，我觉得最重要的一点是，在创作专栏文章的过程中，我学到了很多技术之外的东西，比如换位思考能力和细节把控的能力。\n\n# 01 讲为什么要学习网络协议\n\n《圣经》中有一个通天塔的故事，大致是说，上帝为了阻止人类联合起来，就让人类说不同的语言。人类没法儿沟通，达不成“协议”，通天塔的计划就失败了。\n\n但是千年以后，有一种叫“程序猿”的物种，敲着一种这个群体通用的语言，连接着全世界所有的人，打造这互联网世界的通天塔。如今的世界，正是因为互联网，才连接在一起。\n\n当\"Hello World!\"从显示器打印出来的时候，还记得你激动的心情吗？\n\n```typescript\npublic class HelloWorld {\n  public static void main(String[] args){\n    System.out.println(\"Hello World!\");\n  }\n}\n```\n\n如果你是程序员，一定看得懂上面这一段文字。这是每一个程序员向计算机世界说“你好，世界”的方式。但是，你不一定知道，这段文字也是一种协议，是人类和计算机沟通的协议，**只有通过这种协议，计算机才知道我们想让它做什么。**\n\n## 协议三要素\n\n当然，这种协议还是更接近人类语言，机器不能直接读懂，需要进行翻译，翻译的工作教给编译器，也就是程序员常说的compile。这个过程比较复杂，其中的编译原理非常复杂，我在这里不进行详述。\n\n﻿![img](47f340b2d76fd29bb937006f19dd3e7a-20230115031640924.png)\n\n但是可以看得出，计算机语言作为程序员控制一台计算机工作的协议，具备了协议的三要素。\n\n- **语法**，就是这一段内容要符合一定的规则和格式。例如，括号要成对，结束要使用分号等。\n- **语义**，就是这一段内容要代表某种意义。例如数字减去数字是有意义的，数字减去文本一般来说就没有意义。\n- **顺序**，就是先干啥，后干啥。例如，可以先加上某个数值，然后再减去某个数值。\n\n会了计算机语言，你就能够教给一台计算机完成你的工作了。恭喜你，入门了！\n\n但是，要想打造互联网世界的通天塔，只教给一台机器做什么是不够的，你需要学会教给一大片机器做什么。这就需要网络协议。**只有通过网络协议，才能使一大片机器互相协作、共同完成一件事。**\n\n这个时候，你可能会问，网络协议长啥样，这么神奇，能干成啥事？我先拿一个简单的例子，让你尝尝鲜，然后再讲一个大事。\n\n当你想要买一个商品，常规的做法就是打开浏览器，输入购物网站的地址。浏览器就会给你显示一个缤纷多彩的页面。\n\n那你有没有深入思考过，浏览器是如何做到这件事情的？它之所以能够显示缤纷多彩的页面，是因为它收到了一段来自HTTP协议的“东西”。我拿网易考拉来举例，格式就像下面这样：\n\n```xml\nHTTP/1.1 200 OK\nDate: Tue, 27 Mar 2018 16:50:26 GMT\nContent-Type: text/html;charset=UTF-8\nContent-Language: zh-CN\n\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n\u003cbase href=\"https://pages.kaola.com/\" /\u003e\n\u003cmeta charset=\"utf-8\"/\u003e \u003ctitle\u003e网易考拉3周年主会场\u003c/title\u003e\n```\n\n这符合协议的三要素吗？我带你来看一下。\n\n首先，符合语法，也就是说，只有按照上面那个格式来，浏览器才认。例如，上来是**状态**，然后是**首部**，然后是**内容**。\n\n第二，符合语义，就是要按照约定的意思来。例如，状态200，表述的意思是网页成功返回。如果不成功，就是我们常见的“404”。\n\n第三，符合顺序，你一点浏览器，就是发送出一个HTTP请求，然后才有上面那一串HTTP返回的东西。\n\n浏览器显然按照协议商定好的做了，最后一个五彩缤纷的页面就出现在你面前了。\n\n## 我们常用的网络协议有哪些？\n\n接下来揭秘我要说的大事情，“双十一”。这和我们要讲的网络协议有什么关系呢？\n\n在经济学领域，有个伦纳德·里德（Leonard E. Read）创作的《铅笔的故事》。这个故事通过一个铅笔的诞生过程，来讲述复杂的经济学理论。这里，我也用一个下单的过程，看看互联网世界的运行过程中，都使用了哪些网络协议。\n\n你先在浏览器里面输入 [https://www.kaola.com](https://www.kaola.com/) ，这是一个**URL**。浏览器只知道名字是“www.kaola.com”，但是不知道具体的地点，所以不知道应该如何访问。于是，它打开地址簿去查找。可以使用一般的地址簿协议**DNS**去查找，还可以使用另一种更加精准的地址簿查找协议**HTTPDNS**。\n\n无论用哪一种方法查找，最终都会得到这个地址：106.114.138.24。这个是**IP**地址，是互联网世界的“门牌号”。\n\n知道了目标地址，浏览器就开始打包它的请求。对于普通的浏览请求，往往会使用**HTTP**协议；但是对于购物的请求，往往需要进行加密传输，因而会使用**HTTPS**协议。无论是什么协议，里面都会写明“你要买什么和买多少”。﻿\n\n![img](d8a65ca347ad26acc9f1de49b10320c6-20230115031640843.png)\n\nDNS、HTTP、HTTPS所在的层我们称为**应用层**。经过应用层封装后，浏览器会将应用层的包交给下一层去完成，通过socket编程来实现。下一层是**传输层**。传输层有两种协议，一种是无连接的协议**UDP**，一种是面向连接的协议**TCP**。对于支付来讲，往往使用TCP协议。所谓的面向连接就是，TCP会保证这个包能够到达目的地。如果不能到达，就会重新发送，直至到达。\n\nTCP协议里面会有两个端口，一个是浏览器监听的端口，一个是电商的服务器监听的端口。操作系统往往通过端口来判断，它得到的包应该给哪个进程。\n\n![img](53c753a7d49c9dfe3cfeb26497e47eee-20230115031640930.png)\n\n传输层封装完毕后，浏览器会将包交给操作系统的**网络层**。网络层的协议是IP协议。在IP协议里面会有源IP地址，即浏览器所在机器的IP地址和目标IP地址，也即电商网站所在服务器的IP地址。\n\n![img](459a421975b27f6187d2aa4673171f1b-20230115031640904.png)\n\n操作系统既然知道了目标IP地址，就开始想如何根据这个门牌号找到目标机器。操作系统往往会判断，这个目标IP地址是本地人，还是外地人。如果是本地人，从门牌号就能看出来，但是显然电商网站不在本地，而在遥远的地方。\n\n操作系统知道要离开本地去远方。虽然不知道远方在何处，但是可以这样类比一下：如果去国外要去海关，去外地就要去**网关**。而操作系统启动的时候，就会被DHCP协议配置IP地址，以及默认的网关的IP地址192.168.1.1。\n\n操作系统如何将IP地址发给网关呢？在本地通信基本靠吼，于是操作系统大吼一声，谁是192.168.1.1啊？网关会回答它，我就是，我的本地地址在村东头。这个本地地址就是**MAC**地址，而大吼的那一声是**ARP**协议。\n\n![img](cc02190ac57af7fb6c3839534f2b674f-20230115031640902.png)\n\n于是操作系统将IP包交给了下一层，也就是**MAC层**。网卡再将包发出去。由于这个包里面是有MAC地址的，因而它能够到达网关。\n\n网关收到包之后，会根据自己的知识，判断下一步应该怎么走。网关往往是一个路由器，到某个IP地址应该怎么走，这个叫作路由表。\n\n路由器有点像玄奘西行路过的一个个国家的一个个城关。每个城关都连着两个国家，每个国家相当于一个局域网，在每个国家内部，都可以使用本地的地址MAC进行通信。\n\n一旦跨越城关，就需要拿出IP头来，里面写着贫僧来自东土大唐（就是源IP地址），欲往西天拜佛求经（指的是目标IP地址）。路过宝地，借宿一晚，明日启行，请问接下来该怎么走啊？﻿\n\n![img](f7ea602aec91c67b35e710fb72a975e2-20230115031641074.png)\n\n城关往往是知道这些“知识”的，因为城关和临近的城关也会经常沟通。到哪里应该怎么走，这种沟通的协议称为**路由协议**，常用的有**OSPF**和**BGP**。﻿\n\n![img](b25ad7afba7b79331d95875dd0f451d4-20230115031640998.png)\n\n城关与城关之间是一个国家，当网络包知道了下一步去哪个城关，还是要使用国家内部的MAC地址，通过下一个城关的MAC地址，找到下一个城关，然后再问下一步的路怎么走，一直到走出最后一个城关。\n\n最后一个城关知道这个网络包要去的地方。于是，对着这个国家吼一声，谁是目标IP啊？目标服务器就会回复一个MAC地址。网络包过关后，通过这个MAC地址就能找到目标服务器。\n\n目标服务器发现MAC地址对上了，取下MAC头来，发送给操作系统的网络层。发现IP也对上了，就取下IP头。IP头里会写上一层封装的是TCP协议，然后将其交给传输层，即**TCP层**。\n\n在这一层里，对于收到的每个包，都会有一个回复的包说明收到了。这个回复的包绝非这次下单请求的结果，例如购物是否成功，扣了多少钱等，而仅仅是TCP层的一个说明，即收到之后的回复。当然这个回复，会沿着刚才来的方向走回去，报个平安。\n\n因为一旦出了国门，西行路上千难万险，如果在这个过程中，网络包走丢了，例如进了大沙漠，或者被强盗抢劫杀害怎么办呢？因而到了要报个平安。\n\n如果过一段时间还是没到，发送端的TCP层会重新发送这个包，还是上面的过程，直到有一天收到平安到达的回复。**这个重试绝非你的浏览器重新将下单这个动作重新请求一次**。对于浏览器来讲，就发送了一次下单请求，TCP层不断自己闷头重试。除非TCP这一层出了问题，例如连接断了，才轮到浏览器的应用层重新发送下单请求。\n\n当网络包平安到达TCP层之后，TCP头中有目标端口号，通过这个端口号，可以找到电商网站的进程正在监听这个端口号，假设一个Tomcat，将这个包发给电商网站。﻿\n\n![img](b465ccfafe333bfdfb9daf78f96e123f-20230115031641104.png)\n\n电商网站的进程得到HTTP请求的内容，知道了要买东西，买多少。往往一个电商网站最初接待请求的这个Tomcat只是个接待员，负责统筹处理这个请求，而不是所有的事情都自己做。例如，这个接待员要告诉专门管理订单的进程，登记要买某个商品，买多少，要告诉管理库存的进程，库存要减少多少，要告诉支付的进程，应该付多少钱，等等。\n\n如何告诉相关的进程呢？往往通过RPC调用，即远程过程调用的方式来实现。远程过程调用就是当告诉管理订单进程的时候，接待员不用关心中间的网络互连问题，会由RPC框架统一处理。RPC框架有很多种，有基于HTTP协议放在HTTP的报文里面的，有直接封装在TCP报文里面的。\n\n当接待员发现相应的部门都处理完毕，就回复一个HTTPS的包，告知下单成功。这个HTTPS的包，会像来的时候一样，经过千难万险到达你的个人电脑，最终进入浏览器，显示支付成功。\n\n## 小结\n\n看到了吧，一个简简单单的下单过程，中间牵扯到这么多的协议。而管理一大片机器，更是一件特别有技术含量的事情。除此之外，像最近比较火的云计算、容器、微服务等技术，也都需要借助各种协议，来达成大规模机器之间的合作。\n\n我在这里列一下之后要讲的网络协议，之后我会按照从底层到上层的顺序来讲述。﻿\n\n# 02 讲网络分层的真实含义是什么\n\n长时间从事计算机网络相关的工作，我发现，计算机网络有一个显著的特点，就是这是一个不仅需要背诵，而且特别需要将原理烂熟于胸的学科。很多问题看起来懂了，但是就怕往细里问，一问就发现你懂得没有那么透彻。\n\n我们上一节列了之后要讲的网络协议。这些协议本来没什么稀奇，每一本教科书都会讲，并且都要求你背下来。因为考试会考，面试会问。可以这么说，毕业了去找工作还答不出这类题目的，那你的笔试基本上也就挂了。\n\n当你听到什么二层设备、三层设备、四层LB和七层LB中层的时候，是否有点一头雾水，不知道这些所谓的层，对应的各种协议具体要做什么“工作”？\n\n## 这四个问题你真的懂了吗？\n\n因为教科书或者老师往往会打一个十分不恰当的比喻：为什么网络要分层呀？因为不同的层次之间有不同的沟通方式，这个叫作协议。例如，一家公司也是分“层次”的，分总经理、经理、组长、员工。总经理之间有他们的沟通方式，经理和经理之间也有沟通方式，同理组长和员工。有没有听过类似的比喻？\n\n那么**第一个问题**来了。请问经理在握手的时候，员工在干什么？很多人听过TCP建立连接的**三次握手协议**，也会把它当知识点背诵。同理问你，TCP在进行三次握手的时候，IP层和MAC层对应都有什么操作呢？\n\n除了上面这个不恰当的比喻，教科书还会列出每个层次所包含的协议，然后开始逐层地去讲这些协议。但是这些协议之间的关系呢？却很少有教科书会讲。\n\n学习第三层的时候会提到，IP协议里面包含**目标地址**和**源地址。第三层里往往还会学习路由协议**。路由就像中转站，我们从原始地址A到目标地址D，中间经过两个中转站A-\u003eB-\u003eC-\u003eD，是通过路由转发的。\n\n那么**第二个问题**来了。A知道自己的下一个中转站是B，那从A发出来的包，应该把B的IP地址放在哪里呢？B知道自己的下一个中转站是C，从B发出来的包，应该把C的IP地址放在哪里呢？如果放在IP协议中的目标地址，那包到了中转站，怎么知道最终的目的地址是D呢？\n\n教科书不会通过场景化的例子，将网络包的生命周期讲出来，所以你就会很困惑，不知道这些协议实际的应用场景是什么。\n\n我**再问你一个问题**。你一定经常听说二层设备、三层设备。二层设备处理的通常是MAC层的东西。那我发送一个HTTP的包，是在第七层工作的，那是不是不需要经过二层设备？或者即便经过了，二层设备也不处理呢？或者换一种问法，二层设备处理的包里，有没有HTTP层的内容呢？\n\n最终，我想问你**一个综合的问题**。从你的电脑，通过SSH登录到公有云主机里面，都需要经历哪些过程？或者说你打开一个电商网站，都需要经历哪些过程？说得越详细越好。\n\n实际情况可能是，很多人会答不上来。尽管对每一层都很熟悉，但是知识点却串不起来。\n\n上面的这些问题，有的在这一节就会有一个解释，有的则会贯穿我们整个课程。好在后面一节中我会举一个贯穿的例子，将很多层的细节讲过后，你很容易就能把这些知识点串起来。\n\n## 网络为什么要分层？\n\n这里我们先探讨第一个问题，网络为什么要分层？因为，是个复杂的程序都要分层。\n\n理解计算机网络中的概念，一个很好的角度是，想象网络包就是一段Buffer，或者一块内存，是有格式的。同时，想象自己是一个处理网络包的程序，而且这个程序可以跑在电脑上，可以跑在服务器上，可以跑在交换机上，也可以跑在路由器上。你想象自己有很多的网口，从某个口拿进一个网络包来，用自己的程序处理一下，再从另一个网口发送出去。\n\n当然网络包的格式很复杂，这个程序也很复杂。**复杂的程序都要分层，这是程序设计的要求。**比如，复杂的电商还会分数据库层、缓存层、Compose层、Controller层和接入层，每一层专注做本层的事情。\n\n## 程序是如何工作的？\n\n我们可以简单地想象“你”这个程序的工作过程。\n\n![img](06b355394f525c54f200d8a1af63ddea-20230115031641176.jpg)\n\n当一个网络包从一个网口经过的时候，你看到了，首先先看看要不要请进来，处理一把。有的网口配置了混杂模式，凡是经过的，全部拿进来。\n\n拿进来以后，就要交给一段程序来处理。于是，你调用**process_layer2(buffer)**。当然，这是一个假的函数。但是你明白其中的意思，知道肯定是有这么个函数的。那这个函数是干什么的呢？从Buffer中，摘掉二层的头，看一看，应该根据头里面的内容做什么操作。\n\n假设你发现这个包的MAC地址和你的相符，那说明就是发给你的，于是需要调用**process_layer3(buffer)**。这个时候，Buffer里面往往就没有二层的头了，因为已经在上一个函数的处理过程中拿掉了，或者将开始的偏移量移动了一下。在这个函数里面，摘掉三层的头，看看到底是发送给自己的，还是希望自己转发出去的。\n\n如何判断呢？如果IP地址不是自己的，那就应该转发出去；如果IP地址是自己的，那就是发给自己的。根据IP头里面的标示，拿掉三层的头，进行下一层的处理，到底是调用process_tcp(buffer)呢，还是调用process_udp(buffer)呢？\n\n假设这个地址是TCP的，则会调用**process_tcp(buffer)**。这时候，Buffer里面没有三层的头，就需要查看四层的头，看这是一个发起，还是一个应答，又或者是一个正常的数据包，然后分别由不同的逻辑进行处理。如果是发起或者应答，接下来可能要发送一个回复包；如果是一个正常的数据包，就需要交给上层了。交给谁呢？是不是有process_http(buffer)函数呢？\n\n没有的，如果你是一个网络包处理程序，你不需要有process_http(buffer)，而是应该交给应用去处理。交给哪个应用呢？在四层的头里面有端口号，不同的应用监听不同的端口号。如果发现浏览器应用在监听这个端口，那你发给浏览器就行了。至于浏览器怎么处理，和你没有关系。\n\n浏览器自然是解析HTML，显示出页面来。电脑的主人看到页面很开心，就点了鼠标。点击鼠标的动作被浏览器捕获。浏览器知道，又要发起另一个HTTP请求了，于是使用端口号，将请求发给了你。\n\n你应该调用**send_tcp(buffer)**。不用说，Buffer里面就是HTTP请求的内容。这个函数里面加一个TCP的头，记录下源端口号。浏览器会给你目的端口号，一般为80端口。\n\n然后调用**send_layer3(buffer)**。Buffer里面已经有了HTTP的头和内容，以及TCP的头。在这个函数里面加一个IP的头，记录下源IP的地址和目标IP的地址。\n\n然后调用**send_layer2(buffer)**。Buffer里面已经有了HTTP的头和内容、TCP的头，以及IP的头。这个函数里面要加一下MAC的头，记录下源MAC地址，得到的就是本机器的MAC地址和目标的MAC地址。不过，这个还要看当前知道不知道，知道就直接加上；不知道的话，就要通过一定的协议处理过程，找到MAC地址。反正要填一个，不能空着。\n\n万事俱备，只要Buffer里面的内容完整，就可以从网口发出去了，你作为一个程序的任务就算告一段落了。\n\n## 揭秘层与层之间的关系\n\n知道了这个过程之后，我们再来看一下原来困惑的问题。\n\n首先是分层的比喻。**所有不能表示出层层封装含义的比喻，都是不恰当的。**总经理握手，不需要员工在吧，总经理之间谈什么，不需要员工参与吧，但是网络世界不是这样的。正确的应该是，总经理之间沟通的时候，经理将总经理放在自己兜里，然后组长把经理放自己兜里，员工把组长放自己兜里，像套娃娃一样。那员工直接沟通，不带上总经理，就不恰当了。\n\n现实生活中，往往是员工说一句，组长补充两句，然后经理补充两句，最后总经理再补充两句。但是在网络世界，应该是总经理说话，经理补充两句，组长补充两句，员工再补充两句。\n\n那TCP在三次握手的时候，IP层和MAC层在做什么呢？当然是TCP发送每一个消息，都会带着IP层和MAC层了。因为，TCP每发送一个消息，IP层和MAC层的所有机制都要运行一遍。而你只看到TCP三次握手了，其实，IP层和MAC层为此也忙活好久了。\n\n这里要记住一点：**只要是在网络上跑的包，都是完整的。可以有下层没上层，绝对不可能有上层没下层。**\n\n所以，**对TCP协议来说，三次握手也好，重试也好，只要想发出去包，就要有IP层和MAC层，不然是发不出去的。**\n\n经常有人会问这样一个问题，我都知道那台机器的IP地址了，直接发给他消息呗，要MAC地址干啥？这里的关键就是，没有MAC地址消息是发不出去的。\n\n所以如果一个HTTP协议的包跑在网络上，它一定是完整的。无论这个包经过哪些设备，它都是完整的。\n\n所谓的二层设备、三层设备，都是这些设备上跑的程序不同而已。一个HTTP协议的包经过一个二层设备，二层设备收进去的是整个网络包。这里面HTTP、TCP、 IP、 MAC都有。什么叫二层设备呀，就是只把MAC头摘下来，看看到底是丢弃、转发，还是自己留着。那什么叫三层设备呢？就是把MAC头摘下来之后，再把IP头摘下来，看看到底是丢弃、转发，还是自己留着。\n\n## 小结\n\n总结一下今天的内容，理解网络协议的工作模式，有两个小窍门：\n\n- 始终想象自己是一个处理网络包的程序：如何拿到网络包，如何根据规则进行处理，如何发出去；\n- 始终牢记一个原则：只要是在网络上跑的包，都是完整的。可以有下层没上层，绝对不可能有上层没下层。\n\n# 03 讲ifconfig：最熟悉又陌生的命令行\n\n上一节结尾给你留的一个思考题是，你知道怎么查看IP地址吗？\n\n当面试听到这个问题的时候，面试者常常会觉得走错了房间。我面试的是技术岗位啊，怎么问这么简单的问题？\n\n的确，即便没有专业学过计算机的人，只要倒腾过电脑，重装过系统，大多也会知道这个问题的答案：在Windows上是ipconfig，在Linux上是ifconfig。\n\n那你知道在Linux上还有什么其他命令可以查看IP地址吗？答案是ip addr。如果回答不上来这个问题，那你可能没怎么用过Linux。\n\n那你知道ifconfig和ip addr的区别吗？这是一个有关net-tools和iproute2的“历史”故事，你刚来到第三节，暂时不用了解这么细，但这也是一个常考的知识点。\n\n想象一下，你登录进入一个被裁剪过的非常小的Linux系统中，发现既没有ifconfig命令，也没有ip addr命令，你是不是感觉这个系统压根儿没法用？这个时候，你可以自行安装net-tools和iproute2这两个工具。当然，大多数时候这两个命令是系统自带的。\n\n安装好后，我们来运行一下ip addr。不出意外，应该会输出下面的内容。\n\n```sql\nroot@test:~# ip addr\n1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether fa:16:3e:c7:79:75 brd ff:ff:ff:ff:ff:ff\n    inet 10.100.122.2/24 brd 10.100.122.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::f816:3eff:fec7:7975/64 scope link \n       valid_lft forever preferred_lft forever\n```\n\n这个命令显示了这台机器上所有的网卡。大部分的网卡都会有一个IP地址，当然，这不是必须的。在后面的分享中，我们会遇到没有IP地址的情况。\n\nIP地址是一个网卡在网络世界的通讯地址，相当于我们现实世界的门牌号码。既然是门牌号码，不能大家都一样，不然就会起冲突。比方说，假如大家都叫六单元1001号，那快递就找不到地方了。所以，有时候咱们的电脑弹出网络地址冲突，出现上不去网的情况，多半是IP地址冲突了。\n\n如上输出的结果，10.100.122.2就是一个IP地址。这个地址被点分隔为四个部分，每个部分8个bit，所以IP地址总共是32位。这样产生的IP地址的数量很快就不够用了。因为当时设计IP地址的时候，哪知道今天会有这么多的计算机啊！因为不够用，于是就有了IPv6，也就是上面输出结果里面inet6 fe80::f816:3eff:fec7:7975/64。这个有128位，现在看来是够了，但是未来的事情谁知道呢？\n\n本来32位的IP地址就不够，还被分成了5类。现在想想，当时分配地址的时候，真是太奢侈了。\n\n![img](0b32d6e35ff0bbc5d46cfb87f6669d9e-20230115031641502.jpg)\n\n在网络地址中，至少在当时设计的时候，对于A、B、 C类主要分两部分，前面一部分是网络号，后面一部分是主机号。这很好理解，大家都是六单元1001号，我是小区A的六单元1001号，而你是小区B的六单元1001号。\n\n下面这个表格，详细地展示了A、B、C三类地址所能包含的主机的数量。在后文中，我也会多次借助这个表格来讲解。\n\n![img](e9c59a4b2f0b804356759b10440ea7be-20230115031641260.jpg)\n\n这里面有个尴尬的事情，就是C类地址能包含的最大主机数量实在太少了，只有254个。当时设计的时候恐怕没想到，现在估计一个网吧都不够用吧。而B类地址能包含的最大主机数量又太多了。6万多台机器放在一个网络下面，一般的企业基本达不到这个规模，闲着的地址就是浪费。\n\n## 无类型域间选路（CIDR）\n\n于是有了一个折中的方式叫作**无类型域间选路**，简称**CIDR**。这种方式打破了原来设计的几类地址的做法，将32位的IP地址一分为二，前面是**网络号**，后面是**主机号**。从哪里分呢？你如果注意观察的话可以看到，10.100.122.2/24，这个IP地址中有一个斜杠，斜杠后面有个数字24。这种地址表示形式，就是CIDR。后面24的意思是，32位中，前24位是网络号，后8位是主机号。\n\n伴随着CIDR存在的，一个是**广播地址**，10.100.122.255。如果发送这个地址，所有10.100.122网络里面的机器都可以收到。另一个是**子网掩码**，255.255.255.0。\n\n将子网掩码和IP地址进行AND计算。前面三个255，转成二进制都是1。1和任何数值取AND，都是原来数值，因而前三个数不变，为10.100.122。后面一个0，转换成二进制是0，0和任何数值取AND，都是0，因而最后一个数变为0，合起来就是10.100.122.0。这就是**网络号**。**将子网掩码和IP地址按位计算AND，就可得到网络号。**\n\n## 公有IP地址和私有IP地址\n\n在日常的工作中，几乎不用划分A类、B类或者C类，所以时间长了，很多人就忘记了这个分类，而只记得CIDR。但是有一点还是要注意的，就是公有IP地址和私有IP地址。\n\n![img](901778433f2d6e27b916e9e53c232d93-20230115031641163.jpg)\n\n我们继续看上面的表格。表格最右列是私有IP地址段。平时我们看到的数据中心里，办公室、家里或学校的IP地址，一般都是私有IP地址段。因为这些地址允许组织内部的IT人员自己管理、自己分配，而且可以重复。因此，你学校的某个私有IP地址段和我学校的可以是一样的。\n\n这就像每个小区有自己的楼编号和门牌号，你们小区可以叫6栋，我们小区也叫6栋，没有任何问题。但是一旦出了小区，就需要使用公有IP地址。就像人民路888号，是国家统一分配的，不能两个小区都叫人民路888号。\n\n公有IP地址有个组织统一分配，你需要去买。如果你搭建一个网站，给你学校的人使用，让你们学校的IT人员给你一个IP地址就行。但是假如你要做一个类似网易163这样的网站，就需要有公有IP地址，这样全世界的人才能访问。\n\n表格中的192.168.0.x是最常用的私有IP地址。你家里有Wi-Fi，对应就会有一个IP地址。一般你家里地上网设备不会超过256个，所以/24基本就够了。有时候我们也能见到/16的CIDR，这两种是最常见的，也是最容易理解的。\n\n不需要将十进制转换为二进制32位，就能明显看出192.168.0是网络号，后面是主机号。而整个网络里面的第一个地址192.168.0.1，往往就是你这个私有网络的出口地址。例如，你家里的电脑连接Wi-Fi，Wi-Fi路由器的地址就是192.168.0.1，而192.168.0.255就是广播地址。一旦发送这个地址，整个192.168.0网络里面的所有机器都能收到。\n\n但是也不总都是这样的情况。因此，其他情况往往就会很难理解，还容易出错。\n\n## 举例：一个容易“犯错”的CIDR\n\n我们来看16.158.165.91/22这个CIDR。求一下这个网络的第一个地址、子网掩码和广播地址。\n\n你要是上来就写16.158.165.1，那就大错特错了。\n\n/22不是8的整数倍，不好办，只能先变成二进制来看。16.158的部分不会动，它占了前16位。中间的165，变为二进制为‭10100101‬。除了前面的16位，还剩6位。所以，这8位中前6位是网络号，16.158.\u003c101001\u003e，而\u003c01\u003e.91是机器号。\n\n第一个地址是16.158.\u003c101001\u003e\u003c00\u003e.1，即16.158.164.1。子网掩码是255.255.\u003c111111\u003e\u003c00\u003e.0，即255.255.252.0。广播地址为16.158.\u003c101001\u003e\u003c11\u003e.255，即16.158.167.255。\n\n这五类地址中，还有一类D类是**组播地址**。使用这一类地址，属于某个组的机器都能收到。这有点类似在公司里面大家都加入了一个邮件组。发送邮件，加入这个组的都能收到。组播地址在后面讲述VXLAN协议的时候会提到。\n\n讲了这么多，才讲了上面的输出结果中很小的一部分，是不是觉得原来并没有真的理解ip addr呢？我们接着来分析。\n\n在IP地址的后面有个scope，对于eth0这张网卡来讲，是global，说明这张网卡是可以对外的，可以接收来自各个地方的包。对于lo来讲，是host，说明这张网卡仅仅可以供本机相互通信。\n\nlo全称是**loopback**，又称**环回接口**，往往会被分配到127.0.0.1这个地址。这个地址用于本机通信，经过内核处理后直接返回，不会在任何网络中出现。\n\n## MAC地址\n\n在IP地址的上一行是link/ether fa:16:3e:c7:79:75 brd ff:ff:ff:ff:ff:ff，这个被称为**MAC地址**，是一个网卡的物理地址，用十六进制，6个byte表示。\n\nMAC地址是一个很容易让人“误解”的地址。因为MAC地址号称全局唯一，不会有两个网卡有相同的MAC地址，而且网卡自生产出来，就带着这个地址。很多人看到这里就会想，既然这样，整个互联网的通信，全部用MAC地址好了，只要知道了对方的MAC地址，就可以把信息传过去。\n\n这样当然是不行的。 **一个网络包要从一个地方传到另一个地方，除了要有确定的地址，还需要有定位功能。** 而有门牌号码属性的IP地址，才是有远程定位功能的。\n\n例如，你去杭州市网商路599号B楼6层找刘超，你在路上问路，可能被问的人不知道B楼是哪个，但是可以给你指网商路怎么去。但是如果你问一个人，你知道这个身份证号的人在哪里吗？可想而知，没有人知道。\n\nMAC地址更像是身份证，是一个唯一的标识。它的唯一性设计是为了组网的时候，不同的网卡放在一个网络里面的时候，可以不用担心冲突。从硬件角度，保证不同的网卡有不同的标识。\n\nMAC地址是有一定定位功能的，只不过范围非常有限。你可以根据IP地址，找到杭州市网商路599号B楼6层，但是依然找不到我，你就可以靠吼了，大声喊身份证XXXX的是哪位？我听到了，我就会站起来说，是我啊。但是如果你在上海，到处喊身份证XXXX的是哪位，我不在现场，当然不会回答，因为我在杭州不在上海。\n\n所以，MAC地址的通信范围比较小，局限在一个子网里面。例如，从192.168.0.2/24访问192.168.0.3/24是可以用MAC地址的。一旦跨子网，即从192.168.0.2/24到192.168.1.2/24，MAC地址就不行了，需要IP地址起作用了。\n\n## 网络设备的状态标识\n\n解析完了MAC地址，我们再来看 \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e是干什么的？这个叫作**net_device flags**，**网络设备的状态标识**。\n\nUP表示网卡处于启动的状态；BROADCAST表示这个网卡有广播地址，可以发送广播包；MULTICAST表示网卡可以发送多播包；LOWER_UP表示L1是启动的，也即网线插着呢。MTU1500是指什么意思呢？是哪一层的概念呢？最大传输单元MTU为1500，这是以太网的默认值。\n\n上一节，我们讲过网络包是层层封装的。MTU是二层MAC层的概念。MAC层有MAC的头，以太网规定连MAC头带正文合起来，不允许超过1500个字节。正文里面有IP的头、TCP的头、HTTP的头。如果放不下，就需要分片来传输。\n\nqdisc pfifo_fast是什么意思呢？qdisc全称是**queueing discipline**，中文叫**排队规则**。内核如果需要通过某个网络接口发送数据包，它都需要按照为这个接口配置的qdisc（排队规则）把数据包加入队列。\n\n最简单的qdisc是pfifo，它不对进入的数据包做任何的处理，数据包采用先入先出的方式通过队列。pfifo_fast稍微复杂一些，它的队列包括三个波段（band）。在每个波段里面，使用先进先出规则。\n\n三个波段（band）的优先级也不相同。band 0的优先级最高，band 2的最低。如果band 0里面有数据包，系统就不会处理band 1里面的数据包，band 1和band 2之间也是一样。\n\n数据包是按照服务类型（**Type of Service，TOS**）被分配到三个波段（band）里面的。TOS是IP头里面的一个字段，代表了当前的包是高优先级的，还是低优先级的。\n\n队列是个好东西，后面我们讲云计算中的网络的时候，会有很多用户共享一个网络出口的情况，这个时候如何排队，每个队列有多粗，队列处理速度应该怎么提升，我都会详细为你讲解。\n\n## 小结\n\n怎么样，看起来很简单的一个命令，里面学问很大吧？通过这一节，希望你能记住以下的知识点，后面都能用得上：\n\n- IP是地址，有定位功能；MAC是身份证，无定位功能；\n- CIDR可以用来判断是不是本地人；\n- IP分公有的IP和私有的IP。后面的章节中我会谈到“出国门”，就与这个有关。\n\n最后，给你留两个思考题。\n\n1. 你知道net-tools和iproute2的“历史”故事吗？\n2. 这一节讲的是如何查看IP地址，那你知道IP地址是怎么来的吗？\n\n# 04 讲DHCP与PXE：IP是怎么来的，又是怎么没的？\n\n上一节，我们讲了IP的一些基本概念。如果需要和其他机器通讯，我们就需要一个通讯地址，我们需要给网卡配置这么一个地址。\n\n## 如何配置IP地址？\n\n那如何配置呢？如果有相关的知识和积累，你可以用命令行自己配置一个地址。可以使用ifconfig，也可以使用ip addr。设置好了以后，用这两个命令，将网卡up一下，就可以开始工作了。\n\n**使用net-tools：**\n\n```ruby\n$ sudo ifconfig eth1 10.0.0.1/24\n$ sudo ifconfig eth1 up\n```\n\n**使用iproute2：**\n\n```shell\n$ sudo ip addr add 10.0.0.1/24 dev eth1\n$ sudo ip link set up eth1\n```\n\n你可能会问了，自己配置这个自由度太大了吧，我是不是配置什么都可以？如果配置一个和谁都不搭边的地址呢？例如，旁边的机器都是192.168.1.x，我非得配置一个16.158.23.6，会出现什么现象呢？\n\n不会出现任何现象，就是包发不出去呗。为什么发不出去呢？我来举例说明。\n\n192.168.1.6就在你这台机器的旁边，甚至是在同一个交换机上，而你把机器的地址设为了16.158.23.6。在这台机器上，你企图去ping192.168.1.6，你觉得只要将包发出去，同一个交换机的另一台机器马上就能收到，对不对？\n\n可是Linux系统不是这样的，它没你想得那么智能。你用肉眼看到那台机器就在旁边，它则需要根据自己的逻辑进行处理。\n\n还记得我们在第二节说过的原则吗？**只要是在网络上跑的包，都是完整的，可以有下层没上层，绝对不可能有上层没下层。**\n\n所以，你看着它有自己的源IP地址16.158.23.6，也有目标IP地址192.168.1.6，但是包发不出去，这是因为MAC层还没填。\n\n自己的MAC地址自己知道，这个容易。但是目标MAC填什么呢？是不是填192.168.1.6这台机器的MAC地址呢？\n\n当然不是。Linux首先会判断，要去的这个地址和我是一个网段的吗，或者和我的一个网卡是同一网段的吗？只有是一个网段的，它才会发送ARP请求，获取MAC地址。如果发现不是呢？\n\n**Linux默认的逻辑是，如果这是一个跨网段的调用，它便不会直接将包发送到网络上，而是企图将包发送到网关。**\n\n如果你配置了网关的话，Linux会获取网关的MAC地址，然后将包发出去。对于192.168.1.6这台机器来讲，虽然路过它家门的这个包，目标IP是它，但是无奈MAC地址不是它的，所以它的网卡是不会把包收进去的。\n\n如果没有配置网关呢？那包压根就发不出去。\n\n如果将网关配置为192.168.1.6呢？不可能，Linux不会让你配置成功的，因为网关要和当前的网络至少一个网卡是同一个网段的，怎么可能16.158.23.6的网关是192.168.1.6呢？\n\n所以，当你需要手动配置一台机器的网络IP时，一定要好好问问你的网络管理员。如果在机房里面，要去网络管理员那里申请，让他给你分配一段正确的IP地址。当然，真正配置的时候，一定不是直接用命令配置的，而是放在一个配置文件里面。**不同系统的配置文件格式不同，但是无非就是CIDR、子网掩码、广播地址和网关地址**。\n\n## 动态主机配置协议（DHCP）\n\n原来配置IP有这么多门道儿啊。你可能会问了，配置了IP之后一般不能变的，配置一个服务端的机器还可以，但是如果是客户端的机器呢？我抱着一台笔记本电脑在公司里走来走去，或者白天来晚上走，每次使用都要配置IP地址，那可怎么办？还有人事、行政等非技术人员，如果公司所有的电脑都需要IT人员配置，肯定忙不过来啊。\n\n因此，我们需要有一个自动配置的协议，也就是称**动态主机配置协议（Dynamic Host Configuration Protocol）**，简称**DHCP**。\n\n有了这个协议，网络管理员就轻松多了。他只需要配置一段共享的IP地址。每一台新接入的机器都通过DHCP协议，来这个共享的IP地址里申请，然后自动配置好就可以了。等人走了，或者用完了，还回去，这样其他的机器也能用。\n\n所以说，**如果是数据中心里面的服务器，IP一旦配置好，基本不会变，这就相当于买房自己装修。DHCP的方式就相当于租房。你不用装修，都是帮你配置好的。你暂时用一下，用完退租就可以了。**\n\n## 解析DHCP的工作方式\n\n当一台机器新加入一个网络的时候，肯定一脸懵，啥情况都不知道，只知道自己的MAC地址。怎么办？先吼一句，我来啦，有人吗？这时候的沟通基本靠“吼”。这一步，我们称为**DHCP Discover。**\n\n新来的机器使用IP地址0.0.0.0发送了一个广播包，目的IP地址为255.255.255.255。广播包封装了UDP，UDP封装了BOOTP。其实DHCP是BOOTP的增强版，但是如果你去抓包的话，很可能看到的名称还是BOOTP协议。\n\n在这个广播包里面，新人大声喊：我是新来的（Boot request），我的MAC地址是这个，我还没有IP，谁能给租给我个IP地址！\n\n格式就像这样：\n\n![img](395b304f49559034af34c882bd86f11f-20230115031641073.jpg)\n\n如果一个网络管理员在网络里面配置了**DHCP Server**的话，他就相当于这些IP的管理员。他立刻能知道来了一个“新人”。这个时候，我们可以体会MAC地址唯一的重要性了。当一台机器带着自己的MAC地址加入一个网络的时候，MAC是它唯一的身份，如果连这个都重复了，就没办法配置了。\n\n只有MAC唯一，IP管理员才能知道这是一个新人，需要租给它一个IP地址，这个过程我们称为**DHCP Offer**。同时，DHCP Server为此客户保留为它提供的IP地址，从而不会为其他DHCP客户分配此IP地址。\n\nDHCP Offer的格式就像这样，里面有给新人分配的地址。\n\n![img](54ffefbe4f493f0f4a39f45504bd5086-20230115031641100.jpg)\n\nDHCP Server仍然使用广播地址作为目的地址，因为，此时请求分配IP的新人还没有自己的IP。DHCP Server回复说，我分配了一个可用的IP给你，你看如何？除此之外，服务器还发送了子网掩码、网关和IP地址租用期等信息。\n\n新来的机器很开心，它的“吼”得到了回复，并且有人愿意租给它一个IP地址了，这意味着它可以在网络上立足了。当然更令人开心的是，如果有多个DHCP Server，这台新机器会收到多个IP地址，简直受宠若惊。\n\n它会选择其中一个DHCP Offer，一般是最先到达的那个，并且会向网络发送一个DHCP Request广播数据包，包中包含客户端的MAC地址、接受的租约中的IP地址、提供此租约的DHCP服务器地址等，并告诉所有DHCP Server它将接受哪一台服务器提供的IP地址，告诉其他DHCP服务器，谢谢你们的接纳，并请求撤销它们提供的IP地址，以便提供给下一个IP租用请求者。\n\n![img](e1e45ba0d86d2774ec80a1d86f87b724-20230115031641171.jpg)\n\n此时，由于还没有得到DHCP Server的最后确认，客户端仍然使用0.0.0.0为源IP地址、255.255.255.255为目标地址进行广播。在BOOTP里面，接受某个DHCP Server的分配的IP。\n\n当DHCP Server接收到客户机的DHCP request之后，会广播返回给客户机一个DHCP ACK消息包，表明已经接受客户机的选择，并将这一IP地址的合法租用信息和其他的配置信息都放入该广播包，发给客户机，欢迎它加入网络大家庭。\n\n![img](7da571c18b974582a9cfe4718c5dea0e-20230115031641169.jpg)\n\n最终租约达成的时候，还是需要广播一下，让大家都知道。\n\n## IP地址的收回和续租\n\n既然是租房子，就是有租期的。租期到了，管理员就要将IP收回。\n\n如果不用的话，收回就收回了。就像你租房子一样，如果还要续租的话，不能到了时间再续租，而是要提前一段时间给房东说。DHCP也是这样。\n\n客户机会在租期过去50%的时候，直接向为其提供IP地址的DHCP Server发送DHCP request消息包。客户机接收到该服务器回应的DHCP ACK消息包，会根据包中所提供的新的租期以及其他已经更新的TCP/IP参数，更新自己的配置。这样，IP租用更新就完成了。\n\n好了，一切看起来完美。DHCP协议大部分人都知道，但是其实里面隐藏着一个细节，很多人可能不会去注意。接下来，我就讲一个有意思的事情：网络管理员不仅能自动分配IP地址，还能帮你自动安装操作系统！\n\n## 预启动执行环境（PXE）\n\n普通的笔记本电脑，一般不会有这种需求。因为你拿到电脑时，就已经有操作系统了，即便你自己重装操作系统，也不是很麻烦的事情。但是，在数据中心里就不一样了。数据中心里面的管理员可能一下子就拿到几百台空的机器，一个个安装操作系统，会累死的。\n\n所以管理员希望的不仅仅是自动分配IP地址，还要自动安装系统。装好系统之后自动分配IP地址，直接启动就能用了，这样当然最好了！\n\n这事儿其实仔细一想，还是挺有难度的。安装操作系统，应该有个光盘吧。数据中心里不能用光盘吧，想了一个办法就是，可以将光盘里面要安装的操作系统放在一个服务器上，让客户端去下载。但是客户端放在哪里呢？它怎么知道去哪个服务器上下载呢？客户端总得安装在一个操作系统上呀，可是这个客户端本来就是用来安装操作系统的呀？\n\n其实，这个过程和操作系统启动的过程有点儿像。首先，启动BIOS。这是一个特别小的小系统，只能干特别小的一件事情。其实就是读取硬盘的MBR启动扇区，将GRUB启动起来；然后将权力交给GRUB，GRUB加载内核、加载作为根文件系统的initramfs文件；然后将权力交给内核；最后内核启动，初始化整个操作系统。\n\n那我们安装操作系统的过程，只能插在BIOS启动之后了。因为没安装系统之前，连启动扇区都没有。因而这个过程叫做**预启动执行环境（Pre-boot Execution Environment）**，简称**PXE。**\n\nPXE协议分为客户端和服务器端，由于还没有操作系统，只能先把客户端放在BIOS里面。当计算机启动时，BIOS把PXE客户端调入内存里面，就可以连接到服务端做一些操作了。\n\n首先，PXE客户端自己也需要有个IP地址。因为PXE的客户端启动起来，就可以发送一个DHCP的请求，让DHCP Server给它分配一个地址。PXE客户端有了自己的地址，那它怎么知道PXE服务器在哪里呢？对于其他的协议，都好办，要么人告诉他。例如，告诉浏览器要访问的IP地址，或者在配置中告诉它；例如，微服务之间的相互调用。\n\n但是PXE客户端启动的时候，啥都没有。好在DHCP Server除了分配IP地址以外，还可以做一些其他的事情。这里有一个DHCP Server的一个样例配置：\n\n```lua\nddns-update-style interim;\nignore client-updates;\nallow booting;\nallow bootp;\nsubnet 192.168.1.0 netmask 255.255.255.0\n{\noption routers 192.168.1.1;\noption subnet-mask 255.255.255.0;\noption time-offset -18000;\ndefault-lease-time 21600;\nmax-lease-time 43200;\nrange dynamic-bootp 192.168.1.240 192.168.1.250;\nfilename \"pxelinux.0\";\nnext-server 192.168.1.180;\n}\n```\n\n按照上面的原理，默认的DHCP Server是需要配置的，无非是我们配置IP的时候所需要的IP地址段、子网掩码、网关地址、租期等。如果想使用PXE，则需要配置next-server，指向PXE服务器的地址，另外要配置初始启动文件filename。\n\n这样PXE客户端启动之后，发送DHCP请求之后，除了能得到一个IP地址，还可以知道PXE服务器在哪里，也可以知道如何从PXE服务器上下载某个文件，去初始化操作系统。\n\n## 解析PXE的工作过程\n\n接下来我们来详细看一下PXE的工作过程。\n\n首先，启动PXE客户端。第一步是通过DHCP协议告诉DHCP Server，我刚来，一穷二白，啥都没有。DHCP Server便租给它一个IP地址，同时也给它PXE服务器的地址、启动文件pxelinux.0。\n\n其次，PXE客户端知道要去PXE服务器下载这个文件后，就可以初始化机器。于是便开始下载，下载的时候使用的是TFTP协议。所以PXE服务器上，往往还需要有一个TFTP服务器。PXE客户端向TFTP服务器请求下载这个文件，TFTP服务器说好啊，于是就将这个文件传给它。\n\n然后，PXE客户端收到这个文件后，就开始执行这个文件。这个文件会指示PXE客户端，向TFTP服务器请求计算机的配置信息pxelinux.cfg。TFTP服务器会给PXE客户端一个配置文件，里面会说内核在哪里、initramfs在哪里。PXE客户端会请求这些文件。\n\n最后，启动Linux内核。一旦启动了操作系统，以后就啥都好办了。\n\n![img](6e69007db3fc68ff6da8496266abf6a4-20230115031641219.jpg)\n\n## 小结\n\n好了，这一节就到这里了。我来总结一下今天的内容：\n\n- DHCP协议主要是用来给客户租用IP地址，和房产中介很像，要商谈、签约、续租，广播还不能“抢单”；\n- DHCP协议能给客户推荐“装修队”PXE，能够安装操作系统，这个在云计算领域大有用处。\n\n最后，学完了这一节，给你留两个思考题吧。\n\n1. PXE协议可以用来安装操作系统，但是如果每次重启都安装操作系统，就会很麻烦。你知道如何使得第一次安装操作系统，后面就正常启动吗？\n2. 现在上网很简单了，买个家用路由器，连上WIFI，给DHCP分配一个IP地址，就可以上网了。那你是否用过更原始的方法自己组过简单的网呢？说来听听。\n\n# 05 讲从物理层到MAC层：如何在宿舍里自己组网玩联机游戏？\n\n上一节，我们见证了IP地址的诞生，或者说是整个操作系统的诞生。一旦机器有了IP，就可以在网络的环境里和其他的机器展开沟通了。\n\n故事就从我的大学宿舍开始讲起吧。作为一个八零后，我要暴露年龄了。\n\n我们宿舍四个人，大一的时候学校不让上网，不给开通网络。但是，宿舍有一个人比较有钱，率先买了一台电脑。那买了电脑干什么呢？\n\n首先，有单机游戏可以打，比如说《拳皇》。两个人用一个键盘，照样打得火热。后来有第二个人买了电脑，那两台电脑能不能连接起来呢？你会说，当然能啊，买个路由器不就行了。\n\n现在一台家用路由器非常便宜，一百多块的事情。那时候路由器绝对是奢侈品。一直到大四，我们宿舍都没有买路由器。可能是因为那时候技术没有现在这么发达，导致我对网络技术的认知是逐渐深入的，而且每一层都是实实在在接触到的。\n\n## 第一层（物理层）\n\n使用路由器，是在第三层上。我们先从第一层物理层开始说。\n\n物理层能折腾啥？现在的同学可能想不到，我们当时去学校配电脑的地方买网线，卖网线的师傅都会问，你的网线是要电脑连电脑啊，还是电脑连网口啊？\n\n我们要的是电脑连电脑。这种方式就是一根网线，有两个头。一头插在一台电脑的网卡上，另一头插在另一台电脑的网卡上。但是在当时，普通的网线这样是通不了的，所以水晶头要做交叉线，用的就是所谓的**1－3**、**2－6交叉接法**。\n\n水晶头的第1、2和第3、6脚，它们分别起着收、发信号的作用。将一端的1号和3号线、2号和6号线互换一下位置，就能够在物理层实现一端发送的信号，另一端能收到。\n\n当然电脑连电脑，除了网线要交叉，还需要配置这两台电脑的IP地址、子网掩码和默认网关。这三个概念上一节详细描述过了。要想两台电脑能够通信，这三项必须配置成为一个网络，可以一个是192.168.0.1/24，另一个是192.168.0.2/24，否则是不通的。\n\n这里我想问你一个问题，两台电脑之间的网络包，包含MAC层吗？当然包含，要完整。IP层要封装了MAC层才能将包放入物理层。\n\n到此为止，两台电脑已经构成了一个最小的**局域网**，也即**LAN。**可以玩联机局域网游戏啦！\n\n等到第三个哥们也买了一台电脑，怎么把三台电脑连在一起呢？\n\n先别说交换机，当时交换机也贵。有一个叫作**Hub**的东西，也就是**集线器**。这种设备有多个口，可以将宿舍里的多台电脑连接起来。但是，和交换机不同，集线器没有大脑，它完全在物理层工作。它会将自己收到的每一个字节，都复制到其他端口上去。这是第一层物理层联通的方案。\n\n## 第二层（数据链路层）\n\n你可能已经发现问题了。Hub采取的是广播的模式，如果每一台电脑发出的包，宿舍的每个电脑都能收到，那就麻烦了。这就需要解决几个问题：\n\n1. 这个包是发给谁的？谁应该接收？\n2. 大家都在发，会不会产生混乱？有没有谁先发、谁后发的规则？\n3. 如果发送的时候出现了错误，怎么办？\n\n这几个问题，都是第二层，数据链路层，也即MAC层要解决的问题。**MAC**的全称是**Medium Access Control**，即**媒体访问控制。控制什么呢？其实就是控制在往媒体上发数据的时候，谁先发、谁后发的问题。防止发生混乱。这解决的是第二个问题。这个问题中的规则，学名叫多路访问**。有很多算法可以解决这个问题。就像车管所管束马路上跑的车，能想的办法都想过了。\n\n比如接下来这三种方式：\n\n- 方式一：分多个车道。每个车一个车道，你走你的，我走我的。这在计算机网络里叫作**信道划分；**\n- 方式二：今天单号出行，明天双号出行，轮着来。这在计算机网络里叫作**轮流协议；**\n- 方式三：不管三七二十一，有事儿先出门，发现特堵，就回去。错过高峰再出。我们叫作**随机接入协议。**著名的以太网，用的就是这个方式。\n\n解决了第二个问题，就是解决了媒体接入控制的问题，MAC的问题也就解决好了。这和MAC地址没什么关系。\n\n接下来要解决第一个问题：发给谁，谁接收？这里用到一个物理地址，叫作**链路层地址。但是因为第二层主要解决媒体接入控制的问题，所以它常被称为MAC地址**。\n\n解决第一个问题就牵扯到第二层的网络包**格式**。对于以太网，第二层的最开始，就是目标的MAC地址和源的MAC地址。\n\n![img](cef93d665ca863fef40f7f854d5d33ed-20230115031641262.jpg)\n\n接下来是**类型**，大部分的类型是IP数据包，然后IP里面包含TCP、UDP，以及HTTP等，这都是里层封装的事情。\n\n有了这个目标MAC地址，数据包在链路上广播，MAC的网卡才能发现，这个包是给它的。MAC的网卡把包收进来，然后打开IP包，发现IP地址也是自己的，再打开TCP包，发现端口是自己，也就是80，而nginx就是监听80。\n\n于是将请求提交给nginx，nginx返回一个网页。然后将网页需要发回请求的机器。然后层层封装，最后到MAC层。因为来的时候有源MAC地址，返回的时候，源MAC就变成了目标MAC，再返给请求的机器。\n\n对于以太网，第二层的最后面是**CRC**，也就是**循环冗余检测**。通过XOR异或的算法，来计算整个包是否在发送的过程中出现了错误，主要解决第三个问题。\n\n这里还有一个没有解决的问题，当源机器知道目标机器的时候，可以将目标地址放入包里面，如果不知道呢？一个广播的网络里面接入了N台机器，我怎么知道每个MAC地址是谁呢？这就是**ARP协议**，也就是已知IP地址，求MAC地址的协议。\n\n![img](17ac2f46ef531e2b4380300f10267e3d-20230115031641259.jpg)\n\n在一个局域网里面，当知道了IP地址，不知道MAC怎么办呢？靠“吼”。\n\n![img](5fe88a40a8b5d507601968efb50ac668-20230115031641343.jpg)\n\n广而告之，发送一个广播包，谁是这个IP谁来回答。具体询问和回答的报文就像下面这样：\n\n![img](2bc53afb25515e96d0e646e297b1ce2f-20230115031641263.jpg)\n\n为了避免每次都用ARP请求，机器本地也会进行ARP缓存。当然机器会不断地上线下线，IP也可能会变，所以ARP的MAC地址缓存过一段时间就会过期。\n\n## 局域网\n\n好了，至此我们宿舍四个电脑就组成了一个局域网。用Hub连接起来，就可以玩局域网版的《魔兽争霸》了。\n\n![img](33d180e376439ca10e3f126eb2e36bac-20230115031641367.jpg)\n\n打开游戏，进入“局域网选项”，选择一张地图，点击“创建游戏”，就可以进入这张地图的房间中。等同一个局域网里的其他小伙伴加入后，游戏就可以开始了。\n\n这种组网的方法，对一个宿舍来说没有问题，但是一旦机器数目增多，问题就出现了。因为Hub是广播的，不管某个接口是否需要，所有的Bit都会被发送出去，然后让主机来判断是不是需要。这种方式路上的车少就没问题，车一多，产生冲突的概率就提高了。而且把不需要的包转发过去，纯属浪费。看来Hub这种不管三七二十一都转发的设备是不行了，需要点儿智能的。因为每个口都只连接一台电脑，这台电脑又不怎么换IP和MAC地址，只要记住这台电脑的MAC地址，如果目标MAC地址不是这台电脑的，这个口就不用转发了。\n\n谁能知道目标MAC地址是否就是连接某个口的电脑的MAC地址呢？这就需要一个能把MAC头拿下来，检查一下目标MAC地址，然后根据策略转发的设备，按第二节课中讲过的，这个设备显然是个二层设备，我们称为**交换机**。\n\n交换机怎么知道每个口的电脑的MAC地址呢？这需要交换机会学习。\n\n一台MAC1电脑将一个包发送给另一台MAC2电脑，当这个包到达交换机的时候，一开始交换机也不知道MAC2的电脑在哪个口，所以没办法，它只能将包转发给除了来的那个口之外的其他所有的口。但是，这个时候，交换机会干一件非常聪明的事情，就是交换机会记住，MAC1是来自一个明确的口。以后有包的目的地址是MAC1的，直接发送到这个口就可以了。\n\n当交换机作为一个关卡一样，过了一段时间之后，就有了整个网络的一个结构了，这个时候，基本上不用广播了，全部可以准确转发。当然，每个机器的IP地址会变，所在的口也会变，因而交换机上的学习的结果，我们称为**转发表**，是有一个过期时间的。\n\n有了交换机，一般来说，你接个几十台、上百台机器打游戏，应该没啥问题。你可以组个战队了。能上网了，就可以玩网游了。\n\n这里，给你推荐一个课程，极客时间新上线了**《从0开始学游戏开发》**，由原网易游戏引擎架构师、资深底层技术专家蔡能老师，手把手带你梳理游戏开发的流程和细节，为你剖析热门游戏的成功之道。帮助普通程序员成为游戏开发工程师，步入游戏开发之路。**你可以点击文末的图片进入课程。**\n\n## 小结\n\n好了，今天的内容差不多了，我们来总结一下，有三个重点需要你记住：\n\n第一，MAC层是用来解决多路访问的堵车问题的；\n\n第二，ARP是通过吼的方式来寻找目标MAC地址的，吼完之后记住一段时间，这个叫作缓存；\n\n第三，交换机是有MAC地址学习能力的，学完了它就知道谁在哪儿了，不用广播了。\n\n最后，给你留两个思考题吧。\n\n1. 在二层中我们讲了ARP协议，即已知IP地址求MAC；还有一种RARP协议，即已知MAC求IP的，你知道它可以用来干什么吗？\n2. 如果一个局域网里面有多个交换机，ARP广播的模式会出现什么问题呢？\n\n# 06 讲交换机与VLAN：办公室太复杂，我要回学校\n\n上一次，我们在宿舍里组建了一个本地的局域网LAN，可以愉快地玩游戏了。这是一个非常简单的场景，因为只有一台交换机，电脑数目很少。今天，让我们切换到一个稍微复杂一点的场景，办公室。\n\n## 拓扑结构是怎么形成的？\n\n我们常见到的办公室大多是一排排的桌子，每个桌子都有网口，一排十几个座位就有十几个网口，一个楼层就会有几十个甚至上百个网口。如果算上所有楼层，这个场景自然比你宿舍里的复杂多了。具体哪里复杂呢？我来给你具体讲解。\n\n首先，这个时候，一个交换机肯定不够用，需要多台交换机，交换机之间连接起来，就形成一个稍微复杂的**拓扑结构**。\n\n我们先来看**两台交换机**的情形。两台交换机连接着三个局域网，每个局域网上都有多台机器。如果机器1只知道机器4的IP地址，当它想要访问机器4，把包发出去的时候，它必须要知道机器4的MAC地址。\n\n![img](7a40046c5a2c7f7cd3c95b54488b9773-20230115031641292.jpg)\n\n于是机器1发起广播，机器2收到这个广播，但是这不是找它的，所以没它什么事。交换机A一开始是不知道任何拓扑信息的，在它收到这个广播后，采取的策略是，除了广播包来的方向外，它还要转发给其他所有的网口。于是机器3也收到广播信息了，但是这和它也没什么关系。\n\n当然，交换机B也是能够收到广播信息的，但是这时候它也是不知道任何拓扑信息的，因而也是进行广播的策略，将包转发到局域网三。这个时候，机器4和机器5都收到了广播信息。机器4主动响应说，这是找我的，这是我的MAC地址。于是一个ARP请求就成功完成了。\n\n在上面的过程中，交换机A和交换机B都是能够学习到这样的信息：机器1是在左边这个网口的。当了解到这些拓扑信息之后，情况就好转起来。当机器2要访问机器1的时候，机器2并不知道机器1的MAC地址，所以机器2会发起一个ARP请求。这个广播消息会到达机器1，也同时会到达交换机A。这个时候交换机A已经知道机器1是不可能在右边的网口的，所以这个广播信息就不会广播到局域网二和局域网三。\n\n当机器3要访问机器1的时候，也需要发起一个广播的ARP请求。这个时候交换机A和交换机B都能够收到这个广播请求。交换机A当然知道主机A是在左边这个网口的，所以会把广播消息转发到局域网一。同时，交换机B收到这个广播消息之后，由于它知道机器1是不在右边这个网口的，所以不会将消息广播到局域网三。\n\n## 如何解决常见的环路问题？\n\n这样看起来，两台交换机工作得非常好。随着办公室越来越大，交换机数目肯定越来越多。当整个拓扑结构复杂了，这么多网线，绕过来绕过去，不可避免地会出现一些意料不到的情况。其中常见的问题就是**环路问题**。\n\n例如这个图，当两个交换机将两个局域网同时连接起来的时候。你可能会觉得，这样反而有了高可用性。但是却不幸地出现了环路。出现了环路会有什么结果呢？\n\n![img](c829b28978c3d9686680e4b62fdf53d2-20230115031641375.jpg)\n\n我们来想象一下机器1访问机器2的过程。一开始，机器1并不知道机器2的MAC地址，所以它需要发起一个ARP的广播。广播到达机器2，机器2会把MAC地址返回来，看起来没有这两个交换机什么事情。\n\n但是问题来了，这两个交换机还是都能够收到广播包的。交换机A一开始是不知道机器2在哪个局域网的，所以它会把广播消息放到局域网二，在局域网二广播的时候，交换机B右边这个网口也是能够收到广播消息的。交换机B会将这个广播息信息发送到局域网一。局域网一的这个广播消息，又会到达交换机A左边的这个接口。交换机A这个时候还是不知道机器2在哪个局域网，于是将广播包又转发到局域网二。左转左转左转，好像是个圈哦。\n\n可能有人会说，当两台交换机都能够逐渐学习到拓扑结构之后，是不是就可以了？\n\n别想了，压根儿学不会的。机器1的广播包到达交换机A和交换机B的时候，本来两个交换机都学会了机器1是在局域网一的，但是当交换机A将包广播到局域网二之后，交换机B右边的网口收到了来自交换机A的广播包。根据学习机制，这彻底损坏了交换机B的三观，刚才机器1还在左边的网口呢，怎么又出现在右边的网口呢？哦，那肯定是机器1换位置了，于是就误会了，交换机B就学会了，机器1是从右边这个网口来的，把刚才学习的那一条清理掉。同理，交换机A右边的网口，也能收到交换机B转发过来的广播包，同样也误会了，于是也学会了，机器1从右边的网口来，不是从左边的网口来。\n\n然而当广播包从左边的局域网一广播的时候，两个交换机再次刷新三观，原来机器1是在左边的，过一会儿，又发现不对，是在右边的，过一会，又发现不对，是在左边的。\n\n这还是一个包转来转去，每台机器都会发广播包，交换机转发也会复制广播包，当广播包越来越多的时候，按照上一节讲过一个共享道路的算法，也就是路会越来越堵，最后谁也别想走。所以，必须有一个方法解决环路的问题，怎么破除环路呢？\n\n## STP协议中那些难以理解的概念\n\n在数据结构中，有一个方法叫作**最小生成树**。有环的我们常称为**图**。将图中的环破了，就生成了**树**。在计算机网络中，生成树的算法叫作**STP**，全称**Spanning Tree Protocol**。\n\nSTP协议比较复杂，一开始很难看懂，但是其实这是一场血雨腥风的武林比武或者华山论剑，最终决出五岳盟主的方式。\n\n![img](5d3ba40babdacc735f617cc2356fefba-20230115031641381.jpg)\n\n在STP协议里面有很多概念，译名就非常拗口，但是我一作比喻，你很容易就明白了。\n\n- **Root Bridge**，也就是**根交换机**。这个比较容易理解，可以比喻为“掌门”交换机，是某棵树的老大，是掌门，最大的大哥。\n- **Designated Bridges**，有的翻译为**指定交换机**。这个比较难理解，可以想像成一个“小弟”，对于树来说，就是一棵树的树枝。所谓“指定”的意思是，我拜谁做大哥，其他交换机通过这个交换机到达根交换机，也就相当于拜他做了大哥。这里注意是树枝，不是叶子，因为叶子往往是主机。\n- **Bridge Protocol Data Units （BPDU）** ，**网桥协议数据单元**。可以比喻为“相互比较实力”的协议。行走江湖，比的就是武功，拼的就是实力。当两个交换机碰见的时候，也就是相连的时候，就需要互相比一比内力了。BPDU只有掌门能发，已经隶属于某个掌门的交换机只能传达掌门的指示。\n- **Priority Vector**，**优先级向量**。可以比喻为实力 （值越小越牛）。实力是啥？就是一组ID数目，[Root Bridge ID, Root Path Cost, Bridge ID, and Port ID]。为什么这样设计呢？这是因为要看怎么来比实力。先看Root Bridge ID。拿出老大的ID看看，发现掌门一样，那就是师兄弟；再比Root Path Cost，也即我距离我的老大的距离，也就是拿和掌门关系比，看同一个门派内谁和老大关系铁；最后比Bridge ID，比我自己的ID，拿自己的本事比。\n\n## STP的工作过程是怎样的？\n\n接下来，我们来看STP的工作过程。\n\n一开始，江湖纷争，异常混乱。大家都觉得自己是掌门，谁也不服谁。于是，所有的交换机都认为自己是掌门，每个网桥都被分配了一个ID。这个ID里有管理员分配的优先级，当然网络管理员知道哪些交换机贵，哪些交换机好，就会给它们分配高的优先级。这种交换机生下来武功就很高，起步就是乔峰。\n\n![img](2666530a121ff1d1b079a330427efb54-20230115031641383.jpg)\n\n既然都是掌门，互相都连着网线，就互相发送BPDU来比功夫呗。这一比就发现，有人是岳不群，有人是封不平，赢的接着当掌门，输的就只好做小弟了。当掌门的还会继续发BPDU，而输的人就没有机会了。它们只有在收到掌门发的BPDU的时候，转发一下，表示服从命令。\n\n![img](e7c0deeff171c9eb7e95dde2c73fe011-20230115031641411.jpg)\n\n数字表示优先级。就像这个图，5和6碰见了，6的优先级低，所以乖乖做小弟。于是一个小门派形成，5是掌门，6是小弟。其他诸如1-7、2-8、3-4这样的小门派，也诞生了。于是江湖出现了很多小的门派，小的门派，接着合并。\n\n合并的过程会出现以下四种情形，我分别来介绍。\n\n### 情形一：掌门遇到掌门\n\n当5碰到了1，掌门碰见掌门，1觉得自己是掌门，5也刚刚跟别人PK完成为掌门。这俩掌门比较功夫，最终1胜出。于是输掉的掌门5就会率领所有的小弟归顺。结果就是1成为大掌门。\n\n![img](24fdcb9a11c5998e80738f502b2eff11-20230115031641453.jpg)\n\n### 情形二：同门相遇\n\n同门相遇可以是掌门与自己的小弟相遇，这说明存在“环”了。这个小弟已经通过其他门路拜在你门下，结果你还不认识，就PK了一把。结果掌门发现这个小弟功夫不错，不应该级别这么低，就把它招到门下亲自带，那这个小弟就相当于升职了。\n\n我们再来看，假如1和6相遇。6原来就拜在1的门下，只不过6的上司是5，5的上司是1。1发现，6距离我才只有2，比从5这里过来的5（=4+1）近多了，那6就直接汇报给我吧。于是，5和6分别汇报给1。\n\n![img](899077ce9678c84d8b1a265ced5efb90-20230115031641430.jpg)\n\n同门相遇还可以是小弟相遇。这个时候就要比较谁和掌门的关系近，当然近的当大哥。刚才5和6同时汇报给1了，后来5和6再比较功夫的时候发现，5你直接汇报给1距离是4，如果5汇报给6再汇报给1，距离只有2+1=3，所以5干脆拜6为上司。\n\n### 情形三：掌门与其他帮派小弟相遇\n\n小弟拿本帮掌门和这个掌门比较，赢了，这个掌门拜入门来。输了，会拜入新掌门，并且逐渐拉拢和自己连接的兄弟，一起弃暗投明。\n\n![img](dadcac2d0f6ccb36b065aece06b89813-20230115031641468.jpg)\n\n例如，2和7相遇，虽然7是小弟，2是掌门。就个人武功而言，2比7强，但是7的掌门是1，比2牛，所以没办法，2要拜入7的门派，并且连同自己的小弟都一起拜入。\n\n### 情形四：不同门小弟相遇\n\n各自拿掌门比较，输了的拜入赢的门派，并且逐渐将与自己连接的兄弟弃暗投明。\n\n![img](2d524bfa633e830a27c0a34aad28e609-20230115031641506.jpg)\n\n例如，5和4相遇。虽然4的武功好于5，但是5的掌门是1，比4牛，于是4拜入5的门派。后来当3和4相遇的时候，3发现4已经叛变了，4说我现在老大是1，比你牛，要不你也来吧，于是3也拜入1。\n\n最终，生成一棵树，武林一统，天下太平。但是天下大势，分久必合，合久必分，天下统一久了，也会有相应的问题。\n\n### 如何解决广播问题和安全问题？\n\n毕竟机器多了，交换机也多了，就算交换机比Hub智能一些，但是还是难免有广播的问题，一大波机器，相关的部门、不相关的部门，广播一大堆，性能就下来了。就像一家公司，创业的时候，一二十个人，坐在一个会议室，有事情大家讨论一下，非常方便。但是如果变成了50个人，全在一个会议室里面吵吵，就会乱的不得了。\n\n你们公司有不同的部门，有的部门需要保密的，比如人事部门，肯定要讨论升职加薪的事儿。由于在同一个广播域里面，很多包都会在一个局域网里面飘啊飘，碰到了一个会抓包的程序员，就能抓到这些包，如果没有加密，就能看到这些敏感信息了。还是上面的例子，50个人在一个会议室里面七嘴八舌的讨论，其中有两个HR，那他们讨论的问题，肯定被其他人偷偷听走了。\n\n那咋办，分部门，分会议室呗。那我们就来看看怎么分。\n\n有两种分的方法，一个是**物理隔离**。每个部门设一个单独的会议室，对应到网络方面，就是每个部门有单独的交换机，配置单独的子网，这样部门之间的沟通就需要路由器了。路由器咱们还没讲到，以后再说。这样的问题在于，有的部门人多，有的部门人少。人少的部门慢慢人会变多，人多的部门也可能人越变越少。如果每个部门有单独的交换机，口多了浪费，少了又不够用。\n\n另外一种方式是**虚拟隔离**，就是用我们常说的**VLAN**，或者叫**虚拟局域网**。使用VLAN，一个交换机上会连属于多个局域网的机器，那交换机怎么区分哪个机器属于哪个局域网呢？\n\n![img](2ede82f511ccac2570c17a62ffc749ed-20230115031641458.jpg)\n\n我们只需要在原来的二层的头上加一个TAG，里面有一个VLAN ID，一共12位。为什么是12位呢？因为12位可以划分4096个VLAN。这样是不是还不够啊。现在的情况证明，目前云计算厂商里面绝对不止4096个用户。当然每个用户需要一个VLAN了啊，怎么办呢，这个我们在后面的章节再说。\n\n如果我们买的交换机是支持VLAN的，当这个交换机把二层的头取下来的时候，就能够识别这个VLAN ID。这样只有相同VLAN的包，才会互相转发，不同VLAN的包，是看不到的。这样广播问题和安全问题就都能够解决了。\n\n![img](a134027334616274cf27f18841f7504c-20230115031641539.jpg)\n\n我们可以设置交换机每个口所属的VLAN。如果某个口坐的是程序员，他们属于VLAN 10；如果某个口坐的是人事，他们属于VLAN 20；如果某个口坐的是财务，他们属于VLAN 30。这样，财务发的包，交换机只会转发到VLAN 30的口上。程序员啊，你就监听VLAN 10吧，里面除了代码，啥都没有。\n\n而且对于交换机来讲，每个VLAN的口都是可以重新设置的。一个财务走了，把他所在的作为的口从VLAN 30移除掉，来了一个程序员，坐在财务的位置上，就把这个口设置为VLAN 10，十分灵活。\n\n有人会问交换机之间怎么连接呢？将两个交换机连接起来的口应该设置成什么VLAN呢？对于支持VLAN的交换机，有一种口叫作**Trunk口**。它可以转发属于任何VLAN的口。交换机之间可以通过这种口相互连接。\n\n好了，解决这么多交换机连接在一起的问题，办公室的问题似乎搞定了。然而这只是一般复杂的场景，因为你能接触到的网络，到目前为止，不管是你的台式机，还是笔记本所连接的网络，对于带宽、高可用等都要求不高。就算出了问题，一会儿上不了网，也不会有什么大事。\n\n我们在宿舍、学校或者办公室，经常会访问一些网站，这些网站似乎永远不会“挂掉”。那是因为这些网站都生活在一个叫做数据中心的地方，那里的网络世界更加复杂。在后面的章节，我会为你详细讲解。\n\n## 小结\n\n好了，这节就到这里，我们这里来总结一下：\n\n- 当交换机的数目越来越多的时候，会遭遇环路问题，让网络包迷路，这就需要使用STP协议，通过华山论剑比武的方式，将有环路的图变成没有环路的树，从而解决环路问题。\n- 交换机数目多会面临隔离问题，可以通过VLAN形成虚拟局域网，从而解决广播问题和安全问题。\n\n最后，给你留两个思考题。\n\n1. STP协议能够很好的解决环路问题，但是也有它的缺点，你能举几个例子吗？\n2. 在一个比较大的网络中，如果两台机器不通，你知道应该用什么方式调试吗？\n\n# 07 讲ICMP与ping：投石问路的侦察兵\n\n无论是在宿舍，还是在办公室，或者运维一个数据中心，我们常常会遇到网络不通的问题。那台机器明明就在那里，你甚至都可以通过机器的终端连上去看。它看着好好的，可是就是连不上去，究竟是哪里出了问题呢？\n\n## ICMP协议的格式\n\n一般情况下，你会想到ping一下。那你知道ping是如何工作的吗？\n\nping是基于ICMP协议工作的。**ICMP**全称**Internet Control Message Protocol**，就是**互联网控制报文协议**。这里面的关键词是“控制”，那具体是怎么控制的呢？\n\n网络包在异常复杂的网络环境中传输时，常常会遇到各种各样的问题。当遇到问题的时候，总不能“死个不明不白”，要传出消息来，报告情况，这样才可以调整传输策略。这就相当于我们经常看到的电视剧里，古代行军的时候，为将为帅者需要通过侦察兵、哨探或传令兵等人肉的方式来掌握情况，控制整个战局。\n\nICMP报文是封装在IP包里面的。因为传输指令的时候，肯定需要源地址和目标地址。它本身非常简单。因为作为侦查兵，要轻装上阵，不能携带大量的包袱。\n\n![img](23aecf653d60dd94b7c5c6dc21ca21ff-20230115031641487.jpg)\n\nICMP报文有很多的类型，不同的类型有不同的代码。**最常用的类型是主动请求为8，主动请求的应答为0**。\n\n## 查询报文类型\n\n我们经常在电视剧里听到这样的话：主帅说，来人哪！前方战事如何，快去派人打探，一有情况，立即通报！\n\n这种是主帅发起的，主动查看敌情，对应ICMP的**查询报文类型**。例如，常用的**ping就是查询报文，是一种主动请求，并且获得主动应答的ICMP协议。**所以，ping发的包也是符合ICMP协议格式的，只不过它在后面增加了自己的格式。\n\n对ping的主动请求，进行网络抓包，称为**ICMP ECHO REQUEST。同理主动请求的回复，称为ICMP ECHO REPLY**。比起原生的ICMP，这里面多了两个字段，一个是**标识符**。这个很好理解，你派出去两队侦查兵，一队是侦查战况的，一队是去查找水源的，要有个标识才能区分。另一个是**序号**，你派出去的侦查兵，都要编个号。如果派出去10个，回来10个，就说明前方战况不错；如果派出去10个，回来2个，说明情况可能不妙。\n\n在选项数据中，ping还会存放发送请求的时间值，来计算往返时间，说明路程的长短。\n\n## 差错报文类型\n\n当然也有另外一种方式，就是差错报文。\n\n主帅骑马走着走着，突然来了一匹快马，上面的小兵气喘吁吁的：报告主公，不好啦！张将军遭遇埋伏，全军覆没啦！这种是异常情况发起的，来报告发生了不好的事情，对应ICMP的**差错报文类型**。\n\n我举几个ICMP差错报文的例子：**终点不可达为3，源抑制为4，超时为11，重定向为5**。这些都是什么意思呢？我给你具体解释一下。\n\n**第一种是终点不可达**。小兵：报告主公，您让把粮草送到张将军那里，结果没有送到。\n\n如果你是主公，你肯定会问，为啥送不到？具体的原因在代码中表示就是，网络不可达代码为0，主机不可达代码为1，协议不可达代码为2，端口不可达代码为3，需要进行分片但设置了不分片位代码为4。\n\n具体的场景就像这样：\n\n- 网络不可达：主公，找不到地方呀？\n- 主机不可达：主公，找到地方没这个人呀？\n- 协议不可达：主公，找到地方，找到人，口号没对上，人家天王盖地虎，我说12345！\n- 端口不可达：主公，找到地方，找到人，对了口号，事儿没对上，我去送粮草，人家说他们在等救兵。\n- 需要进行分片但设置了不分片位：主公，走到一半，山路狭窄，想换小车，但是您的将令，严禁换小车，就没办法送到了。\n\n**第二种是源站抑制**，也就是让源站放慢发送速度。小兵：报告主公，您粮草送的太多了吃不完。\n\n**第三种是时间超时**，也就是超过网络包的生存时间还是没到。小兵：报告主公，送粮草的人，自己把粮草吃完了，还没找到地方，已经饿死啦。\n\n**第四种是路由重定向**，也就是让下次发给另一个路由器。小兵：报告主公，上次送粮草的人本来只要走一站地铁，非得从五环绕，下次别这样了啊。\n\n差错报文的结构相对复杂一些。除了前面还是IP，ICMP的前8字节不变，后面则跟上出错的那个IP包的IP头和IP正文的前8个字节。\n\n而且这类侦查兵特别恪尽职守，不但自己返回来报信，还把一部分遗物也带回来。\n\n- 侦察兵：报告主公，张将军已经战死沙场，这是张将军的印信和佩剑。\n- 主公：神马？张将军是怎么死的（可以查看ICMP的前8字节）？没错，这是张将军的剑，是他的剑（IP数据包的头及正文前8字节）。\n\n## ping：查询报文类型的使用\n\n接下来，我们重点来看ping的发送和接收过程。\n\n![img](e5270427819fc51c88e81a5c1cc4b8fc-20230115031641878.jpg)\n\n假定主机A的IP地址是192.168.1.1，主机B的IP地址是192.168.1.2，它们都在同一个子网。那当你在主机A上运行“ping 192.168.1.2”后，会发生什么呢?\n\nping命令执行的时候，源主机首先会构建一个ICMP请求数据包，ICMP数据包内包含多个字段。最重要的是两个，第一个是**类型字段**，对于请求数据包而言该字段为 8；另外一个是**顺序号**，主要用于区分连续ping的时候发出的多个数据包。每发出一个请求数据包，顺序号会自动加1。为了能够计算往返时间RTT，它会在报文的数据部分插入发送时间。\n\n然后，由ICMP协议将这个数据包连同地址192.168.1.2一起交给IP层。IP层将以192.168.1.2作为目的地址，本机IP地址作为源地址，加上一些其他控制信息，构建一个IP数据包。\n\n接下来，需要加入MAC头。如果在本节ARP映射表中查找出IP地址192.168.1.2所对应的MAC地址，则可以直接使用；如果没有，则需要发送ARP协议查询MAC地址，获得MAC地址后，由数据链路层构建一个数据帧，目的地址是IP层传过来的MAC地址，源地址则是本机的MAC地址；还要附加上一些控制信息，依据以太网的介质访问规则，将它们传送出去。\n\n主机B收到这个数据帧后，先检查它的目的MAC地址，并和本机的MAC地址对比，如符合，则接收，否则就丢弃。接收后检查该数据帧，将IP数据包从帧中提取出来，交给本机的IP层。同样，IP层检查后，将有用的信息提取后交给ICMP协议。\n\n主机B会构建一个 ICMP 应答包，应答数据包的类型字段为 0，顺序号为接收到的请求数据包中的顺序号，然后再发送出去给主机A。\n\n在规定的时候间内，源主机如果没有接到 ICMP 的应答包，则说明目标主机不可达；如果接收到了 ICMP 应答包，则说明目标主机可达。此时，源主机会检查，用当前时刻减去该数据包最初从源主机上发出的时刻，就是 ICMP 数据包的时间延迟。\n\n当然这只是最简单的，同一个局域网里面的情况。如果跨网段的话，还会涉及网关的转发、路由器的转发等等。但是对于ICMP的头来讲，是没什么影响的。会影响的是根据目标IP地址，选择路由的下一跳，还有每经过一个路由器到达一个新的局域网，需要换MAC头里面的MAC地址。这个过程后面几节会详细描述，这里暂时不多说。\n\n如果在自己的可控范围之内，当遇到网络不通的问题的时候，除了直接ping目标的IP地址之外，还应该有一个清晰的网络拓扑图。并且从理论上来讲，应该要清楚地知道一个网络包从源地址到目标地址都需要经过哪些设备，然后逐个ping中间的这些设备或者机器。如果可能的话，在这些关键点，通过tcpdump -i eth0 icmp，查看包有没有到达某个点，回复的包到达了哪个点，可以更加容易推断出错的位置。\n\n经常会遇到一个问题，如果不在我们的控制范围内，很多中间设备都是禁止ping的，但是ping不通不代表网络不通。这个时候就要使用telnet，通过其他协议来测试网络是否通，这个就不在本篇的讲述范围了。\n\n说了这么多，你应该可以看出ping这个程序是使用了ICMP里面的ECHO REQUEST和ECHO REPLY类型的。\n\n## Traceroute：差错报文类型的使用\n\n那其他的类型呢？是不是只有真正遇到错误的时候，才能收到呢？那也不是，有一个程序Traceroute，是个“大骗子”。它会使用ICMP的规则，故意制造一些能够产生错误的场景。\n\n所以，**Traceroute的第一个作用就是故意设置特殊的TTL，来追踪去往目的地时沿途经过的路由器**。Traceroute的参数指向某个目的IP地址，它会发送一个UDP的数据包。将TTL设置成1，也就是说一旦遇到一个路由器或者一个关卡，就表示它“牺牲”了。\n\n如果中间的路由器不止一个，当然碰到第一个就“牺牲”。于是，返回一个ICMP包，也就是网络差错包，类型是时间超时。那大军前行就带一顿饭，试一试走多远会被饿死，然后找个哨探回来报告，那我就知道大军只带一顿饭能走多远了。\n\n接下来，将TTL设置为2。第一关过了，第二关就“牺牲”了，那我就知道第二关有多远。如此反复，直到到达目的主机。这样，Traceroute就拿到了所有的路由器IP。当然，有的路由器压根不会回这个ICMP。这也是Traceroute一个公网的地址，看不到中间路由的原因。\n\n怎么知道UDP有没有到达目的主机呢？Traceroute程序会发送一份UDP数据报给目的主机，但它会选择一个不可能的值作为UDP端口号（大于30000）。当该数据报到达时，将使目的主机的 UDP模块产生一份“端口不可达”错误ICMP报文。如果数据报没有到达，则可能是超时。\n\n这就相当于故意派人去西天如来那里去请一本《道德经》，结果人家信佛不信道，消息就会被打出来。被打的消息传回来，你就知道西天是能够到达的。为什么不去取《心经》呢？因为UDP是无连接的。也就是说这人一派出去，你就得不到任何音信。你无法区别到底是半路走丢了，还是真的信佛遁入空门了，只有让人家打出来，你才会得到消息。\n\n**Traceroute还有一个作用是故意设置不分片，从而确定路径的MTU。**要做的工作首先是发送分组，并设置“不分片”标志。发送的第一个分组的长度正好与出口MTU相等。如果中间遇到窄的关口会被卡住，会发送ICMP网络差错包，类型为“需要进行分片但设置了不分片位”。其实，这是人家故意的好吧，每次收到ICMP“不能分片”差错时就减小分组的长度，直到到达目标主机。\n\n## 小结\n\n好了，这一节内容差不多了，我来总结一下：\n\n- ICMP相当于网络世界的侦察兵。我讲了两种类型的ICMP报文，一种是主动探查的查询报文，一种异常报告的差错报文；\n- ping使用查询报文，Traceroute使用差错报文。\n\n最后，给你留两个思考题吧。\n\n1. 当发送的报文出问题的时候，会发送一个ICMP的差错报文来报告错误，但是如果ICMP的差错报文也出问题了呢？\n2. 这一节只说了一个局域网互相ping的情况。如果跨路由器、跨网关的过程会是什么样的呢？\n\n# 08 讲世界这么大，我想出网关：欧洲十国游与玄奘西行\n\n前几节，我主要跟你讲了宿舍里和办公室里用到的网络协议。你已经有了一些基础，是时候去外网逛逛了！\n\n## 怎么在宿舍上网？\n\n还记得咱们在宿舍的时候买了台交换机，几台机器组了一个局域网打游戏吗？可惜啊，只能打局域网的游戏，不能上网啊！盼啊盼啊，终于盼到大二，允许宿舍开通网络了。学校给每个宿舍的网口分配了一个IP地址。这个IP是校园网的IP，完全由网管部门控制。宿舍网的IP地址多为192.168.1.x。校园网的IP地址，假设是10.10.x.x。\n\n这个时候，你要在宿舍上网，有两个办法：\n\n第一个办法，让你们宿舍长再买一个网卡。这个时候，你们宿舍长的电脑里就有两张网卡。一张网卡的线插到你们宿舍的交换机上，另一张网卡的线插到校园网的网口。而且，这张新的网卡的IP地址要按照学校网管部门分配的配置，不然上不了网。**这种情况下，如果你们宿舍的人要上网，就需要一直开着宿舍长的电脑。**\n\n第二个办法，你们共同出钱买个家庭路由器（反正当时我们买不起）。家庭路由器会有内网网口和外网网口。把外网网口的线插到校园网的网口上，将这个外网网口配置成和网管部的一样。内网网口连上你们宿舍的所有的电脑。**这种情况下，如果你们宿舍的人要上网，就需要一直开着路由器。**\n\n这两种方法其实是一样的。只不过第一种方式，让你的宿舍长的电脑，变成一个有多个口的路由器而已。而你买的家庭路由器，里面也跑着程序，和你宿舍长电脑里的功能一样，只不过是一个嵌入式的系统。\n\n当你的宿舍长能够上网之后，接下来，就是其他人的电脑怎么上网的问题。这就需要配置你们的**网卡。当然DHCP是可以默认配置的。在进行网卡配置的时候，除了IP地址，还需要配置一个Gateway**的东西，这个就是**网关**。\n\n## 你了解MAC头和IP头的细节吗？\n\n一旦配置了IP地址和网关，往往就能够指定目标地址进行访问了。由于在跨网关访问的时候，牵扯到MAC地址和IP地址的变化，这里有必要详细描述一下MAC头和IP头的细节。\n\n![img](798467df661ecf5632d67b9c58bc53fc-20230115031641597.jpg)\n\n在MAC头里面，先是目标MAC地址，然后是源MAC地址，然后有一个协议类型，用来说明里面是IP协议。IP头里面的版本号，目前主流的还是IPv4，服务类型TOS在第三节讲ip addr命令的时候讲过，TTL在第7节讲ICMP协议的时候讲过。另外，还有8位标识协议。这里到了下一层的协议，也就是，是TCP还是UDP。最重要的就是源IP和目标IP。先是源IP地址，然后是目标IP地址。\n\n在任何一台机器上，当要访问另一个IP地址的时候，都会先判断，这个目标IP地址，和当前机器的IP地址，是否在同一个网段。怎么判断同一个网段呢？需要CIDR和子网掩码，这个在第三节的时候也讲过了。\n\n**如果是同一个网段**，例如，你访问你旁边的兄弟的电脑，那就没网关什么事情，直接将源地址和目标地址放入IP头中，然后通过ARP获得MAC地址，将源MAC和目的MAC放入MAC头中，发出去就可以了。\n\n**如果不是同一网段**，例如，你要访问你们校园网里面的BBS，该怎么办？这就需要发往默认网关Gateway。Gateway的地址一定是和源IP地址是一个网段的。往往不是第一个，就是第二个。例如192.168.1.0/24这个网段，Gateway往往会是192.168.1.1/24或者192.168.1.2/24。\n\n如何发往默认网关呢？网关不是和源IP地址是一个网段的么？这个过程就和发往同一个网段的其他机器是一样的：将源地址和目标IP地址放入IP头中，通过ARP获得网关的MAC地址，将源MAC和网关的MAC放入MAC头中，发送出去。网关所在的端口，例如192.168.1.1/24将网络包收进来，然后接下来怎么做，就完全看网关的了。\n\n**网关往往是一个路由器，是一个三层转发的设备。**啥叫三层设备？前面也说过了，就是把MAC头和IP头都取下来，然后根据里面的内容，看看接下来把包往哪里转发的设备。\n\n在你的宿舍里面，网关就是你宿舍长的电脑。一个路由器往往有多个网口，如果是一台服务器做这个事情，则就有多个网卡，其中一个网卡是和源IP同网段的。\n\n很多情况下，人们把网关就叫作路由器。其实不完全准确，而另一种比喻更加恰当：**路由器是一台设备，它有五个网口或者网卡，相当于有五只手，分别连着五个局域网。每只手的IP地址都和局域网的IP地址相同的网段，每只手都是它握住的那个局域网的网关。**\n\n任何一个想发往其他局域网的包，都会到达其中一只手，被拿进来，拿下MAC头和IP头，看看，根据自己的路由算法，选择另一只手，加上IP头和MAC头，然后扔出去。\n\n## 静态路由是什么？\n\n这个时候，问题来了，该选择哪一只手？IP头和MAC头加什么内容，哪些变、哪些不变呢？这个问题比较复杂，大致可以分为两类，一个是**静态路由**，一个是**动态路由**。动态路由下一节我们详细地讲。这一节我们先说静态路由。\n\n**静态路由，其实就是在路由器上，配置一条一条规则。**这些规则包括：想访问BBS站（它肯定有个网段），从2号口出去，下一跳是IP2；想访问教学视频站（它也有个自己的网段），从3号口出去，下一跳是IP3，然后保存在路由器里。\n\n每当要选择从哪只手抛出去的时候，就一条一条的匹配规则，找到符合的规则，就按规则中设置的那样，从某个口抛出去，找下一跳IPX。\n\n## IP头和MAC头哪些变、哪些不变？\n\n对于IP头和MAC头哪些变、哪些不变的问题，可以分两种类型。我把它们称为“**欧洲十国游”型**和“**玄奘西行”型**。\n\n之前我说过，MAC地址是一个局域网内才有效的地址。因而，MAC地址只要过网关，就必定会改变，因为已经换了局域网。两者主要的区别在于IP地址是否改变。不改变IP地址的网关，我们称为**转发网关；改变IP地址的网关，我们称为NAT网关**。\n\n### “欧洲十国游”型\n\n结合这个图，我们先来看“欧洲十国游”型。\n\n![img](d54cb2140c25831d6ec9b3e505796a8f-20230115031641547.jpg)\n\n服务器A要访问服务器B。首先，服务器A会思考，192.168.4.101和我不是一个网段的，因而需要先发给网关。那网关是谁呢？已经静态配置好了，网关是192.168.1.1。网关的MAC地址是多少呢？发送ARP获取网关的MAC地址，然后发送包。包的内容是这样的：\n\n- 源MAC：服务器A的MAC\n- 目标MAC：192.168.1.1这个网口的MAC\n- 源IP：192.168.1.101\n- 目标IP：192.168.4.101\n\n包到达192.168.1.1这个网口，发现MAC一致，将包收进来，开始思考往哪里转发。\n\n在路由器A中配置了静态路由之后，要想访问192.168.4.0/24，要从192.168.56.1这个口出去，下一跳为192.168.56.2。\n\n于是，路由器A思考的时候，匹配上了这条路由，要从192.168.56.1这个口发出去，发给192.168.56.2，那192.168.56.2的MAC地址是多少呢？路由器A发送ARP获取192.168.56.2的MAC地址，然后发送包。包的内容是这样的：\n\n- 源MAC：192.168.56.1的MAC地址\n- 目标MAC：192.168.56.2的MAC地址\n- 源IP：192.168.1.101\n- 目标IP：192.168.4.101\n\n包到达192.168.56.2这个网口，发现MAC一致，将包收进来，开始思考往哪里转发。\n\n在路由器B中配置了静态路由，要想访问192.168.4.0/24，要从192.168.4.1这个口出去，没有下一跳了。因为我右手这个网卡，就是这个网段的，我是最后一跳了。\n\n于是，路由器B思考的时候，匹配上了这条路由，要从192.168.4.1这个口发出去，发给192.168.4.101。那192.168.4.101的MAC地址是多少呢？路由器B发送ARP获取192.168.4.101的MAC地址，然后发送包。包的内容是这样的：\n\n- 源MAC：192.168.4.1的MAC地址\n- 目标MAC：192.168.4.101的MAC地址\n- 源IP：192.168.1.101\n- 目标IP：192.168.4.101\n\n包到达服务器B，MAC地址匹配，将包收进来。\n\n通过这个过程可以看出，每到一个新的局域网，MAC都是要变的，但是IP地址都不变。在IP头里面，不会保存任何网关的IP地址。**所谓的下一跳是，某个IP要将这个IP地址转换为MAC放入MAC头。**\n\n之所以将这种模式比喻称为欧洲十国游，是因为在整个过程中，IP头里面的地址都是不变的。IP地址在三个局域网都可见，在三个局域网之间的网段都不会冲突。在三个网段之间传输包，IP头不改变。这就像在欧洲各国之间旅游，一个签证就能搞定。 ﻿﻿![img](016120f1bf46100812f1d1ccec1e517f-20230115031641570.jpg)\n\n### “玄奘西行”型\n\n我们再来看“玄奘西行”型。\n\n这里遇见的第一个问题是，局域网之间没有商量过，各定各的网段，因而IP段冲突了。最左面大唐的地址是192.168.1.101，最右面印度的地址也是192.168.1.101，如果单从IP地址上看，简直是自己访问自己，其实是大唐的192.168.1.101要访问印度的192.168.1.101。\n\n怎么解决这个问题呢？既然局域网之间没有商量过，你们各管各的，那到国际上，也即中间的局域网里面，就需要使用另外的地址。就像出国，不能用咱们自己的身份证，而要改用护照一样，玄奘西游也要拿着专门取经的通关文牒，而不能用自己国家的身份证。\n\n首先，目标服务器B在国际上要有一个国际的身份，我们给它一个192.168.56.2。在网关B上，我们记下来，国际身份192.168.56.2对应国内身份192.168.1.101。凡是要访问192.168.56.2，都转成192.168.1.101。\n\n于是，源服务器A要访问目标服务器B，要指定的目标地址为192.168.56.2。这是它的国际身份。服务器A想，192.168.56.2和我不是一个网段的，因而需要发给网关，网关是谁？已经静态配置好了，网关是192.168.1.1，网关的MAC地址是多少？发送ARP获取网关的MAC地址，然后发送包。包的内容是这样的：\n\n- 源MAC：服务器A的MAC\n- 目标MAC：192.168.1.1这个网口的MAC\n- 源IP：192.168.1.101\n- 目标IP：192.168.56.2\n\n包到达192.168.1.1这个网口，发现MAC一致，将包收进来，开始思考往哪里转发。\n\n在路由器A中配置了静态路由：要想访问192.168.56.2/24，要从192.168.56.1这个口出去，没有下一跳了，因为我右手这个网卡，就是这个网段的，我是最后一跳了。\n\n于是，路由器A思考的时候，匹配上了这条路由，要从192.168.56.1这个口发出去，发给192.168.56.2。那192.168.56.2的MAC地址是多少呢？路由器A发送ARP获取192.168.56.2的MAC地址。\n\n当网络包发送到中间的局域网的时候，服务器A也需要有个国际身份，因而在国际上，源IP地址也不能用192.168.1.101，需要改成192.168.56.1。发送包的内容是这样的：\n\n- 源MAC：192.168.56.1的MAC地址\n- 目标MAC：192.168.56.2的MAC地址\n- 源IP：192.168.56.1\n- 目标IP：192.168.56.2\n\n包到达192.168.56.2这个网口，发现MAC一致，将包收进来，开始思考往哪里转发。\n\n路由器B是一个NAT网关，它上面配置了，要访问国际身份192.168.56.2对应国内身份192.168.1.101，于是改为访问192.168.1.101。\n\n在路由器B中配置了静态路由：要想访问192.168.1.0/24，要从192.168.1.1这个口出去，没有下一跳了，因为我右手这个网卡，就是这个网段的，我是最后一跳了。\n\n于是，路由器B思考的时候，匹配上了这条路由，要从192.168.1.1这个口发出去，发给192.168.1.101。\n\n那192.168.1.101的MAC地址是多少呢？路由器B发送ARP获取192.168.1.101的MAC地址，然后发送包。内容是这样的：\n\n- 源MAC：192.168.1.1的MAC地址\n- 目标MAC：192.168.1.101的MAC地址\n- 源IP：192.168.56.1\n- 目标IP：192.168.1.101\n\n包到达服务器B，MAC地址匹配，将包收进来。\n\n从服务器B接收的包可以看出，源IP为服务器A的国际身份，因而发送返回包的时候，也发给这个国际身份，由路由器A做NAT，转换为国内身份。\n\n从这个过程可以看出，IP地址也会变。这个过程用英文说就是**Network Address Translation**，简称**NAT**。\n\n其实这第二种方式我们经常见，现在大家每家都有家用路由器，家里的网段都是192.168.1.x，所以你肯定访问不了你邻居家的这个私网的IP地址的。所以，当我们家里的包发出去的时候，都被家用路由器NAT成为了运营商的地址了。\n\n很多办公室访问外网的时候，也是被NAT过的，因为不可能办公室里面的IP也是公网可见的，公网地址实在是太贵了，所以一般就是整个办公室共用一个到两个出口IP地址。你可以通过 https://www.whatismyip.com/ 查看自己的出口IP地址。\n\n## 小结\n\n好了，这一节内容差不多了，我来总结一下：\n\n- 如果离开本局域网，就需要经过网关，网关是路由器的一个网口；\n- 路由器是一个三层设备，里面有如何寻找下一跳的规则；\n- 经过路由器之后MAC头要变，如果IP不变，相当于不换护照的欧洲旅游，如果IP变，相当于换护照的玄奘西行。\n\n最后，给你留两个思考题吧。\n\n1. 当在你家里要访问163网站的时候，你的包需要NAT成为公网IP，返回的包又要NAT成你的私有IP，返回包怎么知道这是你的请求呢？它怎么就这么智能的NAT成了你的IP而非别人的IP呢？\n2. 对于路由规则，这一节讲述了静态路由，需要手动配置，如果要自动配置，你觉得应该怎么办呢？\n\n# 09 讲路由协议：西出网关无故人，敢问路在何方\n\n俗话说得好，在家千日好，出门一日难。网络包一旦出了网关，就像玄奘西行一样踏上了江湖漂泊的路。\n\n上一节我们描述的是一个相对简单的情形。出了网关之后，只有一条路可以走。但是，网络世界复杂得多，一旦出了网关，会面临着很多路由器，有很多条道路可以选。如何选择一个更快速的道路求取真经呢？这里面还有很多门道可以讲。\n\n## 如何配置路由？\n\n通过上一节的内容，你应该已经知道，路由器就是一台网络设备，它有多张网卡。当一个入口的网络包送到路由器时，它会根据一个本地的转发信息库，来决定如何正确地转发流量。这个转发信息库通常被称为**路由表**。\n\n一张路由表中会有多条路由规则。每一条规则至少包含这三项信息。\n\n- 目的网络：这个包想去哪儿？\n- 出口设备：将包从哪个口扔出去？\n- 下一跳网关：下一个路由器的地址。\n\n通过route命令和ip route命令都可以进行查询或者配置。\n\n例如，我们设置ip route add 10.176.48.0/20 via 10.173.32.1 dev eth0，就说明要去10.176.48.0/20这个目标网络，要从eth0端口出去，经过10.173.32.1。\n\n上一节的例子中，网关上的路由策略就是按照这三项配置信息进行配置的。这种配置方式的一个核心思想是：**根据目的IP地址来配置路由**。\n\n## 如何配置策略路由？\n\n当然，在真实的复杂的网络环境中，除了可以根据目的ip地址配置路由外，还可以根据多个参数来配置路由，这就称为**策略路由**。\n\n可以配置多个路由表，可以根据源IP地址、入口设备、TOS等选择路由表，然后在路由表中查找路由。这样可以使得来自不同来源的包走不同的路由。\n\n例如，我们设置：\n\n```sql\nip rule add from 192.168.1.0/24 table 10 \nip rule add from 192.168.2.0/24 table 20\n```\n\n表示从192.168.1.10/24这个网段来的，使用table 10中的路由表，而从192.168.2.0/24网段来的，使用table20的路由表。\n\n在一条路由规则中，也可以走多条路径。例如，在下面的路由规则中：\n\n```sql\nip route add default scope global nexthop via 100.100.100.1 weight 1 nexthop via 200.200.200.1 weight 2\n```\n\n下一跳有两个地方，分别是100.100.100.1和200.200.200.1，权重分别为1比2。\n\n在什么情况下会用到如此复杂的配置呢？我来举一个现实中的例子。\n\n我是房东，家里从运营商那儿拉了两根网线。这两根网线分别属于两个运行商。一个带宽大一些，一个带宽小一些。这个时候，我就不能买普通的家用路由器了，得买个高级点的，可以接两个外网的。\n\n家里的网络呢，就是普通的家用网段192.168.1.x/24。家里有两个租户，分别把线连到路由器上。IP地址为192.168.1.101/24和192.168.1.102/24，网关都是192.168.1.1/24，网关在路由器上。\n\n就像上一节说的一样，家里的网段是私有网段，出去的包需要NAT成公网的IP地址，因而路由器是一个NAT路由器。\n\n两个运营商都要为这个网关配置一个公网的IP地址。如果你去查看你们家路由器里的网段，基本就是我图中画的样子。\n\n![img](03df39f76b60ac2c0a61b75a4dc25869-20230115031641883.jpg)\n\n运行商里面也有一个IP地址，在运营商网络里面的网关。不同的运营商方法不一样，有的是/32的，也即一个一对一连接。\n\n例如，运营商1给路由器分配的地址是183.134.189.34/32，而运营商网络里面的网关是183.134.188.1/32。有的是/30的，也就是分了一个特别小的网段。运营商2给路由器分配的地址是60.190.27.190/30，运营商网络里面的网关是60.190.27.189/30。\n\n根据这个网络拓扑图，可以将路由配置成这样：\n\n```sql\n$ ip route list table main \n60.190.27.189/30 dev eth3  proto kernel  scope link  src 60.190.27.190\n183.134.188.1 dev eth2  proto kernel  scope link  src 183.134.189.34\n192.168.1.0/24 dev eth1  proto kernel  scope link  src 192.168.1.1\n127.0.0.0/8 dev lo  scope link\ndefault via 183.134.188.1 dev eth2\n```\n\n当路由这样配置的时候，就告诉这个路由器如下的规则：\n\n- 如果去运营商二，就走eth3；\n- 如果去运营商一呢，就走eth2；\n- 如果访问内网，就走eth1；\n- 如果所有的规则都匹配不上，默认走运营商一，也即走快的网络。\n\n但是问题来了，租户A不想多付钱，他说我就上上网页，从不看电影，凭什么收我同样贵的网费啊？没关系，咱有技术可以解决。\n\n下面我添加一个Table，名字叫**chao**。\n\n```shell\n# echo 200 chao \u003e\u003e /etc/iproute2/rt_tables\n```\n\n添加一条规则：\n\n```sql\n# ip rule add from 192.168.1.101 table chao\n# ip rule ls\n0:    from all lookup local \n32765:    from 10.0.0.10 lookup chao\n32766:    from all lookup main \n32767:    from all lookup default\n```\n\n设定规则为：从192.168.1.101来的包都查看个chao这个新的路由表。\n\n在chao路由表中添加规则：\n\n```bash\n# ip route add default via 60.190.27.189 dev eth3 table chao\n# ip route flush cache\n```\n\n默认的路由走慢的，谁让你不付钱。\n\n上面说的都是静态的路由，一般来说网络环境简单的时候，在自己的可控范围之内，自己捣鼓还是可以的。但是有时候网络环境复杂并且多变，如果总是用静态路由，一旦网络结构发生变化，让网络管理员手工修改路由太复杂了，因而需要动态路由算法。\n\n## 动态路由算法\n\n使用动态路由路由器，可以根据路由协议算法生成动态路由表，随网络运行状况的变化而变化。那路由算法是什么样的呢？\n\n我们可以想象唐僧西天取经，需要解决两大问题，一个是在每个国家如何找到正确的路，去换通关文牒、吃饭、休息；一个是在国家之间，野外行走的时候，如何找到正确的路、水源的问题。\n\n![img](6d1f0be048a04a08a40b16010f1180bb-20230115031642254.jpg)\n\n无论是一个国家内部，还是国家之间，我们都可以将复杂的路径，抽象为一种叫作图的数据结构。至于唐僧西行取经，肯定想走得路越少越好，道路越短越好，因而这就转化成为**如何在途中找到最短路径**的问题。\n\n咱们在大学里面学习计算机网络与数据结构的时候，知道求最短路径常用的有两种方法，一种是Bellman-Ford算法，一种是Dijkstra算法。在计算机网络中基本也是用这两种方法计算的。\n\n### 1.距离矢量路由算法\n\n第一大类的算法称为**距离矢量路由**（**distance vector routing**）。它是基于Bellman-Ford算法的。\n\n这种算法的基本思路是，每个路由器都保存一个路由表，包含多行，每行对应网络中的一个路由器，每一行包含两部分信息，一个是要到目标路由器，从那条线出去，另一个是到目标路由器的距离。\n\n由此可以看出，每个路由器都是知道全局信息的。那这个信息如何更新呢？每个路由器都知道自己和邻居之间的距离，每过几秒，每个路由器都将自己所知的到达所有的路由器的距离告知邻居，每个路由器也能从邻居那里得到相似的信息。\n\n每个路由器根据新收集的信息，计算和其他路由器的距离，比如自己的一个邻居距离目标路由器的距离是M，而自己距离邻居是x，则自己距离目标路由器是x+M。\n\n这个算法比较简单，但是还是有问题。\n\n**第一个问题就是好消息传得快，坏消息传得慢。**如果有个路由器加入了这个网络，它的邻居就能很快发现它，然后将消息广播出去。要不了多久，整个网络就都知道了。但是一旦一个路由器挂了，挂的消息是没有广播的。当每个路由器发现原来的道路到不了这个路由器的时候，感觉不到它已经挂了，而是试图通过其他的路径访问，直到试过了所有的路径，才发现这个路由器是真的挂了。\n\n我再举个例子。\n\n![img](641b14ede460105d15e23b077532013f-20230115031641661.jpg)\n\n原来的网络包括两个节点，B和C。A加入了网络，它的邻居B很快就发现A启动起来了。于是它将自己和A的距离设为1，同样C也发现A起来了，将自己和A的距离设置为2。但是如果A挂掉，情况就不妙了。B本来和A是邻居，发现连不上A了，但是C还是能够连上，只不过距离远了点，是2，于是将自己的距离设置为3。殊不知C的距离2其实是基于原来自己的距离为1计算出来的。C发现自己也连不上A，并且发现B设置为3，于是自己改成距离4。依次类推，数越来越大，直到超过一个阈值，我们才能判定A真的挂了。\n\n这个道理有点像有人走丢了。当你突然发现找不到这个人了。于是你去学校问，是不是在他姨家呀？找到他姨家，他姨说，是不是在他舅舅家呀？他舅舅说，是不是在他姥姥家呀？他姥姥说，是不是在学校呀？总归要问一圈，或者是超过一定的时间，大家才会认为这个人的确走丢了。如果这个人其实只是去见了一个谁都不认识的网友去了，当这个人回来的时候，只要他随便见到其中的一个亲戚，这个亲戚就会拉着他到他的家长那里，说你赶紧回家，你妈都找你一天了。\n\n**这种算法的第二个问题是，每次发送的时候，要发送整个全局路由表。**网络大了，谁也受不了，所以最早的路由协议RIP就是这个算法。它适用于小型网络（小于15跳）。当网络规模都小的时候，没有问题。现在一个数据中心内部路由器数目就很多，因而不适用了。\n\n所以上面的两个问题，限制了距离矢量路由的网络规模。\n\n### 2.链路状态路由算法\n\n第二大类算法是**链路状态路由**（**link state routing**），基于Dijkstra算法。\n\n这种算法的基本思路是：当一个路由器启动的时候，首先是发现邻居，向邻居say hello，邻居都回复。然后计算和邻居的距离，发送一个echo，要求马上返回，除以二就是距离。然后将自己和邻居之间的链路状态包广播出去，发送到整个网络的每个路由器。这样每个路由器都能够收到它和邻居之间的关系的信息。因而，每个路由器都能在自己本地构建一个完整的图，然后针对这个图使用Dijkstra算法，找到两点之间的最短路径。\n\n不像距离距离矢量路由协议那样，更新时发送整个路由表。链路状态路由协议只广播更新的或改变的网络拓扑，这使得更新信息更小，节省了带宽和CPU利用率。而且一旦一个路由器挂了，它的邻居都会广播这个消息，可以使得坏消息迅速收敛。\n\n## 动态路由协议\n\n### 1.基于链路状态路由算法的OSPF\n\n**OSPF**（**Open Shortest Path First**，**开放式最短路径优先**）就是这样一个基于链路状态路由协议，广泛应用在数据中心中的协议。由于主要用在数据中心内部，用于路由决策，因而称为**内部网关协议**（**Interior Gateway Protocol**，简称**IGP**）。\n\n内部网关协议的重点就是找到最短的路径。在一个组织内部，路径最短往往最优。当然有时候OSPF可以发现多个最短的路径，可以在这多个路径中进行负载均衡，这常常被称为**等价路由**。\n\n![img](6791cdd30119e78fcb3c350223d5049b-20230115031641675.jpg)\n\n这一点非常重要。有了等价路由，到一个地方去可以有相同的两个路线，可以分摊流量，还可以当一条路不通的时候，走另外一条路。这个在后面我们讲数据中心的网络的时候，一般应用的接入层会有负载均衡LVS。它可以和OSPF一起，实现高吞吐量的接入层设计。\n\n有了内网的路由协议，在一个国家内，唐僧可以想怎么走怎么走了，两条路选一条也行。\n\n### 2.基于距离矢量路由算法的BGP\n\n但是外网的路由协议，也即国家之间的，又有所不同。我们称为**外网路由协议**（**Border Gateway Protocol**，简称**BGP**）。\n\n在一个国家内部，有路当然选近的走。但是国家之间，不光远近的问题，还有政策的问题。例如，唐僧去西天取经，有的路近。但是路过的国家看不惯僧人，见了僧人就抓。例如灭法国，连光头都要抓。这样的情况即便路近，也最好绕远点走。\n\n对于网络包同样，每个数据中心都设置自己的Policy。例如，哪些外部的IP可以让内部知晓，哪些内部的IP可以让外部知晓，哪些可以通过，哪些不能通过。这就好比，虽然从我家里到目的地最近，但是不能谁都能从我家走啊！\n\n在网络世界，这一个个国家成为自治系统**AS**（Autonomous System）。自治系统分几种类型。\n\n- Stub AS：对外只有一个连接。这类AS不会传输其他AS的包。例如，个人或者小公司的网络。\n- Multihomed AS：可能有多个连接连到其他的AS，但是大多拒绝帮其他的AS传输包。例如一些大公司的网络。\n- Transit AS：有多个连接连到其他的AS，并且可以帮助其他的AS传输包。例如主干网。\n\n每个自治系统都有边界路由器，通过它和外面的世界建立联系。\n\n![img](c3bce0ec298138d8e36e6ebf1375d843-20230115031641734.jpg)\n\n**BGP又分为两类，eBGP和iBGP。**自治系统间，边界路由器之间使用eBGP广播路由。内部网络也需要访问其他的自治系统。边界路由器如何将BGP学习到的路由导入到内部网络呢？就是通过运行iBGP，使得内部的路由器能够找到到达外网目的地的最好的边界路由器。\n\nBGP协议使用的算法是**路径矢量路由协议**（path-vector protocol）。它是距离矢量路由协议的升级版。\n\n前面说了距离矢量路由协议的缺点。其中一个是收敛慢。在BGP里面，除了下一跳hop之外，还包括了自治系统AS的路径，从而可以避免坏消息传的慢的问题，也即上面所描述的，B知道C原来能够到达A，是因为通过自己，一旦自己都到达不了A了，就不用假设C还能到达A了。\n\n另外，在路径中将一个自治系统看成一个整体，不区分自治系统内部的路由器，这样自治系统的数目是非常有限的。就像大家都能记住出去玩，从中国出发先到韩国然后到日本，只要不计算细到具体哪一站，就算是发送全局信息，也是没有问题的。\n\n## 小结\n\n好了，这一节就到这里了，我来做个总结：\n\n- 路由分静态路由和动态路由，静态路由可以配置复杂的策略路由，控制转发策略；\n- 动态路由主流算法有两种，距离矢量算法和链路状态算法。基于两种算法产生两种协议，BGP协议和OSPF协议。\n\n最后，再给你留两个思考题：\n\n1. 路由协议要在路由器之间交换信息，这些信息的交换还需要走路由吗？不是死锁了吗？\n2. 路由器之间信息的交换使用什么协议呢？报文格式是什么样呢？\n\n# 10 讲UDP协议：因性善而简单，难免碰到“城会玩”\n\n讲完了IP层以后，接下来我们开始讲传输层。传输层里比较重要的两个协议，一个是TCP，一个是UDP。对于不从事底层开发的人员来讲，或者对于开发应用的人来讲，最常用的就是这两个协议。由于面试的时候，这两个协议经常会被放在一起问，因而我在讲的时候，也会结合着来讲。\n\n## TCP和UDP有哪些区别？\n\n一般面试的时候我问这两个协议的区别，大部分人会回答，TCP是面向连接的，UDP是面向无连接的。\n\n什么叫面向连接，什么叫无连接呢？在互通之前，面向连接的协议会先建立连接。例如，TCP会三次握手，而UDP不会。为什么要建立连接呢？你TCP三次握手，我UDP也可以发三个包玩玩，有什么区别吗？\n\n**所谓的建立连接，是为了在客户端和服务端维护连接，而建立一定的数据结构来维护双方交互的状态，用这样的数据结构来保证所谓的面向连接的特性。**\n\n例如，**TCP提供可靠交付**。通过TCP连接传输的数据，无差错、不丢失、不重复、并且按序到达。我们都知道IP包是没有任何可靠性保证的，一旦发出去，就像西天取经，走丢了、被妖怪吃了，都只能随它去。但是TCP号称能做到那个连接维护的程序做的事情，这个下两节我会详细描述。而**UDP继承了IP包的特性，不保证不丢失，不保证按顺序到达。**\n\n再如，**TCP是面向字节流的**。发送的时候发的是一个流，没头没尾。IP包可不是一个流，而是一个个的IP包。之所以变成了流，这也是TCP自己的状态维护做的事情。而**UDP继承了IP的特性，基于数据报的，一个一个地发，一个一个地收。**\n\n还有**TCP是可以有拥塞控制的**。它意识到包丢弃了或者网络的环境不好了，就会根据情况调整自己的行为，看看是不是发快了，要不要发慢点。**UDP就不会，应用让我发，我就发，管它洪水滔天。**\n\n因而**TCP其实是一个有状态服务**，通俗地讲就是有脑子的，里面精确地记着发送了没有，接收到没有，发送到哪个了，应该接收哪个了，错一点儿都不行。而**UDP则是无状态服务。**通俗地说是没脑子的，天真无邪的，发出去就发出去了。\n\n我们可以这样比喻，如果MAC层定义了本地局域网的传输行为，IP层定义了整个网络端到端的传输行为，这两层基本定义了这样的基因：网络传输是以包为单位的，二层叫帧，网络层叫包，传输层叫段。我们笼统地称为包。包单独传输，自行选路，在不同的设备封装解封装，不保证到达。基于这个基因，生下来的孩子UDP完全继承了这些特性，几乎没有自己的思想。\n\n## UDP包头是什么样的？\n\n我们来看一下UDP包头。\n\n前面章节我已经讲过包的传输过程，这里不再赘述。当我发送的UDP包到达目标机器后，发现MAC地址匹配，于是就取下来，将剩下的包传给处理IP层的代码。把IP头取下来，发现目标IP匹配，接下来呢？这里面的数据包是给谁呢？\n\n发送的时候，我知道我发的是一个UDP的包，收到的那台机器咋知道的呢？所以在IP头里面有个8位协议，这里会存放，数据里面到底是TCP还是UDP，当然这里是UDP。于是，如果我们知道UDP头的格式，就能从数据里面，将它解析出来。解析出来以后呢？数据给谁处理呢？\n\n处理完传输层的事情，内核的事情基本就干完了，里面的数据应该交给应用程序自己去处理，可是一台机器上跑着这么多的应用程序，应该给谁呢？\n\n无论应用程序写的使用TCP传数据，还是UDP传数据，都要监听一个端口。正是这个端口，用来区分应用程序，要不说端口不能冲突呢。两个应用监听一个端口，到时候包给谁呀？所以，按理说，无论是TCP还是UDP包头里面应该有端口号，根据端口号，将数据交给相应的应用程序。 ﻿﻿![img](6d1313f51b9dfd7ab454b2cef1cb37bf-20230115031641738.jpg)\n\n当我们看到UDP包头的时候，发现的确有端口号，有源端口号和目标端口号。因为是两端通信嘛，这很好理解。但是你还会发现，UDP除了端口号，再没有其他的了。和下两节要讲的TCP头比起来，这个简直简单得一塌糊涂啊！\n\n## UDP的三大特点\n\nUDP就像小孩子一样，有以下这些特点：\n\n第一，**沟通简单**，不需要一肚子花花肠子（大量的数据结构、处理逻辑、包头字段）。前提是它相信网络世界是美好的，秉承性善论，相信网络通路默认就是很容易送达的，不容易被丢弃的。\n\n第二，**轻信他人**。它不会建立连接，虽然有端口号，但是监听在这个地方，谁都可以传给他数据，他也可以传给任何人数据，甚至可以同时传给多个人数据。\n\n第三，**愣头青，做事不懂权变**。不知道什么时候该坚持，什么时候该退让。它不会根据网络的情况进行发包的拥塞控制，无论网络丢包丢成啥样了，它该怎么发还怎么发。\n\n## UDP的三大使用场景\n\n基于UDP这种“小孩子”的特点，我们可以考虑在以下的场景中使用。\n\n第一，**需要资源少，在网络情况比较好的内网，或者对于丢包不敏感的应用**。这很好理解，就像如果你是领导，你会让你们组刚毕业的小朋友去做一些没有那么难的项目，打一些没有那么难的客户，或者做一些失败了也能忍受的实验性项目。\n\n我们在第四节讲的DHCP就是基于UDP协议的。一般的获取IP地址都是内网请求，而且一次获取不到IP又没事，过一会儿还有机会。我们讲过PXE可以在启动的时候自动安装操作系统，操作系统镜像的下载使用的TFTP，这个也是基于UDP协议的。在还没有操作系统的时候，客户端拥有的资源很少，不适合维护一个复杂的状态机，而是因为是内网，一般也没啥问题。\n\n第二，**不需要一对一沟通，建立连接，而是可以广播的应用**。咱们小时候人都很简单，大家在班级里面，谁成绩好，谁写作好，应该表扬谁惩罚谁，谁得几个小红花都是当着全班的面讲的，公平公正公开。长大了人心复杂了，薪水、奖金要背靠背，和员工一对一沟通。\n\nUDP的不面向连接的功能，可以使得可以承载广播或者多播的协议。DHCP就是一种广播的形式，就是基于UDP协议的，而广播包的格式前面说过了。\n\n对于多播，我们在讲IP地址的时候，讲过一个D类地址，也即组播地址，使用这个地址，可以将包组播给一批机器。当一台机器上的某个进程想监听某个组播地址的时候，需要发送IGMP包，所在网络的路由器就能收到这个包，知道有个机器上有个进程在监听这个组播地址。当路由器收到这个组播地址的时候，会将包转发给这台机器，这样就实现了跨路由器的组播。\n\n在后面云中网络部分，有一个协议VXLAN，也是需要用到组播，也是基于UDP协议的。\n\n第三，**需要处理速度快，时延低，可以容忍少数丢包，但是要求即便网络拥塞，也毫不退缩，一往无前的时候**。记得曾国藩建立湘军的时候，专门招出生牛犊不怕虎的新兵，而不用那些“老油条”的八旗兵，就是因为八旗兵经历的事情多，遇到敌军不敢舍死忘生。\n\n同理，UDP简单、处理速度快，不像TCP那样，操这么多的心，各种重传啊，保证顺序啊，前面的不收到，后面的没法处理啊。不然等这些事情做完了，时延早就上去了。而TCP在网络不好出现丢包的时候，拥塞控制策略会主动的退缩，降低发送速度，这就相当于本来环境就差，还自断臂膀，用户本来就卡，这下更卡了。\n\n当前很多应用都是要求低时延的，它们可不想用TCP如此复杂的机制，而是想根据自己的场景，实现自己的可靠和连接保证。例如，如果应用自己觉得，有的包丢了就丢了，没必要重传了，就可以算了，有的比较重要，则应用自己重传，而不依赖于TCP。有的前面的包没到，后面的包到了，那就先给客户展示后面的嘛，干嘛非得等到齐了呢？如果网络不好，丢了包，那不能退缩啊，要尽快传啊，速度不能降下来啊，要挤占带宽，抢在客户失去耐心之前到达。\n\n由于UDP十分简单，基本啥都没做，也就给了应用“城会玩”的机会。就像在和平年代，每个人应该有独立的思考和行为，应该可靠并且礼让；但是如果在战争年代，往往不太需要过于独立的思考，而需要士兵简单服从命令就可以了。\n\n曾国藩说哪支部队需要诱敌牺牲，也就牺牲了，相当于包丢了就丢了。两军狭路相逢的时候，曾国藩说上，没有带宽也要上，这才给了曾国藩运筹帷幄，城会玩的机会。同理如果你实现的应用需要有自己的连接策略，可靠保证，时延要求，使用UDP，然后再应用层实现这些是再好不过了。\n\n## 基于UDP的“城会玩”的五个例子\n\n我列举几种“城会玩”的例子。\n\n### “城会玩”一：网页或者APP的访问\n\n原来访问网页和手机APP都是基于HTTP协议的。HTTP协议是基于TCP的，建立连接都需要多次交互，对于时延比较大的目前主流的移动互联网来讲，建立一次连接需要的时间会比较长，然而既然是移动中，TCP可能还会断了重连，也是很耗时的。而且目前的HTTP协议，往往采取多个数据通道共享一个连接的情况，这样本来为了加快传输速度，但是TCP的严格顺序策略使得哪怕共享通道，前一个不来，后一个和前一个即便没关系，也要等着，时延也会加大。\n\n而**QUIC**（全称**Quick UDP Internet Connections**，**快速UDP互联网连接**）是Google提出的一种基于UDP改进的通信协议，其目的是降低网络通信的延迟，提供更好的用户互动体验。\n\nQUIC在应用层上，会自己实现快速连接建立、减少重传时延，自适应拥塞控制，是应用层“城会玩”的代表。这一节主要是讲UDP，QUIC我们放到应用层去讲。\n\n### “城会玩”二：流媒体的协议\n\n现在直播比较火，直播协议多使用RTMP，这个协议我们后面的章节也会讲，而这个RTMP协议也是基于TCP的。TCP的严格顺序传输要保证前一个收到了，下一个才能确认，如果前一个收不到，下一个就算包已经收到了，在缓存里面，也需要等着。对于直播来讲，这显然是不合适的，因为老的视频帧丢了其实也就丢了，就算再传过来用户也不在意了，他们要看新的了，如果老是没来就等着，卡顿了，新的也看不了，那就会丢失客户，所以直播，实时性比较比较重要，宁可丢包，也不要卡顿的。\n\n另外，对于丢包，其实对于视频播放来讲，有的包可以丢，有的包不能丢，因为视频的连续帧里面，有的帧重要，有的不重要，如果必须要丢包，隔几个帧丢一个，其实看视频的人不会感知，但是如果连续丢帧，就会感知了，因而在网络不好的情况下，应用希望选择性的丢帧。\n\n还有就是当网络不好的时候，TCP协议会主动降低发送速度，这对本来当时就卡的看视频来讲是要命的，应该应用层马上重传，而不是主动让步。因而，很多直播应用，都基于UDP实现了自己的视频传输协议。\n\n### “城会玩”三：实时游戏\n\n游戏有一个特点，就是实时性比较高。快一秒你干掉别人，慢一秒你被别人爆头，所以很多职业玩家会买非常专业的鼠标和键盘，争分夺秒。\n\n因而，实时游戏中客户端和服务端要建立长连接，来保证实时传输。但是游戏玩家很多，服务器却不多。由于维护TCP连接需要在内核维护一些数据结构，因而一台机器能够支撑的TCP连接数目是有限的，然后UDP由于是没有连接的，在异步IO机制引入之前，常常是应对海量客户端连接的策略。\n\n另外还是TCP的强顺序问题，对战的游戏，对网络的要求很简单，玩家通过客户端发送给服务器鼠标和键盘行走的位置，服务器会处理每个用户发送过来的所有场景，处理完再返回给客户端，客户端解析响应，渲染最新的场景展示给玩家。\n\n如果出现一个数据包丢失，所有事情都需要停下来等待这个数据包重发。客户端会出现等待接收数据，然而玩家并不关心过期的数据，激战中卡1秒，等能动了都已经死了。\n\n游戏对实时要求较为严格的情况下，采用自定义的可靠UDP协议，自定义重传策略，能够把丢包产生的延迟降到最低，尽量减少网络问题对游戏性造成的影响。\n\n### “城会玩”四：IoT物联网\n\n一方面，物联网领域终端资源少，很可能只是个内存非常小的嵌入式系统，而维护TCP协议代价太大；另一方面，物联网对实时性要求也很高，而TCP还是因为上面的那些原因导致时延大。Google旗下的Nest建立Thread Group，推出了物联网通信协议Thread，就是基于UDP协议的。\n\n### “城会玩”五：移动通信领域\n\n在4G网络里，移动流量上网的数据面对的协议GTP-U是基于UDP的。因为移动网络协议比较复杂，而GTP协议本身就包含复杂的手机上线下线的通信协议。如果基于TCP，TCP的机制就显得非常多余，这部分协议我会在后面的章节单独讲解。\n\n## 小结\n\n好了，这节就到这里了，我们来总结一下：\n\n- 如果将TCP比作成熟的社会人，UDP则是头脑简单的小朋友。TCP复杂，UDP简单；TCP维护连接，UDP谁都相信；TCP会坚持知进退；UDP愣头青一个，勇往直前；\n- UDP虽然简单，但它有简单的用法。它可以用在环境简单、需要多播、应用层自己控制传输的地方。例如DHCP、VXLAN、QUIC等。\n\n最后，给你留两个思考题吧。\n\n1. 都说TCP是面向连接的，在计算机看来，怎么样才算一个连接呢？\n2. 你知道TCP的连接是如何建立，又是如何关闭的吗？\n\n# 11 讲TCP协议（上）：因性恶而复杂，先恶后善反轻松\n\n上一节，我们讲的UDP，基本上包括了传输层所必须的端口字段。它就像我们小时候一样简单，相信“网之初，性本善，不丢包，不乱序”。\n\n后来呢，我们都慢慢长大，了解了社会的残酷，变得复杂而成熟，就像TCP协议一样。它之所以这么复杂，那是因为它秉承的是“性恶论”。它天然认为网络环境是恶劣的，丢包、乱序、重传，拥塞都是常有的事情，一言不合就可能送达不了，因而要从算法层面来保证可靠性。\n\n## TCP包头格式\n\n我们先来看TCP头的格式。从这个图上可以看出，它比UDP复杂得多。\n\n![img](a795461effcce686a43f48e094c9adbf-20230115031641865.jpg)\n\n首先，源端口号和目标端口号是不可少的，这一点和UDP是一样的。如果没有这两个端口号。数据就不知道应该发给哪个应用。\n\n接下来是包的序号。为什么要给包编号呢？当然是为了解决乱序的问题。不编好号怎么确认哪个应该先来，哪个应该后到呢。编号是为了解决乱序问题。既然是社会老司机，做事当然要稳重，一件件来，面临再复杂的情况，也临危不乱。\n\n还应该有的就是确认序号。发出去的包应该有确认，要不然我怎么知道对方有没有收到呢？如果没有收到就应该重新发送，直到送达。这个可以解决不丢包的问题。作为老司机，做事当然要靠谱，答应了就要做到，暂时做不到也要有个回复。\n\nTCP是靠谱的协议，但是这不能说明它面临的网络环境好。从IP层面来讲，如果网络状况的确那么差，是没有任何可靠性保证的，而作为IP的上一层TCP也无能为力，唯一能做的就是更加努力，不断重传，通过各种算法保证。也就是说，对于TCP来讲，IP层你丢不丢包，我管不着，但是我在我的层面上，会努力保证可靠性。\n\n这有点像如果你在北京，和客户约十点见面，那么你应该清楚堵车是常态，你干预不了，也控制不了，你唯一能做的就是早走。打车不行就改乘地铁，尽力不失约。\n\n接下来有一些状态位。例如SYN是发起一个连接，ACK是回复，RST是重新连接，FIN是结束连接等。TCP是面向连接的，因而双方要维护连接的状态，这些带状态位的包的发送，会引起双方的状态变更。\n\n不像小时候，随便一个不认识的小朋友都能玩在一起，人大了，就变得礼貌，优雅而警觉，人与人遇到会互相热情的寒暄，离开会不舍的道别，但是人与人之间的信任会经过多次交互才能建立。\n\n还有一个重要的就是窗口大小。TCP要做流量控制，通信双方各声明一个窗口，标识自己当前能够的处理能力，别发送的太快，撑死我，也别发的太慢，饿死我。\n\n作为老司机，做事情要有分寸，待人要把握尺度，既能适当提出自己的要求，又不强人所难。除了做流量控制以外，TCP还会做拥塞控制，对于真正的通路堵车不堵车，它无能为力，唯一能做的就是控制自己，也即控制发送的速度。不能改变世界，就改变自己嘛。\n\n作为老司机，要会自我控制，知进退，知道什么时候应该坚持，什么时候应该让步。\n\n通过对TCP头的解析，我们知道要掌握TCP协议，重点应该关注以下几个问题：\n\n- 顺序问题 ，稳重不乱；\n- 丢包问题，承诺靠谱；\n- 连接维护，有始有终；\n- 流量控制，把握分寸；\n- 拥塞控制，知进知退。\n\n## TCP的三次握手\n\n所有的问题，首先都要先建立一个连接，所以我们先来看连接维护问题。\n\nTCP的连接建立，我们常常称为三次握手。\n\nA：您好，我是A。\n\nB：您好A，我是B。\n\nA：您好B。\n\n我们也常称为“请求-\u003e应答-\u003e应答之应答”的三个回合。这个看起来简单，其实里面还是有很多的学问，很多的细节。\n\n首先，为什么要三次，而不是两次？按说两个人打招呼，一来一回就可以了啊？为了可靠，为什么不是四次？\n\n我们还是假设这个通路是非常不可靠的，A要发起一个连接，当发了第一个请求杳无音信的时候，会有很多的可能性，比如第一个请求包丢了，再如没有丢，但是绕了弯路，超时了，还有B没有响应，不想和我连接。\n\nA不能确认结果，于是再发，再发。终于，有一个请求包到了B，但是请求包到了B的这个事情，目前A还是不知道的，A还有可能再发。\n\nB收到了请求包，就知道了A的存在，并且知道A要和它建立连接。如果B不乐意建立连接，则A会重试一阵后放弃，连接建立失败，没有问题；如果B是乐意建立连接的，则会发送应答包给A。\n\n当然对于B来说，这个应答包也是一入网络深似海，不知道能不能到达A。这个时候B自然不能认为连接是建立好了，因为应答包仍然会丢，会绕弯路，或者A已经挂了都有可能。\n\n而且这个时候B还能碰到一个诡异的现象就是，A和B原来建立了连接，做了简单通信后，结束了连接。还记得吗？A建立连接的时候，请求包重复发了几次，有的请求包绕了一大圈又回来了，B会认为这也是一个正常的的请求的话，因此建立了连接，可以想象，这个连接不会进行下去，也没有个终结的时候，纯属单相思了。因而两次握手肯定不行。\n\nB发送的应答可能会发送多次，但是只要一次到达A，A就认为连接已经建立了，因为对于A来讲，他的消息有去有回。A会给B发送应答之应答，而B也在等这个消息，才能确认连接的建立，只有等到了这个消息，对于B来讲，才算它的消息有去有回。\n\n当然A发给B的应答之应答也会丢，也会绕路，甚至B挂了。按理来说，还应该有个应答之应答之应答，这样下去就没底了。所以四次握手是可以的，四十次都可以，关键四百次也不能保证就真的可靠了。只要双方的消息都有去有回，就基本可以了。\n\n好在大部分情况下，A和B建立了连接之后，A会马上发送数据的，一旦A发送数据，则很多问题都得到了解决。例如A发给B的应答丢了，当A后续发送的数据到达的时候，B可以认为这个连接已经建立，或者B压根就挂了，A发送的数据，会报错，说B不可达，A就知道B出事情了。\n\n当然你可以说A比较坏，就是不发数据，建立连接后空着。我们在程序设计的时候，可以要求开启keepalive机制，即使没有真实的数据包，也有探活包。\n\n另外，你作为服务端B的程序设计者，对于A这种长时间不发包的客户端，可以主动关闭，从而空出资源来给其他客户端使用。\n\n三次握手除了双方建立连接外，主要还是为了沟通一件事情，就是**TCP包的序号的问题**。\n\nA要告诉B，我这面发起的包的序号起始是从哪个号开始的，B同样也要告诉A，B发起的包的序号起始是从哪个号开始的。为什么序号不能都从1开始呢？因为这样往往会出现冲突。\n\n例如，A连上B之后，发送了1、2、3三个包，但是发送3的时候，中间丢了，或者绕路了，于是重新发送，后来A掉线了，重新连上B后，序号又从1开始，然后发送2，但是压根没想发送3，但是上次绕路的那个3又回来了，发给了B，B自然认为，这就是下一个包，于是发生了错误。\n\n因而，每个连接都要有不同的序号。这个序号的起始序号是随着时间变化的，可以看成一个32位的计数器，每4ms加一，如果计算一下，如果到重复，需要4个多小时，那个绕路的包早就死翘翘了，因为我们都知道IP包头里面有个TTL，也即生存时间。\n\n好了，双方终于建立了信任，建立了连接。前面也说过，为了维护这个连接，双方都要维护一个状态机，在连接建立的过程中，双方的状态变化时序图就像这样。\n\n![img](666d7d20aa907d8317af3770411f5aa2-20230115031641811.jpg)\n\n一开始，客户端和服务端都处于CLOSED状态。先是服务端主动监听某个端口，处于LISTEN状态。然后客户端主动发起连接SYN，之后处于SYN-SENT状态。服务端收到发起的连接，返回SYN，并且ACK客户端的SYN，之后处于SYN-RCVD状态。客户端收到服务端发送的SYN和ACK之后，发送ACK的ACK，之后处于ESTABLISHED状态，因为它一发一收成功了。服务端收到ACK的ACK之后，处于ESTABLISHED状态，因为它也一发一收了。\n\n## TCP四次挥手\n\n好了，说完了连接，接下来说一说“拜拜”，好说好散。这常被称为四次挥手。\n\nA：B啊，我不想玩了。\n\nB：哦，你不想玩了啊，我知道了。\n\n这个时候，还只是A不想玩了，也即A不会再发送数据，但是B能不能在ACK的时候，直接关闭呢？当然不可以了，很有可能A是发完了最后的数据就准备不玩了，但是B还没做完自己的事情，还是可以发送数据的，所以称为半关闭的状态。\n\n这个时候A可以选择不再接收数据了，也可以选择最后再接收一段数据，等待B也主动关闭。\n\nB：A啊，好吧，我也不玩了，拜拜。\n\nA：好的，拜拜。\n\n这样整个连接就关闭了。但是这个过程有没有异常情况呢？当然有，上面是和平分手的场面。\n\nA开始说“不玩了”，B说“知道了”，这个回合，是没什么问题的，因为在此之前，双方还处于合作的状态，如果A说“不玩了”，没有收到回复，则A会重新发送“不玩了”。但是这个回合结束之后，就有可能出现异常情况了，因为已经有一方率先撕破脸。\n\n一种情况是，A说完“不玩了”之后，直接跑路，是会有问题的，因为B还没有发起结束，而如果A跑路，B就算发起结束，也得不到回答，B就不知道该怎么办了。另一种情况是，A说完“不玩了”，B直接跑路，也是有问题的，因为A不知道B是还有事情要处理，还是过一会儿会发送结束。\n\n那怎么解决这些问题呢？TCP协议专门设计了几个状态来处理这些问题。我们来看断开连接的时候的**状态时序图**。\n\n![img](1f6a5e17b34f00d28722428b7b8ccb11-20230115031641811.jpg)\n\n断开的时候，我们可以看到，当A说“不玩了”，就进入FIN_WAIT_1的状态，B收到“A不玩”的消息后，发送知道了，就进入CLOSE_WAIT的状态。\n\nA收到“B说知道了”，就进入FIN_WAIT_2的状态，如果这个时候B直接跑路，则A将永远在这个状态。TCP协议里面并没有对这个状态的处理，但是Linux有，可以调整tcp_fin_timeout这个参数，设置一个超时时间。\n\n如果B没有跑路，发送了“B也不玩了”的请求到达A时，A发送“知道B也不玩了”的ACK后，从FIN_WAIT_2状态结束，按说A可以跑路了，但是最后的这个ACK万一B收不到呢？则B会重新发一个“B不玩了”，这个时候A已经跑路了的话，B就再也收不到ACK了，因而TCP协议要求A最后等待一段时间TIME_WAIT，这个时间要足够长，长到如果B没收到ACK的话，“B说不玩了”会重发的，A会重新发一个ACK并且足够时间到达B。\n\nA直接跑路还有一个问题是，A的端口就直接空出来了，但是B不知道，B原来发过的很多包很可能还在路上，如果A的端口被一个新的应用占用了，这个新的应用会收到上个连接中B发过来的包，虽然序列号是重新生成的，但是这里要上一个双保险，防止产生混乱，因而也需要等足够长的时间，等到原来B发送的所有的包都死翘翘，再空出端口来。\n\n等待的时间设为2MSL，**MSL**是**Maximum Segment Lifetime**，**报文最大生存时间**，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为TCP报文基于是IP协议的，而IP头中有一个TTL域，是IP数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减1，当此值为0则数据报将被丢弃，同时发送ICMP报文通知源主机。协议规定MSL为2分钟，实际应用中常用的是30秒，1分钟和2分钟等。\n\n还有一个异常情况就是，B超过了2MSL的时间，依然没有收到它发的FIN的ACK，怎么办呢？按照TCP的原理，B当然还会重发FIN，这个时候A再收到这个包之后，A就表示，我已经在这里等了这么长时间了，已经仁至义尽了，之后的我就都不认了，于是就直接发送RST，B就知道A早就跑了。\n\n## TCP状态机\n\n将连接建立和连接断开的两个时序状态图综合起来，就是这个著名的TCP的状态机。学习的时候比较建议将这个状态机和时序状态机对照着看，不然容易晕。\n\n![img](dab9f6ee2908b05ed6f15f3e21be88ab-20230115031641962.jpg)\n\n在这个图中，加黑加粗的部分，是上面说到的主要流程，其中阿拉伯数字的序号，是连接过程中的顺序，而大写中文数字的序号，是连接断开过程中的顺序。加粗的实线是客户端A的状态变迁，加粗的虚线是服务端B的状态变迁。\n\n## 小结\n\n好了，这一节就到这里了，我来做一个总结：\n\n- TCP包头很复杂，但是主要关注五个问题，顺序问题，丢包问题，连接维护，流量控制，拥塞控制；\n- 连接的建立是经过三次握手，断开的时候四次挥手，一定要掌握的我画的那个状态图。\n\n最后，给你留两个思考题。\n\n1. TCP的连接有这么多的状态，你知道如何在系统中查看某个连接的状态吗？\n2. 这一节仅仅讲了连接维护问题，其实为了维护连接的状态，还有其他的数据结构来处理其他的四个问题，那你知道是什么吗？\n\n# 12 讲TCP协议（下）：西行必定多妖孽，恒心智慧消磨难\n\n我们前面说到玄奘西行，要出网关。既然出了网关，那就是在公网上传输数据，公网往往是不可靠的，因而需要很多的机制去保证传输的可靠性，这里面需要恒心，也即各种**重传的策略**，还需要有智慧，也就是说，这里面包含着**大量的算法**。\n\n## 如何做个靠谱的人？\n\nTCP想成为一个成熟稳重的人，成为一个靠谱的人。那一个人怎么样才算靠谱呢？咱们工作中经常就有这样的场景，比如你交代给下属一个事情以后，下属到底能不能做到，做到什么程度，什么时候能够交付，往往就会有应答，有回复。这样，处理事情的过程中，一旦有异常，你也可以尽快知道，而不是交代完之后就石沉大海，过了一个月再问，他说，啊我不记得了。\n\n对应到网络协议上，就是客户端每发送的一个包，服务器端都应该有个回复，如果服务器端超过一定的时间没有回复，客户端就会重新发送这个包，直到有回复。\n\n这个发送应答的过程是什么样呢？可以是**上一个收到了应答，再发送下一个**。这种模式有点像两个人直接打电话，你一句，我一句。但是这种方式的缺点是效率比较低。如果一方在电话那头处理的时间比较长，这一头就要干等着，双方都没办法干其他事情。咱们在日常工作中也不是这样的，不能你交代你的下属办一件事情，就一直打着电话看着他做，而是应该他按照你的安排，先将事情记录下来，办完一件回复一件。在他办事情的过程中，你还可以同时交代新的事情，这样双方就并行了。\n\n如果使⽤这种模式，其实需要你和你的下属就不能靠脑⼦了，⽽是要都准备⼀个本⼦，你每交代下属⼀个事情，双方的本子都要记录⼀下。\n\n当你的下属做完⼀件事情，就回复你，做完了，你就在你的本⼦上将这个事情划去。同时你的本⼦上每件事情都有时限，如果超过了时限下属还没有回复，你就要主动重新交代⼀下：上次那件事情，你还没回复我，咋样啦？\n\n既然多件事情可以一起处理，那就需要给每个事情编个号，防止弄错了。例如，程序员平时看任务的时候，都会看JIRA的ID，而不是每次都要描述一下具体的事情。在大部分情况下，对于事情的处理是按照顺序来的，先来的先处理，这就给应答和汇报工作带来了方便。等开周会的时候，每个程序员都可以将JIRA ID的列表拉出来，说以上的都做完了，⽽不⽤⼀个个说。\n\n## 如何实现一个靠谱的协议？\n\nTCP协议使用的也是同样的模式。为了保证顺序性，每一个包都有一个ID。在建立连接的时候，会商定起始的ID是什么，然后按照ID一个个发送。为了保证不丢包，对于发送的包都要进行应答，但是这个应答也不是一个一个来的，而是会应答某个之前的ID，表示都收到了，这种模式称为**累计确认**或者**累计应答**（**cumulative acknowledgment**）。\n\n为了记录所有发送的包和接收的包，TCP也需要发送端和接收端分别都有缓存来保存这些记录。发送端的缓存里是按照包的ID一个个排列，根据处理的情况分成四个部分。\n\n第一部分：发送了并且已经确认的。这部分就是你交代下属的，并且也做完了的，应该划掉的。\n\n第二部分：发送了并且尚未确认的。这部分是你交代下属的，但是还没做完的，需要等待做完的回复之后，才能划掉。\n\n第三部分：没有发送，但是已经等待发送的。这部分是你还没有交代给下属，但是马上就要交代的。\n\n第四部分：没有发送，并且暂时还不会发送的。这部分是你还没有交代给下属，而且暂时还不会交代给下属的。\n\n这里面为什么要区分第三部分和第四部分呢？没交代的，一下子全交代了不就完了吗？\n\n这就是我们上一节提到的十个词口诀里的“流量控制，把握分寸”。作为项目管理人员，你应该根据以往的工作情况和这个员工反馈的能力、抗压力等，先在心中估测一下，这个人一天能做多少工作。如果工作布置少了，就会不饱和；如果工作布置多了，他就会做不完；如果你使劲逼迫，人家可能就要辞职了。\n\n到底一个员工能够同时处理多少事情呢？在TCP里，接收端会给发送端报一个窗口的大小，叫**Advertised window**。这个窗口的大小应该等于上面的第二部分加上第三部分，就是已经交代了没做完的加上马上要交代的。超过这个窗口的，接收端做不过来，就不能发送了。\n\n于是，发送端需要保持下面的数据结构。\n\n![img](16dcd6fb8105a1caa75887b5ffa0bd7b-20230115031642102.jpg)\n\n- LastByteAcked：第一部分和第二部分的分界线\n- LastByteSent：第二部分和第三部分的分界线\n- LastByteAcked + AdvertisedWindow：第三部分和第四部分的分界线\n\n对于接收端来讲，它的缓存里记录的内容要简单一些。\n\n第一部分：接受并且确认过的。也就是我领导交代给我，并且我做完的。\n\n第二部分：还没接收，但是马上就能接收的。也即是我自己的能够接受的最大工作量。\n\n第三部分：还没接收，也没法接收的。也即超过工作量的部分，实在做不完。\n\n对应的数据结构就像这样。 ﻿﻿![img](f7b1d3bc6b6d8e55f0951e82294c8ba4-20230115031641904.jpg)\n\n- MaxRcvBuffer：最大缓存的量；\n- LastByteRead之后是已经接收了，但是还没被应用层读取的；\n- NextByteExpected是第一部分和第二部分的分界线。\n\n第二部分的窗口有多大呢？\n\nNextByteExpected和LastByteRead的差其实是还没被应用层读取的部分占用掉的MaxRcvBuffer的量，我们定义为A。\n\nAdvertisedWindow其实是MaxRcvBuffer减去A。\n\n也就是：AdvertisedWindow=MaxRcvBuffer-((NextByteExpected-1)-LastByteRead)。\n\n那第二部分和第三部分的分界线在哪里呢？NextByteExpected加AdvertisedWindow就是第二部分和第三部分的分界线，其实也就是LastByteRead加上MaxRcvBuffer。\n\n其中第二部分里面，由于受到的包可能不是顺序的，会出现空挡，只有和第一部分连续的，可以马上进行回复，中间空着的部分需要等待，哪怕后面的已经来了。\n\n## 顺序问题与丢包问题\n\n接下来我们结合一个例子来看。\n\n还是刚才的图，在发送端来看，1、2、3已经发送并确认；4、5、6、7、8、9都是发送了还没确认；10、11、12是还没发出的；13、14、15是接收方没有空间，不准备发的。\n\n在接收端来看，1、2、3、4、5是已经完成ACK，但是没读取的；6、7是等待接收的；8、9是已经接收，但是没有ACK的。\n\n发送端和接收端当前的状态如下：\n\n- 1、2、3没有问题，双方达成了一致。\n- 4、5接收方说ACK了，但是发送方还没收到，有可能丢了，有可能在路上。\n- 6、7、8、9肯定都发了，但是8、9已经到了，但是6、7没到，出现了乱序，缓存着但是没办法ACK。\n\n根据这个例子，我们可以知道，顺序问题和丢包问题都有可能发生，所以我们先来看**确认与重发的机制**。\n\n假设4的确认到了，不幸的是，5的ACK丢了，6、7的数据包丢了，这该怎么办呢？\n\n一种方法就是**超时重试**，也即对每一个发送了，但是没有ACK的包，都有设一个定时器，超过了一定的时间，就重新尝试。但是这个超时的时间如何评估呢？这个时间不宜过短，时间必须大于往返时间RTT，否则会引起不必要的重传。也不宜过长，这样超时时间变长，访问就变慢了。\n\n估计往返时间，需要TCP通过采样RTT的时间，然后进行加权平均，算出一个值，而且这个值还是要不断变化的，因为网络状况不断的变化。除了采样RTT，还要采样RTT的波动范围，计算出一个估计的超时时间。由于重传时间是不断变化的，我们称为**自适应重传算法**（**Adaptive Retransmission Algorithm**）。\n\n如果过一段时间，5、6、7都超时了，就会重新发送。接收方发现5原来接收过，于是丢弃5；6收到了，发送ACK，要求下一个是7，7不幸又丢了。当7再次超时的时候，有需要重传的时候，TCP的策略是**超时间隔加倍**。**每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍**。**两次超时，就说明网络环境差，不宜频繁反复发送。**\n\n超时触发重传存在的问题是，超时周期可能相对较长。那是不是可以有更快的方式呢？\n\n有一个可以快速重传的机制，当接收方收到一个序号大于下一个所期望的报文段时，就检测到了数据流中的一个间格，于是发送三个冗余的ACK，客户端收到后，就在定时器过期之前，重传丢失的报文段。\n\n例如，接收方发现6、8、9都已经接收了，就是7没来，那肯定是丢了，于是发送三个6的ACK，要求下一个是7。客户端收到3个，就会发现7的确又丢了，不等超时，马上重发。\n\n还有一种方式称为**Selective Acknowledgment** （**SACK**）。这种方式需要在TCP头里加一个SACK的东西，可以将缓存的地图发送给发送方。例如可以发送ACK6、SACK8、SACK9，有了地图，发送方一下子就能看出来是7丢了。\n\n## 流量控制问题\n\n我们再来看流量控制机制，在对于包的确认中，同时会携带一个窗口的大小。\n\n我们先假设窗口不变的情况，窗口始终为9。4的确认来的时候，会右移一个，这个时候第13个包也可以发送了。\n\n![img](7339fd8973865164d25227cac206ca33-20230115031641942.jpg)\n\n这个时候，假设发送端发送过猛，会将第三部分的10、11、12、13全部发送完毕，之后就停止发送了，未发送可发送部分为0。\n\n![img](06cc25118730fbf611eb315705420ed2-1584286441684-20230115031642184.jpg)\n\n当对于包5的确认到达的时候，在客户端相当于窗口再滑动了一格，这个时候，才可以有更多的包可以发送了，例如第14个包才可以发送。\n\n![img](bddb59ebbf7eecc4853cafce0bb1dcc3-20230115031642079.jpg)\n\n如果接收方实在处理的太慢，导致缓存中没有空间了，可以通过确认信息修改窗口的大小，甚至可以设置为0，则发送方将暂时停止发送。\n\n我们假设一个极端情况，接收端的应用一直不读取缓存中的数据，当数据包6确认后，窗口大小就不能再是9了，就要缩小一个变为8。\n\n![img](92f66b1556b76c46c669aba232d35a31-20230115031641991.jpg)\n\n这个新的窗口8通过6的确认消息到达发送端的时候，你会发现窗口没有平行右移，而是仅仅左面的边右移了，窗口的大小从9改成了8。\n\n![img](a78f5195ebf9b4f9dc4ea5a9b91e94ba-20230115031642018.jpg)\n\n如果接收端还是一直不处理数据，则随着确认的包越来越多，窗口越来越小，直到为0。\n\n![img](150f28d9e745952f5968eff05e3f0ad2-20230115031642034.jpg)\n\n当这个窗口通过包14的确认到达发送端的时候，发送端的窗口也调整为0，停止发送。\n\n![img](3014a6a259f74b0c950bf3067581ac30-20230115031642309.jpg)\n\n如果这样的话，发送方会定时发送窗口探测数据包，看是否有机会调整窗口的大小。当接收方比较慢的时候，要防止低能窗口综合征，别空出一个字节来就赶快告诉发送方，然后马上又填满了，可以当窗口太小的时候，不更新窗口，直到达到一定大小，或者缓冲区一半为空，才更新窗口。\n\n这就是我们常说的流量控制。\n\n## 拥塞控制问题\n\n最后，我们看一下拥塞控制的问题，也是通过窗口的大小来控制的，前面的滑动窗口rwnd是怕发送方把接收方缓存塞满，而拥塞窗口cwnd，是怕把网络塞满。\n\n这里有一个公式 LastByteSent - LastByteAcked \u003c= min {cwnd, rwnd} ，是拥塞窗口和滑动窗口共同控制发送的速度。\n\n那发送方怎么判断网络是不是满呢？这其实是个挺难的事情，因为对于TCP协议来讲，他压根不知道整个网络路径都会经历什么，对他来讲就是一个黑盒。TCP发送包常被比喻为往一个水管里面灌水，而TCP的拥塞控制就是在不堵塞，不丢包的情况下，尽量发挥带宽。\n\n水管有粗细，网络有带宽，也即每秒钟能够发送多少数据；水管有长度，端到端有时延。在理想状态下，水管里面水的量=水管粗细 x 水管长度。对于到网络上，通道的容量 = 带宽 × 往返延迟。\n\n如果我们设置发送窗口，使得发送但未确认的包为为通道的容量，就能够撑满整个管道。\n\n![img](db8510541662281175803c7f9d1fcae6-1584286462337-20230115031642095.jpg)\n\n如图所示，假设往返时间为8s，去4s，回4s，每秒发送一个包，每个包1024byte。已经过去了8s，则8个包都发出去了，其中前4个包已经到达接收端，但是ACK还没有返回，不能算发送成功。5-8后四个包还在路上，还没被接收。这个时候，整个管道正好撑满，在发送端，已发送未确认的为8个包，正好等于带宽，也即每秒发送1个包，乘以来回时间8s。\n\n如果我们在这个基础上再调大窗口，使得单位时间内更多的包可以发送，会出现什么现象呢？\n\n我们来想，原来发送一个包，从一端到达另一端，假设一共经过四个设备，每个设备处理一个包时间耗费1s，所以到达另一端需要耗费4s，如果发送的更加快速，则单位时间内，会有更多的包到达这些中间设备，这些设备还是只能每秒处理一个包的话，多出来的包就会被丢弃，这是我们不想看到的。\n\n这个时候，我们可以想其他的办法，例如这个四个设备本来每秒处理一个包，但是我们在这些设备上加缓存，处理不过来的在队列里面排着，这样包就不会丢失，但是缺点是会增加时延，这个缓存的包，4s肯定到达不了接收端了，如果时延达到一定程度，就会超时重传，也是我们不想看到的。\n\n于是TCP的拥塞控制主要来避免两种现象，**包丢失**和**超时重传**。一旦出现了这些现象就说明，发送速度太快了，要慢一点。但是一开始我怎么知道速度多快呢，我怎么知道应该把窗口调整到多大呢？\n\n如果我们通过漏斗往瓶子里灌水，我们就知道，不能一桶水一下子倒进去，肯定会溅出来，要一开始慢慢的倒，然后发现总能够倒进去，就可以越倒越快。这叫作慢启动。\n\n一条TCP连接开始，cwnd设置为一个报文段，一次只能发送一个；当收到这一个确认的时候，cwnd加一，于是一次能够发送两个；当这两个的确认到来的时候，每个确认cwnd加一，两个确认cwnd加二，于是一次能够发送四个；当这四个的确认到来的时候，每个确认cwnd加一，四个确认cwnd加四，于是一次能够发送八个。可以看出这是**指数性的增长**。\n\n涨到什么时候是个头呢？有一个值ssthresh为65535个字节，当超过这个值的时候，就要小心一点了，不能倒这么快了，可能快满了，再慢下来。\n\n每收到一个确认后，cwnd增加1/cwnd，我们接着上面的过程来，一次发送八个，当八个确认到来的时候，每个确认增加1/8，八个确认一共cwnd增加1，于是一次能够发送九个，变成了线性增长。\n\n但是线性增长还是增长，还是越来越多，直到有一天，水满则溢，出现了拥塞，这时候一般就会一下子降低倒水的速度，等待溢出的水慢慢渗下去。\n\n拥塞的一种表现形式是丢包，需要超时重传，这个时候，将sshresh设为cwnd/2，将cwnd设为1，重新开始慢启动。这真是一旦超时重传，马上回到解放前。但是这种方式太激进了，将一个高速的传输速度一下子停了下来，会造成网络卡顿。\n\n前面我们讲过**快速重传算法**。当接收端发现丢了一个中间包的时候，发送三次前一个包的ACK，于是发送端就会快速的重传，不必等待超时再重传。TCP认为这种情况不严重，因为大部分没丢，只丢了一小部分，cwnd减半为cwnd/2，然后sshthresh = cwnd，当三个包返回的时候，cwnd = sshthresh + 3，也就是没有一夜回到解放前，而是还在比较高的值，呈线性增长。\n\n![img](1910bc1a0048d4de7b2128eb0f5dbcd2-1584286484075-20230115031642177.jpg)\n\n就像前面说的一样，正是这种知进退，使得时延很重要的情况下，反而降低了速度。但是如果你仔细想一下，TCP的拥塞控制主要来避免的两个现象都是有问题的。\n\n**第一个问题**是丢包并不代表着通道满了，也可能是管子本来就漏水。例如公网上带宽不满也会丢包，这个时候就认为拥塞了，退缩了，其实是不对的。\n\n**第二个问题**是TCP的拥塞控制要等到将中间设备都填充满了，才发生丢包，从而降低速度，这时候已经晚了。其实TCP只要填满管道就可以了，不应该接着填，直到连缓存也填满。\n\n为了优化这两个问题，后来有了**TCP BBR拥塞算法**。它企图找到一个平衡点，就是通过不断的加快发送速度，将管道填满，但是不要填满中间设备的缓存，因为这样时延会增加，在这个平衡点可以很好的达到高带宽和低时延的平衡。\n\n![img](a2b3a5df5eca52e302b75824e4bbbd4c-1584286500709-20230115031642172.jpg)\n\n## 小结\n\n好了，这一节我们就到这里，总结一下：\n\n- 顺序问题、丢包问题、流量控制都是通过滑动窗口来解决的，这其实就相当于你领导和你的工作备忘录，布置过的工作要有编号，干完了有反馈，活不能派太多，也不能太少；\n- 拥塞控制是通过拥塞窗口来解决的，相当于往管道里面倒水，快了容易溢出，慢了浪费带宽，要摸着石头过河，找到最优值。\n\n最后留两个思考题：\n\n1. TCP的BBR听起来很牛，你知道他是如何达到这个最优点的嘛？\n2. 学会了UDP和TCP，你知道如何基于这两种协议写程序吗？这样的程序会有什么坑呢？\n\n# 13 讲套接字Socket：Talkischeap,showmethecode\n\n前面讲完了TCP和UDP协议，还没有上手过，这一节咱们讲讲基于TCP和UDP协议的Socket编程。\n\n在讲TCP和UDP协议的时候，我们分客户端和服务端，在写程序的时候，我们也同样这样分。\n\nSocket这个名字很有意思，可以作插口或者插槽讲。虽然我们是写软件程序，但是你可以想象为弄一根网线，一头插在客户端，一头插在服务端，然后进行通信。所以在通信之前，双方都要建立一个Socket。\n\n在建立Socket的时候，应该设置什么参数呢？Socket编程进行的是端到端的通信，往往意识不到中间经过多少局域网，多少路由器，因而能够设置的参数，也只能是端到端协议之上网络层和传输层的。\n\n在网络层，Socket函数需要指定到底是IPv4还是IPv6，分别对应设置为AF_INET和AF_INET6。另外，还要指定到底是TCP还是UDP。还记得咱们前面讲过的，TCP协议是基于数据流的，所以设置为SOCK_STREAM，而UDP是基于数据报的，因而设置为SOCK_DGRAM。\n\n## 基于TCP协议的Socket程序函数调用过程\n\n两端创建了Socket之后，接下来的过程中，TCP和UDP稍有不同，我们先来看TCP。\n\nTCP的服务端要先监听一个端口，一般是先调用bind函数，给这个Socket赋予一个IP地址和端口。为什么需要端口呢？要知道，你写的是一个应用程序，当一个网络包来的时候，内核要通过TCP头里面的这个端口，来找到你这个应用程序，把包给你。为什么要IP地址呢？有时候，一台机器会有多个网卡，也就会有多个IP地址，你可以选择监听所有的网卡，也可以选择监听一个网卡，这样，只有发给这个网卡的包，才会给你。\n\n当服务端有了IP和端口号，就可以调用listen函数进行监听。在TCP的状态图里面，有一个listen状态，当调用这个函数之后，服务端就进入了这个状态，这个时候客户端就可以发起连接了。\n\n在内核中，为每个Socket维护两个队列。一个是已经建立了连接的队列，这时候连接三次握手已经完毕，处于established状态；一个是还没有完全建立连接的队列，这个时候三次握手还没完成，处于syn_rcvd的状态。\n\n接下来，服务端调用accept函数，拿出一个已经完成的连接进行处理。如果还没有完成，就要等着。\n\n在服务端等待的时候，客户端可以通过connect函数发起连接。先在参数中指明要连接的IP地址和端口号，然后开始发起三次握手。内核会给客户端分配一个临时的端口。一旦握手成功，服务端的accept就会返回另一个Socket。\n\n这是一个经常考的知识点，就是监听的Socket和真正用来传数据的Socket是两个，一个叫作**监听Socket**，一个叫作**已连接Socket**。\n\n连接建立成功之后，双方开始通过read和write函数来读写数据，就像往一个文件流里面写东西一样。\n\n这个图就是基于TCP协议的Socket程序函数调用过程。\n\n![img](77d5eeb659d5347874bda5e8f711f692-20230115031642585.jpg)\n\n说TCP的Socket就是一个文件流，是非常准确的。因为，Socket在Linux中就是以文件的形式存在的。除此之外，还存在文件描述符。写入和读出，也是通过文件描述符。\n\n在内核中，Socket是一个文件，那对应就有文件描述符。每一个进程都有一个数据结构task_struct，里面指向一个文件描述符数组，来列出这个进程打开的所有文件的文件描述符。文件描述符是一个整数，是这个数组的下标。\n\n这个数组中的内容是一个指针，指向内核中所有打开的文件的列表。既然是一个文件，就会有一个inode，只不过Socket对应的inode不像真正的文件系统一样，保存在硬盘上的，而是在内存中的。在这个inode中，指向了Socket在内核中的Socket结构。\n\n在这个结构里面，主要的是两个队列，一个是**发送队列**，一个是**接收队列**。在这两个队列里面保存的是一个缓存sk_buff。这个缓存里面能够看到完整的包的结构。看到这个，是不是能和前面讲过的收发包的场景联系起来了？\n\n整个数据结构我也画了一张图。\n\n![img](602d09290bd4f9e0183f530e9653348c-20230115031642298.jpg)\n\n## 基于UDP协议的Socket程序函数调用过程\n\n对于UDP来讲，过程有些不一样。UDP是没有连接的，所以不需要三次握手，也就不需要调用listen和connect，但是，UDP的的交互仍然需要IP和端口号，因而也需要bind。UDP是没有维护连接状态的，因而不需要每对连接建立一组Socket，而是只要有一个Socket，就能够和多个客户端通信。也正是因为没有连接状态，每次通信的时候，都调用sendto和recvfrom，都可以传入IP地址和端口。\n\n这个图的内容就是基于UDP协议的Socket程序函数调用过程。\n\n![img](778687d1a02ffc0c24078c33be2ac1ef-20230115031642299.jpg)﻿﻿\n\n## 服务器如何接更多的项目？\n\n会了这几个基本的Socket函数之后，你就可以轻松地写一个网络交互的程序了。就像上面的过程一样，在建立连接后，进行一个while循环。客户端发了收，服务端收了发。\n\n当然这只是万里长征的第一步，因为如果使用这种方法，基本上只能一对一沟通。如果你是一个服务器，同时只能服务一个客户，肯定是不行的。这就相当于老板成立一个公司，只有自己一个人，自己亲自上来服务客户，只能干完了一家再干下一家，这样赚不来多少钱。\n\n那作为老板你就要想了，我最多能接多少项目呢？当然是越多越好。\n\n我们先来算一下理论值，也就是**最大连接数**，系统会用一个四元组来标识一个TCP连接。\n\n```undefined\n{本机IP, 本机端口, 对端IP, 对端端口}\n```\n\n服务器通常固定在某个本地端口上监听，等待客户端的连接请求。因此，服务端端TCP连接四元组中只有对端IP, 也就是客户端的IP和对端的端口，也即客户端的端口是可变的，因此，最大TCP连接数=客户端IP数×客户端端口数。对IPv4，客户端的IP数最多为2的32次方，客户端的端口数最多为2的16次方，也就是服务端单机最大TCP连接数，约为2的48次方。\n\n当然，服务端最大并发TCP连接数远不能达到理论上限。首先主要是**文件描述符限制**，按照上面的原理，Socket都是文件，所以首先要通过ulimit配置文件描述符的数目；另一个限制是**内存**，按上面的数据结构，每个TCP连接都要占用一定内存，操作系统是有限的。\n\n所以，作为老板，在资源有限的情况下，要想接更多的项目，就需要降低每个项目消耗的资源数目。\n\n### 方式一：将项目外包给其他公司（多进程方式）\n\n这就相当于你是一个代理，在那里监听来的请求。一旦建立了一个连接，就会有一个已连接Socket，这时候你可以创建一个子进程，然后将基于已连接Socket的交互交给这个新的子进程来做。就像来了一个新的项目，但是项目不一定是你自己做，可以再注册一家子公司，招点人，然后把项目转包给这家子公司做，以后对接就交给这家子公司了，你又可以去接新的项目了。\n\n这里有一个问题是，如何创建子公司，并如何将项目移交给子公司呢？\n\n在Linux下，创建子进程使用fork函数。通过名字可以看出，这是在父进程的基础上完全拷贝一个子进程。在Linux内核中，会复制文件描述符的列表，也会复制内存空间，还会复制一条记录当前执行到了哪一行程序的进程。显然，复制的时候在调用fork，复制完毕之后，父进程和子进程都会记录当前刚刚执行完fork。这两个进程刚复制完的时候，几乎一模一样，只是根据fork的返回值来区分到底是父进程，还是子进程。如果返回值是0，则是子进程；如果返回值是其他的整数，就是父进程。\n\n进程复制过程我画在这里。\n\n![img](d353eee3c387332e378c1e517c642f1c-20230115031642593.jpg)\n\n因为复制了文件描述符列表，而文件描述符都是指向整个内核统一的打开文件列表的，因而父进程刚才因为accept创建的已连接Socket也是一个文件描述符，同样也会被子进程获得。\n\n接下来，子进程就可以通过这个已连接Socket和客户端进行互通了，当通信完毕之后，就可以退出进程，那父进程如何知道子进程干完了项目，要退出呢？还记得fork返回的时候，如果是整数就是父进程吗？这个整数就是子进程的ID，父进程可以通过这个ID查看子进程是否完成项目，是否需要退出。\n\n### 方式二：将项目转包给独立的项目组（多线程方式）\n\n上面这种方式你应该也能发现问题，如果每次接一个项目，都申请一个新公司，然后干完了，就注销掉这个公司，实在是太麻烦了。毕竟一个新公司要有新公司的资产，有新的办公家具，每次都买了再卖，不划算。\n\n于是你应该想到了，我们可以使用**线程**。相比于进程来讲，这样要轻量级的多。如果创建进程相当于成立新公司，购买新办公家具，而创建线程，就相当于在同一个公司成立项目组。一个项目做完了，那这个项目组就可以解散，组成另外的项目组，办公家具可以共用。\n\n在Linux下，通过pthread_create创建一个线程，也是调用do_fork。不同的是，虽然新的线程在task列表会新创建一项，但是很多资源，例如文件描述符列表、进程空间，还是共享的，只不过多了一个引用而已。\n\n![img](ab6e0ecfee5e21f7a563999a94bd8bd7-20230115031642341.jpg)﻿ 新的线程也可以通过已连接Socket处理请求，从而达到并发处理的目的。\n\n上面基于进程或者线程模型的，其实还是有问题的。新到来一个TCP连接，就需要分配一个进程或者线程。一台机器无法创建很多进程或者线程。有个**C10K**，它的意思是一台机器要维护1万个连接，就要创建1万个进程或者线程，那么操作系统是无法承受的。如果维持1亿用户在线需要10万台服务器，成本也太高了。\n\n其实C10K问题就是，你接项目接的太多了，如果每个项目都成立单独的项目组，就要招聘10万人，你肯定养不起，那怎么办呢？\n\n### 方式三：一个项目组支撑多个项目（IO多路复用，一个线程维护多个Socket）\n\n当然，一个项目组可以看多个项目了。这个时候，每个项目组都应该有个项目进度墙，将自己组看的项目列在那里，然后每天通过项目墙看每个项目的进度，一旦某个项目有了进展，就派人去盯一下。\n\n由于Socket是文件描述符，因而某个线程盯的所有的Socket，都放在一个文件描述符集合fd_set中，这就是**项目进度墙**，然后调用select函数来监听文件描述符集合是否有变化。一旦有变化，就会依次查看每个文件描述符。那些发生变化的文件描述符在fd_set对应的位都设为1，表示Socket可读或者可写，从而可以进行读写操作，然后再调用select，接着盯着下一轮的变化。。\n\n### 方式四：一个项目组支撑多个项目（IO多路复用，从“派人盯着”到“有事通知”）\n\n上面select函数还是有问题的，因为每次Socket所在的文件描述符集合中有Socket发生变化的时候，都需要通过轮询的方式，也就是需要将全部项目都过一遍的方式来查看进度，这大大影响了一个项目组能够支撑的最大的项目数量。因而使用select，能够同时盯的项目数量由FD_SETSIZE限制。\n\n如果改成事件通知的方式，情况就会好很多，项目组不需要通过轮询挨个盯着这些项目，而是当项目进度发生变化的时候，主动通知项目组，然后项目组再根据项目进展情况做相应的操作。\n\n能完成这件事情的函数叫epoll，它在内核中的实现不是通过轮询的方式，而是通过注册callback函数的方式，当某个文件描述符发送变化的时候，就会主动通知。\n\n![img](cff688ede147809da4d65fe4152ffb19-20230115031642396.jpg)\n\n如图所示，假设进程打开了Socket m, n, x等多个文件描述符，现在需要通过epoll来监听是否这些Socket都有事件发生。其中epoll_create创建一个epoll对象，也是一个文件，也对应一个文件描述符，同样也对应着打开文件列表中的一项。在这项里面有一个红黑树，在红黑树里，要保存这个epoll要监听的所有Socket。\n\n当epoll_ctl添加一个Socket的时候，其实是加入这个红黑树，同时红黑树里面的节点指向一个结构，将这个结构挂在被监听的Socket的事件列表中。当一个Socket来了一个事件的时候，可以从这个列表中得到epoll对象，并调用call back通知它。\n\n这种通知方式使得监听的Socket数据增加的时候，效率不会大幅度降低，能够同时监听的Socket的数目也非常的多了。上限就为系统定义的、进程打开的最大文件描述符个数。因而，**epoll被称为解决C10K问题的利器**。\n\n## 小结\n\n好了，这一节就到这里了，我们来总结一下：\n\n- 你需要记住TCP和UDP的Socket的编程中，客户端和服务端都需要调用哪些函数；\n- 写一个能够支撑大量连接的高并发的服务端不容易，需要多进程、多线程，而epoll机制能解决C10K问题。\n\n最后，给你留两个思考题：\n\n1. epoll是Linux上的函数，那你知道Windows上对应的机制是什么吗？如果想实现一个跨平台的程序，你知道应该怎么办吗？\n2. 自己写Socket还是挺复杂的，写个HTTP的应用可能简单一些。那你知道HTTP的工作机制吗？\n\n# 14 讲HTTP协议：看个新闻原来这么麻烦\n\n前面讲述完**传输层**，接下来开始讲**应用层**的协议。从哪里开始讲呢，就从咱们最常用的HTTP协议开始。\n\nHTTP协议，几乎是每个人上网用的第一个协议，同时也是很容易被人忽略的协议。\n\n既然说看新闻，咱们就先登录 [http://www.163.com](https://www.163.com/) 。\n\n[http://www.163.com](https://www.163.com/) 是个URL，叫作**统一资源定位符**。之所以叫统一，是因为它是有格式的。HTTP称为协议，www.163.com是一个域名，表示互联网上的一个位置。有的URL会有更详细的位置标识，例如 [http://www.163.com/index.html](https://www.163.com/index.html) 。正是因为这个东西是统一的，所以当你把这样一个字符串输入到浏览器的框里的时候，浏览器才知道如何进行统一处理。\n\n## HTTP请求的准备\n\n浏览器会将www.163.com这个域名发送给DNS服务器，让它解析为IP地址。有关DNS的过程，其实非常复杂，这个在后面专门介绍DNS的时候，我会详细描述，这里我们先不管，反正它会被解析成为IP地址。那接下来是发送HTTP请求吗？\n\n不是的，HTTP是基于TCP协议的，当然是要先建立TCP连接了，怎么建立呢？还记得第11节讲过的三次握手吗？\n\n目前使用的HTTP协议大部分都是1.1。在1.1的协议里面，默认是开启了Keep-Alive的，这样建立的TCP连接，就可以在多次请求中复用。\n\n学习了TCP之后，你应该知道，TCP的三次握手和四次挥手，还是挺费劲的。如果好不容易建立了连接，然后就做了一点儿事情就结束了，有点儿浪费人力和物力。\n\n## HTTP请求的构建\n\n建立了连接以后，浏览器就要发送HTTP的请求。\n\n请求的格式就像这样。\n\n![img](10ff27d1032bf32393195f23ef2f9874-20230115031642371.jpg)\n\nHTTP的报文大概分为三大部分。第一部分是**请求行**，第二部分是请求的**首部**，第三部分才是请求的**正文实体**。\n\n### 第一部分：请求行\n\n在请求行中，URL就是 [http://www.163.com](https://www.163.com/) ，版本为HTTP 1.1。这里要说一下的，就是方法。方法有几种类型。\n\n对于访问网页来讲，最常用的类型就是**GET**。顾名思义，GET就是去服务器获取一些资源。对于访问网页来讲，要获取的资源往往是一个页面。其实也有很多其他的格式，比如说返回一个JSON字符串，到底要返回什么，是由服务器端的实现决定的。\n\n例如，在云计算中，如果我们的服务器端要提供一个基于HTTP协议的API，获取所有云主机的列表，这就会使用GET方法得到，返回的可能是一个JSON字符串。字符串里面是一个列表，列表里面是一项的云主机的信息。\n\n另外一种类型叫做**POST**。它需要主动告诉服务端一些信息，而非获取。要告诉服务端什么呢？一般会放在正文里面。正文可以有各种各样的格式。常见的格式也是JSON。\n\n例如，我们下一节要讲的支付场景，客户端就需要把“我是谁？我要支付多少？我要买啥？”告诉服务器，这就需要通过POST方法。\n\n再如，在云计算里，如果我们的服务器端，要提供一个基于HTTP协议的创建云主机的API，也会用到POST方法。这个时候往往需要将“我要创建多大的云主机？多少CPU多少内存？多大硬盘？”这些信息放在JSON字符串里面，通过POST的方法告诉服务器端。\n\n还有一种类型叫**PUT**，就是向指定资源位置上传最新内容。但是，HTTP的服务器往往是不允许上传文件的，所以PUT和POST就都变成了要传给服务器东西的方法。\n\n在实际使用过程中，这两者还会有稍许的区别。POST往往是用来创建一个资源的，而PUT往往是用来修改一个资源的。\n\n例如，云主机已经创建好了，我想对这个云主机打一个标签，说明这个云主机是生产环境的，另外一个云主机是测试环境的。那怎么修改这个标签呢？往往就是用PUT方法。\n\n再有一种常见的就是**DELETE**。这个顾名思义就是用来删除资源的。例如，我们要删除一个云主机，就会调用DELETE方法。\n\n### 第二部分：首部字段\n\n请求行下面就是我们的首部字段。首部是key value，通过冒号分隔。这里面，往往保存了一些非常重要的字段。\n\n例如，**Accept-Charset**，表示**客户端可以接受的字符集**。防止传过来的是另外的字符集，从而导致出现乱码。\n\n再如，**Content-Type**是指**正文的格式**。例如，我们进行POST的请求，如果正文是JSON，那么我们就应该将这个值设置为JSON。\n\n这里需要重点说一下的就是**缓存**。为啥要使用缓存呢？那是因为一个非常大的页面有很多东西。\n\n例如，我浏览一个商品的详情，里面有这个商品的价格、库存、展示图片、使用手册等等。商品的展示图片会保持较长时间不变，而库存会根据用户购买的情况经常改变。如果图片非常大，而库存数非常小，如果我们每次要更新数据的时候都要刷新整个页面，对于服务器的压力就会很大。\n\n对于这种高并发场景下的系统，在真正的业务逻辑之前，都需要有个接入层，将这些静态资源的请求拦在最外面。\n\n这个架构的图就像这样。\n\n![img](c81af7a52305f7de27e32e34a02d0eac-20230115031642595.jpg)\n\n其中DNS、CDN我在后面的章节会讲。和这一节关系比较大的就是Nginx这一层，它如何处理HTTP协议呢？对于静态资源，有Vanish缓存层。当缓存过期的时候，才会访问真正的Tomcat应用集群。\n\n在HTTP头里面，**Cache-control**是用来**控制缓存**的。当客户端发送的请求中包含max-age指令时，如果判定缓存层中，资源的缓存时间数值比指定时间的数值小，那么客户端可以接受缓存的资源；当指定max-age值为0，那么缓存层通常需要将请求转发给应用集群。\n\n另外，**If-Modified-Since**也是一个关于缓存的。也就是说，如果服务器的资源在某个时间之后更新了，那么客户端就应该下载最新的资源；如果没有更新，服务端会返回“304 Not Modified”的响应，那客户端就不用下载了，也会节省带宽。\n\n到此为止，我们仅仅是拼凑起了HTTP请求的报文格式，接下来，浏览器会把它交给下一层传输层。怎么交给传输层呢？其实也无非是用Socket这些东西，只不过用的浏览器里，这些程序不需要你自己写，有人已经帮你写好了。\n\n## HTTP请求的发送\n\nHTTP协议是基于TCP协议的，所以它使用面向连接的方式发送请求，通过stream二进制流的方式传给对方。当然，到了TCP层，它会把二进制流变成一个的报文段发送给服务器。\n\n在发送给每个报文段的时候，都需要对方有一个回应ACK，来保证报文可靠地到达了对方。如果没有回应，那么TCP这一层会进行重新传输，直到可以到达。同一个包有可能被传了好多次，但是HTTP这一层不需要知道这一点，因为是TCP这一层在埋头苦干。\n\nTCP层发送每一个报文的时候，都需要加上自己的地址（即源地址）和它想要去的地方（即目标地址），将这两个信息放到IP头里面，交给IP层进行传输。\n\nIP层需要查看目标地址和自己是否是在同一个局域网。如果是，就发送ARP协议来请求这个目标地址对应的MAC地址，然后将源MAC和目标MAC放入MAC头，发送出去即可；如果不在同一个局域网，就需要发送到网关，还要需要发送ARP协议，来获取网关的MAC地址，然后将源MAC和网关MAC放入MAC头，发送出去。\n\n网关收到包发现MAC符合，取出目标IP地址，根据路由协议找到下一跳的路由器，获取下一跳路由器的MAC地址，将包发给下一跳路由器。\n\n这样路由器一跳一跳终于到达目标的局域网。这个时候，最后一跳的路由器能够发现，目标地址就在自己的某一个出口的局域网上。于是，在这个局域网上发送ARP，获得这个目标地址的MAC地址，将包发出去。\n\n目标的机器发现MAC地址符合，就将包收起来；发现IP地址符合，根据IP头中协议项，知道自己上一层是TCP协议，于是解析TCP的头，里面有序列号，需要看一看这个序列包是不是我要的，如果是就放入缓存中然后返回一个ACK，如果不是就丢弃。\n\nTCP头里面还有端口号，HTTP的服务器正在监听这个端口号。于是，目标机器自然知道是HTTP服务器这个进程想要这个包，于是将包发给HTTP服务器。HTTP服务器的进程看到，原来这个请求是要访问一个网页，于是就把这个网页发给客户端。\n\n## HTTP返回的构建\n\nHTTP的返回报文也是有一定格式的。这也是基于HTTP 1.1的。\n\n![img](1c2cfd4326d0dfca652ac8501321fac1-20230115031642423.jpg)\n\n状态码会反应HTTP请求的结果。“200”意味着大吉大利；而我们最不想见的，就是“404”，也就是“服务端无法响应这个请求”。然后，短语会大概说一下原因。\n\n接下来是返回首部的**key value**。\n\n这里面，**Retry-After**表示，告诉客户端应该在多长时间以后再次尝试一下。“503错误”是说“服务暂时不再和这个值配合使用”。\n\n在返回的头部里面也会有**Content-Type**，表示返回的是HTML，还是JSON。\n\n构造好了返回的HTTP报文，接下来就是把这个报文发送出去。还是交给Socket去发送，还是交给TCP层，让TCP层将返回的HTML，也分成一个个小的段，并且保证每个段都可靠到达。\n\n这些段加上TCP头后会交给IP层，然后把刚才的发送过程反向走一遍。虽然两次不一定走相同的路径，但是逻辑过程是一样的，一直到达客户端。\n\n客户端发现MAC地址符合、IP地址符合，于是就会交给TCP层。根据序列号看是不是自己要的报文段，如果是，则会根据TCP头中的端口号，发给相应的进程。这个进程就是浏览器，浏览器作为客户端也在监听某个端口。\n\n当浏览器拿到了HTTP的报文。发现返回“200”，一切正常，于是就从正文中将HTML拿出来。HTML是一个标准的网页格式。浏览器只要根据这个格式，展示出一个绚丽多彩的网页。\n\n这就是一个正常的HTTP请求和返回的完整过程。\n\n## HTTP 2.0\n\n当然HTTP协议也在不断地进化过程中，在HTTP1.1基础上便有了HTTP 2.0。\n\nHTTP 1.1在应用层以纯文本的形式进行通信。每次通信都要带完整的HTTP的头，而且不考虑pipeline模式的话，每次的过程总是像上面描述的那样一去一回。这样在实时性、并发性上都存在问题。\n\n为了解决这些问题，HTTP 2.0会对HTTP的头进行一定的压缩，将原来每次都要携带的大量key value在两端建立一个索引表，对相同的头只发送索引表中的索引。\n\n另外，HTTP 2.0协议将一个TCP的连接中，切分成多个流，每个流都有自己的ID，而且流可以是客户端发往服务端，也可以是服务端发往客户端。它其实只是一个虚拟的通道。流是有优先级的。\n\nHTTP 2.0还将所有的传输信息分割为更小的消息和帧，并对它们采用二进制格式编码。常见的帧有**Header帧**，用于传输Header内容，并且会开启一个新的流。再就是**Data帧**，用来传输正文实体。多个Data帧属于同一个流。\n\n通过这两种机制，HTTP 2.0的客户端可以将多个请求分到不同的流中，然后将请求内容拆成帧，进行二进制传输。这些帧可以打散乱序发送， 然后根据每个帧首部的流标识符重新组装，并且可以根据优先级，决定优先处理哪个流的数据。\n\n我们来举一个例子。\n\n假设我们的一个页面要发送三个独立的请求，一个获取css，一个获取js，一个获取图片jpg。如果使用HTTP 1.1就是串行的，但是如果使用HTTP 2.0，就可以在一个连接里，客户端和服务端都可以同时发送多个请求或回应，而且不用按照顺序一对一对应。\n\n![img](0bc51f8f887aae04ef89a1a88cb5a17a-20230115031642441.jpg)\n\nHTTP 2.0其实是将三个请求变成三个流，将数据分成帧，乱序发送到一个TCP连接中。\n\n![img](03d4a216c024a9e761ed43c6787bf7dd-20230115031642493.jpg)\n\nHTTP 2.0成功解决了HTTP 1.1的队首阻塞问题，同时，也不需要通过HTTP 1.x的pipeline机制用多条TCP连接来实现并行请求与响应；减少了TCP连接数对服务器性能的影响，同时将页面的多个数据css、js、 jpg等通过一个数据链接进行传输，能够加快页面组件的传输速度。\n\n## QUIC协议的“城会玩”\n\nHTTP 2.0虽然大大增加了并发性，但还是有问题的。因为HTTP 2.0也是基于TCP协议的，TCP协议在处理包时是有严格顺序的。\n\n当其中一个数据包遇到问题，TCP连接需要等待这个包完成重传之后才能继续进行。虽然HTTP 2.0通过多个stream，使得逻辑上一个TCP连接上的并行内容，进行多路数据的传输，然而这中间并没有关联的数据。一前一后，前面stream 2的帧没有收到，后面stream 1的帧也会因此阻塞。\n\n于是，就又到了从TCP切换到UDP，进行“城会玩”的时候了。这就是Google的QUIC协议，接下来我们来看它是如何“城会玩”的。\n\n### 机制一：自定义连接机制\n\n我们都知道，一条TCP连接是由四元组标识的，分别是源 IP、源端口、目的 IP、目的端口。一旦一个元素发生变化时，就需要断开重连，重新连接。在移动互联情况下，当手机信号不稳定或者在WIFI和 移动网络切换时，都会导致重连，从而进行再次的三次握手，导致一定的时延。\n\n这在TCP是没有办法的，但是基于UDP，就可以在QUIC自己的逻辑里面维护连接的机制，不再以四元组标识，而是以一个64位的随机数作为ID来标识，而且UDP是无连接的，所以当IP或者端口变化的时候，只要ID不变，就不需要重新建立连接。\n\n### 机制二：自定义重传机制\n\n前面我们讲过，TCP为了保证可靠性，通过使用**序号**和**应答**机制，来解决顺序问题和丢包问题。\n\n任何一个序号的包发过去，都要在一定的时间内得到应答，否则一旦超时，就会重发这个序号的包。那怎么样才算超时呢？还记得我们提过的**自适应重传算法**吗？这个超时是通过**采样往返时间RTT**不断调整的。\n\n其实，在TCP里面超时的采样存在不准确的问题。例如，发送一个包，序号为100，发现没有返回，于是再发送一个100，过一阵返回一个ACK101。这个时候客户端知道这个包肯定收到了，但是往返时间是多少呢？是ACK到达的时间减去后一个100发送的时间，还是减去前一个100发送的时间呢？事实是，第一种算法把时间算短了，第二种算法把时间算长了。\n\nQUIC也有个序列号，是递增的。任何一个序列号的包只发送一次，下次就要加一了。例如，发送一个包，序号是100，发现没有返回；再次发送的时候，序号就是101了；如果返回的ACK 100，就是对第一个包的响应。如果返回ACK 101就是对第二个包的响应，RTT计算相对准确。\n\n但是这里有一个问题，就是怎么知道包100和包101发送的是同样的内容呢？QUIC定义了一个offset概念。QUIC既然是面向连接的，也就像TCP一样，是一个数据流，发送的数据在这个数据流里面有个偏移量offset，可以通过offset查看数据发送到了哪里，这样只要这个offset的包没有来，就要重发；如果来了，按照offset拼接，还是能够拼成一个流。\n\n![img](da2af1e419db66929dc85107c7250fc4-20230115031642511.jpg)\n\n### 机制三：无阻塞的多路复用\n\n有了自定义的连接和重传机制，我们就可以解决上面HTTP 2.0的多路复用问题。\n\n同HTTP 2.0一样，同一条QUIC连接上可以创建多个stream，来发送多个 HTTP 请求。但是，QUIC是基于UDP的，一个连接上的多个stream之间没有依赖。这样，假如stream2丢了一个UDP包，后面跟着stream3的一个UDP包，虽然stream2的那个包需要重传，但是stream3的包无需等待，就可以发给用户。\n\n### 机制四：自定义流量控制\n\nTCP的流量控制是通过**滑动窗口协议**。QUIC的流量控制也是通过window_update，来告诉对端它可以接受的字节数。但是QUIC的窗口是适应自己的多路复用机制的，不但在一个连接上控制窗口，还在一个连接中的每个stream控制窗口。\n\n还记得吗？在TCP协议中，接收端的窗口的起始点是下一个要接收并且ACK的包，即便后来的包都到了，放在缓存里面，窗口也不能右移，因为TCP的ACK机制是基于序列号的累计应答，一旦ACK了一个系列号，就说明前面的都到了，所以只要前面的没到，后面的到了也不能ACK，就会导致后面的到了，也有可能超时重传，浪费带宽。\n\nQUIC的ACK是基于offset的，每个offset的包来了，进了缓存，就可以应答，应答后就不会重发，中间的空挡会等待到来或者重发即可，而窗口的起始位置为当前收到的最大offset，从这个offset到当前的stream所能容纳的最大缓存，是真正的窗口大小。显然，这样更加准确。\n\n![img](a66563b46906e7708cc69a02d43afb22-20230115031642523.jpg)\n\n另外，还有整个连接的窗口，需要对于所有的stream的窗口做一个统计。\n\n## 小结\n\n好了，今天就讲到这里，我们来总结一下：\n\n- HTTP协议虽然很常用，也很复杂，重点记住GET、POST、 PUT、DELETE这几个方法，以及重要的首部字段；\n- HTTP 2.0通过头压缩、分帧、二进制编码、多路复用等技术提升性能；\n- QUIC协议通过基于UDP自定义的类似TCP的连接、重试、多路复用、流量控制技术，进一步提升性能。\n\n接下来，给你留两个思考题吧。\n\n1. QUIC是一个精巧的协议，所以它肯定不止今天我提到的四种机制，你知道它还有哪些吗？\n2. 这一节主要讲了如何基于HTTP浏览网页，如果要传输比较敏感的银行卡信息，该怎么办呢？\n\n# 15 讲HTTPS协议：点外卖的过程原来这么复杂\n\n用HTTP协议，看个新闻还没有问题，但是换到更加严肃的场景中，就存在很多的安全风险。例如，你要下单做一次支付，如果还是使用普通的HTTP协议，那你很可能会被黑客盯上。\n\n你发送一个请求，说我要点个外卖，但是这个网络包被截获了，于是在服务器回复你之前，黑客先假装自己就是外卖网站，然后给你回复一个假的消息说：“好啊好啊，来来来，银行卡号、密码拿来。”如果这时候你真把银行卡密码发给它，那你就真的上套了。\n\n那怎么解决这个问题呢？当然一般的思路就是**加密**。加密分为两种方式一种是**对称加密**，一种是**非对称加密**。\n\n在对称加密算法中，加密和解密使用的密钥是相同的。也就是说，加密和解密使用的是同一个密钥。因此，对称加密算法要保证安全性的话，密钥要做好保密。只能让使用的人知道，不能对外公开。\n\n在非对称加密算法中，加密使用的密钥和解密使用的密钥是不相同的。一把是作为公开的公钥，另一把是作为谁都不能给的私钥。公钥加密的信息，只有私钥才能解密。私钥加密的信息，只有公钥才能解密。\n\n因为对称加密算法相比非对称加密算法来说，效率要高得多，性能也好，所以交互的场景下多用对称加密。\n\n## 对称加密\n\n假设你和外卖网站约定了一个密钥，你发送请求的时候用这个密钥进行加密，外卖网站用同样的密钥进行解密。这样就算中间的黑客截获了你的请求，但是它没有密钥，还是破解不了。\n\n这看起来很完美，但是中间有个问题，你们两个怎么来约定这个密钥呢？如果这个密钥在互联网上传输，也是很有可能让黑客截获的。黑客一旦截获这个秘钥，它可以佯作不知，静静地等着你们两个交互。这时候你们之间互通的任何消息，它都能截获并且查看，就等你把银行卡账号和密码发出来。\n\n我们在谍战剧里面经常看到这样的场景，就是特工破译的密码会有个密码本，截获无线电台，通过密码本就能将原文破解出来。怎么把密码本给对方呢？只能通过**线下传输**。\n\n比如，你和外卖网站偷偷约定时间地点，它给你一个纸条，上面写着你们两个的密钥，然后说以后就用这个密钥在互联网上定外卖了。当然你们接头的时候，也会先约定一个口号，什么“天王盖地虎”之类的，口号对上了，才能把纸条给它。但是，“天王盖地虎”同样也是对称加密密钥，同样存在如何把“天王盖地虎”约定成口号的问题。而且在谍战剧中一对一接头可能还可以，在互联网应用中，客户太多，这样是不行的。\n\n## 非对称加密\n\n所以，只要是对称加密，就会永远在这个死循环里出不来，这个时候，就需要非对称加密介入进来。\n\n非对称加密的私钥放在外卖网站这里，不会在互联网上传输，这样就能保证这个秘钥的私密性。但是，对应私钥的公钥，是可以在互联网上随意传播的，只要外卖网站把这个公钥给你，你们就可以愉快地互通了。\n\n比如说你用公钥加密，说“我要定外卖”，黑客在中间就算截获了这个报文，因为它没有私钥也是解不开的，所以这个报文可以顺利到达外卖网站，外卖网站用私钥把这个报文解出来，然后回复，“那给我银行卡和支付密码吧”。\n\n先别太乐观，这里还是有问题的。回复的这句话，是外卖网站拿私钥加密的，互联网上人人都可以把它打开，当然包括黑客。那外卖网站可以拿公钥加密吗？当然不能，因为它自己的私钥只有它自己知道，谁也解不开。\n\n另外，这个过程还有一个问题，黑客也可以模拟发送“我要定外卖”这个过程的，因为它也有外卖网站的公钥。\n\n为了解决这个问题，看来一对公钥私钥是不够的，客户端也需要有自己的公钥和私钥，并且客户端要把自己的公钥，给外卖网站。\n\n这样，客户端给外卖网站发送的时候，用外卖网站的公钥加密。而外卖网站给客户端发送消息的时候，使用客户端的公钥。这样就算有黑客企图模拟客户端获取一些信息，或者半路截获回复信息，但是由于它没有私钥，这些信息它还是打不开。\n\n## 数字证书\n\n不对称加密也会有同样的问题，如何将不对称加密的公钥给对方呢？一种是放在一个公网的地址上，让对方下载；另一种就是在建立连接的时候，传给对方。\n\n这两种方法有相同的问题，那就是，作为一个普通网民，你怎么鉴别别人给你的公钥是对的。会不会有人冒充外卖网站，发给你一个它的公钥。接下来，你和它所有的互通，看起来都是没有任何问题的。毕竟每个人都可以创建自己的公钥和私钥。\n\n例如，我自己搭建了一个网站cliu8site，可以通过这个命令先创建私钥。\n\n```csharp\nopenssl genrsa -out cliu8siteprivate.key 1024\n```\n\n然后，再根据这个私钥，创建对应的公钥。\n\n```vbnet\nopenssl rsa -in cliu8siteprivate.key -pubout -outcliu8sitepublic.pem\n```\n\n这个时候就需要权威部门的介入了，就像每个人都可以打印自己的简历，说自己是谁，但是有公安局盖章的，就只有户口本，这个才能证明你是你。这个由权威部门颁发的称为**证书**（**Certificate**）。\n\n证书里面有什么呢？当然应该有**公钥**，这是最重要的；还有证书的**所有者**，就像户口本上有你的姓名和身份证号，说明这个户口本是你的；另外还有证书的**发布机构**和证书的**有效期**，这个有点像身份证上的机构是哪个区公安局，有效期到多少年。\n\n这个证书是怎么生成的呢？会不会有人假冒权威机构颁发证书呢？就像有假身份证、假户口本一样。生成证书需要发起一个证书请求，然后将这个请求发给一个权威机构去认证，这个权威机构我们称为**CA**（ **Certificate Authority**）。\n\n证书请求可以通过这个命令生成。\n\n```vbnet\nopenssl req -key cliu8siteprivate.key -new -out cliu8sitecertificate.req\n```\n\n将这个请求发给权威机构，权威机构会给这个证书卡一个章，我们称为**签名算法。**问题又来了，那怎么签名才能保证是真的权威机构签名的呢？当然只有用只掌握在权威机构手里的东西签名了才行，这就是CA的私钥。\n\n签名算法大概是这样工作的：一般是对信息做一个Hash计算，得到一个Hash值，这个过程是不可逆的，也就是说无法通过Hash值得出原来的信息内容。在把信息发送出去时，把这个Hash值加密后，作为一个签名和信息一起发出去。\n\n权威机构给证书签名的命令是这样的。\n\n```objectivec\nopenssl x509 -req -in cliu8sitecertificate.req -CA cacertificate.pem -CAkey caprivate.key -out cliu8sitecertificate.pem\n```\n\n这个命令会返回Signature ok，而cliu8sitecertificate.pem就是签过名的证书。CA用自己的私钥给外卖网站的公钥签名，就相当于给外卖网站背书，形成了外卖网站的证书。\n\n我们来查看这个证书的内容。\n\n```scss\nopenssl x509 -in cliu8sitecertificate.pem -noout -text \n```\n\n这里面有个Issuer，也即证书是谁颁发的；Subject，就是证书颁发给谁；Validity是证书期限；Public-key是公钥内容；Signature Algorithm是签名算法。\n\n这下好了，你不会从外卖网站上得到一个公钥，而是会得到一个证书，这个证书有个发布机构CA，你只要得到这个发布机构CA的公钥，去解密外卖网站证书的签名，如果解密成功了，Hash也对的上，就说明这个外卖网站的公钥没有啥问题。\n\n你有没有发现，又有新问题了。要想验证证书，需要CA的公钥，问题是，你怎么确定CA的公钥就是对的呢？\n\n所以，CA的公钥也需要更牛的CA给它签名，然后形成CA的证书。要想知道某个CA的证书是否可靠，要看CA的上级证书的公钥，能不能解开这个CA的签名。就像你不相信区公安局，可以打电话问市公安局，让市公安局确认区公安局的合法性。这样层层上去，直到全球皆知的几个著名大CA，称为**root CA**，做最后的背书。通过这种**层层授信背书**的方式，从而保证了非对称加密模式的正常运转。\n\n除此之外，还有一种证书，称为**Self-Signed Certificate**，就是自己给自己签名。这个给人一种“我就是我，你爱信不信”的感觉。这里我就不多说了。\n\n## HTTPS的工作模式\n\n我们可以知道，非对称加密在性能上不如对称加密，那是否能将两者结合起来呢？例如，公钥私钥主要用于传输对称加密的秘钥，而真正的双方大数据量的通信都是通过对称加密进行的。\n\n当然是可以的。这就是HTTPS协议的总体思路。\n\n![img](7042f5c3d9e3437d5b0b30b30f43c802-20230115031642600.jpg)\n\n当你登录一个外卖网站的时候，由于是HTTPS，客户端会发送Client Hello消息到服务器，以明文传输TLS版本信息、加密套件候选列表、压缩算法候选列表等信息。另外，还会有一个随机数，在协商对称密钥的时候使用。\n\n这就类似在说：“您好，我想定外卖，但你要保密我吃的是什么。这是我的加密套路，再给你个随机数，你留着。”\n\n然后，外卖网站返回Server Hello消息, 告诉客户端，服务器选择使用的协议版本、加密套件、压缩算法等，还有一个随机数，用于后续的密钥协商。\n\n这就类似在说：“您好，保密没问题，你的加密套路还挺多，咱们就按套路2来吧，我这里也有个随机数，你也留着。”\n\n然后，外卖网站会给你一个服务器端的证书，然后说：“Server Hello Done，我这里就这些信息了。”\n\n你当然不相信这个证书，于是你从自己信任的CA仓库中，拿CA的证书里面的公钥去解密外卖网站的证书。如果能够成功，则说明外卖网站是可信的。这个过程中，你可能会不断往上追溯CA、CA的CA、CA的CA的CA，反正直到一个授信的CA，就可以了。\n\n证书验证完毕之后，觉得这个外卖网站可信，于是客户端计算产生随机数字Pre-master，发送Client Key Exchange，用证书中的公钥加密，再发送给服务器，服务器可以通过私钥解密出来。\n\n到目前为止，无论是客户端还是服务器，都有了三个随机数，分别是：自己的、对端的，以及刚生成的Pre-Master随机数。通过这三个随机数，可以在客户端和服务器产生相同的对称密钥。\n\n有了对称密钥，客户端就可以说：“Change Cipher Spec，咱们以后都采用协商的通信密钥和加密算法进行加密通信了。”\n\n然后发送一个Encrypted Handshake Message，将已经商定好的参数等，采用协商密钥进行加密，发送给服务器用于数据与握手验证。\n\n同样，服务器也可以发送Change Cipher Spec，说：“没问题，咱们以后都采用协商的通信密钥和加密算法进行加密通信了”，并且也发送Encrypted Handshake Message的消息试试。当双方握手结束之后，就可以通过对称密钥进行加密传输了。\n\n这个过程除了加密解密之外，其他的过程和HTTP是一样的，过程也非常复杂。\n\n上面的过程只包含了HTTPS的单向认证，也即客户端验证服务端的证书，是大部分的场景，也可以在更加严格安全要求的情况下，启用双向认证，双方互相验证证书。\n\n## 重放与篡改\n\n其实，这里还有一些没有解决的问题，例如重放和篡改的问题。\n\n没错，有了加密和解密，黑客截获了包也打不开了，但是它可以发送N次。这个往往通过Timestamp和Nonce随机数联合起来，然后做一个不可逆的签名来保证。\n\nNonce随机数保证唯一，或者Timestamp和Nonce合起来保证唯一，同样的，请求只接受一次，于是服务器多次受到相同的Timestamp和Nonce，则视为无效即可。\n\n如果有人想篡改Timestamp和Nonce，还有签名保证不可篡改性，如果改了用签名算法解出来，就对不上了，可以丢弃了。\n\n## 小结\n\n好了，这一节就到这里了，我们来总结一下。\n\n- 加密分对称加密和非对称加密。对称加密效率高，但是解决不了密钥传输问题；非对称加密可以解决这个问题，但是效率不高。\n- 非对称加密需要通过证书和权威机构来验证公钥的合法性。\n- HTTPS是综合了对称加密和非对称加密算法的HTTP协议。既保证传输安全，也保证传输效率。\n\n最后，给你留两个思考题：\n\n1. HTTPS协议比较复杂，沟通过程太繁复，这样会导致效率问题，那你知道有哪些手段可以解决这些问题吗？\n2. HTTP和HTTPS协议的正文部分传输个JSON什么的还好，如果播放视频，就有问题了，那这个时候，应该使用什么协议呢？\n\n# 16 讲流媒体协议：如何在直播里看到美女帅哥？\n\n最近直播比较火，很多人都喜欢看直播，那一个直播系统里面都有哪些组成部分，都使用了什么协议呢？\n\n无论是直播还是点播，其实都是对于视频数据的传输。一提到视频，大家都爱看，但是一提到视频技术，大家都头疼，因为名词实在是太多了。\n\n## 三个名词系列\n\n我这里列三个名词系列，你先大致有个印象。\n\n- **名词系列一**：AVI、MPEG、RMVB、MP4、MOV、FLV、WebM、WMV、ASF、MKV。例如RMVB和MP4，看着是不是很熟悉？\n- **名词系列二**：H.261、 H.262、H.263、H.264、H.265。这个是不是就没怎么听过了？别着急，你先记住，要重点关注H.264。\n- **名词系列**三：MPEG-1、MPEG-2、MPEG-4、MPEG-7。MPEG好像听说过，但是后面的数字是怎么回事？是不是又熟悉又陌生？\n\n这里，我想问你个问题，视频是什么？我说，其实就是快速播放一连串连续的图片。\n\n每一张图片，我们称为一**帧**。只要每秒钟帧的数据足够多，也即播放得足够快。比如每秒30帧，以人的眼睛的敏感程度，是看不出这是一张张独立的图片的，这就是我们常说的**帧率**（**FPS**）。\n\n每一张图片，都是由**像素**组成的，假设为1024*768（这个像素数不算多）。每个像素由RGB组成，每个8位，共24位。\n\n我们来算一下，每秒钟的视频有多大？\n\n30帧 × 1024 × 768 × 24 = 566,231,040Bits = 70,778,880Bytes\n\n如果一分钟呢？4,246,732,800Bytes，已经是4个G了。\n\n是不是不算不知道，一算吓一跳？这个数据量实在是太大，根本没办法存储和传输。如果这样存储，你的硬盘很快就满了；如果这样传输，那多少带宽也不够用啊！\n\n怎么办呢？人们想到了**编码**，就是看如何用尽量少的Bit数保存视频，使播放的时候画面看起来仍然很精美。**编码是一个压缩的过程。**\n\n## 视频和图片的压缩过程有什么特点？\n\n之所以能够对视频流中的图片进行压缩，因为视频和图片有这样一些特点。\n\n1. **空间冗余**：图像的相邻像素之间有较强的相关性，一张图片相邻像素往往是渐变的，不是突变的，没必要每个像素都完整地保存，可以隔几个保存一个，中间的用算法计算出来。\n2. **时间冗余**：视频序列的相邻图像之间内容相似。一个视频中连续出现的图片也不是突变的，可以根据已有的图片进行预测和推断。\n3. **视觉冗余**：人的视觉系统对某些细节不敏感，因此不会每一个细节都注意到，可以允许丢失一些数据。\n4. **编码冗余**：不同像素值出现的概率不同，概率高的用的字节少，概率低的用的字节多，类似[霍夫曼编码（Huffman Coding）](https://zh.wikipedia.org/wiki/霍夫曼编码)的思路。\n\n总之，用于编码的算法非常复杂，而且多种多样，但是编码过程其实都是类似的。\n\n![img](433a51e15d0ed50e313454ceccd61cb4-20230115031642583.jpg)﻿﻿\n\n## 视频编码的两大流派\n\n能不能形成一定的标准呢？要不然开发视频播放的人得累死了。当然能，我这里就给你介绍，视频编码的两大流派。\n\n- 流派一：ITU（International Telecommunications Union）的VCEG（Video Coding Experts Group），这个称为**国际电联下的VCEG**。既然是电信，可想而知，他们最初做视频编码，主要侧重传输。名词系列二，就是这个组织制定的标准。\n- 流派二：ISO（International Standards Organization）的MPEG（Moving Picture Experts Group），这个是**ISO旗下的MPEG**，本来是做视频存储的。例如，编码后保存在VCD和DVD中。当然后来也慢慢侧重视频传输了。名词系列三，就是这个组织制定的标准。\n\n后来，ITU-T（国际电信联盟电信标准化部门，ITU Telecommunication Standardization Sector）与MPEG联合制定了H.264/MPEG-4 AVC，这才是我们这一节要重点关注的。\n\n经过编码之后，生动活泼的一帧一帧的图像，就变成了一串串让人看不懂的二进制，这个二进制可以放在一个文件里面，按照一定的格式保存起来，这就是名词系列一。\n\n其实这些就是视频保存成文件的格式。例如，前几个字节是什么意义，后几个字节是什么意义，然后是数据，数据中保存的就是编码好的结果。\n\n## 如何在直播里看到帅哥美女？\n\n当然，这个二进制也可以通过某种网络协议进行封装，放在互联网上传输，这个时候就可以进行网络直播了。\n\n网络协议将**编码**好的视频流，从主播端推送到服务器，在服务器上有个运行了同样协议的服务端来接收这些网络包，从而得到里面的视频流，这个过程称为**接流**。\n\n服务端接到视频流之后，可以对视频流进行一定的处理，例如**转码**，也即从一个编码格式，转成另一种格式。因为观众使用的客户端千差万别，要保证他们都能看到直播。\n\n**流处理**完毕之后，就可以等待观众的客户端来请求这些视频流。观众的客户端请求的过程称为**拉流**。\n\n如果有非常多的观众，同时看一个视频直播，那都从一个服务器上**拉流**，压力太大了，因而需要一个视频的**分发**网络，将视频预先加载到就近的边缘节点，这样大部分观众看的视频，是从边缘节点拉取的，就能降低服务器的压力。\n\n当观众的客户端将视频流拉下来之后，就需要进行**解码**，也即通过上述过程的逆过程，将一串串看不懂的二进制，再转变成一帧帧生动的图片，在客户端**播放**出来，这样你就能看到美女帅哥啦。\n\n整个直播过程，可以用这个的图来描述。\n\n![img](e4d4b538c434ec0eade37028a34391f8-20230115031642588.jpg)\n\n接下来，我们依次来看一下每个过程。\n\n### 编码：如何将丰富多彩的图片变成二进制流？\n\n虽然我们说视频是一张张图片的序列，但是如果每张图片都完整，就太大了，因而会将视频序列分成三种帧。\n\n- **I帧**，也称关键帧。里面是完整的图片，只需要本帧数据，就可以完成解码。\n- **P帧**，前向预测编码帧。P帧表示的是这一帧跟之前的一个关键帧（或P帧）的差别，解码时需要用之前缓存的画面，叠加上和本帧定义的差别，生成最终画面。\n- **B帧**，双向预测内插编码帧。B帧记录的是本帧与前后帧的差别。要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面的数据与本帧数据的叠加，取得最终的画面。\n\n可以看出，I帧最完整，B帧压缩率最高，而压缩后帧的序列，应该是在IBBP的间隔出现的。这就是**通过时序进行编码**。\n\n![img](b8f215697ce950005a532d3be341f570-20230115031642657.jpg)\n\n在一帧中，分成多个片，每个片中分成多个宏块，每个宏块分成多个子块，这样将一张大的图分解成一个个小块，可以方便进行**空间上的编码**。\n\n尽管时空非常立体的组成了一个序列，但是总归还是要压缩成一个二进制流。这个流是有结构的，是一个个的**网络提取层单元**（**NALU**，**Network Abstraction Layer Unit**）。变成这种格式就是为了传输，因为网络上的传输，默认的是一个个的包，因而这里也就分成了一个个的单元。\n\n![img](42dcd0705e3b1bad05d59fd9d6707d60-20230115031642662.jpg)﻿﻿\n\n每一个NALU首先是一个起始标识符，用于标识NALU之间的间隔；然后是NALU的头，里面主要配置了NALU的类型；最终Payload里面是NALU承载的数据。\n\n在NALU头里面，主要的内容是类型**NAL Type**。\n\n- 0x07表示SPS，是序列参数集， 包括一个图像序列的所有信息，如图像尺寸、视频格式等。\n- 0x08表示PPS，是图像参数集，包括一个图像的所有分片的所有相关信息，包括图像类型、序列号等。\n\n在传输视频流之前，必须要传输这两类参数，不然无法解码。为了保证容错性，每一个I帧前面，都会传一遍这两个参数集合。\n\n如果NALU Header里面的表示类型是SPS或者PPS，则Payload中就是真正的参数集的内容。\n\n如果类型是帧，则Payload中才是正的视频数据，当然也是一帧一帧存放的，前面说了，一帧的内容还是挺多的，因而每一个NALU里面保存的是一片。对于每一片，到底是I帧，还是P帧，还是B帧，在片结构里面也有个Header，这里面有个类型，然后是片的内容。\n\n这样，整个格式就出来了，**一个视频，可以拆分成一系列的帧，每一帧拆分成一系列的片，每一片都放在一个NALU里面，NALU之间都是通过特殊的起始标识符分隔，在每一个I帧的第一片前面，要插入单独保存SPS和PPS的NALU，最终形成一个长长的NALU序列**。\n\n### 推流：如何把数据流打包传输到对端？\n\n那这个格式是不是就能够直接在网上传输到对端，开始直播了呢？其实还不是，还需要将这个二进制的流打包成网络包进行发送，这里我们使用**RTMP协议**。这就进入了第二个过程，**推流**。\n\nRTMP是基于TCP的，因而肯定需要双方建立一个TCP的连接。在有TCP的连接的基础上，还需要建立一个RTMP的连接，也即在程序里面，你需要调用RTMP类库的Connect函数，显示创建一个连接。\n\nRTMP为什么需要建立一个单独的连接呢？\n\n因为它们需要商量一些事情，保证以后的传输能正常进行。主要就是两个事情，一个是**版本号**，如果客户端、服务器的版本号不一致，则不能工作。另一个就是**时间戳**，视频播放中，时间是很重要的，后面的数据流互通的时候，经常要带上时间戳的差值，因而一开始双方就要知道对方的时间戳。\n\n未来沟通这些事情，需要发送六条消息：客户端发送C0、C1、 C2，服务器发送S0、 S1、 S2。\n\n首先，客户端发送C0表示自己的版本号，不必等对方的回复，然后发送C1表示自己的时间戳。\n\n服务器只有在收到C0的时候，才能返回S0，表明自己的版本号，如果版本不匹配，可以断开连接。\n\n服务器发送完S0后，也不用等什么，就直接发送自己的时间戳S1。客户端收到S1的时候，发一个知道了对方时间戳的ACK C2。同理服务器收到C1的时候，发一个知道了对方时间戳的ACK S2。\n\n于是，握手完成。\n\n![img](de6301500d02c5afa3e6c6f5fa47bac7-20230115031642915.jpg)﻿﻿\n\n握手之后，双方需要互相传递一些控制信息，例如Chunk块的大小、窗口大小等。\n\n真正传输数据的时候，还是需要创建一个流Stream，然后通过这个Stream来推流publish。\n\n推流的过程，就是将NALU放在Message里面发送，这个也称为**RTMP Packet包**。Message的格式就像这样。\n\n![img](1a97a0b90c2304cbdf22a2bc8a8ce94b-20230115031642747.jpg)﻿﻿\n\n发送的时候，去掉NALU的起始标识符。因为这部分对于RTMP协议来讲没有用。接下来，将SPS和PPS参数集封装成一个RTMP包发送，然后发送一个个片的NALU。\n\nRTMP在收发数据的时候并不是以Message为单位的，而是把Message拆分成Chunk发送，而且必须在一个Chunk发送完成之后，才能开始发送下一个Chunk。每个Chunk中都带有Message ID，表示属于哪个Message，接收端也会按照这个ID将Chunk组装成Message。\n\n前面连接的时候，设置的Chunk块大小就是指这个Chunk。将大的消息变为小的块再发送，可以在低带宽的情况下，减少网络拥塞。\n\n这有一个分块的例子，你可以看一下。\n\n假设一个视频的消息长度为307，但是Chunk大小约定为128，于是会拆分为三个Chunk。\n\n第一个Chunk的Type＝0，表示Chunk头是完整的；头里面Timestamp为1000，总长度Length 为307，类型为9，是个视频，Stream ID为12346，正文部分承担128个字节的Data。\n\n第二个Chunk也要发送128个字节，Chunk头由于和第一个Chunk一样，因此采用Chunk Type＝3，表示头一样就不再发送了。\n\n第三个Chunk要发送的Data的长度为307-128-128=51个字节，还是采用Type＝3。\n\n![img](41abff0d11198fcf8b8308f3222b8c2f-20230115031642669.jpg)\n\n就这样数据就源源不断到达流媒体服务器，整个过程就像这样。\n\n![img](14221e482876b0b243f5213c7a1cc62e-20230115031642683.jpg)\n\n这个时候，大量观看直播的观众就可以通过RTMP协议从流媒体服务器上拉取，但是这么多的用户量，都去同一个地方拉取，服务器压力会很大，而且用户分布在全国甚至全球，如果都去统一的一个地方下载，也会时延比较长，需要有分发网络。\n\n分发网络分为**中心**和**边缘**两层。边缘层服务器部署在全国各地及横跨各大运营商里，和用户距离很近。中心层是流媒体服务集群，负责内容的转发。智能负载均衡系统，根据用户的地理位置信息，就近选择边缘服务器，为用户提供推/拉流服务。中心层也负责转码服务，例如，把RTMP协议的码流转换为HLS码流。\n\n![img](6cdbe17d580f46d60d5f6380262834db-20230115031642787.jpg)﻿﻿\n\n这套机制在后面的DNS、HTTPDNS、CDN的章节会更有详细的描述。\n\n### 拉流：观众的客户端如何看到视频？\n\n接下来，我们再来看观众的客户端通过RTMP拉流的过程。\n\n![img](52d2e201f87462bc3afed8ce4d743aee-20230115031643112.jpg)﻿﻿\n\n先读到的是H.264的解码参数，例如SPS和PPS，然后对收到的NALU组成的一个个帧，进行解码，交给播发器播放，一个绚丽多彩的视频画面就出来了。\n\n## 小结\n\n好了，今天的内容就到这里了，我们来总结一下：\n\n- 视频名词比较多，编码两大流派达成了一致，都是通过时间、空间的各种算法来压缩数据；\n- 压缩好的数据，为了传输组成一系列NALU，按照帧和片依次排列；\n- 排列好的NALU，在网络传输的时候，要按照RTMP包的格式进行包装，RTMP的包会拆分成Chunk进行传输；\n- 推送到流媒体集群的视频流经过转码和分发，可以被客户端通过RTMP协议拉取，然后组合为NALU，解码成视频格式进行播放。\n\n最后，给你留两个思考题：\n\n1. 你觉得基于RTMP的视频流传输的机制存在什么问题？如何进行优化？\n2. 在线看视频之前，大家都是把电影下载下来看的，电影这么大，你知道如何快速下载吗？\n\n# 17 讲P2P协议：我下小电影，99%急死你\n\n如果你想下载一个电影，一般会通过什么方式呢？\n\n当然，最简单的方式就是通过**HTTP**进行下载。但是相信你有过这样的体验，通过浏览器下载的时候，只要文件稍微大点，下载的速度就奇慢无比。\n\n还有种下载文件的方式，就是通过**FTP**，也即**文件传输协议**。FTP采用两个TCP连接来传输一个文件。\n\n- **控制连接**：服务器以被动的方式，打开众所周知用于FTP的端口21，客户端则主动发起连接。该连接将命令从客户端传给服务器，并传回服务器的应答。常用的命令有：list——获取文件目录；reter——取一个文件；store——存一个文件。\n- **数据连接**：每当一个文件在客户端与服务器之间传输时，就创建一个数据连接。\n\n## FTP的两种工作模式\n\n每传输一个文件，都要建立一个全新的数据连接。FTP有两种工作模式，分别是**主动模式**（**PORT**）和**被动模式**（**PASV**），这些都是站在FTP服务器的角度来说的。\n\n主动模式下，客户端随机打开一个大于1024的端口N，向服务器的命令端口21发起连接，同时开放N+1端口监听，并向服务器发出 “port N+1” 命令，由服务器从自己的数据端口20，主动连接到客户端指定的数据端口N+1。\n\n被动模式下，当开启一个FTP连接时，客户端打开两个任意的本地端口N（大于1024）和N+1。第一个端口连接服务器的21端口，提交PASV命令。然后，服务器会开启一个任意的端口P（大于1024），返回“227 entering passive mode”消息，里面有FTP服务器开放的用来进行数据传输的端口。客户端收到消息取得端口号之后，会通过N+1号端口连接服务器的端口P，然后在两个端口之间进行数据传输。\n\n## P2P是什么？\n\n但是无论是HTTP的方式，还是FTP的方式，都有一个比较大的缺点，就是**难以解决单一服务器的带宽压力**， 因为它们使用的都是传统的客户端服务器的方式。\n\n后来，一种创新的、称为P2P的方式流行起来。**P2P**就是**peer-to-peer**。资源开始并不集中地存储在某些设备上，而是分散地存储在多台设备上。这些设备我们姑且称为peer。\n\n想要下载一个文件的时候，你只要得到那些已经存在了文件的peer，并和这些peer之间，建立点对点的连接，而不需要到中心服务器上，就可以就近下载文件。一旦下载了文件，你也就成为peer中的一员，你旁边的那些机器，也可能会选择从你这里下载文件，所以当你使用P2P软件的时候，例如BitTorrent，往往能够看到，既有下载流量，也有上传的流量，也即你自己也加入了这个P2P的网络，自己从别人那里下载，同时也提供给其他人下载。可以想象，这种方式，参与的人越多，下载速度越快，一切完美。\n\n## 种子（.torrent）文件\n\n但是有一个问题，当你想下载一个文件的时候，怎么知道哪些peer有这个文件呢？\n\n这就用到**种子**啦，也即咱们比较熟悉的**.torrent文件**。.torrent文件由两部分组成，分别是：**announce**（**tracker URL**）和**文件信息**。\n\n文件信息里面有这些内容。\n\n- **info区**：这里指定的是该种子有几个文件、文件有多长、目录结构，以及目录和文件的名字。\n- **Name字段**：指定顶层目录名字。\n- **每个段的大小**：BitTorrent（简称BT）协议把一个文件分成很多个小段，然后分段下载。\n- **段哈希值**：将整个种子中，每个段的SHA-1哈希值拼在一起。\n\n下载时，BT客户端首先解析.torrent文件，得到tracker地址，然后连接tracker服务器。tracker服务器回应下载者的请求，将其他下载者（包括发布者）的IP提供给下载者。下载者再连接其他下载者，根据.torrent文件，两者分别对方告知自己已经有的块，然后交换对方没有的数据。此时不需要其他服务器参与，并分散了单个线路上的数据流量，因此减轻了服务器的负担。\n\n下载者每得到一个块，需要算出下载块的Hash验证码，并与.torrent文件中的对比。如果一样，则说明块正确，不一样则需要重新下载这个块。这种规定是为了解决下载内容的准确性问题。\n\n从这个过程也可以看出，这种方式特别依赖tracker。tracker需要收集下载者信息的服务器，并将此信息提供给其他下载者，使下载者们相互连接起来，传输数据。虽然下载的过程是非中心化的，但是加入这个P2P网络的时候，都需要借助tracker中心服务器，这个服务器是用来登记有哪些用户在请求哪些资源。\n\n所以，这种工作方式有一个弊端，一旦tracker服务器出现故障或者线路遭到屏蔽，BT工具就无法正常工作了。\n\n## 去中心化网络（DHT）\n\n那能不能彻底非中心化呢？\n\n于是，后来就有了一种叫作**DHT**（**Distributed Hash Table**）的去中心化网络。每个加入这个DHT网络的人，都要负责存储这个网络里的资源信息和其他成员的联系信息，相当于所有人一起构成了一个庞大的分布式存储数据库。\n\n有一种著名的DHT协议，叫**Kademlia协议**。这个和区块链的概念一样，很抽象，我来详细讲一下这个协议。\n\n任何一个BitTorrent启动之后，它都有两个角色。一个是**peer**，监听一个TCP端口，用来上传和下载文件，这个角色表明，我这里有某个文件。另一个角色**DHT node**，监听一个UDP的端口，通过这个角色，这个节点加入了一个DHT的网络。\n\n![img](8ece62f3f99cb3fe7ee0274a1ad79fcf-20230115031642801.jpg)\n\n在DHT网络里面，每一个DHT node都有一个ID。这个ID是一个很长的串。每个DHT node都有责任掌握一些知识，也就是**文件索引**，也即它应该知道某些文件是保存在哪些节点上。它只需要有这些知识就可以了，而它自己本身不一定就是保存这个文件的节点。\n\n## 哈希值\n\n当然，每个DHT node不会有全局的知识，也即不知道所有的文件保存在哪里，它只需要知道一部分。那应该知道哪一部分呢？这就需要用哈希算法计算出来。\n\n每个文件可以计算出一个哈希值，而**DHT node的ID是和哈希值相同长度的串**。\n\nDHT算法是这样规定的：如果一个文件计算出一个哈希值，则和这个哈希值一样的那个DHT node，就有责任知道从哪里下载这个文件，即便它自己没保存这个文件。\n\n当然不一定这么巧，总能找到和哈希值一模一样的，有可能一模一样的DHT node也下线了，所以DHT算法还规定：除了一模一样的那个DHT node应该知道，ID和这个哈希值非常接近的N个DHT node也应该知道。\n\n什么叫和哈希值接近呢？例如只修改了最后一位，就很接近；修改了倒数2位，也不远；修改了倒数3位，也可以接受。总之，凑齐了规定的N这个数就行。\n\n刚才那个图里，文件1通过哈希运算，得到匹配ID的DHT node为node C，当然还会有其他的，我这里没有画出来。所以，node C有责任知道文件1的存放地址，虽然node C本身没有存放文件1。\n\n同理，文件2通过哈希运算，得到匹配ID的DHT node为node E，但是node D和E的ID值很近，所以node D也知道。当然，文件2本身没有必要一定在node D和E里，但是碰巧这里就在E那有一份。\n\n接下来一个新的节点node new上线了。如果想下载文件1，它首先要加入DHT网络，如何加入呢？\n\n在这种模式下，种子.torrent文件里面就不再是tracker的地址了，而是一个list的node的地址，而所有这些node都是已经在DHT网络里面的。当然随着时间的推移，很可能有退出的，有下线的，但是我们假设，不会所有的都联系不上，总有一个能联系上。\n\nnode new只要在种子里面找到一个DHT node，就加入了网络。\n\nnode new会计算文件1的哈希值，并根据这个哈希值了解到，和这个哈希值匹配，或者很接近的node上知道如何下载这个文件，例如计算出来的哈希值就是node C。\n\n但是node new不知道怎么联系上node C，因为种子里面的node列表里面很可能没有node C，但是它可以问，DHT网络特别像一个社交网络，node new只有去它能联系上的node问，你们知道不知道node C的联系方式呀？\n\n在DHT网络中，每个node都保存了一定的联系方式，但是肯定没有node的所有联系方式。DHT网络中，节点之间通过互相通信，也会交流联系方式，也会删除联系方式。和人们的方式一样，你有你的朋友圈，你的朋友有它的朋友圈，你们互相加微信，就互相认识了，过一段时间不联系，就删除朋友关系。\n\n有个理论是，社交网络中，任何两个人直接的距离不超过六度，也即你想联系比尔盖茨，也就六个人就能够联系到了。\n\n所以，node new想联系node C，就去万能的朋友圈去问，并且求转发，朋友再问朋友，很快就能找到。如果找不到C，也能找到和C的ID很像的节点，它们也知道如何下载文件1。\n\n在node C上，告诉node new，下载文件1，要去B、D、 F，于是node new选择和node B进行peer连接，开始下载，它一旦开始下载，自己本地也有文件1了，于是node new告诉node C以及和node C的ID很像的那些节点，我也有文件1了，可以加入那个文件拥有者列表了。\n\n但是你会发现node new上没有文件索引，但是根据哈希算法，一定会有某些文件的哈希值是和node new的ID匹配上的。在DHT网络中，会有节点告诉它，你既然加入了咱们这个网络，你也有责任知道某些文件的下载地址。\n\n好了，一切都分布式了。\n\n这里面遗留几个细节的问题。\n\n- DHT node ID以及文件哈希是个什么东西？\n\n节点ID是一个随机选择的160bits（20字节）空间，文件的哈希也使用这样的160bits空间。\n\n- 所谓ID相似，具体到什么程度算相似？\n\n在Kademlia网络中，距离是通过异或（XOR）计算的。我们就不以160bits举例了。我们以5位来举例。\n\n01010与01000的距离，就是两个ID之间的异或值，为00010，也即为2。 01010与00010的距离为01000，也即为8,。01010与00011的距离为01001，也即8+1=9 。以此类推，高位不同的，表示距离更远一些；低位不同的，表示距离更近一些，总的距离为所有的不同的位的距离之和。\n\n这个距离不能比喻为地理位置，因为在Kademlia网络中，位置近不算近，ID近才算近，所以我把这个距离比喻为社交距离，也即在朋友圈中的距离，或者社交网络中的距离。这个和你住的位置没有关系，和人的经历关系比较大。\n\n还是以5位ID来举例，就像在领英中，排第一位的表示最近一份工作在哪里，第二位的表示上一份工作在哪里，然后第三位的是上上份工作，第四位的是研究生在哪里读，第五位的表示大学在哪里读。\n\n如果你是一个猎头，在上面找候选人，当然最近的那份工作是最重要的。而对于工作经历越丰富的候选人，大学在哪里读的反而越不重要。\n\n## DHT网络中的朋友圈是怎么维护的？\n\n就像人一样，虽然我们常联系人的只有少数，但是朋友圈里肯定是远近都有。DHT网络的朋友圈也是一样，远近都有，并且**按距离分层**。\n\n假设某个节点的ID为01010，如果一个节点的ID，前面所有位数都与它相同，只有最后1位不同。这样的节点只有1个，为01011。与基础节点的异或值为00001，即距离为1；对于01010而言，这样的节点归为“k-bucket 1”。\n\n如果一个节点的ID，前面所有位数都相同，从倒数第2位开始不同，这样的节点只有2个，即01000和01001，与基础节点的异或值为00010和00011，即距离范围为2和3；对于01010而言，这样的节点归为“k-bucket 2”。\n\n如果一个节点的ID，前面所有位数相同，从倒数第i位开始不同，这样的节点只有2^(i-1)个，与基础节点的距离范围为[2^(i-1), 2^i)；对于01010而言，这样的节点归为“k-bucket i”。\n\n最终到从倒数160位就开始都不同。\n\n你会发现，差距越大，陌生人越多，但是朋友圈不能都放下，所以每一层都只放K个，这是参数可以配置。\n\n## DHT网络是如何查找朋友的？\n\n假设，node A 的ID为00110，要找node B ID为10000，异或距离为10110，距离范围在[2^4, 2^5)，所以这个目标节点可能在“k-bucket 5”中，这就说明B的ID与A的ID从第5位开始不同，所以B可能在“k-bucket 5”中。\n\n然后，A看看自己的k-bucket 5有没有B。如果有，太好了，找到你了；如果没有，在k-bucket 5里随便找一个C。因为是二进制，C、B都和A的第5位不同，那么C的ID第5位肯定与B相同，即它与B的距离会小于2^4，相当于比A、B之间的距离缩短了一半以上。\n\n再请求C，在它自己的通讯录里，按同样的查找方式找一下B。如果C知道B，就告诉A；如果C也不知道B，那C按同样的搜索方法，可以在自己的通讯录里找到一个离B更近的D朋友（D、B之间距离小于2^3），把D推荐给A，A请求D进行下一步查找。\n\nKademlia的这种查询机制，是通过折半查找的方式来收缩范围，对于总的节点数目为N，最多只需要查询log2(N)次，就能够找到。\n\n例如，图中这个最差的情况。\n\n![img](1d9409a21f318e8b6c51db7f8ad93f31-20230115031642790.jpg)\n\nA和B每一位都不一样，所以相差31，A找到的朋友C，不巧正好在中间。和A的距离是16，和B距离为15，于是C去自己朋友圈找的时候，不巧找到D，正好又在中间，距离C为8，距离B为7。于是D去自己朋友圈找的时候，不巧找到E，正好又在中间，距离D为4，距离B为3，E在朋友圈找到F，距离E为2，距离B为1，最终在F的朋友圈距离1的地方找到B。当然这是最最不巧的情况，每次找到的朋友都不远不近，正好在中间。\n\n如果碰巧了，在A的朋友圈里面有G，距离B只有3，然后在G的朋友圈里面一下子就找到了B，两次就找到了。\n\n在DHT网络中，朋友之间怎么沟通呢？\n\nKademlia算法中，每个节点只有4个指令。\n\n- PING：测试一个节点是否在线，还活着没，相当于打个电话，看还能打通不。\n- STORE：要求一个节点存储一份数据，既然加入了组织，有义务保存一份数据。\n- FIND_NODE：根据节点ID查找一个节点，就是给一个160位的ID，通过上面朋友圈的方式找到那个节点。\n- FIND_VALUE：根据KEY查找一个数据，实则上跟FIND_NODE非常类似。KEY就是文件对应的160位的ID，就是要找到保存了文件的节点。\n\nDHT网络中，朋友圈如何更新呢？\n\n- 每个bucket里的节点，都按最后一次接触的时间倒序排列，这就相当于，朋友圈里面最近联系过的人往往是最熟的。\n- 每次执行四个指令中的任意一个都会触发更新。\n- 当一个节点与自己接触时，检查它是否已经在k-bucket中，也就是说是否已经在朋友圈。如果在，那么将它挪到k-bucket列表的最底，也就是最新的位置，刚联系过，就置顶一下，方便以后多联系；如果不在，新的联系人要不要加到通讯录里面呢？假设通讯录已满的情况，PING一下列表最上面，也即最旧的一个节点。如果PING通了，将旧节点挪到列表最底，并丢弃新节点，老朋友还是留一下；如果PING不通，删除旧节点，并将新节点加入列表，这人联系不上了，删了吧。\n\n这个机制保证了任意节点加入和离开都不影响整体网络。\n\n## 小结\n\n好了，今天的讲解就到这里了，我们总结一下：\n\n- 下载一个文件可以使用HTTP或FTP，这两种都是集中下载的方式，而P2P则换了一种思路，采取非中心化下载的方式；\n- P2P也是有两种，一种是依赖于tracker的，也即元数据集中，文件数据分散；另一种是基于分布式的哈希算法，元数据和文件数据全部分散。\n\n接下来，给你留两个思考题：\n\n1. 除了这种去中心化分布式哈希的算法，你还能想到其他的应用场景吗？\n2. 在前面所有的章节中，要下载一个文件，都需要使用域名。但是网络通信是使用IP的，那你知道怎么实现两者的映射机制吗？\n\n我们的专栏马上更新过半了，不知你掌握得如何？每节课后我留的思考题，你都有没有认真思考，并在留言区写下答案呢？我会从已发布的文章中选出一批认真留言的同学，赠送学习奖励礼券和我整理的独家网络协议知识图谱。\n\n# 18 讲DNS协议：网络世界的地址簿\n\n前面我们讲了平时常见的看新闻、支付、直播、下载等场景，现在网站的数目非常多，常用的网站就有二三十个，如果全部用IP地址进行访问，恐怕很难记住。于是，就需要一个地址簿，根据名称，就可以查看具体的地址。\n\n例如，我要去西湖边的“外婆家”，这就是名称，然后通过地址簿，查看到底是哪条路多少号。\n\n## DNS服务器\n\n在网络世界，也是这样的。你肯定记得住网站的名称，但是很难记住网站的IP地址，因而也需要一个地址簿，就是**DNS服务器**。\n\n由此可见，DNS在日常生活中多么重要。每个人上网，都需要访问它，但是同时，这对它来讲也是非常大的挑战。一旦它出了故障，整个互联网都将瘫痪。另外，上网的人分布在全世界各地，如果大家都去同一个地方访问某一台服务器，时延将会非常大。因而，**DNS服务器，一定要设置成高可用、高并发和分布式的**。\n\n于是，就有了这样**树状的层次结构**。\n\n![img](59f79cba26904ff721aabfcdc0c27da6-20230115031642915.jpg)﻿\n\n- 根DNS服务器 ：返回顶级域DNS服务器的IP地址\n- 顶级域DNS服务器：返回权威DNS服务器的IP地址\n- 权威DNS服务器 ：返回相应主机的IP地址\n\n## DNS解析流程\n\n为了提高DNS的解析性能，很多网络都会就近部署DNS缓存服务器。于是，就有了以下的DNS解析流程。\n\n1. 电脑客户端会发出一个DNS请求，问www.163.com的IP是啥啊，并发给本地域名服务器 (本地DNS)。那本地域名服务器 (本地DNS) 是什么呢？如果是通过DHCP配置，本地DNS由你的网络服务商（ISP），如电信、移动等自动分配，它通常就在你网络服务商的某个机房。\n2. 本地DNS收到来自客户端的请求。你可以想象这台服务器上缓存了一张域名与之对应IP地址的大表格。如果能找到 www.163.com，它直接就返回IP地址。如果没有，本地DNS会去问它的根域名服务器：“老大，能告诉我www.163.com的IP地址吗？”根域名服务器是最高层次的，全球共有13套。它不直接用于域名解析，但能指明一条道路。\n3. 根DNS收到来自本地DNS的请求，发现后缀是 .com，说：“哦，www.163.com啊，这个域名是由.com区域管理，我给你它的顶级域名服务器的地址，你去问问它吧。”\n4. 本地DNS转向问顶级域名服务器：“老二，你能告诉我www.163.com的IP地址吗？”顶级域名服务器就是大名鼎鼎的比如 .com、.net、 .org这些一级域名，它负责管理二级域名，比如 163.com，所以它能提供一条更清晰的方向。\n5. 顶级域名服务器说：“我给你负责 www.163.com 区域的权威DNS服务器的地址，你去问它应该能问到。”\n6. 本地DNS转向问权威DNS服务器：“您好，www.163.com 对应的IP是啥呀？”163.com的权威DNS服务器，它是域名解析结果的原出处。为啥叫权威呢？就是我的域名我做主。\n7. 权限DNS服务器查询后将对应的IP地址X.X.X.X告诉本地DNS。\n8. 本地DNS再将IP地址返回客户端，客户端和目标建立连接。\n\n至此，我们完成了DNS的解析过程。现在总结一下，整个过程我画成了一个图。\n\n![img](ff7e8f824ebd1f7e16ef5d70cd79bdf2-20230115031642861.jpg)\n\n## 负载均衡\n\n站在客户端角度，这是一次**DNS递归查询过程。因为本地DNS全权为它效劳，它只要坐等结果即可。在这个过程中，DNS除了可以通过名称映射为IP地址，它还可以做另外一件事，就是负载均衡**。\n\n还是以访问“外婆家”为例，还是我们开头的“外婆家”，但是，它可能有很多地址，因为它在杭州可以有很多家。所以，如果一个人想去吃“外婆家”，他可以就近找一家店，而不用大家都去同一家，这就是负载均衡。\n\nDNS首先可以做**内部负载均衡**。\n\n例如，一个应用要访问数据库，在这个应用里面应该配置这个数据库的IP地址，还是应该配置这个数据库的域名呢？显然应该配置域名，因为一旦这个数据库，因为某种原因，换到了另外一台机器上，而如果有多个应用都配置了这台数据库的话，一换IP地址，就需要将这些应用全部修改一遍。但是如果配置了域名，则只要在DNS服务器里，将域名映射为新的IP地址，这个工作就完成了，大大简化了运维。\n\n在这个基础上，我们可以再进一步。例如，某个应用要访问另外一个应用，如果配置另外一个应用的IP地址，那么这个访问就是一对一的。但是当被访问的应用撑不住的时候，我们其实可以部署多个。但是，访问它的应用，如何在多个之间进行负载均衡？只要配置成为域名就可以了。在域名解析的时候，我们只要配置策略，这次返回第一个IP，下次返回第二个IP，就可以实现负载均衡了。\n\n另外一个更加重要的是，DNS还可以做**全局负载均衡**。\n\n为了保证我们的应用高可用，往往会部署在多个机房，每个地方都会有自己的IP地址。当用户访问某个域名的时候，这个IP地址可以轮询访问多个数据中心。如果一个数据中心因为某种原因挂了，只要在DNS服务器里面，将这个数据中心对应的IP地址删除，就可以实现一定的高可用。\n\n另外，我们肯定希望北京的用户访问北京的数据中心，上海的用户访问上海的数据中心，这样，客户体验就会非常好，访问速度就会超快。这就是全局负载均衡的概念。\n\n## 示例：DNS访问数据中心中对象存储上的静态资源\n\n我们通过DNS访问数据中心中对象存储上的静态资源为例，看一看整个过程。\n\n假设全国有多个数据中心，托管在多个运营商，每个数据中心三个可用区（Available Zone）。对象存储通过跨可用区部署，实现高可用性。在每个数据中心中，都至少部署两个内部负载均衡器，内部负载均衡器后面对接多个对象存储的前置服务器（Proxy-server）。\n\n![img](569ddad1a0c5a5f60341fbe023b47cd1-20230115031642905.jpg)﻿\n\n1. 当一个客户端要访问object.yourcompany.com的时候，需要将域名转换为IP地址进行访问，所以它要请求本地DNS解析器。\n2. 本地DNS解析器先查看看本地的缓存是否有这个记录。如果有则直接使用，因为上面的过程太复杂了，如果每次都要递归解析，就太麻烦了。\n3. 如果本地无缓存，则需要请求本地的DNS服务器。\n4. 本地的DNS服务器一般部署在你的数据中心或者你所在的运营商的网络中，本地DNS服务器也需要看本地是否有缓存，如果有则返回，因为它也不想把上面的递归过程再走一遍。\n5. 至 7. 如果本地没有，本地DNS才需要递归地从根DNS服务器，查到.com的顶级域名服务器，最终查到 yourcompany.com 的权威DNS服务器，给本地DNS服务器，权威DNS服务器按说会返回真实要访问的IP地址。\n\n对于不需要做全局负载均衡的简单应用来讲，yourcompany.com的权威DNS服务器可以直接将 object.yourcompany.com这个域名解析为一个或者多个IP地址，然后客户端可以通过多个IP地址，进行简单的轮询，实现简单的负载均衡。\n\n但是对于复杂的应用，尤其是跨地域跨运营商的大型应用，则需要更加复杂的全局负载均衡机制，因而需要专门的设备或者服务器来做这件事情，这就是**全局负载均衡器**（**GSLB**，**Global Server Load Balance**）。\n\n在yourcompany.com的DNS服务器中，一般是通过配置CNAME的方式，给 object.yourcompany.com起一个别名，例如 object.vip.yourcomany.com，然后告诉本地DNS服务器，让它请求GSLB解析这个域名，GSLB就可以在解析这个域名的过程中，通过自己的策略实现负载均衡。\n\n图中画了两层的GSLB，是因为分运营商和地域。我们希望不同运营商的客户，可以访问相同运营商机房中的资源，这样不跨运营商访问，有利于提高吞吐量，减少时延。\n\n1. 第一层GSLB，通过查看请求它的本地DNS服务器所在的运营商，就知道用户所在的运营商。假设是移动，通过CNAME的方式，通过另一个别名 object.yd.yourcompany.com，告诉本地DNS服务器去请求第二层的GSLB。\n2. 第二层GSLB，通过查看请求它的本地DNS服务器所在的地址，就知道用户所在的地理位置，然后将距离用户位置比较近的Region里面，六个**内部负载均衡**（**SLB**，S**erver Load Balancer**）的地址，返回给本地DNS服务器。\n3. 本地DNS服务器将结果返回给本地DNS解析器。\n4. 本地DNS解析器将结果缓存后，返回给客户端。\n5. 客户端开始访问属于相同运营商的距离较近的Region 1中的对象存储，当然客户端得到了六个IP地址，它可以通过负载均衡的方式，随机或者轮询选择一个可用区进行访问。对象存储一般会有三个备份，从而可以实现对存储读写的负载均衡。\n\n## 小结\n\n好了，这节内容就到这里了，我们来总结一下：\n\n- DNS是网络世界的地址簿，可以通过域名查地址，因为域名服务器是按照树状结构组织的，因而域名查找是使用递归的方法，并通过缓存的方式增强性能；\n- 在域名和IP的映射过程中，给了应用基于域名做负载均衡的机会，可以是简单的负载均衡，也可以根据地址和运营商做全局的负载均衡。\n\n最后，给你留两个思考题：\n\n1. 全局负载均衡为什么要分地址和运营商呢？\n2. 全局负载均衡使用过程中，常常遇到失灵的情况，你知道具体有哪些情况吗？对应应该怎么来解决呢？\n\n# 19 讲HTTPDNS：网络世界的地址簿也会指错路\n\n上一节我们知道了DNS的两项功能，第一是根据名称查到具体的地址，另外一个是可以针对多个地址做负载均衡，而且可以在多个地址中选择一个距离你近的地方访问。\n\n然而有时候这个地址簿也经常给你指错路，明明距离你500米就有个吃饭的地方，非要把你推荐到5公里外。为什么会出现这样的情况呢？\n\n还记得吗？当我们发出请求解析DNS的时候，首先，会先连接到运营商本地的DNS服务器，由这个服务器帮我们去整棵DNS树上进行解析，然后将解析的结果返回给客户端。但是本地的DNS服务器，作为一个本地导游，往往有自己的“小心思”。\n\n## 传统DNS存在哪些问题？\n\n### 1.域名缓存问题\n\n它可以在本地做一个缓存，也就是说，不是每一个请求，它都会去访问权威DNS服务器，而是访问过一次就把结果缓存到自己本地，当其他人来问的时候，直接就返回这个缓存数据。\n\n这就相当于导游去过一个饭店，自己脑子记住了地址，当有一个游客问的时候，他就凭记忆回答了，不用再去查地址簿。这样经常存在的一个问题是，人家那个饭店明明都已经搬了，结果作为导游，他并没有刷新这个缓存，结果你辛辛苦苦到了这个地点，发现饭店已经变成了服装店，你是不是会非常失望？\n\n另外，有的运营商会把一些静态页面，缓存到本运营商的服务器内，这样用户请求的时候，就不用跨运营商进行访问，这样既加快了速度，也减少了运营商之间流量计算的成本。在域名解析的时候，不会将用户导向真正的网站，而是指向这个缓存的服务器。\n\n很多情况下是看不出问题的，但是当页面更新，用户会访问到老的页面，问题就出来了。例如，你听说一个餐馆推出了一个新菜，你想去尝一下。结果导游告诉你，在这里吃也是一样的。有的游客会觉得没问题，但是对于想尝试新菜的人来说，如果导游说带你去，但其实并没有吃到新菜，你是不是也会非常失望呢？\n\n再就是本地的缓存，往往使得全局负载均衡失败，因为上次进行缓存的时候，缓存中的地址不一定是这次访问离客户最近的地方，如果把这个地址返回给客户，那肯定就会绕远路。\n\n就像上一次客户要吃西湖醋鱼的事，导游知道西湖边有一家，因为当时游客就在西湖边，可是，下一次客户在灵隐寺，想吃西湖醋鱼的时候，导游还指向西湖边的那一家，那这就绕的太远了。\n\n![img](50ea6116ce6deadcc1a42587480e3bdf-20230115031642869.jpg)\n\n### 2.域名转发问题\n\n缓存问题还是说本地域名解析服务，还是会去权威DNS服务器中查找，只不过不是每次都要查找。可以说这还是大导游、大中介。还有一些小导游、小中介，有了请求之后，直接转发给其他运营商去做解析，自己只是外包了出去。\n\n这样的问题是，如果是A运营商的客户，访问自己运营商的DNS服务器，如果A运营商去权威DNS服务器查询的话，权威DNS服务器知道你是A运营商的，就返回给一个部署在A运营商的网站地址，这样针对相同运营商的访问，速度就会快很多。\n\n但是A运营商偷懒，将解析的请求转发给B运营商，B运营商去权威DNS服务器查询的话，权威服务器会误认为，你是B运营商的，那就返回给你一个在B运营商的网站地址吧，结果客户的每次访问都要跨运营商，速度就会很慢。\n\n![img](095d2a687f311d22481b51d97d9a9141-20230115031642937.jpg)﻿﻿\n\n### 3.出口NAT问题\n\n前面讲述网关的时候，我们知道，出口的时候，很多机房都会配置**NAT**，也即**网络地址转换**，使得从这个网关出去的包，都换成新的IP地址，当然请求返回的时候，在这个网关，再将IP地址转换回去，所以对于访问来说是没有任何问题。\n\n但是一旦做了网络地址的转换，权威的DNS服务器，就没办法通过这个地址，来判断客户到底是来自哪个运营商，而且极有可能因为转换过后的地址，误判运营商，导致跨运营商的访问。\n\n### 4.域名更新问题\n\n本地DNS服务器是由不同地区、不同运营商独立部署的。对域名解析缓存的处理上，实现策略也有区别，有的会偷懒，忽略域名解析结果的TTL时间限制，在权威DNS服务器解析变更的时候，解析结果在全网生效的周期非常漫长。但是有的时候，在DNS的切换中，场景对生效时间要求比较高。\n\n例如双机房部署的时候，跨机房的负载均衡和容灾多使用DNS来做。当一个机房出问题之后，需要修改权威DNS，将域名指向新的IP地址，但是如果更新太慢，那很多用户都会出现访问异常。\n\n这就像，有的导游比较勤快、敬业，时时刻刻关注酒店、餐馆、交通的变化，问他的时候，往往会得到最新情况。有的导游懒一些，8年前背的导游词就没换过，问他的时候，指的路往往就是错的。\n\n### 5.解析延迟问题\n\n从上一节的DNS查询过程来看，DNS的查询过程需要递归遍历多个DNS服务器，才能获得最终的解析结果，这会带来一定的时延，甚至会解析超时。\n\n## HTTPDNS的工作模式\n\n既然DNS解析中有这么多问题，那怎么办呢？难不成退回到直接用IP地址？这样显然不合适，所以就有了**HTTPDNS**。\n\n**HTTPNDS其实就是，不走传统的DNS解析，而是自己搭建基于HTTP协议的DNS服务器集群，分布在多个地点和多个运营商。当客户端需要DNS解析的时候，直接通过HTTP协议进行请求这个服务器集群，得到就近的地址。**\n\n这就相当于每家基于HTTP协议，自己实现自己的域名解析，自己做一个自己的地址簿，而不使用统一的地址簿。但是默认的域名解析都是走DNS的，因而使用HTTPDNS需要绕过默认的DNS路径，就不能使用默认的客户端。使用HTTPDNS的，往往是手机应用，需要在手机端嵌入支持HTTPDNS的客户端SDK。\n\n通过自己的HTTPDNS服务器和自己的SDK，实现了从依赖本地导游，到自己上网查询做旅游攻略，进行自由行，爱怎么玩怎么玩。这样就能够避免依赖导游，而导游又不专业，你还不能把他怎么样的尴尬。\n\n下面我来解析一下**HTTPDNS的工作模式**。\n\n在客户端的SDK里动态请求服务端，获取HTTPDNS服务器的IP列表，缓存到本地。随着不断地解析域名，SDK也会在本地缓存DNS域名解析的结果。\n\n当手机应用要访问一个地址的时候，首先看是否有本地的缓存，如果有就直接返回。这个缓存和本地DNS的缓存不一样的是，这个是手机应用自己做的，而非整个运营商统一做的。如何更新、何时更新，手机应用的客户端可以和服务器协调来做这件事情。\n\n如果本地没有，就需要请求HTTPDNS的服务器，在本地HTTPDNS服务器的IP列表中，选择一个发出HTTP的请求，会返回一个要访问的网站的IP列表。\n\n请求的方式是这样的。\n\n```bash\ncurl http://106.2.xxx.xxx/d?dn=c.m.163.com\n{\"dns\":[{\"host\":\"c.m.163.com\",\"ips\":[\"223.252.199.12\"],\"ttl\":300,\"http2\":0}],\"client\":{\"ip\":\"106.2.81.50\",\"line\":269692944}}\n```\n\n手机客户端自然知道手机在哪个运营商、哪个地址。由于是直接的HTTP通信，HTTPDNS服务器能够准确知道这些信息，因而可以做精准的全局负载均衡。\n\n![img](914d44e3d9246804b1b670b216146100-20230115031642938.jpg)\n\n当然，当所有这些都不工作的时候，可以切换到传统的LocalDNS来解析，慢也比访问不到好。那HTTPDNS是如何解决上面的问题的呢？\n\n其实归结起来就是两大问题。一是解析速度和更新速度的平衡问题，二是智能调度的问题，对应的解决方案是HTTPDNS的缓存设计和调度设计。\n\n### HTTPDNS的缓存设计\n\n解析DNS过程复杂，通信次数多，对解析速度造成很大影响。为了加快解析，因而有了缓存，但是这又会产生缓存更新速度不及时的问题。最要命的是，这两个方面都掌握在别人手中，也即本地DNS服务器手中，它不会为你定制，你作为客户端干着急没办法。\n\n而HTTPDNS就是将解析速度和更新速度全部掌控在自己手中。一方面，解析的过程，不需要本地DNS服务递归的调用一大圈，一个HTTP的请求直接搞定，要实时更新的时候，马上就能起作用；另一方面为了提高解析速度，本地也有缓存，缓存是在客户端SDK维护的，过期时间、更新时间，都可以自己控制。\n\nHTTPDNS的缓存设计策略也是咱们做应用架构中常用的缓存设计模式，也即分为客户端、缓存、数据源三层。\n\n- 对于应用架构来讲，就是应用、缓存、数据库。常见的是Tomcat、Redis、MySQL。\n- 对于HTTPDNS来讲，就是手机客户端、DNS缓存、HTTPDNS服务器。\n\n![img](022198aa7ac5584330aae0cb35a82f29-20230115031642980.jpg)\n\n只要是缓存模式，就存在缓存的过期、更新、不一致的问题，解决思路也是很像的。\n\n例如DNS缓存在内存中，也可以持久化到存储上，从而APP重启之后，能够尽快从存储中加载上次累积的经常访问的网站的解析结果，就不需要每次都全部解析一遍，再变成缓存。这有点像Redis是基于内存的缓存，但是同样提供持久化的能力，使得重启或者主备切换的时候，数据不会完全丢失。\n\nSDK中的缓存会严格按照缓存过期时间，如果缓存没有命中，或者已经过期，而且客户端不允许使用过期的记录，则会发起一次解析，保障记录是更新的。\n\n解析可以**同步进行**，也就是直接调用HTTPDNS的接口，返回最新的记录，更新缓存；也可以**异步进行**，添加一个解析任务到后台，由后台任务调用HTTPDNS的接口。\n\n**同步更新**的**优点**是实时性好，缺点是如果有多个请求都发现过期的时候，同时会请求HTTPDNS多次，其实是一种浪费。\n\n同步更新的方式对应到应用架构中缓存的**Cache-Aside机制**，也即先读缓存，不命中读数据库，同时将结果写入缓存。\n\n![img](a9ae8782b23c73bcc0c824dcf9fc370b-20230115031642981.jpg)\n\n**异步更新**的**优点**是，可以将多个请求都发现过期的情况，合并为一个对于HTTPDNS的请求任务，只执行一次，减少HTTPDNS的压力。同时可以在即将过期的时候，就创建一个任务进行预加载，防止过期之后再刷新，称为**预加载**。\n\n它的**缺点**是当前请求拿到过期数据的时候，如果客户端允许使用过期数据，需要冒一次风险。如果过期的数据还能请求，就没问题；如果不能请求，则失败一次，等下次缓存更新后，再请求方能成功。\n\n![img](e35240b0992c260602c5cff53299bf44-20230115031643079.jpg)\n\n异步更新的机制对应到应用架构中缓存的**Refresh-Ahead机制**，即业务仅仅访问缓存，当过期的时候定期刷新。在著名的应用缓存Guava Cache中，有个RefreshAfterWrite机制，对于并发情况下，多个缓存访问不命中从而引发并发回源的情况，可以采取只有一个请求回源的模式。在应用架构的缓存中，也常常用**数据预热**或者**预加载**的机制。\n\n![img](962250440c7e0bc39e510d7a9d075acd-20230115031643006.jpg)\n\n### HTTPDNS的调度设计\n\n由于客户端嵌入了SDK，因而就不会因为本地DNS的各种缓存、转发、NAT，让权威DNS服务器误会客户端所在的位置和运营商，而可以拿到第一手资料。\n\n在**客户端**，可以知道手机是哪个国家、哪个运营商、哪个省，甚至哪个市，HTTPDNS服务端可以根据这些信息，选择最佳的服务节点返回。\n\n如果有多个节点，还会考虑错误率、请求时间、服务器压力、网络状况等，进行综合选择，而非仅仅考虑地理位置。当有一个节点宕机或者性能下降的时候，可以尽快进行切换。\n\n要做到这一点，需要客户端使用HTTPDNS返回的IP访问业务应用。客户端的SDK会收集网络请求数据，如错误率、请求时间等网络请求质量数据，并发送到统计后台，进行分析、聚合，以此查看不同的IP的服务质量。\n\n在**服务端**，应用可以通过调用HTTPDNS的管理接口，配置不同服务质量的优先级、权重。HTTPDNS会根据这些策略综合地理位置和线路状况算出一个排序，优先访问当前那些优质的、时延低的IP地址。\n\nHTTPDNS通过智能调度之后返回的结果，也会缓存在客户端。为了不让缓存使得调度失真，客户端可以根据不同的移动网络运营商WIFI的SSID来分维度缓存。不同的运营商或者WIFI解析出来的结果会不同。\n\n![img](9edb73c0e7b369de8784376485427e38-20230115031643012.jpg)\n\n## 小结\n\n好了，这节就到这里了，我们来总结一下，你需要记住这两个重点：\n\n- 传统的DNS有很多问题，例如解析慢、更新不及时。因为缓存、转发、NAT问题导致客户端误会自己所在的位置和运营商，从而影响流量的调度。\n- HTTPDNS通过客户端SDK和服务端，通过HTTP直接调用解析DNS的方式，绕过了传统DNS的这些缺点，实现了智能的调度。\n\n最后，给你留两个思考题。\n\n1. 使用HTTPDNS，需要向HTTPDNS服务器请求解析域名，可是客户端怎么知道HTTPDNS服务器的地址或者域名呢？\n2. HTTPDNS的智能调度，主要是让客户端选择最近的服务器，而有另一种机制，使得资源分发到离客户端更近的位置，从而加快客户端的访问，你知道是什么技术吗？\n\n# 20 讲CDN：你去小卖部取过快递么？\n\n上一节，我们看到了网站的一般访问模式。\n\n当一个用户想访问一个网站的时候，指定这个网站的域名，DNS就会将这个域名解析为地址，然后用户请求这个地址，返回一个网页。就像你要买个东西，首先要查找商店的位置，然后去商店里面找到自己想要的东西，最后拿着东西回家。\n\n**那这里面还有没有可以优化的地方呢？**\n\n例如你去电商网站下单买个东西，这个东西一定要从电商总部的中心仓库送过来吗？原来基本是这样的，每一单都是单独配送，所以你可能要很久才能收到你的宝贝。但是后来电商网站的物流系统学聪明了，他们在全国各地建立了很多仓库，而不是只有总部的中心仓库才可以发货。\n\n电商网站根据统计大概知道，北京、上海、广州、深圳、杭州等地，每天能够卖出去多少书籍、卫生纸、包、电器等存放期比较长的物品。这些物品用不着从中心仓库发出，所以平时就可以将它们分布在各地仓库里，客户一下单，就近的仓库发出，第二天就可以收到了。\n\n这样，用户体验大大提高。当然，这里面也有个难点就是，生鲜这类东西保质期太短，如果提前都备好货，但是没有人下单，那肯定就坏了。这个问题，我后文再说。\n\n**我们先说，我们的网站访问可以借鉴“就近配送”这个思路。**\n\n全球有这么多的数据中心，无论在哪里上网，临近不远的地方基本上都有数据中心。是不是可以在这些数据中心里部署几台机器，形成一个缓存的集群来缓存部分数据，那么用户访问数据的时候，就可以就近访问了呢？\n\n当然是可以的。这些分布在各个地方的各个数据中心的节点，就称为**边缘节点**。\n\n由于边缘节点数目比较多，但是每个集群规模比较小，不可能缓存下来所有东西，因而可能无法命中，这样就会在边缘节点之上。有区域节点，规模就要更大，缓存的数据会更多，命中的概率也就更大。在区域节点之上是中心节点，规模更大，缓存数据更多。如果还不命中，就只好回源网站访问了。\n\n![img](d8c77f59d6b7ac894b5192252239cfcc-20230115031643106.jpg)\n\n这就是**CDN的分发系统的架构**。CDN系统的缓存，也是一层一层的，能不访问后端真正的源，就不打扰它。这也是电商网站物流系统的思路，北京局找不到，找华北局，华北局找不到，再找北方局。\n\n有了这个分发系统之后，接下来就是，**客户端如何找到相应的边缘节点进行访问呢？**\n\n还记得我们讲过的基于DNS的全局负载均衡吗？这个负载均衡主要用来选择一个就近的同样运营商的服务器进行访问。你会发现，CDN分发网络也是一个分布在多个区域、多个运营商的分布式系统，也可以用相同的思路选择最合适的边缘节点。\n\n![img](a94d543020d85c8feb9cd665eb4a3502-20230115031643086.jpg)\n\n**在没有CDN的情况下**，用户向浏览器输入www.web.com这个域名，客户端访问本地DNS服务器的时候，如果本地DNS服务器有缓存，则返回网站的地址；如果没有，递归查询到网站的权威DNS服务器，这个权威DNS服务器是负责web.com的，它会返回网站的IP地址。本地DNS服务器缓存下IP地址，将IP地址返回，然后客户端直接访问这个IP地址，就访问到了这个网站。\n\n然而**有了CDN之后，情况发生了变化**。在web.com这个权威DNS服务器上，会设置一个CNAME别名，指向另外一个域名 [www.web.cdn.com](http://www.web.cdn.com/)，返回给本地DNS服务器。\n\n当本地DNS服务器拿到这个新的域名时，需要继续解析这个新的域名。这个时候，再访问的就不是web.com的权威DNS服务器了，而是web.cdn.com的权威DNS服务器，这是CDN自己的权威DNS服务器。在这个服务器上，还是会设置一个CNAME，指向另外一个域名，也即CDN网络的全局负载均衡器。\n\n接下来，本地DNS服务器去请求CDN的全局负载均衡器解析域名，全局负载均衡器会为用户选择一台合适的缓存服务器提供服务，选择的依据包括：\n\n- 根据用户IP地址，判断哪一台服务器距用户最近；\n- 用户所处的运营商；\n- 根据用户所请求的URL中携带的内容名称，判断哪一台服务器上有用户所需的内容；\n- 查询各个服务器当前的负载情况，判断哪一台服务器尚有服务能力。\n\n基于以上这些条件，进行综合分析之后，全局负载均衡器会返回一台缓存服务器的IP地址。\n\n本地DNS服务器缓存这个IP地址，然后将IP返回给客户端，客户端去访问这个边缘节点，下载资源。缓存服务器响应用户请求，将用户所需内容传送到用户终端。如果这台缓存服务器上并没有用户想要的内容，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本地。\n\n**CDN可以进行缓存的内容有很多种。**\n\n保质期长的日用品比较容易缓存，因为不容易过期，对应到就像电商仓库系统里，就是静态页面、图片等，因为这些东西也不怎么变，所以适合缓存。\n\n![img](c81af7a52305f7de27e32e34a02d0eac-1584286108427-20230115031643118.jpg)\n\n还记得这个**接入层缓存的架构**吗？在进入数据中心的时候，我们希望通过最外层接入层的缓存，将大部分静态资源的访问拦在边缘。而CDN则更进一步，将这些静态资源缓存到离用户更近的数据中心外。越接近客户，访问性能越好，时延越低。\n\n但是静态内容中，有一种特殊的内容，也大量使用了CDN，这个就是前面讲过的[流媒体]。\n\nCDN支持**流媒体协议**，例如前面讲过的RTMP协议。在很多情况下，这相当于一个代理，从上一级缓存读取内容，转发给用户。由于流媒体往往是连续的，因而可以进行预先缓存的策略，也可以预先推送到用户的客户端。\n\n对于静态页面来讲，内容的分发往往采取**拉取**的方式，也即当发现未命中的时候，再去上一级进行拉取。但是，流媒体数据量大，如果出现**回源**，压力会比较大，所以往往采取主动**推送**的模式，将热点数据主动推送到边缘节点。\n\n对于流媒体来讲，很多CDN还提供**预处理服务**，也即文件在分发之前，经过一定的处理。例如将视频转换为不同的码流，以适应不同的网络带宽的用户需求；再如对视频进行分片，降低存储压力，也使得客户端可以选择使用不同的码率加载不同的分片。这就是我们常见的，“我要看超清、标清、流畅等”。\n\n对于流媒体CDN来讲，有个关键的问题是**防盗链**问题。因为视频是要花大价钱买版权的，为了挣点钱，收点广告费，如果流媒体被其他的网站盗走，在人家的网站播放，那损失可就大了。\n\n最常用也最简单的方法就是**HTTP头的refer字段**， 当浏览器发送请求的时候，一般会带上referer，告诉服务器是从哪个页面链接过来的，服务器基于此可以获得一些信息用于处理。如果refer信息不是来自本站，就阻止访问或者跳到其它链接。\n\n**refer的机制相对比较容易破解，所以还需要配合其他的机制。**\n\n一种常用的机制是**时间戳防盗链**。使用CDN的管理员可以在配置界面上，和CDN厂商约定一个加密字符串。\n\n客户端取出当前的时间戳，要访问的资源及其路径，连同加密字符串进行签名算法得到一个字符串，然后生成一个下载链接，带上这个签名字符串和截止时间戳去访问CDN。\n\n在CDN服务端，根据取出过期时间，和当前 CDN 节点时间进行比较，确认请求是否过期。然后CDN服务端有了资源及路径，时间戳，以及约定的加密字符串，根据相同的签名算法计算签名，如果匹配则一致，访问合法，才会将资源返回给客户。\n\n然而比如在电商仓库中，我在前面提过，有关生鲜的缓存就是非常麻烦的事情，这对应着就是动态的数据，比较难以缓存。怎么办呢？现在也有**动态CDN，主要有两种模式**。\n\n- 一种为**生鲜超市模式**，也即**边缘计算的模式**。既然数据是动态生成的，所以数据的逻辑计算和存储，也相应的放在边缘的节点。其中定时从源数据那里同步存储的数据，然后在边缘进行计算得到结果。就像对生鲜的烹饪是动态的，没办法事先做好缓存，因而将生鲜超市放在你家旁边，既能够送货上门，也能够现场烹饪，也是边缘计算的一种体现。\n- 另一种是**冷链运输模式**，也即**路径优化的模式**。数据不是在边缘计算生成的，而是在源站生成的，但是数据的下发则可以通过CDN的网络，对路径进行优化。因为CDN节点较多，能够找到离源站很近的边缘节点，也能找到离用户很近的边缘节点。中间的链路完全由CDN来规划，选择一个更加可靠的路径，使用类似专线的方式进行访问。\n\n对于常用的TCP连接，在公网上传输的时候经常会丢数据，导致TCP的窗口始终很小，发送速度上不去。根据前面的TCP流量控制和拥塞控制的原理，在CDN加速网络中可以调整TCP的参数，使得TCP可以更加激进地传输数据。\n\n可以通过多个请求复用一个连接，保证每次动态请求到达时。连接都已经建立了，不必临时三次握手或者建立过多的连接，增加服务器的压力。另外，可以通过对传输数据进行压缩，增加传输效率。\n\n所有这些手段就像冷链运输，整个物流优化了，全程冷冻高速运输。不管生鲜是从你旁边的超市送到你家的，还是从产地送的，保证到你家是新鲜的。\n\n## 小结\n\n好了，这节就到这里了。咱们来总结一下，你记住这两个重点就好。\n\n- CDN和电商系统的分布式仓储系统一样，分为中心节点、区域节点、边缘节点，而数据缓存在离用户最近的位置。\n- CDN最擅长的是缓存静态数据，除此之外还可以缓存流媒体数据，这时候要注意使用防盗链。它也支持动态数据的缓存，一种是边缘计算的生鲜超市模式，另一种是链路优化的冷链运输模式。\n\n最后，给你留两个思考题：\n\n1. 这一节讲了CDN使用DNS进行全局负载均衡的例子，CDN如何使用HTTPDNS呢？\n2. 客户端对DNS、HTTPDNS、CDN访问了半天，还没进数据中心，你知道数据中心里面什么样吗？\n\n# 21 讲数据中心：我是开发商，自己拿地盖别墅\n\n无论你是看新闻、下订单、看视频、下载文件，最终访问的目的地都在数据中心里面。我们前面学了这么多的网络协议和网络相关的知识，你是不是很好奇，数据中心究竟长啥样呢？\n\n数据中心是一个大杂烩，几乎要用到前面学过的所有知识。\n\n前面讲办公室网络的时候，我们知道办公室里面有很多台电脑。如果要访问外网，需要经过一个叫**网关**的东西，而网关往往是一个路由器。\n\n数据中心里面也有一大堆的电脑，但是它和咱们办公室里面的笔记本或者台式机不一样。数据中心里面是服务器。服务器被放在一个个叫作**机架**（**Rack**）的架子上面。\n\n数据中心的入口和出口也是路由器，由于在数据中心的边界，就像在一个国家的边境，称为**边界路由器**（**Border Router**）。为了高可用，边界路由器会有多个。\n\n一般家里只会连接一个运营商的网络，而为了高可用， 为了当一个运营商出问题的时候，还可以通过另外一个运营商来提供服务，所以数据中心的边界路由器会连接多个运营商网络。\n\n既然是路由器，就需要跑路由协议，数据中心往往就是路由协议中的自治区域（AS）。数据中心里面的机器要想访问外面的网站，数据中心里面也是有对外提供服务的机器，都可以通过BGP协议，获取内外互通的路由信息。这就是我们常听到的**多线BGP**的概念。\n\n如果数据中心非常简单，没几台机器，那就像家里或者宿舍一样，所有的服务器都直接连到路由器上就可以了。但是数据中心里面往往有非常多的机器，当塞满一机架的时候，需要有交换机将这些服务器连接起来，可以互相通信。\n\n这些交换机往往是放在机架顶端的，所以经常称为**TOR**（**Top Of** **Rack**）**交换机**。这一层的交换机常常称为**接入层**（**Access Layer**）。注意这个接入层和原来讲过的应用的接入层不是一个概念。\n\n![img](8fdfb8d4e1cd5a9a086f99b98a7555f8-1584286226108-20230115031643138.jpg)\n\n当一个机架放不下的时候，就需要多个机架，还需要有交换机将多个机架连接在一起。这些交换机对性能的要求更高，带宽也更大。这些交换机称为**汇聚层交换机**（**Aggregation Layer**）。\n\n数据中心里面的每一个连接都是需要考虑高可用的。这里首先要考虑的是，如果一台机器只有一个网卡，上面连着一个网线，接入到TOR交换机上。如果网卡坏了，或者不小心网线掉了，机器就上不去了。所以，需要至少两个网卡、两个网线插到TOR交换机上，但是两个网卡要工作得像一张网卡一样，这就是常说的**网卡绑定**（**bond**）。\n\n这就需要服务器和交换机都支持一种协议**LACP**（**Link Aggregation Control Protocol**）。它们互相通信，将多个网卡聚合称为一个网卡，多个网线聚合成一个网线，在网线之间可以进行负载均衡，也可以为了高可用作准备。\n\n![img](84196dedd044cc135bfc28ede4687d94-1584286228267-20230115031643153.jpg)\n\n网卡有了高可用保证，但交换机还有问题。如果一个机架只有一个交换机，它挂了，那整个机架都不能上网了。因而TOR交换机也需要高可用，同理接入层和汇聚层的连接也需要高可用性，也不能单线连着。\n\n最传统的方法是，部署两个接入交换机、两个汇聚交换机。服务器和两个接入交换机都连接，接入交换机和两个汇聚都连接，当然这样会形成环，所以需要启用STP协议，去除环，但是这样两个汇聚就只能一主一备了。STP协议里我们学过，只有一条路会起作用。\n\n![img](116a168c0eb55fabd7786fca728bd850-1584286230034-20230115031643164.jpg)\n\n交换机有一种技术叫作**堆叠**，所以另一种方法是，将多个交换机形成一个逻辑的交换机，服务器通过多根线分配连到多个接入层交换机上，而接入层交换机多根线分别连接到多个交换机上，并且通过堆叠的私有协议，形成**双活**的连接方式。\n\n![img](10aa7eac3fd38dfc2a09d6475ff4d93a-1584286228468-20230115031643218.jpg)\n\n由于对带宽要钱求更大，而且挂了影响也更大，所以两个堆叠可能就不够了，可以就会有更多的，比如四个堆叠为一个逻辑的交换机。\n\n汇聚层将大量的计算节点相互连接在一起，形成一个集群。在这个集群里面，服务器之间通过二层互通，这个区域常称为一个**POD**（**Point Of Delivery**），有时候也称为一个**可用区**（**Available Zon**e）。\n\n当节点数目再多的时候，一个可用区放不下，需要将多个可用区连在一起，连接多个可用区的交换机称为**核心交换机**。\n\n![img](080ce3bbe673de38e196b5b741a86313-20230115031643311.jpg)\n\n核心交换机吞吐量更大，高可用要求更高，肯定需要堆叠，但是往往仅仅堆叠，不足以满足吞吐量，因而还是需要部署多组核心交换机。核心和汇聚交换机之间为了高可用，也是全互连模式的。\n\n这个时候还存在那个问题，出现环路怎么办？\n\n一种方式是，不同的可用区在不同的二层网络，需要分配不同的网段。汇聚和核心之间通过三层网络互通的，二层都不在一个广播域里面，不会存在二层环路的问题。三层有环是没有问题的，只要通过路由协议选择最佳的路径就可以了。那为啥二层不能有环路，而三层可以呢？你可以回忆一下二层环路的情况。\n\n![img](524e5f104829cca60b953bfb04357898-1584286325381-20230115031643300.jpg)\n\n如图，核心层和汇聚层之间通过内部的路由协议OSPF，找到最佳的路径进行访问，而且还可以通过ECMP等价路由，在多个路径之间进行负载均衡和高可用。\n\n但是随着数据中心里面的机器越来越多，尤其是有了云计算、大数据，集群规模非常大，而且都要求在一个二层网络里面。这就需要二层互连从**汇聚层**上升为**核心层**，也即在核心以下，全部是二层互连，全部在一个广播域里面，这就是常说的**大二层**。\n\n![img](2aa3787c31c52defc7614c53f0a71d2c-1584286334074-20230115031643345.jpg)\n\n如果大二层横向流量不大，核心交换机数目不多，可以做堆叠，但是如果横向流量很大，仅仅堆叠满足不了，就需要部署多组核心交换机，而且要和汇聚层进行全互连。由于堆叠只解决一个核心交换机组内的无环问题，而组之间全互连，还需要其他机制进行解决。\n\n如果是STP，那部署多组核心无法扩大横向流量的能力，因为还是只有一组起作用。\n\n于是大二层就引入了**TRILL**（**Transparent Interconnection of Lots of Link**），即**多链接透明互联协议**。它的基本思想是，二层环有问题，三层环没有问题，那就把三层的路由能力模拟在二层实现。\n\n运行TRILL协议的交换机称为**RBridge**，是**具有路由转发特性的网桥设备**，只不过这个路由是根据MAC地址来的，不是根据IP来的。\n\nRbridage之间通过**链路状态协议**运作。记得这个路由协议吗？通过它可以学习整个大二层的拓扑，知道访问哪个MAC应该从哪个网桥走；还可以计算最短的路径，也可以通过等价的路由进行负载均衡和高可用性。\n\n![img](bc2dc829a1d51d7a5321a7605f93036e-1584286230034-20230115031643709.jpg)\n\nTRILL协议在原来的MAC头外面加上自己的头，以及外层的MAC头。TRILL头里面的Ingress RBridge，有点像IP头里面的源IP地址，Egress RBridge是目标IP地址，这两个地址是端到端的，在中间路由的时候，不会发生改变。而外层的MAC，可以有下一跳的Bridge，就像路由的下一跳，也是通过MAC地址来呈现的一样。\n\n如图中所示的过程，有一个包要从主机A发送到主机B，中间要经过RBridge 1、RBridge 2、RBridge X等等，直到RBridge 3。在RBridge 2收到的包里面，分内外两层，内层就是传统的主机A和主机B的MAC地址以及内层的VLAN。\n\n在外层首先加上一个TRILL头，里面描述这个包从RBridge 1进来的，要从RBridge 3出去，并且像三层的IP地址一样有跳数。然后再外面，目的MAC是RBridge 2，源MAC是RBridge 1，以及外层的VLAN。\n\n当RBridge 2收到这个包之后，首先看MAC是否是自己的MAC，如果是，要看自己是不是Egress RBridge，也即是不是最后一跳；如果不是，查看跳数是不是大于0，然后通过类似路由查找的方式找到下一跳RBridge X，然后将包发出去。\n\nRBridge 2发出去的包，内层的信息是不变的，外层的TRILL头里面。同样，描述这个包从RBridge 1进来的，要从RBridge 3出去，但是跳数要减1。外层的目标MAC变成RBridge X，源MAC变成RBridge 2。\n\n如此一直转发，直到RBridge 3，将外层解出来，发送内层的包给主机B。\n\n这个过程是不是和IP路由很像？\n\n对于大二层的广播包，也需要通过分发树的技术来实现。我们知道STP是将一个有环的图，通过去掉边形成一棵树，而分发树是一个有环的图形成多棵树，不同的树有不同的VLAN，有的广播包从VLAN A广播，有的从VLAN B广播，实现负载均衡和高可用。\n\n![img](0d686918ad9f7ea29791422d6eb41f36-1584286230034-20230115031643272.jpg)\n\n核心交换机之外，就是边界路由器了。至此从服务器到数据中心边界的层次情况已经清楚了。\n\n在核心交换上面，往往会挂一些安全设备，例如入侵检测、DDoS防护等等。这是整个数据中心的屏障，防止来自外来的攻击。核心交换机上往往还有负载均衡器，原理前面的章节已经说过了。\n\n在有的数据中心里面，对于存储设备，还会有一个存储网络，用来连接SAN和NAS。但是对于新的云计算来讲，往往不使用传统的SAN和NAS，而使用部署在x86机器上的软件定义存储，这样存储也是服务器了，而且可以和计算节点融合在一个机架上，从而更加有效率，也就没有了单独的存储网络了。\n\n于是整个数据中心的网络如下图所示。\n\n![img](9002b47ad5dc7762faf37e60250211eb-20230115031643379.jpg)\n\n这是一个典型的三层网络结构。这里的三层不是指IP层，而是指接入层、汇聚层、核心层三层。这种模式非常有利于外部流量请求到内部应用。这个类型的流量，是从外到内或者从内到外，对应到上面那张图里，就是从上到下，从下到上，上北下南，所以称为**南北流量**。\n\n但是随着云计算和大数据的发展，节点之间的交互越来越多，例如大数据计算经常要在不同的节点将数据拷贝来拷贝去，这样需要经过交换机，使得数据从左到右，从右到左，左西右东，所以称为**东西流量**。\n\n为了解决东西流量的问题，演进出了**叶脊网络**（**Spine/Leaf**）。\n\n- **叶子交换机**（**leaf**），直接连接物理服务器。L2/L3网络的分界点在叶子交换机上，叶子交换机之上是三层网络。\n- **脊交换机**（**spine switch**），相当于核心交换机。叶脊之间通过ECMP动态选择多条路径。脊交换机现在只是为叶子交换机提供一个弹性的L3路由网络。南北流量可以不用直接从脊交换机发出，而是通过与leaf交换机并行的交换机，再接到边界路由器出去。\n\n![img](99f86d113a629d81bb52786d80ca5c92-1584286349495-20230115031643397.jpg)\n\n传统的三层网络架构是垂直的结构，而叶脊网络架构是扁平的结构，更易于水平扩展。\n\n## 小结\n\n好了，复杂的数据中心就讲到这里了。我们来总结一下，你需要记住这三个重点。\n\n- 数据中心分为三层。服务器连接到接入层，然后是汇聚层，再然后是核心层，最外面是边界路由器和安全设备。\n- 数据中心的所有链路都需要高可用性。服务器需要绑定网卡，交换机需要堆叠，三层设备可以通过等价路由，二层设备可以通过TRILL协议。\n- 随着云和大数据的发展，东西流量相对于南北流量越来越重要，因而演化为叶脊网络结构。\n\n最后，给你留两个思考题：\n\n1. 对于数据中心来讲，高可用是非常重要的，每个设备都要考虑高可用，那跨机房的高可用，你知道应该怎么做吗？\n2. 前面说的浏览新闻、购物、下载、看视频等行为，都是普通用户通过公网访问数据中心里面的资源。那IT管理员应该通过什么样的方式访问数据中心呢？\n\n# 22 讲VPN：朝中有人好做官\n\n前面我们讲到了数据中心，里面很复杂，但是有的公司有多个数据中心，需要将多个数据中心连接起来，或者需要办公室和数据中心连接起来。这该怎么办呢？\n\n- 第一种方式是走公网，但是公网太不安全，你的隐私可能会被别人偷窥。\n- 第二种方式是租用专线的方式把它们连起来，这是土豪的做法，需要花很多钱。\n- 第三种方式是用VPN来连接，这种方法比较折中，安全又不贵。\n\n![img](9f797934cb5cf40543b716d97e214868-20230115031643382.jpg)\n\n**VPN**，全名**Virtual Private Network**，**虚拟专用网**，就是利用开放的公众网络，建立专用数据传输通道，将远程的分支机构、移动办公人员等连接起来。\n\n## VPN是如何工作的？\n\nVPN通过隧道技术在公众网络上仿真一条点到点的专线，是通过利用一种协议来传输另外一种协议的技术，这里面涉及三种协议：**乘客协议**、**隧道协议**和**承载协议**。\n\n我们以IPsec协议为例来说明。\n\n![img](a7c0a7fd2334d7145d093cf24bc6d7f1-20230115031643380.jpg)\n\n你知道如何通过自驾进行海南游吗？这其中，你的车怎么通过琼州海峡呢？这里用到轮渡，其实这就用到**隧道协议**。\n\n在广州这边开车是有“协议”的，例如靠右行驶、红灯停、绿灯行，这个就相当于“被封装”的**乘客协议**。当然在海南那面，开车也是同样的协议。这就相当于需要连接在一起的一个公司的两个分部。\n\n但是在海上坐船航行，也有它的协议，例如要看灯塔、要按航道航行等。这就是外层的**承载协议**。\n\n那我的车如何从广州到海南呢？这就需要你遵循开车的协议，将车开上轮渡，所有通过轮渡的车都关在船舱里面，按照既定的规则排列好，这就是**隧道协议**。\n\n在大海上，你的车是关在船舱里面的，就像在隧道里面一样，这个时候内部的乘客协议，也即驾驶协议没啥用处，只需要船遵从外层的承载协议，到达海南就可以了。\n\n到达之后，外部承载协议的任务就结束了，打开船舱，将车开出来，就相当于取下承载协议和隧道协议的头。接下来，在海南该怎么开车，就怎么开车，还是内部的乘客协议起作用。\n\n在最前面的时候说了，直接使用公网太不安全，所以接下来我们来看一种十分安全的VPN，**IPsec VPN**。这是基于IP协议的**安全隧道协议**，为了保证在公网上面信息的安全，因而采取了一定的机制保证安全性。\n\n- 机制一：**私密性**，防止信息泄漏给未经授权的个人，通过加密把数据从明文变成无法读懂的密文，从而确保数据的私密性。 前面讲HTTPS的时候，说过加密可以分为对称加密和非对称加密。对称加密速度快一些。而VPN一旦建立，需要传输大量数据，因而我们采取对称加密。但是同样，对称加密还是存在加密秘钥如何传输的问题，这里需要用到因特网密钥交换（IKE，Internet Key Exchange）协议。\n- 机制二：**完整性**，数据没有被非法篡改，通过对数据进行hash运算，产生类似于指纹的数据摘要，以保证数据的完整性。\n- 机制三：**真实性**，数据确实是由特定的对端发出，通过身份认证可以保证数据的真实性。\n\n那如何保证对方就是真正的那个人呢？\n\n- 第一种方法就是**预共享密钥**，也就是双方事先商量好一个暗号，比如“天王盖地虎，宝塔镇河妖”，对上了，就说明是对的。\n- 另外一种方法就是**用数字签名来验证**。咋签名呢？当然是使用私钥进行签名，私钥只有我自己有，所以如果对方能用我的数字证书里面的公钥解开，就说明我是我。\n\n基于以上三个特性，组成了**IPsec VPN的协议簇**。这个协议簇内容比较丰富。\n\n![img](34a952bd1abeec19460b8d5dca5cd0af-20230115031643493.jpg)\n\n在这个协议簇里面，有两种协议，这两种协议的区别在于封装网络包的格式不一样。\n\n- 一种协议称为**AH**（**Authentication Header**），只能进行数据摘要 ，不能实现数据加密。\n- 还有一种**ESP**（**Encapsulating Security Payload**），能够进行数据加密和数据摘要。\n\n在这个协议簇里面，还有两类算法，分别是**加密算法**和**摘要算法**。\n\n这个协议簇还包含两大组件，一个用于VPN的双方要进行对称密钥的交换的**IKE组件**，另一个是VPN的双方要对连接进行维护的**SA（Security Association）组件**。\n\n## IPsec VPN的建立过程\n\n下面来看IPsec VPN的建立过程，这个过程分两个阶段。\n\n**第一个阶段，建立IKE自己的SA**。这个SA用来维护一个通过身份认证和安全保护的通道，为第二个阶段提供服务。在这个阶段，通过DH（Diffie-Hellman）算法计算出一个对称密钥K。\n\nDH算法是一个比较巧妙的算法。客户端和服务端约定两个公开的质数p和q，然后客户端随机产生一个数a作为自己的私钥，服务端随机产生一个b作为自己的私钥，客户端可以根据p、q和a计算出公钥A，服务端根据p、q和b计算出公钥B，然后双方交换公钥A和B。\n\n到此客户端和服务端可以根据已有的信息，各自独立算出相同的结果K，就是**对称密钥**。但是这个过程，对称密钥从来没有在通道上传输过，只传输了生成密钥的材料，通过这些材料，截获的人是无法算出的。\n\n![img](28a3938ab8efaf9fa7f313a46d0e1478-20230115031643455.jpg)\n\n有了这个对称密钥K，接下来是**第二个阶段，建立IPsec SA**。在这个SA里面，双方会生成一个随机的对称密钥M，由K加密传给对方，然后使用M进行双方接下来通信的数据。对称密钥M是有过期时间的，会过一段时间，重新生成一次，从而防止被破解。\n\nIPsec SA里面有以下内容：\n\n- SPI（Security Parameter Index），用于标识不同的连接；\n- 双方商量好的加密算法、哈希算法和封装模式；\n- 生存周期，超过这个周期，就需要重新生成一个IPsec SA，重新生成对称密钥。\n\n![img](87a34817f22ed4a2afb58e8f6496f159-20230115031643457.jpg)\n\n当IPsec建立好，接下来就可以开始打包封装传输了。\n\n![img](71fa17f5bb21c12aec1234eec85a6dca-20230115031643514.jpg)\n\n左面是原始的IP包，在IP头里面，会指定上一层的协议为TCP。ESP要对IP包进行封装，因而IP头里面的上一层协议为ESP。在ESP的正文里面，ESP的头部有双方商讨好的SPI，以及这次传输的序列号。\n\n接下来全部是加密的内容。可以通过对称密钥进行解密，解密后在正文的最后，指明了里面的协议是什么。如果是IP，则需要先解析IP头，然后解析TCP头，这是从隧道出来后解封装的过程。\n\n有了IPsec VPN之后，客户端发送的明文的IP包，都会被加上ESP头和IP头，在公网上传输，由于加密，可以保证不被窃取，到了对端后，去掉ESP的头，进行解密。\n\n![img](dfc5a7934added3cc11d9c95a69a9bb5-20230115031643465.jpg)\n\n这种点对点的基于IP的VPN，能满足互通的要求，但是速度往往比较慢，这是由底层IP协议的特性决定的。IP不是面向连接的，是尽力而为的协议，每个IP包自由选择路径，到每一个路由器，都自己去找下一跳，丢了就丢了，是靠上一层TCP的重发来保证可靠性。\n\n![img](29f4dff1f40a252dda2ac6f9d1d4088e-20230115031643523.jpg)\n\n因为IP网络从设计的时候，就认为是不可靠的，所以即使同一个连接，也可能选择不同的道路，这样的好处是，一条道路崩溃的时候，总有其他的路可以走。当然，带来的代价就是，不断的路由查找，效率比较差。\n\n和IP对应的另一种技术称为ATM。这种协议和IP协议的不同在于，它是面向连接的。你可以说TCP也是面向连接的啊。这两个不同，ATM和IP是一个层次的，和TCP不是一个层次的。\n\n另外，TCP所谓的面向连接，是不停地重试来保证成功，其实下层的IP还是不面向连接的，丢了就丢了。ATM是传输之前先建立一个连接，形成一个虚拟的通路，一旦连接建立了，所有的包都按照相同的路径走，不会分头行事。\n\n![img](afea2a2cecb8bfe6c1c40585000f1c48-20230115031643655.jpg)\n\n好处是不需要每次都查路由表的，虚拟路径已经建立，打上了标签，后续的包傻傻的跟着走就是了，不用像IP包一样，每个包都思考下一步怎么走，都按相同的路径走，这样效率会高很多。\n\n但是一旦虚拟路径上的某个路由器坏了，则这个连接就断了，什么也发不过去了，因为其他的包还会按照原来的路径走，都掉坑里了，它们不会选择其他的路径走。\n\nATM技术虽然没有成功，但其屏弃了繁琐的路由查找，改为简单快速的标签交换，将具有全局意义的路由表改为只有本地意义的标签表，这些都可以大大提高一台路由器的转发功力。\n\n有没有一种方式将两者的优点结合起来呢？这就是**多协议标签交换**（**MPLS**，**Multi-Protocol Label Switching**）。MPLS的格式如图所示，在原始的IP头之外，多了MPLS的头，里面可以打标签。\n\n![img](ab77ad0cec6a26f43bacb3f51b0c8d32-20230115031643522.jpg)\n\n在二层头里面，有类型字段，0x0800表示IP，0x8847表示MPLS Label。\n\n在MPLS头里面，首先是标签值占20位，接着是3位实验位，再接下来是1位栈底标志位，表示当前标签是否位于栈底了。这样就允许多个标签被编码到同一个数据包中，形成标签栈。最后是8位TTL存活时间字段，如果标签数据包的出发TTL值为0，那么该数据包在网络中的生命期被认为已经过期了。\n\n有了标签，还需要设备认这个标签，并且能够根据这个标签转发，这种能够转发标签的路由器称为**标签交换路由器**（LSR，Label Switching Router）。\n\n这种路由器会有两个表格，一个就是传统的FIB，也即路由表，另一个就是LFIB，标签转发表。有了这两个表，既可以进行普通的路由转发，也可以进行基于标签的转发。\n\n![img](782742e09ddddcef169c9982b65c69ee-20230115031643555.jpg)\n\n有了标签转发表，转发的过程如图所示，就不用每次都进行普通路由的查找了。\n\n这里我们区分MPLS区域和非MPLS区域。在MPLS区域中间，使用标签进行转发，非MPLS区域，使用普通路由转发，在边缘节点上，需要有能力将对于普通路由的转发，变成对于标签的转发。\n\n例如图中要访问114.1.1.1，在边界上查找普通路由，发现马上要进入MPLS区域了，进去了对应标签1，于是在IP头外面加一个标签1，在区域里面，标签1要变成标签3，标签3到达出口边缘，将标签去掉，按照路由发出。\n\n这样一个通过标签转换而建立的路径称为LSP，标签交换路径。在一条LSP上，沿数据包传送的方向，相邻的LSR分别叫**上游LSR**（**upstream LSR**）和**下游LSR**（**downstream LSR**）。\n\n有了标签，转发是很简单的事，但是如何生成标签，却是MPLS中最难修炼的部分。在MPLS秘笈中，这部分被称为**LDP**（**Label Distribution Protocol**），是一个动态的生成标签的协议。\n\n其实LDP与IP帮派中的路由协议十分相像，通过LSR的交互，互相告知去哪里应该打哪个标签，称为标签分发，往往是从下游开始的。\n\n![img](947e2fece5e3b2750b0b73c9458f3784-20230115031643603.jpg)\n\n如果有一个边缘节点发现自己的路由表中出现了新的目的地址，它就要给别人说，我能到达一条新的路径了。\n\n如果此边缘节点存在上游LSR，并且尚有可供分配的标签，则该节点为新的路径分配标签，并向上游发出标签映射消息，其中包含分配的标签等信息。\n\n收到标签映射消息的LSR记录相应的标签映射信息，在其标签转发表中增加相应的条目。此LSR为它的上游LSR分配标签，并继续向上游LSR发送标签映射消息。\n\n当入口LSR收到标签映射消息时，在标签转发表中增加相应的条目。这时，就完成了LSP的建立。有了标签，转发轻松多了，但是这个和VPN什么关系呢？\n\n可以想象，如果我们VPN通道里面包的转发，都是通过标签的方式进行，效率就会高很多。所以要想个办法把MPLS应用于VPN。\n\n![img](bae67c4e3f4a4127bad07de4bb577bec-20230115031643596.jpg)\n\n在MPLS VPN中，网络中的路由器分成以下几类：\n\n- PE（Provider Edge）：运营商网络与客户网络相连的边缘网络设备；\n- CE（Customer Edge）：客户网络与PE相连接的边缘设备；\n- P（Provider）：这里特指运营商网络中除PE之外的其他运营商网络设备。\n\n为什么要这样分呢？因为我们发现，在运营商网络里面，也即P Router之间，使用标签是没有问题的，因为都在运营商的管控之下，对于网段，路由都可以自己控制。但是一旦客户要接入这个网络，就复杂得多。\n\n首先是客户地址重复的问题。客户所使用的大多数都是私网的地址(192.168.X.X;10.X.X.X;172.X.X.X)，而且很多情况下都会与其它的客户重复。\n\n比如，机构A和机构B都使用了192.168.101.0/24网段的地址，这就发生了地址空间重叠（Overlapping Address Spaces）。\n\n首先困惑的是BGP协议，既然VPN将两个数据中心连起来，应该看起来像一个数据中心一样，那么如何到达另一端需要通过BGP将路由广播过去，传统BGP无法正确处理地址空间重叠的VPN的路由。\n\n假设机构A和机构B都使用了192.168.101.0/24网段的地址，并各自发布了一条去往此网段的路由，BGP将只会选择其中一条路由，从而导致去往另一个VPN的路由丢失。\n\n所以PE路由器之间使用特殊的MP-BGP来发布VPN路由，在相互沟通的消息中，在一般32位IPv4的地址之前加上一个客户标示的区分符用于客户地址的区分，这种称为VPN-IPv4地址族，这样PE路由器会收到如下的消息，机构A的192.168.101.0/24应该往这面走，机构B的192.168.101.0/24则应该去另外一个方向。\n\n另外困惑的是**路由表**，当两个客户的IP包到达PE的时候，PE就困惑了，因为网段是重复的。\n\n如何区分哪些路由是属于哪些客户VPN内的？如何保证VPN业务路由与普通路由不相互干扰？\n\n在PE上，可以通过VRF（VPN Routing\u0026Forwarding Instance）建立每个客户一个路由表，与其它VPN客户路由和普通路由相互区分。可以理解为专属于客户的小路由器。\n\n远端PE通过MP-BGP协议把业务路由放到近端PE，近端PE根据不同的客户选择出相关客户的业务路由放到相应的VRF路由表中。\n\nVPN报文转发采用两层标签方式：\n\n- 第一层（外层）标签在骨干网内部进行交换，指示从PE到对端PE的一条LSP。VPN报文利用这层标签，可以沿LSP到达对端PE；\n- 第二层（内层）标签在从对端PE到达CE时使用，在PE上，通过查找VRF表项，指示报文应被送到哪个VPN用户，或者更具体一些，到达哪一个CE。这样，对端PE根据内层标签可以找到转发报文的接口。\n\n![img](3cd18d212bf76e7151ea73aa65ef48dd-20230115031643608.jpg)\n\n我们来举一个例子，看MPLS VPN的包发送过程。\n\n1. 机构A和机构B都发出一个目的地址为192.168.101.0/24的IP报文，分别由各自的CE将报文发送至PE。\n2. PE会根据报文到达的接口及目的地址查找VPN实例表项VRF，匹配后将报文转发出去，同时打上内层和外层两个标签。假设通过MP-BGP配置的路由，两个报文在骨干网走相同的路径。\n3. MPLS网络利用报文的外层标签，将报文传送到出口PE，报文在到达出口PE 2前一跳时已经被剥离外层标签，仅含内层标签。\n4. 出口PE根据内层标签和目的地址查找VPN实例表项VRF，确定报文的出接口，将报文转发至各自的CE。\n5. CE根据正常的IP转发过程将报文传送到目的地。\n\n## 小结\n\n好了，这一节就到这里了，我们来总结一下：\n\n- VPN可以将一个机构的多个数据中心通过隧道的方式连接起来，让机构感觉在一个数据中心里面，就像自驾游通过琼州海峡一样；\n- 完全基于软件的IPsec VPN可以保证私密性、完整性、真实性、简单便宜，但是性能稍微差一些；\n- MPLS-VPN综合和IP转发模式和ATM的标签转发模式的优势，性能较好，但是需要从运营商购买。\n\n接下来，给你留两个思考题：\n\n1. 当前业务的高可用性和弹性伸缩很重要，所以很多机构都会在自建私有云之外，采购公有云，你知道私有云和公有云应该如何打通吗？\n2. 前面所有的上网行为，都是基于电脑的，但是移动互联网越来越成为核心，你知道手机上网都需要哪些协议吗？\n\n# 23 讲移动网络：去巴塞罗那，手机也上不了脸书\n\n前面讲的都是电脑上网的场景，那使用手机上网有什么不同呢？\n\n## 移动网络的发展历程\n\n你一定知道手机上网有2G、3G、4G的说法，究竟这都是什么意思呢？有一个通俗的说法就是：用2G看txt，用3G看jpg，用4G看avi。\n\n### 2G网络\n\n手机本来是用来打电话的，不是用来上网的，所以原来在2G时代，上网使用的不是IP网络，而是电话网络，走模拟信号，专业名称为公共交换电话网（PSTN，Public Switched Telephone Network）。\n\n那手机不连网线，也不连电话线，它是怎么上网的呢？\n\n手机是通过收发无线信号来通信的，专业名称是Mobile Station，简称MS，需要嵌入SIM。手机是客户端，而无线信号的服务端，就是基站子系统（BSS，Base Station SubsystemBSS）。至于什么是基站，你可以回想一下，你在爬山的时候，是不是看到过信号塔？我们平时城市里面的基站比较隐蔽，不容易看到，所以只有在山里才会注意到。正是这个信号塔，通过无线信号，让你的手机可以进行通信。\n\n但是你要知道一点，**无论无线通信如何无线，最终还是要连接到有线的网络里**。前面讲[数据中心]的时候我也讲过，电商的应用是放在数据中心的，数据中心的电脑都是插着网线的。\n\n因而，基站子系统分两部分，一部分对外提供无线通信，叫作基站收发信台（BTS，Base Transceiver Station），另一部分对内连接有线网络，叫作基站控制器（BSC，Base Station Controller）。基站收发信台通过无线收到数据后，转发给基站控制器。\n\n这部分属于无线的部分，统称为无线接入网（RAN，Radio Access Network）。\n\n基站控制器通过有线网络，连接到提供手机业务的运营商的数据中心，这部分称为核心网（CN，Core Network）。核心网还没有真的进入互联网，这部分还是主要提供手机业务，是手机业务的有线部分。\n\n首先接待基站来的数据的是移动业务交换中心（MSC，Mobile Service Switching Center），它是进入核心网的入口，但是它不会让你直接连接到互联网上。\n\n因为在让你的手机真正进入互联网之前，提供手机业务的运营商，需要认证是不是合法的手机接入。别你自己造了一张手机卡，就连接上来。鉴权中心（AUC，Authentication Center）和设备识别寄存器（EIR，Equipment Identity Register）主要是负责安全性的。\n\n另外，需要看你是本地的号，还是外地的号，这个牵扯到计费的问题，异地收费还是很贵的。访问位置寄存器（VLR，Visit Location Register）是看你目前在的地方，归属位置寄存器（HLR，Home Location Register）是看你的号码归属地。\n\n当你的手机卡既合法又有钱的时候，才允许你上网，这个时候需要一个网关，连接核心网和真正的互联网。网关移动交换中心（GMSC ，Gateway Mobile Switching Center）就是干这个的，然后是真正的互连网。在2G时代，还是电话网络PSTN。\n\n数据中心里面的这些模块统称为网络子系统（NSS，Network and Switching Subsystem）。\n\n![img](c065c4f15421c7fd9924c472948aa8e2-20230115031643719.jpg)\n\n因而2G时代的上网如图所示，我们总结一下，有这几个核心点：\n\n- **手机通过无线信号连接基站；**\n- **基站一面朝前接无线，一面朝后接核心网；**\n- **核心网一面朝前接到基站请求，一是判断你是否合法，二是判断你是不是本地号，还有没有钱，一面通过网关连接电话网络。**\n\n### 2.5G网络\n\n后来从2G到了2.5G，也即在原来电路交换的基础上，加入了分组交换业务，支持Packet的转发，从而支持IP网络。\n\n在上述网络的基础上，基站一面朝前接无线，一面朝后接核心网。在朝后的组件中，多了一个分组控制单元（PCU，Packet Control Unit），用以提供分组交换通道。\n\n在核心网里面，有个朝前的接待员（SGSN，Service GPRS Supported Node）和朝后连接IP网络的网关型GPRS支持节点（GGSN，Gateway GPRS Supported Node）。\n\n![img](39fa17696761d28f2f04c8555041aa90-20230115031643656.jpg)\n\n### 3G网络\n\n到了3G时代，主要是无线通信技术有了改进，大大增加了无线的带宽。\n\n以W-CDMA为例，理论最高2M的下行速度，因而基站改变了，一面朝外的是Node B，一面朝内连接核心网的是无线网络控制器（RNC，Radio Network Controller）。核心网以及连接的IP网络没有什么变化。\n\n![img](9453ee22b697455ba3f2c4f89936b018-20230115031643670.jpg)\n\n### 4G网络\n\n然后就到了今天的4G网络，基站为eNodeB，包含了原来Node B和RNC的功能，下行速度向百兆级别迈进。另外，核心网实现了控制面和数据面的分离，这个怎么理解呢？\n\n在前面的核心网里面，有接待员MSC或者SGSN，你会发现检查是否合法是它负责，转发数据也是它负责，也即控制面和数据面是合二为一的，这样灵活性比较差，因为控制面主要是指令，多是小包，往往需要高的及时性；数据面主要是流量，多是大包，往往需要吞吐量。\n\n于是有了下面这个架构。\n\n![img](c83a147558c2f8c3a2c2440ca5208835-20230115031643675.jpg)\n\nHSS用于存储用户签约信息的数据库，其实就是你这个号码归属地是哪里的，以及一些认证信息。\n\nMME是核心控制网元，是控制面的核心，当手机通过eNodeB连上的时候，MME会根据HSS的信息，判断你是否合法。如果允许连上来，MME不负责具体的数据的流量，而是MME会选择数据面的SGW和PGW，然后告诉eNodeB，我允许你连上来了，你连接它们吧。\n\n于是手机直接通过eNodeB连接SGW，连上核心网，SGW相当于数据面的接待员，并通过PGW连到IP网络。PGW就是出口网关。在出口网关，有一个组件PCRF，称为策略和计费控制单元，用来控制上网策略和流量的计费。\n\n## 4G网络协议解析\n\n我们来仔细看一下4G网络的协议，真的非常复杂。我们将几个关键组件放大来看。\n\n![img](e40f7cb5754e01ed82ae120d8d571f50-20230115031643721.jpg)\n\n### 控制面协议\n\n其中虚线部分是控制面的协议。当一个手机想上网的时候，先要连接eNodeB，并通过S1-MME接口，请求MME对这个手机进行认证和鉴权。S1-MME协议栈如下图所示。\n\n![img](3b40f3c06f95de2e261609b82163f392-20230115031643719.jpg)\n\nUE就是你的手机，eNodeB还是两面派，朝前对接无线网络，朝后对接核心网络，在控制面对接的是MME。\n\neNodeB和MME之间的连接就是很正常的IP网络，但是这里面在IP层之上，却既不是TCP，也不是UDP，而是SCTP。这也是传输层的协议，也是面向连接的，但是更加适合移动网络。 它继承了TCP较为完善的拥塞控制并改进TCP的一些不足之处。\n\nSCTP的第一个特点是**多宿主**。一台机器可以有多个网卡，而对于TCP连接来讲，虽然服务端可以监听0.0.0.0，也就是从哪个网卡来的连接都能接受，但是一旦建立了连接，就建立了四元组，也就选定了某个网卡。\n\nSCTP引入了联合（association）的概念，将多个接口、多条路径放到一个联合中来。当检测到一条路径失效时，协议就会通过另外一条路径来发送通信数据。应用程序甚至都不必知道发生了故障、恢复，从而提供更高的可用性和可靠性。\n\nSCTP的第二个特点是**将一个联合分成多个流**。一个联合中的所有流都是独立的，但均与该联合相关。每个流都给定了一个流编号，它被编码到SCTP报文中，通过联合在网络上传送。在TCP的机制中，由于强制顺序，导致前一个不到达，后一个就得等待，SCTP的多个流不会相互阻塞。\n\nSCTP的第三个特点是**四次握手，防止SYN攻击**。在TCP中是三次握手，当服务端收到客户的SYN之后，返回一个SYN-ACK之前，就建立数据结构，并记录下状态，等待客户端发送ACK的ACK。当恶意客户端使用虚假的源地址来伪造大量SYN报文时，服务端需要分配大量的资源，最终耗尽资源，无法处理新的请求。\n\nSCTP可以通过四次握手引入Cookie的概念，来有效地防止这种攻击的产生。在SCTP中，客户机使用一个INIT报文发起一个连接。服务器使用一个INIT-ACK报文进行响应，其中就包括了Cookie。然后客户端就使用一个COOKIE-ECHO报文进行响应，其中包含了服务器所发送的Cookie。这个时候，服务器为这个连接分配资源，并通过向客户机发送一个COOKIE-ACK报文对其进行响应。\n\nSCTP的第四个特点是**将消息分帧**。TCP是面向流的，也即发送的数据没头没尾，没有明显的界限。这对于发送数据没有问题，但是对于发送一个个消息类型的数据，就不太方便。有可能客户端写入10个字节，然后再写入20个字节。服务端不是读出10个字节的一个消息，再读出20个字节的一个消息，而有可能读入25个字节，再读入5个字节，需要业务层去组合成消息。\n\nSCTP借鉴了UDP的机制，在数据传输中提供了消息分帧功能。当一端对一个套接字执行写操作时，可确保对等端读出的数据大小与此相同。\n\nSCTP的第五个特点是**断开连接是三次挥手**。在TCP里面，断开连接是四次挥手，允许另一端处于半关闭的状态。SCTP选择放弃这种状态，当一端关闭自己的套接字时，对等的两端全部需要关闭，将来任何一端都不允许再进行数据的移动了。\n\n当MME通过认证鉴权，同意这个手机上网的时候，需要建立一个数据面的数据通路。建立通路的过程还是控制面的事情，因而使用的是控制面的协议GTP-C。\n\n建设的数据通路分两段路，其实是两个隧道。一段是从eNodeB到SGW，这个数据通路由MME通过S1-MME协议告诉eNodeB，它是隧道的一端，通过S11告诉SGW，它是隧道的另一端。第二端是从SGW到PGW，SGW通过S11协议知道自己是其中一端，并主动通过S5协议，告诉PGW它是隧道的另一端。\n\nGTP-C协议是基于UDP的，这是[UDP的“城会玩”]中的一个例子。如果看GTP头，我们可以看到，这里面有隧道的ID，还有序列号。\n\n![img](77eb34d092948fbf8773ef9041305fcd-20230115031643753.jpg)\n\n通过序列号，不用TCP，GTP-C自己就可以实现可靠性，为每个输出信令消息分配一个依次递增的序列号，以确保信令消息的按序传递，并便于检测重复包。对于每个输出信令消息启动定时器，在定时器超时前未接收到响应消息则进行重发。\n\n### 数据面协议\n\n当两个隧道都打通，接在一起的时候，PGW会给手机分配一个IP地址，这个IP地址是隧道内部的IP地址，可以类比为IPsec协议里面的IP地址。这个IP地址是归手机运营商管理的。然后，手机可以使用这个IP地址，连接eNodeB，从eNodeB经过S1-U协议，通过第一段隧道到达SGW，再从SGW经过S8协议，通过第二段隧道到达PGW，然后通过PGW连接到互联网。\n\n数据面的协议都是通过GTP-U，如图所示。\n\n![img](0397a72d0c5d76d4e3591cbe61eef729-20230115031643739.jpg)\n\n手机每发出的一个包，都由GTP-U隧道协议封装起来，格式如下。\n\n![img](84c128c408ac748f1206b9b4d4500132-20230115031643964.jpg)\n\n和IPsec协议很类似，分为乘客协议、隧道协议、承载协议。其中乘客协议是手机发出来的包，IP是手机的IP，隧道协议里面有隧道ID，不同的手机上线会建立不同的隧道，因而需要隧道ID来标识。承载协议的IP地址是SGW和PGW的IP地址。\n\n### 手机上网流程\n\n接下来，我们来看一个手机开机之后上网的流程，这个过程称为**Attach**。可以看出来，移动网络还是很复杂的。因为这个过程要建立很多的隧道，分配很多的隧道ID，所以我画了一个图来详细说明这个过程。\n\n![img](4d3e9282b00410a28e77f91f9375f4d2-20230115031643783.jpg)\n\n1. 手机开机以后，在附近寻找基站eNodeB，找到后给eNodeB发送Attach Request，说“我来啦，我要上网”。\n2. eNodeB将请求发给MME，说“有个手机要上网”。\n3. MME去请求手机，一是认证，二是鉴权，还会请求HSS看看有没有钱，看看是在哪里上网。\n4. 当MME通过了手机的认证之后，开始分配隧道，先告诉SGW，说要创建一个会话（Create Session）。在这里面，会给SGW分配一个隧道ID t1，并且请求SGW给自己也分配一个隧道ID。\n5. SGW转头向PGW请求建立一个会话，为PGW的控制面分配一个隧道ID t2，也给PGW的数据面分配一个隧道ID t3，并且请求PGW给自己的控制面和数据面分配隧道ID。\n6. PGW回复SGW说“创建会话成功”，使用自己的控制面隧道ID t2，回复里面携带着给SGW控制面分配的隧道ID t4和控制面的隧道ID t5，至此SGW和PGW直接的隧道建设完成。双方请求对方，都要带着对方给自己分配的隧道ID，从而标志是这个手机的请求。\n7. 接下来SGW回复MME说“创建会话成功”，使用自己的隧道ID t1访问MME，回复里面有给MME分配隧道ID t6，也有SGW给eNodeB分配的隧道ID t7。\n8. 当MME发现后面的隧道都建设成功之后，就告诉eNodeB，“后面的隧道已经建设完毕，SGW给你分配的隧道ID是t7，你可以开始连上来了，但是你也要给SGW分配一个隧道ID”。\n9. eNodeB告诉MME自己给SGW分配一个隧道，ID为t8。\n10. MME将eNodeB给SGW分配的隧道ID t8告知SGW，从而前面的隧道也建设完毕。\n\n这样，手机就可以通过建立的隧道成功上网了。\n\n## 异地上网问题\n\n接下来我们考虑异地上网的事情。\n\n为什么要分SGW和PGW呢，一个GW不可以吗？SGW是你本地的运营商的设备，而PGW是你所属的运营商的设备。\n\n如果你在巴塞罗那，一下飞机，手机开机，周围搜寻到的肯定是巴塞罗那的eNodeB。通过MME去查寻国内运营商的HSS，看你是否合法，是否还有钱。如果允许上网，你的手机和巴塞罗那的SGW会建立一个隧道，然后巴塞罗那的SGW和国内运营商的PGW建立一个隧道，然后通过国内运营商的PGW上网。\n\n![img](d0ef569269e3f45afbdc276fdec83f2f-20230115031643782.jpg)\n\n这样判断你是否能上网的在国内运营商的HSS，控制你上网策略的是国内运营商的PCRF，给手机分配的IP地址也是国内运营商的PGW负责的，给手机分配的IP地址也是国内运营商里统计的。运营商由于是在PGW里面统计的，这样你的上网流量全部通过国内运营商即可，只不过巴塞罗那运营商也要和国内运营商进行流量结算。\n\n由于你的上网策略是由国内运营商在PCRF中控制的，因而你还是上不了脸书。\n\n## 小结\n\n好了，这一节就到这里了，我们来总结一下：\n\n- 移动网络的发展历程从2G到3G，再到4G，逐渐从打电话的功能为主，向上网的功能为主转变；\n- 请记住4G网络的结构，有eNodeB、MME、SGW、PGW等，分控制面协议和数据面协议，你可以对照着结构，试着说出手机上网的流程；\n- 即便你在国外的运营商下上网，也是要通过国内运营商控制的，因而也上不了脸书。\n\n最后，给你留两个思考题：\n\n1. 咱们上网都有套餐，有交钱多的，有交钱少的，你知道移动网络是如何控制不同优先级的用户的上网流量的吗？\n2. 前面讲过的所有的网络都是基于物理机的，随着云计算兴起，无论是电商，还是移动网络都要部署在云中了，你知道云中网络的设计有哪些要点吗？\n\n# 24 讲云中网络：自己拿地成本高，购买公寓更灵活\n\n前面我们讲了，数据中心里面堆着一大片一大片的机器，用网络连接起来，机器数目一旦非常多，人们就发现，维护这么一大片机器还挺麻烦的，有好多不灵活的地方。\n\n- 采购不灵活：如果客户需要一台电脑，那就需要自己采购、上架、插网线、安装操作系统，周期非常长。一旦采购了，一用就N年，不能退货，哪怕业务不做了，机器还在数据中心里留着。\n- 运维不灵活：一旦需要扩容CPU、内存、硬盘，都需要去机房手动弄，非常麻烦。\n- 规格不灵活：采购的机器往往动不动几百G的内存，而每个应用往往可能只需要4核8G，所以很多应用混合部署在上面，端口各种冲突，容易相互影响。\n- 复用不灵活：一台机器，一旦一个用户不用了，给另外一个用户，那就需要重装操作系统。因为原来的操作系统可能遗留很多数据，非常麻烦。\n\n## 从物理机到虚拟机\n\n为了解决这些问题，人们发明了一种叫虚拟机的东西，并基于它产生了云计算技术。\n\n其实在你的个人电脑上，就可以使用虚拟机。如果你对虚拟机没有什么概念，你可以下载一个桌面虚拟化的软件，自己动手尝试一下。它可以让你灵活地指定CPU的数目、内存的大小、硬盘的大小，可以有多个网卡，然后在一台笔记本电脑里面创建一台或者多台虚拟电脑。不用的时候，一点删除就没有了。\n\n在数据中心里面，也有一种类似的开源技术qemu-kvm，能让你在一台巨大的物理机里面，掏出一台台小的机器。这套软件就能解决上面的问题：一点就能创建，一点就能销毁。你想要多大就有多大，每次创建的系统还都是新的。\n\n**我们常把物理机比喻为自己拿地盖房子，而虚拟机则相当于购买公寓，更加灵活方面，随时可买可卖。** 那这个软件为什么能做到这些事儿呢？\n\n它用的是**软件模拟硬件**的方式。刚才说了，数据中心里面用的qemu-kvm。从名字上来讲，emu就是Emulator（模拟器）的意思，主要会模拟CPU、内存、网络、硬盘，使得虚拟机感觉自己在使用独立的设备，但是真正使用的时候，当然还是使用物理的设备。\n\n例如，多个虚拟机轮流使用物理CPU，内存也是使用虚拟内存映射的方式，最终映射到物理内存上。硬盘在一块大的文件系统上创建一个N个G的文件，作为虚拟机的硬盘。\n\n简单比喻，虚拟化软件就像一个“骗子”，向上“骗”虚拟机里面的应用，让它们感觉独享资源，其实自己啥都没有，全部向下从物理机里面弄。\n\n## 虚拟网卡的原理\n\n那网络是如何“骗”应用的呢？如何将虚拟机的网络和物理机的网络连接起来？\n\n![img](9a257afaa9c8a158a5a99e2df00dcf7e-20230115031643800.jpg)\n\n首先，虚拟机要有一张网卡。对于qemu-kvm来说，这是通过Linux上的一种TUN/TAP技术来实现的。\n\n虚拟机是物理机上跑着的一个软件。这个软件可以像其他应用打开文件一样，打开一个称为TUN/TAP的Char Dev（字符设备文件）。打开了这个字符设备文件之后，在物理机上就能看到一张虚拟TAP网卡。\n\n虚拟化软件作为“骗子”，会将打开的这个文件，在虚拟机里面虚拟出一张网卡，让虚拟机里面的应用觉得它们真有一张网卡。于是，所有的网络包都往这里发。\n\n当然，网络包会到虚拟化软件这里。它会将网络包转换成为文件流，写入字符设备，就像写一个文件一样。内核中TUN/TAP字符设备驱动会收到这个写入的文件流，交给TUN/TAP的虚拟网卡驱动。这个驱动将文件流再次转成网络包，交给TCP/IP协议栈，最终从虚拟TAP网卡发出来，成为标准的网络包。\n\n就这样，几经转手，数据终于从虚拟机里面，发到了虚拟机外面。\n\n## 虚拟网卡连接到云中\n\n我们就这样有了虚拟TAP网卡。接下来就要看，这个卡怎么接入庞大的数据中心网络中。\n\n在接入之前，我们先来看，云计算中的网络都需要注意哪些点。\n\n- **共享**：尽管每个虚拟机都会有一个或者多个虚拟网卡，但是物理机上可能只有有限的网卡。那这么多虚拟网卡如何共享同一个出口？\n- **隔离**：分两个方面，一个是安全隔离，两个虚拟机可能属于两个用户，那怎么保证一个用户的数据不被另一个用户窃听？一个是流量隔离，两个虚拟机，如果有一个疯狂下片，会不会导致另外一个上不了网？\n- **互通**：分两个方面，一个是如果同一台机器上的两个虚拟机，属于同一个用户的话，这两个如何相互通信？另一个是如果不同物理机上的两个虚拟机，属于同一个用户的话，这两个如何相互通信？\n- **灵活**：虚拟机和物理不同，会经常创建、删除，从一个机器漂移到另一台机器，有的互通、有的不通等等，灵活性比物理网络要好得多，需要能够灵活配置。\n\n### 共享与互通问题\n\n这些问题，我们一个个来解决。\n\n首先，一台物理机上有多个虚拟机，有多个虚拟网卡，这些虚拟网卡如何连在一起，进行相互访问，并且可以访问外网呢？\n\n还记得我们在大学宿舍里做的事情吗？你可以想象你的物理机就是你们宿舍，虚拟机就是你的个人电脑，这些电脑应该怎么连接起来呢？当然应该买一个交换机。\n\n在物理机上，应该有一个虚拟的交换机，在Linux上有一个命令叫作brctl，可以创建虚拟的网桥brctl addbr br0。创建出来以后，将两个虚拟机的虚拟网卡，都连接到虚拟网桥brctl addif br0 tap0上，这样将两个虚拟机配置相同的子网网段，两台虚拟机就能够相互通信了。\n\n![img](6976194513a8ec9066c1dd26e7b07ca1-20230115031643813.jpg)\n\n那这些虚拟机如何连外网呢？在桌面虚拟化软件上面，我们能看到以下选项。\n\n![img](ee3424547c32433e04fb174fdbaa9924-20230115031643830.jpg)\n\n这里面，host-only的网络对应的，其实就是上面两个虚拟机连到一个br0虚拟网桥上，而且不考虑访问外部的场景，只要虚拟机之间能够相互访问就可以了。\n\n如果要访问外部，往往有两种方式。\n\n一种方式称为**桥接**。如果在桌面虚拟化软件上选择桥接网络，则在你的笔记本电脑上，就会形成下面的结构。\n\n![img](9347b67409e26924ea9358f052f2e9f4-20230115031643844.jpg)\n\n每个虚拟机都会有虚拟网卡，在你的笔记本电脑上，会发现多了几个网卡，其实是虚拟交换机。这个虚拟交换机将虚拟机连接在一起。在桥接模式下，物理网卡也连接到这个虚拟交换机上，物理网卡在桌面虚拟化软件上，在“界面名称”那里选定。\n\n如果使用桥接网络，当你登录虚拟机里看IP地址的时候会发现，你的虚拟机的地址和你的笔记本电脑的，以及你旁边的同事的电脑的网段是一个网段。这是为什么呢？这其实相当于将物理机和虚拟机放在同一个网桥上，相当于这个网桥上有三台机器，是一个网段的，全部打平了。我将图画成下面的样子你就好理解了。\n\n![img](e55e9a85106d9086e51cd649a182d707-20230115031643921.jpg)\n\n在数据中心里面，采取的也是类似的技术，只不过都是Linux，在每台机器上都创建网桥br0，虚拟机的网卡都连到br0上，物理网卡也连到br0上，所有的br0都通过物理网卡出来连接到物理交换机上。\n\n![img](a859bb918ab4ca53c3542a6b18884ec0-20230115031644232.jpg)\n\n同样我们换一个角度看待这个拓扑图。同样是将网络打平，虚拟机会和你的物理网络具有相同的网段。\n\n![img](a145ea19a844b3e38834efbf0f4cedb7-20230115031643889.jpg)\n\n在这种方式下，不但解决了同一台机器的互通问题，也解决了跨物理机的互通问题，因为都在一个二层网络里面，彼此用相同的网段访问就可以了。但是当规模很大的时候，会存在问题。\n\n你还记得吗？在一个二层网络里面，最大的问题是广播。一个数据中心的物理机已经很多了，广播已经非常严重，需要通过VLAN进行划分。如果使用了虚拟机，假设一台物理机里面创建10台虚拟机，全部在一个二层网络里面，那广播就会很严重，所以除非是你的桌面虚拟机或者数据中心规模非常小，才可以使用这种相对简单的方式。\n\n另外一种方式称为**NAT**。如果在桌面虚拟化软件中使用NAT模式，在你的笔记本电脑上会出现如下的网络结构。\n\n![img](2dd447992fbf4901f92a3dfdf8086bc4-20230115031643903.jpg)\n\n在这种方式下，你登录到虚拟机里面查看IP地址，会发现虚拟机的网络是虚拟机的，物理机的网络是物理机的，两个不相同。虚拟机要想访问物理机的时候，需要将地址NAT成为物理机的地址。\n\n除此之外，它还会在你的笔记本电脑里内置一个DHCP服务器，为笔记本电脑上的虚拟机动态分配IP地址。因为虚拟机的网络自成体系，需要进行IP管理。为什么桥接方式不需要呢？因为桥接将网络打平了，虚拟机的IP地址应该由物理网络的DHCP服务器分配。\n\n在数据中心里面，也是使用类似的方式。这种方式更像是真的将你宿舍里面的情况，搬到一台物理机上来。\n\n![img](72b49cdeeb8846025f310a1f59b6b88b-20230115031643917.jpg)\n\n虚拟机是你的电脑，路由器和DHCP Server相当于家用路由器或者寝室长的电脑，物理网卡相当于你们宿舍的外网网口，用于访问互联网。所有电脑都通过内网网口连接到一个网桥br0上，虚拟机要想访问互联网，需要通过br0连到路由器上，然后通过路由器将请求NAT成为物理网络的地址，转发到物理网络。\n\n如果是你自己登录到物理机上做个简单配置，你可以简化一下。例如将虚拟机所在网络的网关的地址直接配置到br0上，不用DHCP Server，手动配置每台虚拟机的IP地址，通过命令iptables -t nat -A POSTROUTING -o ethX -j MASQUERADE，直接在物理网卡ethX上进行NAT，所有从这个网卡出去的包都NAT成这个网卡的地址。通过设置net.ipv4.ip_forward = 1，开启物理机的转发功能，直接做路由器，而不用单独的路由器，这样虚拟机就能直接上网了。\n\n![img](499e17daa9495ccf9f9497a165cc50f3-20230115031643960.jpg)\n\n### 隔离问题\n\n解决了互通的问题，接下来就是隔离的问题。\n\n如果一台机器上的两个虚拟机不属于同一个用户，怎么办呢？好在brctl创建的网桥也是支持VLAN功能的，可以设置两个虚拟机的tag，这样在这个虚拟网桥上，两个虚拟机是不互通的。\n\n但是如何跨物理机互通，并且实现VLAN的隔离呢？由于brctl创建的网桥上面的tag是没办法在网桥之外的范围内起作用的，于是我们需要寻找其他的方式。\n\n有一个命令**vconfig**，可以基于物理网卡eth0创建带VLAN的虚拟网卡，所有从这个虚拟网卡出去的包，都带这个VLAN，如果这样，跨物理机的互通和隔离就可以通过这个网卡来实现。\n\n![img](dfc95f72325ab13c2f9551cfccc073e0-20230115031643976.jpg)\n\n首先为每个用户分配不同的VLAN，例如有一个用户VLAN 10，一个用户VLAN 20。在一台物理机上，基于物理网卡，为每个用户用vconfig创建一个带VLAN的网卡。不同的用户使用不同的虚拟网桥，带VLAN的虚拟网卡也连接到虚拟网桥上。\n\n这样是否能保证两个用户的隔离性呢？不同的用户由于网桥不通，不能相互通信，一旦出了网桥，由于VLAN不同，也不会将包转发到另一个网桥上。另外，出了物理机，也是带着VLAN ID的。只要物理交换机也是支持VLAN的，到达另一台物理机的时候，VLAN ID依然在，它只会将包转发给相同VLAN的网卡和网桥，所以跨物理机，不同的VLAN也不会相互通信。\n\n使用brctl创建出来的网桥功能是简单的，基于VLAN的虚拟网卡也能实现简单的隔离。但是这都不是大规模云平台能够满足的，一个是VLAN的隔离，数目太少。前面我们学过，VLAN ID只有4096个，明显不够用。另外一点是这个配置不够灵活。谁和谁通，谁和谁不通，流量的隔离也没有实现，还有大量改进的空间。\n\n## 小结\n\n好了，这一节就到这里了，我们来总结一下：\n\n- 云计算的关键技术是虚拟化，这里我们重点关注的是，虚拟网卡通过打开TUN/TAP字符设备的方式，将虚拟机内外连接起来；\n- 云中的网络重点关注四个方面，共享、隔离、互通、灵活。其中共享和互通有两种常用的方式，分别是桥接和NAT，隔离可以通过VLAN的方式。\n\n接下来，给你留两个思考题。\n\n1. 为了直观，这一节的内容我们以桌面虚拟化系统举例。在数据中心里面，有一款著名的开源软件OpenStack，这一节讲的网络连通方式对应OpenStack中的哪些模型呢？\n2. 这一节的最后，我们也提到了，本节提到的网络配置方式比较不灵活，你知道什么更加灵活的方式吗？\n\n# 25 讲软件定义网络：共享基础设施的小区物业管理办法\n\n上一节我们说到，使用原生的VLAN和Linux网桥的方式来进行云平台的管理，但是这样在灵活性、隔离性方面都显得不足，而且整个网络缺少统一的视图、统一的管理。\n\n可以这样比喻，云计算就像大家一起住公寓，要共享小区里面的基础设施，其中网络就相当于小区里面的电梯、楼道、路、大门等，大家都走，往往会常出现问题，尤其在上班高峰期，出门的人太多，对小区的物业管理就带来了挑战。\n\n物业可以派自己的物业管理人员，到每个单元的楼梯那里，将电梯的上下行速度调快一点，可以派人将隔离健身区、景色区的栅栏门暂时打开，让大家可以横穿小区，直接上地铁，还可以派人将多个小区出入口，改成出口多、入口少等等。等过了十点半，上班高峰过去，再派人都改回来。\n\n## 软件定义网络（SDN）\n\n这种模式就像传统的网络设备和普通的Linux网桥的模式，配置整个云平台的网络通路，你需要登录到这台机器上配置这个，再登录到另外一个设备配置那个，才能成功。\n\n如果物业管理人员有一套智能的控制系统，在物业监控室里就能看到小区里每个单元、每个电梯的人流情况，然后在监控室里面，只要通过远程控制的方式，拨弄一个手柄，电梯的速度就调整了，栅栏门就打开了，某个入口就改出口了。\n\n这就是软件定义网络（SDN）。它主要有以下三个特点。\n\n![img](346fe3b3dbe1024e7119ec4ffa9377f9-20230115031644387.jpg)\n\n- **控制与转发分离**：转发平面就是一个个虚拟或者物理的网络设备，就像小区里面的一条条路。控制平面就是统一的控制中心，就像小区物业的监控室。它们原来是一起的，物业管理员要从监控室出来，到路上去管理设备，现在是分离的，路就是走人的，控制都在监控室。\n- **控制平面与转发平面之间的开放接口**：控制器向上提供接口，被应用层调用，就像总控室提供按钮，让物业管理员使用。控制器向下调用接口，来控制网络设备，就像总控室会远程控制电梯的速度。这里经常使用两个名词，前面这个接口称为**北向接口**，后面这个接口称为**南向接口**，上北下南嘛。\n- **逻辑上的集中控制**：逻辑上集中的控制平面可以控制多个转发面设备，也就是控制整个物理网络，因而可以获得全局的网络状态视图，并根据该全局网络状态视图实现对网络的优化控制，就像物业管理员在监控室能够看到整个小区的情况，并根据情况优化出入方案。\n\n## OpenFlow和OpenvSwitch\n\nSDN有很多种实现方式，我们来看一种开源的实现方式。\n\nOpenFlow是SDN控制器和网络设备之间互通的南向接口协议，OpenvSwitch用于创建软件的虚拟交换机。OpenvSwitch是支持OpenFlow协议的，当然也有一些硬件交换机也支持OpenFlow协议。它们都可以被统一的SDN控制器管理，从而实现物理机和虚拟机的网络连通。\n\n![img](383710afd8e2b7e99ba8ccfef69c02d0-20230115031643983.jpg)\n\nSDN控制器是如何通过OpenFlow协议控制网络的呢？\n\n![img](ed8939e33728c6118db3c0283b1dbdf7-20230115031644028.jpg)\n\n在OpenvSwitch里面，有一个流表规则，任何通过这个交换机的包，都会经过这些规则进行处理，从而接收、转发、放弃。\n\n那流表长啥样呢？其实就是一个个表格，每个表格好多行，每行都是一条规则。每条规则都有优先级，先看高优先级的规则，再看低优先级的规则。\n\n![img](91a7f011885ec48ca57d61940b748477-20230115031644039.jpg)\n\n对于每一条规则，要看是否满足匹配条件。这些条件包括，从哪个端口进来的，网络包头里面有什么等等。满足了条件的网络包，就要执行一个动作，对这个网络包进行处理。可以修改包头里的内容，可以跳到任何一个表格，可以转发到某个网口出去，也可以丢弃。\n\n通过这些表格，可以对收到的网络包随意处理。\n\n![img](342ac21bda24e20d8d78f47ca8415a22-20230115031644074.jpg)\n\n具体都能做什么处理呢？通过上面的表格可以看出，简直是想怎么处理怎么处理，可以覆盖TCP/IP协议栈的四层。\n\n对于物理层：\n\n- 匹配规则包括由从哪个口进来；\n- 执行动作包括从哪个口出去。\n\n对于MAC层：\n\n- 匹配规则包括：源MAC地址是多少？（dl_src），目标MAC是多少？（dl_dst），所属vlan是多少？（dl_vlan）；\n- 执行动作包括：修改源MAC（mod_dl_src），修改目标MAC（mod_dl_dst），修改VLAN（mod_vlan_vid），删除VLAN（strip_vlan），MAC地址学习（learn）。\n\n对于网络层：\n\n- 匹配规则包括：源IP地址是多少？(nw_src)，目标IP是多少？（nw_dst）。\n- 执行动作包括：修改源IP地址（mod_nw_src），修改目标IP地址（mod_nw_dst）。\n\n对于传输层：\n\n- 匹配规则包括：源端口是多少？（tp_src），目标端口是多少？（tp_dst）。\n- 执行动作包括：修改源端口（mod_tp_src），修改目标端口（mod_tp_dst）。\n\n总而言之，对于OpenvSwitch来讲，网络包到了我手里，就是一个Buffer，我想怎么改怎么改，想发到哪个端口就发送到哪个端口。\n\nOpenvSwitch有本地的命令行可以进行配置，能够实验咱们前面讲过的一些功能。我们可以通过OpenvSwitch的命令创建一个虚拟交换机。然后可以将多个虚拟端口port添加到这个虚拟交换机上。\n\n```csharp\novs-vsctl add-br ubuntu_br\n```\n\n## 实验一：用OpenvSwitch实现VLAN的功能\n\n下面我们实验一下通过OpenvSwitch实现VLAN的功能，在OpenvSwitch中端口port分两种。\n\n第一类是access port：\n\n- 这个端口配置tag，从这个端口进来的包会被打上这个tag；\n- 如果网络包本身带有的VLAN ID等于tag，则会从这个port发出；\n- 从access port发出的包不带VLAN ID。\n\n第二类是trunk port：\n\n- 这个port不配置tag，配置trunks；\n- 如果trunks为空，则所有的VLAN都trunk，也就意味着对于所有VLAN的包，本身带什么VLAN ID，就是携带者什么VLAN ID，如果没有设置VLAN，就属于VLAN 0，全部允许通过；\n- 如果trunks不为空，则仅仅带着这些VLAN ID的包通过。\n\n我们通过以下命令创建如下的环境：\n\n```csharp\novs-vsctl add-port ubuntu_br first_br\novs-vsctl add-port ubuntu_br second_br\novs-vsctl add-port ubuntu_br third_br\novs-vsctl set Port vnet0 tag=101\novs-vsctl set Port vnet1 tag=102\novs-vsctl set Port vnet2 tag=103\novs-vsctl set Port first_br tag=103\novs-vsctl clear Port second_br tag\novs-vsctl set Port third_br trunks=101,102\n```\n\n另外要配置禁止MAC地址学习。\n\n```bash\novs-vsctl set bridge ubuntu_br flood-vlans=101,102,103\n```\n\n![img](6e1ddc1eb92c85cda32f40b62dd9fcb4-20230115031644051.jpg)\n\n创建好了环境以后，我们来做这个实验。\n\n1. 从192.168.100.102来ping 192.168.100.103，然后用tcpdump进行抓包。first_if收到包了，从first_br出来的包头是没有VLAN ID的。second_if也收到包了，由于second_br是trunk port，因而出来的包头是有VLAN ID的，third_if收不到包。\n2. 从192.168.100.100来ping 192.168.100.105, 则second_if和third_if可以收到包，当然ping不通，因为third_if不属于某个VLAN。first_if是收不到包的。second_if能够收到包，而且包头里面是VLAN ID=101。third_if也能收到包，而且包头里面是VLAN ID=101。\n3. 从192.168.100.101来ping 192.168.100.104， 则second_if和third_if可以收到包。first_if是收不到包的。second_br能够收到包，而且包头里面是VLAN ID=102。third_if也能收到包，而且包头里面是VLAN ID=102。\n\n通过这个例子，我们可以看到，通过OpenvSwitch，不用买一个支持VLAN的交换机，你也能学习VLAN的工作模式了。\n\n## 实验二：用OpenvSwitch模拟网卡绑定，连接交换机\n\n接下来，我们来做另一个实验。在前面，我们还说过，为了高可用，可以使用网卡绑定，连接到交换机，OpenvSwitch也可以模拟这一点。\n\n在OpenvSwitch里面，有个bond_mode，可以设置为以下三个值：\n\n- active-backup：一个连接是active，其他的是backup，当active失效的时候，backup顶上；\n- balance-slb：流量安装源MAC和output VLAN进行负载均衡；\n- balance-tcp：必须在支持LACP协议的情况下才可以，可根据L2, L3, L4进行负载均衡。\n\n我们搭建一个测试环境。\n\n![img](8a1956cb5bbf03de7d6cbaa2e706046c-20230115031644113.jpg)\n\n我们使用下面的命令，建立bond连接。\n\n```csharp\novs-vsctl add-bond br0 bond0 first_br second_br\novs-vsctl add-bond br1 bond1 first_if second_if\novs-vsctl set Port bond0 lacp=active\novs-vsctl set Port bond1 lacp=active\n```\n\n默认情况下bond_mode是active-backup模式，一开始active的是first_br和first_if。\n\n这个时候我们从192.168.100.100 ping 192.168.100.102，以及从192.168.100.101 ping 192.168.100.103的时候，tcpdump可以看到所有的包都是从first_if通过。\n\n如果把first_if设成down，则包的走向会变，发现second_if开始有流量，对于192.168.100.100和192.168.100.101似乎没有收到影响。\n\n如果我们通过以下命令，把bond_mode设为balance-slb。然后我们同时在192.168.100.100 ping 192.168.100.102，在192.168.100.101 ping 192.168.100.103，我们通过tcpdump发现包已经被分流了。\n\n```bash\novs-vsctl set Port bond0 bond_mode=balance-slb\novs-vsctl set Port bond1 bond_mode=balance-slb\n```\n\n通过这个例子，我们可以看到，通过OpenvSwitch，你不用买两台支持bond的交换机，也能看到bond的效果。\n\n那OpenvSwitch是怎么做到这些的呢？我们来看OpenvSwitch的架构图。\n\n![img](d870e5bfcad8ec45d146c3226cdccb14-20230115031644426.jpg)\n\nOpenvSwitch包含很多的模块，在用户态有两个重要的进程，也有两个重要的命令行工具。\n\n- 第一个进程是OVSDB进程。ovs-vsctl命令行会和这个进程通信，去创建虚拟交换机，创建端口，将端口添加到虚拟交换机上，OVSDB会将这些拓扑信息保存在一个本地的文件中。\n- 第一个进程是vswitchd进程。ovs-ofctl命令行会和这个进程通信，去下发流表规则，规则里面会规定如何对网络包进行处理，vswitchd会将流表放在用户态Flow Table中。\n\n在内核态，OpenvSwitch有内核模块OpenvSwitch.ko，对应图中的Datapath部分。在网卡上注册一个函数，每当有网络包到达网卡的时候，这个函数就会被调用。\n\n在内核的这个函数里面，会拿到网络包，将各个层次的重要信息拿出来，例如：\n\n- 在物理层，in_port即包进入的网口的ID；\n- 在MAC层，源和目的MAC地址；\n- 在IP层，源和目的IP地址；\n- 在传输层，源和目的端口号。\n\n在内核中，有一个内核态Flow Table。接下来内核模块在这个内核流表中匹配规则，如果匹配上了，则执行操作、修改包，或者转发或者放弃。如果内核没有匹配上，则需要进入用户态，用户态和内核态之间通过Linux的一个机制Netlink相互通信。\n\n内核通过upcall，告知用户态进程vswitchd在用户态Flow Table里面去匹配规则，这里面的规则是全量的流表规则，而内核Flow Table里面的只是为了快速处理，保留了部分规则，内核里面的规则过一阵就会过期。\n\n当在用户态匹配到了流表规则之后，就在用户态执行操作，同时将这个匹配成功的流表通过reinject下发到内核，从而接下来的包都能在内核找到这个规则。\n\n这里调用openflow协议的，是本地的命令行工具，也可以是远程的SDN控制器，一个重要的SDN控制器是OpenDaylight。\n\n下面这个图就是OpenDaylight中看到的拓扑图。是不是有种物业管理员在监控室里的感觉？\n\n![img](274442ba251fdc63c88bc5dbfc6183a8-20230115031644129.jpg)\n\n我们可以通过在OpenDaylight里，将两个交换机之间配置通，也可以配置不通，还可以配置一个虚拟IP地址VIP，在不同的机器之间实现负载均衡等等，所有的策略都可以灵活配置。\n\n## 如何在云计算中使用OpenvSwitch？\n\nOpenvSwitch这么牛，如何用在云计算中呢？\n\n![img](24b09861632f7ba7211073e2829d4f59-20230115031644173.jpg)\n\n我们还是讨论VLAN的场景。\n\n在没有OpenvSwitch的时候，如果一个新的用户要使用一个新的VLAN，还需要创建一个属于新的VLAN的虚拟网卡，并且为这个租户创建一个单独的虚拟网桥，这样用户越来越多的时候，虚拟网卡和虚拟网桥会越来越多，管理非常复杂。\n\n另一个问题是虚拟机的VLAN和物理环境的VLAN是透传的，也即从一开始规划的时候，就需要匹配起来，将物理环境和虚拟环境强绑定，本来就不灵活。\n\n而引入了OpenvSwitch，状态就得到了改观。\n\n首先，由于OpenvSwitch本身就是支持VLAN的，所有的虚拟机都可以放在一个网桥br0上，通过不同的用户配置不同的tag，就能够实现隔离。例如上面的图，用户A的虚拟机都在br0上，用户B的虚拟机都在br1上，有了OpenvSwitch，就可以都放在br0上，只是设置了不同的tag。\n\n另外，还可以创建一个虚拟交换机br1，将物理网络和虚拟网络进行隔离。物理网络有物理网络的VLAN规划，虚拟机在一台物理机上，所有的VLAN都是从1开始的。由于一台机器上的虚拟机不会超过4096个，所以VLAN在一台物理机上如果从1开始，肯定够用了。\n\n例如在图中，上面的物理机里面，用户A被分配的tag是1，用户B被分配的tag是2，而在下面的物理机里面，用户A被分配的tag是7，用户B被分配的tag是6。\n\n如果物理机之间的通信和隔离还是通过VLAN的话，需要将虚拟机的VLAN和物理环境的VLAN对应起来，但为了灵活性，不一定一致，这样可以实现分别管理物理机的网络和虚拟机的网络。好在OpenvSwitch可以对包的内容进行修改。例如通过匹配dl_vlan，然后执行mod_vlan_vid来改进进出出物理机的网络包。\n\n尽管租户多了，物理环境的VLAN还是不够用，但是有了OpenvSwitch的映射，将物理和虚拟解耦，从而可以让物理环境使用其他技术，而不影响虚拟机环境，这个我们后面再讲。\n\n## 小结\n\n好了，这一节就到这里了，我们来总结一下：\n\n- 用SDN控制整个云里面的网络，就像小区保安从总控室管理整个物业是一样的，将控制面和数据面进行了分离；\n- 一种开源的虚拟交换机的实现OpenvSwitch，它能对经过自己的包做任意修改，从而使得云对网络的控制十分灵活；\n- 将OpenvSwitch引入了云之后，可以使得配置简单而灵活，并且可以解耦物理网络和虚拟网络。\n\n最后，给你留两个思考题：\n\n1. 在这一节中，提到了通过VIP可以通过流表在不同的机器之间实现复杂均衡，你知道怎样才能做到吗？\n2. 虽然OpenvSwitch可以解耦物理网络和虚拟网络，但是在物理网络里面使用VLAN，数目还是不够，你知道该怎么办吗？\n\n# 26 讲云中的网络安全：虽然不是土豪，也需要基本安全和保障\n\n上一节我们看到，做一个小区物业维护一个大家共享的环境，还是挺不容易的。如果都是自觉遵守规则的住户那还好，如果遇上不自觉的住户就会很麻烦。\n\n就像公有云的环境，其实没有你想的那么纯净，各怀鬼胎的黑客到处都是。扫描你的端口呀，探测一下你启动的什么应用啊，看一看是否有各种漏洞啊。这就像小偷潜入小区后，这儿看看，那儿瞧瞧，窗户有没有关严了啊，窗帘有没有拉上啊，主人睡了没，是不是时机潜入室内啊，等等。\n\n假如你创建了一台虚拟机，里面明明跑了一个电商应用，这是你非常重要的一个应用，你会把它进行安全加固。这台虚拟机的操作系统里，不小心安装了另外一个后台应用，监听着一个端口，而你的警觉性没有这么高。\n\n虚拟机的这个端口是对着公网开放的，碰巧这个后台应用本身是有漏洞的，黑客就可以扫描到这个端口，然后通过这个后台应用的端口侵入你的机器，将你加固好的电商网站黑掉。这就像你买了一个五星级的防盗门，卡车都撞不开，但是厕所窗户的门把手是坏的，小偷从厕所里面就进来了。\n\n所以**对于公有云上的虚拟机，我的建议是仅仅开放需要的端口，而将其他的端口一概关闭。这个时候，你只要通过安全措施守护好这个唯一的入口就可以了**。采用的方式常常是用**ACL**（Access Control List，访问控制列表）来控制IP和端口。\n\n设置好了这些规则，只有指定的IP段能够访问指定的开放接口，就算有个有漏洞的后台进程在那里，也会被屏蔽，黑客进不来。在云平台上，这些规则的集合常称为**安全组**。那安全组怎么实现呢？\n\n我们来复习一下，当一个网络包进入一台机器的时候，都会做什么事情。\n\n首先拿下MAC头看看，是不是我的。如果是，则拿下IP头来。得到目标IP之后呢，就开始进行路由判断。在路由判断之前，这个节点我们称为**PREROUTING**。如果发现IP是我的，包就应该是我的，就发给上面的传输层，这个节点叫作**INPUT**。如果发现IP不是我的，就需要转发出去，这个节点称为**FORWARD**。如果是我的，上层处理完毕完毕后，一般会返回一个处理结果，这个处理结果会发出去，这个节点称为**OUTPUT**，无论是FORWARD还是OUTPUT，都是路由判断之后发生的，最后一个节点是**POSTROUTING**。\n\n整个过程如图所示。\n\n![img](a924ccda5d54bcad6f67fdebe0a6c1fc-20230115031644201.jpg)\n\n整个包的处理过程还是原来的过程，只不过为什么要格外关注这**五个节点**呢？\n\n是因为在Linux内核中，有一个框架叫Netfilter。它可以在这些节点插入hook函数。这些函数可以截获数据包，对数据包进行干预。例如做一定的修改，然后决策是否接着交给TCP/IP协议栈处理；或者可以交回给协议栈，那就是**ACCEPT**；或者过滤掉，不再传输，就是**DROP**；还有就是**QUEUE**，发送给某个用户态进程处理。\n\n这个比较难理解，经常用在内部负载均衡，就是过来的数据一会儿传给目标地址1，一会儿传给目标地址2，而且目标地址的个数和权重都可能变。协议栈往往处理不了这么复杂的逻辑，需要写一个函数接管这个数据，实现自己的逻辑。\n\n有了这个Netfilter框架就太好了，你可以在IP转发的过程中，随时干预这个过程，只要你能实现这些hook函数。\n\n一个著名的实现，就是**内核模块ip_tables**。它在这五个节点上埋下函数，从而可以根据规则进行包的处理。按功能可分为四大类：连接跟踪（conntrack）、数据包的过滤（filter）、网络地址转换（nat）和数据包的修改（mangle）。其中连接跟踪是基础功能，被其他功能所依赖。其他三个可以实现包的过滤、修改和网络地址转换。\n\n在用户态，还有一个你肯定知道的客户端程序iptables，用命令行来干预内核的规则。内核的功能对应iptables的命令行来讲，就是**表和链**的概念。\n\n![img](897618c5f5b65e8c35ef5f20a731cdfd-20230115031644528.jpg)\n\niptables的表分为四种：raw–\u003emangle–\u003enat–\u003efilter。这四个优先级依次降低，raw不常用，所以主要功能都在其他三种表里实现。每个表可以设置多个链。\n\nfilter表处理过滤功能，主要包含三个链：\n\n- INPUT链：过滤所有目标地址是本机的数据包；\n- FORWARD链：过滤所有路过本机的数据包；\n- OUTPUT链：过滤所有由本机产生的数据包。\n\nnat表主要是处理网络地址转换，可以进行Snat（改变数据包的源地址）、Dnat（改变数据包的目标地址），包含三个链：\n\n- PREROUTING链：可以在数据包到达防火墙时改变目标地址；\n- OUTPUT链：可以改变本地产生的数据包的目标地址；\n- POSTROUTING链：在数据包离开防火墙时改变数据包的源地址。\n\nmangle表主要是修改数据包，包含：\n\n- PREROUTING链；\n- INPUT链；\n- FORWARD链；\n- OUTPUT链；\n- POSTROUTING链。\n\n将iptables的表和链加入到上面的过程图中，就形成了下面的图和过程。\n\n![img](1a0ba797b9a0f0e32c9e561b97955917-20230115031644264.jpg)\n\n1. 数据包进入的时候，先进mangle表的PREROUTING链。在这里可以根据需要，改变数据包头内容之后，进入nat表的PREROUTING链，在这里可以根据需要做Dnat，也就是目标地址转换。\n2. 进入路由判断，要判断是进入本地的还是转发的。\n3. 如果是进入本地的，就进入INPUT链，之后按条件过滤限制进入。\n4. 之后进入本机，再进入OUTPUT链，按条件过滤限制出去，离开本地。\n5. 如果是转发就进入FORWARD链，根据条件过滤限制转发。\n6. 之后进入POSTROUTING链，这里可以做Snat，离开网络接口。\n\n有了iptables命令，我们就可以在云中实现一定的安全策略。例如我们可以处理前面的偷窥事件。首先我们将所有的门都关闭。\n\n```css\niptables -t filter -A INPUT -s 0.0.0.0/0.0.0.0 -d X.X.X.X -j DROP\n```\n\n-s表示源IP地址段，-d表示目标地址段，DROP表示丢弃，也即无论从哪里来的，要想访问我这台机器，全部拒绝，谁也黑不进来。\n\n但是你发现坏了，ssh也进不来了，都不能远程运维了，可以打开一下。\n\n```css\niptables -I INPUT -s 0.0.0.0/0.0.0.0 -d X.X.X.X -p tcp --dport 22 -j ACCEPT\n```\n\n如果这台机器是提供的是web服务，80端口也应该打开，当然一旦打开，这个80端口就需要很好的防护，但是从规则角度还是要打开。\n\n```css\niptables -A INPUT -s 0.0.0.0/0.0.0.0 -d X.X.X.X -p tcp --dport 80 -j ACCEPT\n```\n\n这样就搞定了，其他的账户都封死，就一个防盗门可以进出，只要防盗门是五星级的，就比较安全了。\n\n这些规则都可以在虚拟机里，自己安装iptables自己配置。但是如果虚拟机数目非常多，都要配置，对于用户来讲就太麻烦了，能不能让云平台把这部分工作做掉呢？\n\n当然可以了。在云平台上，一般允许一个或者多个虚拟机属于某个安全组，而属于不同安全组的虚拟机之间的访问以及外网访问虚拟机，都需要通过安全组进行过滤。\n\n![img](93aa707cb55c47023ae958d6cadde8a3-20230115031644283.jpg)\n\n例如图中，我们会创建一系列的网站，都是前端在Tomcat里面，对外开放8080端口。数据库使用MySQL，开放3306端口。\n\n为了方便运维，我们创建两个安全组，将Tomcat所在的虚拟机放在安全组A里面。在安全组A里面，允许任意IP地址0.0.0.0/0访问8080端口，但是对于ssh的22端口，仅仅允许管理员网段203.0.113.0/24访问。\n\n我们将MySQL所在的虚拟机在安全组B里面。在安全组B里面，仅仅允许来自安全组A的机器访问3306端口，但是对于ssh的22端口，同样允许管理员网段203.0.113.0/24访问。\n\n这些安全组规则都可以自动下发到每个在安全组里面的虚拟机上，从而控制一大批虚拟机的安全策略。这种批量下发是怎么做到的呢？你还记得这幅图吗？\n\n![img](246db57c915d9ccf6e0d66182de0fe24-20230115031644662.jpg)\n\n两个VM都通过tap网卡连接到一个网桥上，但是网桥是二层的，两个VM之间是可以随意互通的，因而需要有一个地方统一配置这些iptables规则。\n\n可以多加一个网桥，在这个网桥上配置iptables规则，将在用户在界面上配置的规则，放到这个网桥上。然后在每台机器上跑一个Agent，将用户配置的安全组变成iptables规则，配置在这个网桥上。\n\n安全问题解决了，iptables真强大！别忙，iptables除了filter，还有nat呢，这个功能也非常重要。\n\n前面的章节我们说过，在设计云平台的时候，我们想让虚拟机之间的网络和物理网络进行隔离，但是虚拟机毕竟还是要通过物理网和外界通信的，因而需要在出物理网的时候，做一次网络地址转换，也即nat，这个就可以用iptables来做。\n\n我们学过，IP头里面包含源IP地址和目标IP地址，这两种IP地址都可以转换成其他地址。转换源IP地址的，我们称为Snat；转换目标IP地址的，我们称为Dnat。\n\n你有没有思考过这个问题，TCP的访问都是一去一回的，而你在你家里连接WIFI的IP地址是一个私网IP，192.168.1.x。当你通过你们家的路由器访问163网站之后，网站的返回结果如何能够到达你的笔记本电脑呢？肯定不能通过192.168.1.x，这是个私网IP，不具有公网上的定位能力，而且用这个网段的人很多，茫茫人海，怎么能够找到你呢？\n\n所以当你从你家里访问163网站的时候，在你路由器的出口，会做Snat的，运营商的出口也可能做Snat，将你的私网IP地址，最终转换为公网IP地址，然后163网站就可以通过这个公网IP地址返回结果，然后再nat回来，直到到达你的笔记本电脑。\n\n云平台里面的虚拟机也是这样子的，它只有私网IP地址，到达外网网口要做一次Snat，转换成为机房网IP，然后出数据中心的时候，再转换为公网IP。\n\n![img](1a5d299c2eb5480eda93a8f8e3b3ca1a-20230115031644336.jpg)\n\n这里有一个问题是，在外网网口上做Snat的时候，是全部转换成一个机房网IP呢，还是每个虚拟机都对应一个机房网IP，最终对应一个公网IP呢？前面也说过了，公网IP非常贵，虚拟机也很多，当然不能每个都有单独的机房网和公网IP了，于是这种Snat是一种特殊的Snat，MASQUERADE（地址伪装）。\n\n这种方式下，所有的虚拟机共享一个机房网和公网的IP地址，所有从外网网口出去的，都转换成为这个IP地址。那又一个问题来了，都变成一个公网IP了，当163网站返回结果的时候，给谁呢，再nat成为哪个私网的IP呢？\n\n这就是Netfilter的连接跟踪（conntrack）功能了。对于TCP协议来讲，肯定是上来先建立一个连接，可以用“源/目的IP+源/目的端口”唯一标识一条连接，这个连接会放在conntrack表里面。当时是这台机器去请求163网站的，虽然源地址已经Snat成公网IP地址了，但是conntrack表里面还是有这个连接的记录的。当163网站返回数据的时候，会找到记录，从而找到正确的私网IP地址。\n\n这是虚拟机做客户端的情况，如果虚拟机做服务器呢？也就是说，如果虚拟机里面部署的就是163网站呢？\n\n这个时候就需要给这个网站配置固定的物理网的IP地址和公网IP地址了。这时候就需要显示的配置Snat规则和Dnat规则了。\n\n当外部访问进来的时候，外网网口会通过Dnat规则将公网IP地址转换为私网IP地址，到达虚拟机，虚拟机里面是163网站，返回结果，外网网口会通过Snat规则，将私网IP地址转换为那个分配给它的固定的公网IP地址。\n\n类似的规则如下：\n\n- 源地址转换(Snat)：iptables -t nat -A -s 私网IP -j Snat --to-source 外网IP\n- 目的地址转换(Dnat)：iptables -t nat -A -PREROUTING -d 外网IP -j Dnat --to-destination 私网IP\n\n到此为止iptables解决了非法偷窥隐私的问题。\n\n## 小结\n\n好了，这一节就讲到这里了，我们来总结一下。\n\n- 云中的安全策略的常用方式是，使用iptables的规则，请记住它的五个阶段，PREROUTING、INPUT、FORWARD、OUTPUT、POSTROUTING。\n- iptables分为四种表，raw、mangle、nat、filter。其中安全策略主要在filter表中实现，而虚拟网络和物理网络地址的转换主要在nat表中实现。\n\n最后，给你留两个思考题。\n\n1. 这一节中重点讲了iptables的filter和nat功能，iptables还可以通过QUEUE实现负载均衡，你知道怎么做吗？\n2. 这一节仅仅讲述了云中偷窥的问题，如果是一个合法的用户，但是不自觉抢占网络通道，应该采取什么策略呢？\n\n# 27 讲云中的网络QoS：邻居疯狂下电影，我该怎么办？\n\n在小区里面，是不是经常有住户不自觉就霸占公共通道，如果你找他理论，他的话就像一个相声《楼道曲》说的一样：“公用公用，你用我用，大家都用，我为什么不能用？”。\n\n除此之外，你租房子的时候，有没有碰到这样的情况：本来合租共享WIFI，一个人狂下小电影，从而你网都上不去，是不是很懊恼？\n\n在云平台上，也有这种现象，好在有一种流量控制的技术，可以实现**QoS**（Quality of Service），从而保障大多数用户的服务质量。\n\n对于控制一台机器的网络的QoS，分两个方向，一个是入方向，一个是出方向。\n\n![img](e7eda457d064071bcfa92219e6aefaa6-20230115031644347.jpg)\n\n其实我们能控制的只有出方向，通过Shaping，将出的流量控制成自己想要的模样。而进入的方向是无法控制的，只能通过Policy将包丢弃。\n\n## 控制网络的QoS有哪些方式？\n\n在Linux下，可以通过TC控制网络的QoS，主要就是通过队列的方式。\n\n### 无类别排队规则\n\n第一大类称为**无类别排队规则**（Classless Queuing Disciplines）。还记得我们讲[ip addr]的时候讲过的**pfifo_fast**，这是一种不把网络包分类的技术。\n\n![img](7e3218260e75bb9f18d68641928ff33e-20230115031644412.jpg)\n\npfifo_fast分为三个先入先出的队列，称为三个Band。根据网络包里面TOS，看这个包到底应该进入哪个队列。TOS总共四位，每一位表示的意思不同，总共十六种类型。\n\n通过命令行tc qdisc show dev eth0，可以输出结果priomap，也是十六个数字。在0到2之间，和TOS的十六种类型对应起来，表示不同的TOS对应的不同的队列。其中Band 0优先级最高，发送完毕后才轮到Band 1发送，最后才是Band 2。\n\n另外一种无类别队列规则叫作**随机公平队列**（Stochastic Fair Queuing）。\n\n![img](da3a4653469877d9d98f1610ccaefd71-20230115031644424.jpg)\n\n会建立很多的FIFO的队列，TCP Session会计算hash值，通过hash值分配到某个队列。在队列的另一端，网络包会通过轮询策略从各个队列中取出发送。这样不会有一个Session占据所有的流量。\n\n当然如果两个Session的hash是一样的，会共享一个队列，也有可能互相影响。hash函数会经常改变，从而session不会总是相互影响。\n\n还有一种无类别队列规则称为**令牌桶规则**（TBF，Token Bucket Filte）。\n\n![img](c2170423769b8dfb6e6ff854287ab115-20230115031644446.jpg)\n\n所有的网络包排成队列进行发送，但不是到了队头就能发送，而是需要拿到令牌才能发送。\n\n令牌根据设定的速度生成，所以即便队列很长，也是按照一定的速度进行发送的。\n\n当没有包在队列中的时候，令牌还是以既定的速度生成，但是不是无限累积的，而是放满了桶为止。设置桶的大小为了避免下面的情况：当长时间没有网络包发送的时候，积累了大量的令牌，突然来了大量的网络包，每个都能得到令牌，造成瞬间流量大增。\n\n### 基于类别的队列规则\n\n另外一大类是**基于类别的队列规则**（Classful Queuing Disciplines），其中典型的为**分层令牌桶规则**（**HTB**， Hierarchical Token Bucket）。\n\nHTB往往是一棵树，接下来我举个具体的例子，通过TC如何构建一棵HTB树来带你理解。\n\n![img](9a1b8a7c0c5403a2b4b3c277545991b5-20230115031644484.jpg)\n\n使用TC可以为某个网卡eth0创建一个HTB的队列规则，需要付给它一个句柄为（1:）。\n\n这是整棵树的根节点，接下来会有分支。例如图中有三个分支，句柄分别为（:10）、（:11）、（:12）。最后的参数default 12，表示默认发送给1:12，也即发送给第三个分支。\n\n```csharp\ntc qdisc add dev eth0 root handle 1: htb default 12\n```\n\n对于这个网卡，需要规定发送的速度。一般有两个速度可以配置，一个是**rate**，表示一般情况下的速度；一个是**ceil**，表示最高情况下的速度。对于根节点来讲，这两个速度是一样的，于是创建一个root class，速度为（rate=100kbps，ceil=100kbps）。\n\n```kotlin\ntc class add dev eth0 parent 1: classid 1:1 htb rate 100kbps ceil 100kbps\n```\n\n接下来要创建分支，也即创建几个子class。每个子class统一有两个速度。三个分支分别为（rate=30kbps，ceil=100kbps）、（rate=10kbps，ceil=100kbps）、（rate=60kbps，ceil=100kbps）。\n\n```kotlin\ntc class add dev eth0 parent 1:1 classid 1:10 htb rate 30kbps ceil 100kbps\ntc class add dev eth0 parent 1:1 classid 1:11 htb rate 10kbps ceil 100kbps\ntc class add dev eth0 parent 1:1 classid 1:12 htb rate 60kbps ceil 100kbps\n```\n\n你会发现三个rate加起来，是整个网卡允许的最大速度。\n\nHTB有个很好的特性，同一个root class下的子类可以相互借流量，如果不直接在队列规则下面创建一个root class，而是直接创建三个class，它们之间是不能相互借流量的。借流量的策略，可以使得当前不使用这个分支的流量的时候，可以借给另一个分支，从而不浪费带宽，使带宽发挥最大的作用。\n\n最后，创建叶子队列规则，分别为**fifo**和**sfq**。\n\n```csharp\ntc qdisc add dev eth0 parent 1:10 handle 20: pfifo limit 5\ntc qdisc add dev eth0 parent 1:11 handle 30: pfifo limit 5\ntc qdisc add dev eth0 parent 1:12 handle 40: sfq perturb 10\n```\n\n基于这个队列规则，我们还可以通过TC设定发送规则：从1.2.3.4来的，发送给port 80的包，从第一个分支1:10走；其他从1.2.3.4发送来的包从第二个分支1:11走；其他的走默认分支。\n\n```sql\ntc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src 1.2.3.4 match ip dport 80 0xffff flowid 1:10\ntc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src 1.2.3.4 flowid 1:11\n```\n\n## 如何控制QoS？\n\n我们讲过，使用OpenvSwitch将云中的网卡连通在一起，那如何控制QoS呢？\n\n就像我们上面说的一样，OpenvSwitch支持两种：\n\n- 对于进入的流量，可以设置策略Ingress policy；\n\n```vbnet\novs-vsctl set Interface tap0 ingress_policing_rate=100000\novs-vsctl set Interface tap0 ingress_policing_burst=10000\n```\n\n- 对于发出的流量，可以设置QoS规则Egress shaping，支持HTB。\n\n我们构建一个拓扑图，来看看OpenvSwitch的QoS是如何工作的。\n\n首先，在port上可以创建QoS规则，一个QoS规则可以有多个队列Queue。\n\n![img](aafe1e0bfc6a87e3341b82485e41185b-20230115031644494.jpg)\n\n```python\novs-vsctl set port first_br qos=@newqos -- --id=@newqos create qos type=linux-htb other-config:max-rate=10000000 queues=0=@q0,1=@q1,2=@q2 -- --id=@q0 create queue other-config:min-rate=3000000 other-config:max-rate=10000000 -- --id=@q1 create queue other-config:min-rate=1000000 other-config:max-rate=10000000 -- --id=@q2 create queue other-config:min-rate=6000000 other-config:max-rate=10000000\n```\n\n上面的命令创建了一个QoS规则，对应三个Queue。min-rate就是上面的rate，max-rate就是上面的ceil。通过交换机的网络包，要通过流表规则，匹配后进入不同的队列。然后我们就可以添加流表规则Flow(first_br是br0上的port 5)。\n\n```csharp\novs-ofctl add-flow br0 \"in_port=6 nw_src=192.168.100.100 actions=enqueue:5:0\"\novs-ofctl add-flow br0 \"in_port=7 nw_src=192.168.100.101 actions=enqueue:5:1\"\novs-ofctl add-flow br0 \"in_port=8 nw_src=192.168.100.102 actions=enqueue:5:2\"\n```\n\n接下来，我们单独测试从192.168.100.100，192.168.100.101，192.168.100.102到192.168.100.103的带宽的时候，每个都是能够打满带宽的。\n\n如果三个一起测试，一起狂发网络包，会发现是按照3:1:6的比例进行的，正是根据配置的队列的带宽比例分配的。\n\n如果192.168.100.100和192.168.100.101一起测试，发现带宽占用比例为3:1，但是占满了总的流量，也即没有发包的192.168.100.102有60%的带宽被借用了。\n\n如果192.168.100.100和192.168.100.102一起测试，发现带宽占用比例为1:2。如果192.168.100.101和192.168.100.102一起测试，发现带宽占用比例为1:6。\n\n## 小结\n\n好了，这一节就讲到这里了，我们来总结一下。\n\n- 云中的流量控制主要通过队列进行的，队列分为两大类：无类别队列规则和基于类别的队列规则。\n- 在云中网络Openvswitch中，主要使用的是分层令牌桶规则（HTB），将总的带宽在一棵树上按照配置的比例进行分配，并且在一个分支不用的时候，可以借给另外的分支，从而增强带宽利用率。\n\n最后，给你留两个思考题。\n\n1. 这一节中提到，入口流量其实没有办法控制，出口流量是可以很好控制的，你能想出一个控制云中的虚拟机的入口流量的方式吗？\n2. 安全性和流量控制大概解决了，但是不同用户在物理网络的隔离还是没有解决，你知道怎么解决吗？\n\n# 28 讲云中网络的隔离GRE、VXLAN：虽然住一个小区，也要保护隐私\n\n对于云平台中的隔离问题，前面咱们用的策略一直都是VLAN，但是我们也说过这种策略的问题，VLAN只有12位，共4096个。当时设计的时候，看起来是够了，但是现在绝对不够用，怎么办呢？\n\n**一种方式是修改这个协议**。这种方法往往不可行，因为当这个协议形成一定标准后，千千万万设备上跑的程序都要按这个规则来。现在说改就放，谁去挨个儿告诉这些程序呢？很显然，这是一项不可能的工程。\n\n**另一种方式就是扩展**，在原来包的格式的基础上扩展出一个头，里面包含足够用于区分租户的ID，外层的包的格式尽量和传统的一样，依然兼容原来的格式。一旦遇到需要区分用户的地方，我们就用这个特殊的程序，来处理这个特殊的包的格式。\n\n这个概念很像咱们[第22讲]讲过的**隧道理论**，还记得自驾游通过摆渡轮到海南岛的那个故事吗？在那一节，我们说过，扩展的包头主要是用于加密的，而我们现在需要的包头是要能够区分用户的。\n\n底层的物理网络设备组成的网络我们称为**Underlay网络**，而用于虚拟机和云中的这些技术组成的网络称为**Overlay网络**，**这是一种基于物理网络的虚拟化网络实现**。这一节我们重点讲两个Overlay的网络技术。\n\n## GRE\n\n第一个技术是**GRE**，全称Generic Routing Encapsulation，它是一种IP-over-IP的隧道技术。它将IP包封装在GRE包里，外面加上IP头，在隧道的一端封装数据包，并在通路上进行传输，到另外一端的时候解封装。你可以认为Tunnel是一个虚拟的、点对点的连接。\n\n![img](b189df0a6ee4b0462818bf2f154c9531-20230115031644496.jpg)\n\n从这个图中可以看到，在GRE头中，前32位是一定会有的，后面的都是可选的。在前4位标识位里面，有标识后面到底有没有可选项？这里面有个很重要的key字段，是一个32位的字段，里面存放的往往就是用于区分用户的Tunnel ID。32位，够任何云平台喝一壶的了！\n\n下面的格式类型专门用于网络虚拟化的GRE包头格式，称为**NVGRE**，也给网络ID号24位，也完全够用了。\n\n除此之外，GRE还需要有一个地方来封装和解封装GRE的包，这个地方往往是路由器或者有路由功能的Linux机器。\n\n使用GRE隧道，传输的过程就像下面这张图。这里面有两个网段、两个路由器，中间要通过GRE隧道进行通信。当隧道建立之后，会多出两个Tunnel端口，用于封包、解封包。\n\n![img](76298ce51d349bc9805fbf317312e4ce-20230115031644996.jpg)\n\n1. 主机A在左边的网络，IP地址为192.168.1.102，它想要访问主机B，主机B在右边的网络，IP地址为192.168.2.115。于是发送一个包，源地址为192.168.1.102，目标地址为192.168.2.115。因为要跨网段访问，于是根据默认的default路由表规则，要发给默认的网关192.168.1.1，也即左边的路由器。\n2. 根据路由表，从左边的路由器，去192.168.2.0/24这个网段，应该走一条GRE的隧道，从隧道一端的网卡Tunnel0进入隧道。\n3. 在Tunnel隧道的端点进行包的封装，在内部的IP头之外加上GRE头。对于NVGRE来讲，是在MAC头之外加上GRE头，然后加上外部的IP地址，也即路由器的外网IP地址。源IP地址为172.17.10.10，目标IP地址为172.16.11.10，然后从E1的物理网卡发送到公共网络里。\n4. 在公共网络里面，沿着路由器一跳一跳地走，全部都按照外部的公网IP地址进行。\n5. 当网络包到达对端路由器的时候，也要到达对端的Tunnel0，然后开始解封装，将外层的IP头取下来，然后根据里面的网络包，根据路由表，从E3口转发出去到达服务器B。\n\n从GRE的原理可以看出，GRE通过隧道的方式，很好地解决了VLAN ID不足的问题。但是，GRE技术本身还是存在一些不足之处。\n\n首先是**Tunnel的数量问题**。GRE是一种点对点隧道，如果有三个网络，就需要在每两个网络之间建立一个隧道。如果网络数目增多，这样隧道的数目会呈指数性增长。\n\n![img](006cc8a4bf7a13fea0f456905c263afe-20230115031644567.jpg)\n\n其次，**GRE不支持组播**，因此一个网络中的一个虚机发出一个广播帧后，GRE会将其广播到所有与该节点有隧道连接的节点。\n\n另外一个问题是目前还是**有很多防火墙和三层网络设备无法解析GRE**，因此它们无法对GRE封装包做合适地过滤和负载均衡。\n\n## VXLAN\n\n第二种Overlay的技术称为VXLAN。和三层外面再套三层的GRE不同，VXLAN则是从二层外面就套了一个VXLAN的头，这里面包含的VXLAN ID为24位，也够用了。在VXLAN头外面还封装了UDP、IP，以及外层的MAC头。\n\n![img](3b02d54c9093e3de6d1a847dd0eb060f-20230115031644571.jpg)\n\nVXLAN作为扩展性协议，也需要一个地方对VXLAN的包进行封装和解封装，实现这个功能的点称为**VTEP**（VXLAN Tunnel Endpoint）。\n\nVTEP相当于虚拟机网络的管家。每台物理机上都可以有一个VTEP。每个虚拟机启动的时候，都需要向这个VTEP管家注册，每个VTEP都知道自己上面注册了多少个虚拟机。当虚拟机要跨VTEP进行通信的时候，需要通过VTEP代理进行，由VTEP进行包的封装和解封装。\n\n和GRE端到端的隧道不同，VXLAN不是点对点的，而是支持通过组播的来定位目标机器的，而非一定是这一端发出，另一端接收。\n\n当一个VTEP启动的时候，它们都需要通过IGMP协议。加入一个组播组，就像加入一个邮件列表，或者加入一个微信群一样，所有发到这个邮件列表里面的邮件，或者发送到微信群里面的消息，大家都能收到。而当每个物理机上的虚拟机启动之后，VTEP就知道，有一个新的VM上线了，它归我管。\n\n![img](e4541dfd1571aab11694343520970a77-20230115031644850.jpg)\n\n如图，虚拟机1、2、3属于云中同一个用户的虚拟机，因而需要分配相同的VXLAN ID=101。在云的界面上，就可以知道它们的IP地址，于是可以在虚拟机1上ping虚拟机2。\n\n虚拟机1发现，它不知道虚拟机2的MAC地址，因而包没办法发出去，于是要发送ARP广播。\n\n![﻿﻿](27b0f23c0e94c10bac63d2ec6401e079-20230115031644609.jpg)\n\nARP请求到达VTEP1的时候，VTEP1知道，我这里有一台虚拟机，要访问一台不归我管的虚拟机，需要知道MAC地址，可是我不知道啊，这该咋办呢？\n\nVTEP1想，我不是加入了一个微信群么？可以在里面@all 一下，问问虚拟机2归谁管。于是VTEP1将ARP请求封装在VXLAN里面，组播出去。\n\n当然在群里面，VTEP2和VTEP3都收到了消息，因而都会解开VXLAN包看，里面是一个ARP。\n\nVTEP3在本地广播了半天，没人回，都说虚拟机2不归自己管。\n\nVTEP2在本地广播，虚拟机2回了，说虚拟机2归我管，MAC地址是这个。通过这次通信，VTEP2也学到了，虚拟机1归VTEP1管，以后要找虚拟机1，去找VTEP1就可以了。\n\n![img](377070660f2474b90489b0f10f8f686c-20230115031644636.jpg)\n\nVTEP2将ARP的回复封装在VXLAN里面，这次不用组播了，直接发回给VTEP1。\n\nVTEP1解开VXLAN的包，发现是ARP的回复，于是发给虚拟机1。通过这次通信，VTEP1也学到了，虚拟机2归VTEP2管，以后找虚拟机2，去找VTEP2就可以了。\n\n虚拟机1的ARP得到了回复，知道了虚拟机2的MAC地址，于是就可以发送包了。\n\n![img](5ba8f197a682de2539594ef5d7e7d56e-20230115031644645.jpg)\n\n虚拟机1发给虚拟机2的包到达VTEP1，它当然记得刚才学的东西，要找虚拟机2，就去VTEP2，于是将包封装在VXLAN里面，外层加上VTEP1和VTEP2的IP地址，发送出去。\n\n网络包到达VTEP2之后，VTEP2解开VXLAN封装，将包转发给虚拟机2。\n\n虚拟机2回复的包，到达VTEP2的时候，它当然也记得刚才学的东西，要找虚拟机1，就去VTEP1，于是将包封装在VXLAN里面，外层加上VTEP1和VTEP2的IP地址，也发送出去。\n\n网络包到达VTEP1之后，VTEP1解开VXLAN封装，将包转发给虚拟机1。\n\n![img](b4d39cfc833be380be67eeee8019d319-20230115031644678.jpg)\n\n有了GRE和VXLAN技术，我们就可以解决云计算中VLAN的限制了。那如何将这个技术融入云平台呢？\n\n还记得将你宿舍里面的情况，所有东西都搬到一台物理机上那个故事吗？\n\n![img](8fc4b26b9e42c37adb87db81e36cc64c-20230115031644705.jpg)\n\n虚拟机是你的电脑，路由器和DHCP Server相当于家用路由器或者寝室长的电脑，外网网口访问互联网，所有的电脑都通过内网网口连接到一个交换机br0上，虚拟机要想访问互联网，需要通过br0连到路由器上，然后通过路由器将请求NAT后转发到公网。\n\n接下来的事情就惨了，你们宿舍闹矛盾了，你们要分成三个宿舍住，对应上面的图，你们寝室长，也即路由器单独在一台物理机上，其他的室友也即VM分别在两台物理机上。这下把一个完整的br0一刀三断，每个宿舍都是单独的一段。\n\n![img](184a02e0ee2404b46409cbf3d34837cb-20230115031644726.jpg)\n\n可是只有你的寝室长有公网口可以上网，于是你偷偷在三个宿舍中间打了一个隧道，用网线通过隧道将三个宿舍的两个br0连接起来，让其他室友的电脑和你寝室长的电脑，看起来还是连到同一个br0上，其实中间是通过你隧道中的网线做了转发。\n\n为什么要多一个br1这个虚拟交换机呢？主要通过br1这一层将虚拟机之间的互联和物理机机之间的互联分成两层来设计，中间隧道可以有各种挖法，GRE、VXLAN都可以。\n\n使用了OpenvSwitch之后，br0可以使用OpenvSwitch的Tunnel功能和Flow功能。\n\nOpenvSwitch支持三类隧道：GRE、VXLAN、IPsec_GRE。在使用OpenvSwitch的时候，虚拟交换机就相当于GRE和VXLAN封装的端点。\n\n我们模拟创建一个如下的网络拓扑结构，来看隧道应该如何工作。\n\n![img](fca6857aaca4f3549b02445ffce71f49-20230115031645100.jpg)\n\n三台物理机，每台上都有两台虚拟机，分别属于两个不同的用户，因而VLAN tag都得打地不一样，这样才不能相互通信。但是不同物理机上的相同用户，是可以通过隧道相互通信的，因而通过GRE隧道可以连接到一起。\n\n接下来，所有的Flow Table规则都设置在br1上，每个br1都有三个网卡，其中网卡1是对内的，网卡2和3是对外的。\n\n下面我们具体来看Flow Table的设计。\n\n![img](5162a39b589901ca3de95de751e90a96-20230115031645149.jpg)\n\n1.Table 0是所有流量的入口，所有进入br1的流量，分为两种流量，一个是进入物理机的流量，一个是从物理机发出的流量。\n\n从port 1进来的，都是发出去的流量，全部由Table 1处理。\n\n```csharp\novs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=1 in_port=1 actions=resubmit(,1)\"\n```\n\n从port 2、3进来的，都是进入物理机的流量，全部由Table 3处理。\n\n```csharp\novs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=1 in_port=2 actions=resubmit(,3)\"\novs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=1 in_port=3 actions=resubmit(,3)\"\n```\n\n如果都没匹配上，就默认丢弃。\n\n```csharp\novs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=0 actions=drop\"\n```\n\n2.Table 1用于处理所有出去的网络包，分为两种情况，一种是单播，一种是多播。\n\n对于单播，由Table 20处理。\n\n```csharp\novs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=1 table=1 dl_dst=00:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,20)\"\n```\n\n对于多播，由Table 21处理。\n\n```csharp\novs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=1 table=1 dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,21)\"\n```\n\n3.Table 2是紧接着Table1的，如果既不是单播，也不是多播，就默认丢弃。\n\n```csharp\novs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=0 table=2 actions=drop\"\n```\n\n4.Table 3用于处理所有进来的网络包，需要将隧道Tunnel ID转换为VLAN ID。\n\n如果匹配不上Tunnel ID，就默认丢弃。\n\n```csharp\novs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=0 table=3 actions=drop\"\n```\n\n如果匹配上了Tunnel ID，就转换为相应的VLAN ID，然后跳到Table 10。\n\n```csharp\novs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=1 table=3 tun_id=0x1 actions=mod_vlan_vid:1,resubmit(,10)\"\novs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=1 table=3 tun_id=0x2 actions=mod_vlan_vid:2,resubmit(,10)\"\n```\n\n5.对于进来的包，Table 10会进行MAC地址学习。这是一个二层交换机应该做的事情，学习完了之后，再从port 1发出去。\n\n```csharp\novs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=1 table=10  actions=learn(table=20,priority=1,hard_timeout=300,NXM_OF_VLAN_TCI[0..11],NXM_OF_ETH_DST[]=NXM_OF_ETH_SRC[],load:0-\u003eNXM_OF_VLAN_TCI[],load:NXM_NX_TUN_ID[]-\u003eNXM_NX_TUN_ID[],output:NXM_OF_IN_PORT[]),output:1\"\n```\n\nTable 10是用来学习MAC地址的，学习的结果放在Table 20里面。Table20被称为MAC learning table。\n\nNXM_OF_VLAN_TCI是VLAN tag。在MAC learning table中，每一个entry都仅仅是针对某一个VLAN来说的，不同VLAN的learning table是分开的。在学习结果的entry中，会标出这个entry是针对哪个VLAN的。\n\nNXM_OF_ETH_DST[]=NXM_OF_ETH_SRC[]表示，当前包里面的MAC Source Address会被放在学习结果的entry里的dl_dst里。这是因为每个交换机都是通过进入的网络包来学习的。某个MAC从某个port进来，交换机就应该记住，以后发往这个MAC的包都要从这个port出去，因而源MAC地址就被放在了目标MAC地址里面，因为这是为了发送才这么做的。\n\nload:0-\u003eNXM_OF_VLAN_TCI[]是说，在Table20中，将包从物理机发送出去的时候，VLAN tag设为0，所以学习完了之后，Table 20中会有actions=strip_vlan。\n\nload:NXM_NX_TUN_ID[]-\u003eNXM_NX_TUN_ID[]的意思是，在Table 20中，将包从物理机发出去的时候，设置Tunnel ID，进来的时候是多少，发送的时候就是多少，所以学习完了之后，Table 20中会有set_tunnel。\n\noutput:NXM_OF_IN_PORT[]是发送给哪个port。例如是从port 2进来的，那学习完了之后，Table 20中会有output:2。\n\n![img](4dc0fe34819ee02a53a97c89811747dd-20230115031644811.jpg)\n\n所以如图所示，通过左边的MAC地址学习规则，学习到的结果就像右边的一样，这个结果会被放在Table 20里面。\n\n6.Table 20是MAC Address Learning Table。如果不为空，就按照规则处理；如果为空，就说明没有进行过MAC地址学习，只好进行广播了，因而要交给Table 21处理。\n\n```csharp\novs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=0 table=20 actions=resubmit(,21)\"\n```\n\n7.Table 21用于处理多播的包。\n\n如果匹配不上VLAN ID，就默认丢弃。\n\n```csharp\novs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=0 table=21 actions=drop\"\n```\n\n如果匹配上了VLAN ID，就将VLAN ID转换为Tunnel ID，从两个网卡port 2和port 3都发出去，进行多播。\n\n```csharp\novs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=1table=21dl_vlan=1 actions=strip_vlan,set_tunnel:0x1,output:2,output:3\"\novs-ofctl add-flow br1 \"hard_timeout=0 idle_timeout=0 priority=1table=21dl_vlan=2 actions=strip_vlan,set_tunnel:0x2,output:2,output:3\"\n```\n\n## 小结\n\n好了，这一节就到这里了，我们来总结一下。\n\n- 要对不同用户的网络进行隔离，解决VLAN数目有限的问题，需要通过Overlay的方式，常用的有GRE和VXLAN。\n- GRE是一种点对点的隧道模式，VXLAN支持组播的隧道模式，它们都要在某个Tunnel Endpoint进行封装和解封装，来实现跨物理机的互通。\n- OpenvSwitch可以作为Tunnel Endpoint，通过设置流表的规则，将虚拟机网络和物理机网络进行隔离、转换。\n\n最后，给你留两个思考题。\n\n1. 虽然VXLAN可以支持组播，但是如果虚拟机数目比较多，在Overlay网络里面，广播风暴问题依然会很严重，你能想到什么办法解决这个问题吗？\n2. 基于虚拟机的云比较复杂，而且虚拟机里面的网卡，到物理网络转换层次比较多，有一种比虚拟机更加轻量级的云的模式，你知道是什么吗？\n\n# 29 讲容器网络：来去自由的日子，不买公寓去合租\n\n如果说虚拟机是买公寓，容器则相当于合租，有一定的隔离，但是隔离性没有那么好。云计算解决了基础资源层的弹性伸缩，却没有解决PaaS层应用随基础资源层弹性伸缩而带来的批量、快速部署问题。于是，容器应运而生。\n\n容器就是Container，而Container的另一个意思是集装箱。其实**容器的思想就是要变成软件交付的集装箱**。集装箱的特点，一是打包，二是标准。\n\n![img](a50157f1084c946b9e27f3b328b8d2dc-20230115031644808.jpg)\n\n在没有集装箱的时代，假设要将货物从A运到B，中间要经过三个码头、换三次船。每次都要将货物卸下船来，弄的乱七八糟，然后还要再搬上船重新整齐摆好。因此在没有集装箱的时候，每次换船，船员们都要在岸上待几天才能干完活。\n\n有了尺寸全部都一样的集装箱以后，可以把所有的货物都打包在一起，所以每次换船的时候，一个箱子整体搬过去就行了，小时级别就能完成，船员再也不用耗费很长时间了。这是集装箱的“打包”“标准”两大特点在生活中的应用。\n\n![img](50c12f33ec178972c315e57b370dffcb-20230115031644877.jpg)\n\n那么容器如何对应用打包呢？\n\n学习集装箱，首先要有个封闭的环境，将货物封装起来，让货物之间互不干扰，互相隔离，这样装货卸货才方便。\n\n封闭的环境主要使用了两种技术，一种是**看起来是隔离的技术**，称为**namespace**，也即每个 namespace中的应用看到的是不同的 IP地址、用户空间、程号等。另一种是**用起来是隔离的技术**，称为**cgroup**，也即明明整台机器有很多的 CPU、内存，而一个应用只能用其中的一部分。\n\n有了这两项技术，就相当于我们焊好了集装箱。接下来的问题就是如何“将这个集装箱标准化”，并在哪艘船上都能运输。这里的标准首先就是**镜像**。\n\n所谓镜像，就是将你焊好集装箱的那一刻，将集装箱的状态保存下来，就像孙悟空说：“定！”，集装箱里的状态就被定在了那一刻，然后将这一刻的状态保存成一系列文件。无论从哪里运行这个镜像，都能完整地还原当时的情况。\n\n接下来我们就具体来看看，这两种网络方面的打包技术。\n\n## 命名空间（namespace）\n\n我们首先来看网络namespace。\n\nnamespace翻译过来就是命名空间。其实很多面向对象的程序设计语言里面，都有命名空间这个东西。大家一起写代码，难免类会起相同的名词，编译就会冲突。而每个功能都有自己的命名空间，在不同的空间里面，类名相同，不会冲突。\n\n在Linux下也是这样的，很多的资源都是全局的。比如进程有全局的进程ID，网络也有全局的路由表。但是，当一台Linux上跑多个进程的时候，如果我们觉得使用不同的路由策略，这些进程可能会冲突，那就需要将这个进程放在一个独立的namespace里面，这样就可以独立配置网络了。\n\n网络的namespace由ip netns命令操作。它可以创建、删除、查询namespace。\n\n我们再来看将你们宿舍放进一台物理机的那个图。你们宿舍长的电脑是一台路由器，你现在应该知道怎么实现这个路由器吧？可以创建一个Router虚拟机来做这件事情，但是还有一个更加简单的办法，就是我在图里画的这条虚线，这个就是通过namespace实现的。\n\n![img](1a5d299c2eb5480eda93a8f8e3b3ca1a-1584286829605-20230115031644899.jpg)\n\n我们创建一个routerns，于是一个独立的网络空间就产生了。你可以在里面尽情设置自己的规则。\n\n```csharp\nip netns add routerns\n```\n\n既然是路由器，肯定要能转发嘛，因而forward开关要打开。\n\n```bash\nip netns exec routerns sysctl -w net.ipv4.ip_forward=1\n```\n\nexec的意思就是进入这个网络空间做点事情。初始化一下iptables，因为这里面要配置NAT规则。\n\n```bash\nip netns exec routerns iptables-save -c \nip netns exec routerns iptables-restore -c\n```\n\n路由器需要有一张网卡连到br0上，因而要创建一个网卡。\n\n```kotlin\novs-vsctl -- add-port br0 taprouter -- set Interface taprouter type=internal -- set Interface taprouter external-ids:iface-status=active -- set Interface taprouter external-ids:attached-mac=fa:16:3e:84:6e:cc\n```\n\n这个网络创建完了，但是是在namespace外面的，如何进去呢？可以通过这个命令：\n\n```bash\nip link set taprouter netns routerns\n```\n\n要给这个网卡配置一个IP地址，当然应该是虚拟机网络的网关地址。例如虚拟机私网网段为192.168.1.0/24，网关的地址往往为192.168.1.1。\n\n```sql\nip netns exec routerns ip -4 addr add 192.168.1.1/24 brd 192.168.1.255 scope global dev taprouter\n```\n\n为了访问外网，还需要另一个网卡连在外网网桥br-ex上，并且塞在namespace里面。\n\n```kotlin\novs-vsctl -- add-port br-ex taprouterex -- set Interface taprouterex type=internal -- set Interface taprouterex external-ids:iface-status=active -- set Interface taprouterex external-ids:attached-mac=fa:16:3e:68:12:c0\nip link set taprouterex netns routerns\n```\n\n我们还需要为这个网卡分配一个地址，这个地址应该和物理外网网络在一个网段。假设物理外网为16.158.1.0/24，可以分配一个外网地址16.158.1.100/24。\n\n```sql\nip netns exec routerns ip -4 addr add 16.158.1.100/24 brd 16.158.1.255 scope global dev taprouterex\n```\n\n接下来，既然是路由器，就需要配置路由表，路由表是这样的：\n\n```sql\nip netns exec routerns route -n\nKernel IP routing table\nDestination   Gateway     Genmask     Flags Metric Ref  Use Iface\n0.0.0.0     16.158.1.1  0.0.0.0     UG  0   0    0 taprouterex\n192.168.1.0    0.0.0.0     255.255.255.0  U   0   0    0 taprouter\n16.158.1.0  0.0.0.0     255.255.255.0  U   0   0    0 taprouterex\n```\n\n路由表中的默认路由是去物理外网的，去192.168.1.0/24也即虚拟机私网，走下面的网卡，去16.158.1.0/24也即物理外网，走上面的网卡。\n\n我们在前面的章节讲过，如果要在虚拟机里面提供服务，提供给外网的客户端访问，客户端需要访问外网IP3，会在外网网口NAT称为虚拟机私网IP。这个NAT规则要在这个namespace里面配置。\n\n```sql\nip netns exec routerns iptables -t nat -nvL\nChain PREROUTING\ntarget  prot opt  in  out  source  destination\nDNAT  all  --  *  *  0.0.0.0/0 16.158.1.103 to:192.168.1.3\nChain POSTROUTING\ntarget  prot opt  in  out  source   destination\nSNAT  all  --  *  *  192.168.1.3  0.0.0.0/0 to:16.158.1.103\n```\n\n这里面有两个规则，一个是SNAT，将虚拟机的私网IP 192.168.1.3 NAT成物理外网IP 16.158.1.103。一个是DNAT，将物理外网IP 16.158.1.103 NAT成虚拟机私网IP 192.168.1.3。\n\n至此为止，基于网络namespace的路由器实现完毕。\n\n## 机制网络（cgroup）\n\n我们再来看打包的另一个机制网络cgroup。\n\ncgroup全称control groups，是Linux内核提供的一种可以限制、隔离进程使用的资源机制。\n\ncgroup能控制哪些资源呢？它有很多子系统：\n\n- CPU子系统使用调度程序为进程控制CPU的访问；\n- cpuset，如果是多核心的CPU，这个子系统会为进程分配单独的CPU和内存；\n- memory子系统，设置进程的内存限制以及产生内存资源报告；\n- blkio子系统，设置限制每个块设备的输入输出控制；\n- net_cls，这个子系统使用等级识别符（classid）标记网络数据包，可允许Linux 流量控制程序（tc）识别从具体cgroup中生成的数据包。\n\n我们这里最关心的是net_cls，它可以和前面讲过的TC关联起来。\n\ncgroup提供了一个虚拟文件系统，作为进行分组管理和各子系统设置的用户接口。要使用cgroup，必须挂载cgroup文件系统，一般情况下都是挂载到/sys/fs/cgroup目录下。\n\n所以首先我们要挂载一个net_cls的文件系统。\n\n```bash\nmkdir /sys/fs/cgroup/net_cls\nmount -t cgroup -onet_cls net_cls /sys/fs/cgroup/net_cls\n```\n\n接下来我们要配置TC了。还记得咱们实验TC的时候那个树吗？\n\n![img](9a1b8a7c0c5403a2b4b3c277545991b5-1584286829655-20230115031645215.jpg)\n\n当时我们通过这个命令设定了规则：从1.2.3.4来的，发送给port 80的包，从1:10走；其他从1.2.3.4发送来的包从1:11走；其他的走默认。\n\n```sql\ntc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src 1.2.3.4 match ip dport 80 0xffff flowid 1:10\ntc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src 1.2.3.4 flowid 1:11\n```\n\n这里是根据源IP来设定的，现在有了cgroup，我们按照cgroup再来设定规则。\n\n```sql\ntc filter add dev eth0 protocol ip parent 1:0 prio 1 handle 1: cgroup\n```\n\n假设我们有两个用户a和b，要对它们进行带宽限制。\n\n首先，我们要创建两个net_cls。\n\n```bash\nmkdir /sys/fs/cgroup/net_cls/a   \nmkdir /sys/fs/cgroup/net_cls/b\n```\n\n假设用户a启动的进程ID为12345，把它放在net_cls/a/tasks文件中。同样假设用户b启动的进程ID为12346，把它放在net_cls/b/tasks文件中。\n\nnet_cls/a目录下面，还有一个文件net_cls.classid，我们放flowid 1:10。net_cls/b目录下面，也创建一个文件net_cls.classid，我们放flowid 1:11。\n\n这个数字怎么放呢？要转换成一个0xAAAABBBB的值，AAAA对应class中冒号前面的数字，而BBBB对应后面的数字。\n\n```bash\necho 0x00010010 \u003e /sys/fs/cgroup/net_cls/a/net_cls.classid    \necho 0x00010011 \u003e /sys/fs/cgroup/net_cls/b/net_cls.classid\n```\n\n这样用户a的进程发的包，会打上1:10这个标签；用户b的进程发的包，会打上1:11这个标签。然后TC根据这两个标签，让用户a的进程的包走左边的分支，用户b的进程的包走右边的分支。\n\n## 容器网络中如何融入物理网络？\n\n了解了容器背后的技术，接下来我们来看，容器网络究竟是如何融入物理网络的？\n\n如果你使用docker run运行一个容器，你应该能看到这样一个拓扑结构。\n\n![img](20e87bc215b9d049a4a504d775d26dd2-20230115031644955.jpg)\n\n是不是和虚拟机很像？容器里面有张网卡，容器外有张网卡，容器外的网卡连到docker0网桥，通过这个网桥，容器直接实现相互访问。\n\n如果你用brctl查看docker0网桥，你会发现它上面连着一些网卡。其实这个网桥和[第24讲]，咱们自己用brctl创建的网桥没什么两样。\n\n那连接容器和网桥的那个网卡和虚拟机一样吗？在虚拟机场景下，有一个虚拟化软件，通过TUN/TAP设备虚拟一个网卡给虚拟机，但是容器场景下并没有虚拟化软件，这该怎么办呢？\n\n在Linux下，可以创建一对veth pair的网卡，从一边发送包，另一边就能收到。\n\n我们首先通过这个命令创建这么一对。\n\n```bash\nip link add name veth1 mtu 1500 type veth peer name veth2 mtu 1500\n```\n\n其中一边可以打到docker0网桥上。\n\n```bash\nip link set veth1 master testbr    \nip link set veth1 up\n```\n\n那另一端如何放到容器里呢？\n\n一个容器的启动会对应一个namespace，我们要先找到这个namespace。对于docker来讲，pid就是namespace的名字，可以通过这个命令获取。\n\n```bash\ndocker inspect '--format={{ .State.Pid }}' test\n```\n\n假设结果为12065，这个就是namespace名字。\n\n默认Docker创建的网络namespace不在默认路径下 ，ip netns看不到，所以需要ln软链接一下。链接完毕以后，我们就可以通过ip netns命令操作了。\n\n```bash\nrm -f /var/run/netns/12065    \nln -s /proc/12065/ns/net /var/run/netns/12065\n```\n\n然后，我们就可以将另一端veth2塞到namespace里面。\n\n```bash\nip link set veth2 netns 12065\n```\n\n然后，将容器内的网卡重命名。\n\n```bash\nip netns exec 12065 ip link set veth2 name eth0\n```\n\n然后，给容器内网卡设置ip地址。\n\n```bash\nip netns exec 12065 ip addr add 172.17.0.2/24 dev eth0    \nip netns exec 12065 ip link set eth0 up\n```\n\n一台机器内部容器的互相访问没有问题了，那如何访问外网呢？\n\n你先想想看有没有思路？对，就是虚拟机里面的桥接模式和NAT模式。Docker默认使用NAT模式。NAT模式分为SNAT和DNAT，如果是容器内部访问外部，就需要通过SNAT。\n\n从容器内部的客户端访问外部网络中的服务器，我画了一张图。在[虚拟机]那一节，也有一张类似的图。\n\n![img](5452971c96e8fea33c3f873860e25c93-20230115031645010.jpg)\n\n在宿主机上，有这么一条iptables规则：\n\n```css\n-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE\n```\n\n所有从容器内部发出来的包，都要做地址伪装，将源IP地址，转换为物理网卡的IP地址。如果有多个容器，所有的容器共享一个外网的IP地址，但是在conntrack表中，记录下这个出去的连接。\n\n当服务器返回结果的时候，到达物理机，会根据conntrack表中的规则，取出原来的私网IP，通过DNAT将地址转换为私网IP地址，通过网桥docker0实现对内的访问。\n\n如果在容器内部属于一个服务，例如部署一个网站，提供给外部进行访问，需要通过Docker的端口映射技术，将容器内部的端口映射到物理机上来。\n\n例如容器内部监听80端口，可以通Docker run命令中的参数-p 10080:80，将物理机上的10080端口和容器的80端口映射起来， 当外部的客户端访问这个网站的时候，通过访问物理机的10080端口，就能访问到容器内的80端口了。\n\n![img](49bb6b2a30fe76b124182980da935ebb-20230115031645050.jpg)\n\nDocker有两种方式，一种是通过一个进程**docker-proxy**的方式，监听10080，转换为80端口。\n\n```bash\n/usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 10080 -container-ip 172.17.0.2 -container-port 80\n```\n\n另外一种方式是通过**DNAT**方式，在-A PREROUTING阶段加一个规则，将到端口10080的DNAT称为容器的私有网络。\n\n```css\n-A DOCKER -p tcp -m tcp --dport 10080 -j DNAT --to-destination 172.17.0.2:80\n```\n\n如此就可以实现容器和物理网络之间的互通了。\n\n## 小结\n\n好了，这一节就到这里了，我们来总结一下。\n\n- 容器是一种比虚拟机更加轻量级的隔离方式，主要通过namespace和cgroup技术进行资源的隔离，namespace用于负责看起来隔离，cgroup用于负责用起来隔离。\n- 容器网络连接到物理网络的方式和虚拟机很像，通过桥接的方式实现一台物理机上的容器进行相互访问，如果要访问外网，最简单的方式还是通过NAT。\n\n最后，给你留两个思考题：\n\n1. 容器内的网络和物理机网络可以使用NAT的方式相互访问，如果这种方式用于部署应用，有什么问题呢？\n2. 和虚拟机一样，不同物理机上的容器需要相互通信，你知道容器是怎么做到这一点吗？\n\n# 30 讲容器网络之Flannel：每人一亩三分地\n\n上一节我们讲了容器网络的模型，以及如何通过NAT的方式与物理网络进行互通。\n\n每一台物理机上面安装好了Docker以后，都会默认分配一个172.17.0.0/16的网段。一台机器上新创建的第一个容器，一般都会给172.17.0.2这个地址，当然一台机器这样玩玩倒也没啥问题。但是容器里面是要部署应用的，就像上一节讲过的一样，它既然是集装箱，里面就需要装载货物。\n\n如果这个应用是比较传统的单体应用，自己就一个进程，所有的代码逻辑都在这个进程里面，上面的模式没有任何问题，只要通过NAT就能访问进来。\n\n但是因为无法解决快速迭代和高并发的问题，单体应用越来越跟不上时代发展的需要了。\n\n你可以回想一下，无论是各种网络直播平台，还是共享单车，是不是都是很短时间内就要积累大量用户，否则就会错过风口。所以应用需要在很短的时间内快速迭代，不断调整，满足用户体验；还要在很短的时间内，具有支撑高并发请求的能力。\n\n单体应用作为个人英雄主义的时代已经过去了。如果所有的代码都在一个工程里面，开发的时候必然存在大量冲突，上线的时候，需要开大会进行协调，一个月上线一次就很不错了。而且所有的流量都让一个进程扛，怎么也扛不住啊！\n\n没办法，一个字：拆！拆开了，每个子模块独自变化，减少相互影响。拆开了，原来一个进程扛流量，现在多个进程一起扛。所以，微服务就是从个人英雄主义，变成集团军作战。\n\n容器作为集装箱，可以保证应用在不同的环境中快速迁移，提高迭代的效率。但是如果要形成容器集团军，还需要一个集团军作战的调度平台，这就是Kubernetes。它可以灵活地将一个容器调度到任何一台机器上，并且当某个应用扛不住的时候，只要在Kubernetes上修改容器的副本数，一个应用马上就能变八个，而且都能提供服务。\n\n然而集团军作战有个重要的问题，就是通信。这里面包含两个问题，第一个是集团军的A部队如何实时地知道B部队的位置变化，第二个是两个部队之间如何相互通信。\n\n第一个问题位置变化，往往是通过一个称为注册中心的地方统一管理的，这个是应用自己做的。当一个应用启动的时候，将自己所在环境的IP地址和端口，注册到注册中心指挥部，这样其他的应用请求它的时候，到指挥部问一下它在哪里就好了。当某个应用发生了变化，例如一台机器挂了，容器要迁移到另一台机器，这个时候IP改变了，应用会重新注册，则其他的应用请求它的时候，还是能够从指挥部得到最新的位置。\n\n![img](06ba300a78aef37b9d190aba61c37865-20230115031645061.jpg)\n\n接下来是如何相互通信的问题。NAT这种模式，在多个主机的场景下，是存在很大问题的。在物理机A上的应用A看到的IP地址是容器A的，是172.17.0.2，在物理机B上的应用B看到的IP地址是容器B的，不巧也是172.17.0.2，当它们都注册到注册中心的时候，注册中心就是这个图里这样子。\n\n![img](a2bd259417b173ee641d2d16a0da54f1-20230115031645078.jpg)\n\n这个时候，应用A要访问应用B，当应用A从注册中心将应用B的IP地址读出来的时候，就彻底困惑了，这不是自己访问自己吗？\n\n怎么解决这个问题呢？一种办法是不去注册容器内的IP地址，而是注册所在物理机的IP地址，端口也要是物理机上映射的端口。\n\n![img](dfb2d3b6ae5ce31280812b64442a7519-20230115031645115.jpg)\n\n这样存在的问题是，应用是在容器里面的，它怎么知道物理机上的IP地址和端口呢？这明明是运维人员配置的，除非应用配合，读取容器平台的接口获得这个IP和端口。一方面，大部分分布式框架都是容器诞生之前就有了，它们不会适配这种场景；另一方面，让容器内的应用意识到容器外的环境，本来就是非常不好的设计。\n\n说好的集装箱，说好的随意迁移呢？难道要让集装箱内的货物意识到自己传的信息？而且本来Tomcat都是监听8080端口的，结果到了物理机上，就不能大家都用这个端口了，否则端口就冲突了，因而就需要随机分配端口，于是在注册中心就出现了各种各样奇怪的端口。无论是注册中心，还是调用方都会觉得很奇怪，而且不是默认的端口，很多情况下也容易出错。\n\nKubernetes作为集团军作战管理平台，提出指导意见，说网络模型要变平，但是没说怎么实现。于是业界就涌现了大量的方案，Flannel就是其中之一。\n\n对于IP冲突的问题，如果每一个物理机都是网段172.17.0.0/16，肯定会冲突啊，但是这个网段实在太大了，一台物理机上根本启动不了这么多的容器，所以能不能每台物理机在这个大网段里面，抠出一个小的网段，每个物理机网段都不同，自己看好自己的一亩三分地，谁也不和谁冲突。\n\n例如物理机A是网段172.17.8.0/24，物理机B是网段172.17.9.0/24，这样两台机器上启动的容器IP肯定不一样，而且就看IP地址，我们就一下子识别出，这个容器是本机的，还是远程的，如果是远程的，也能从网段一下子就识别出它归哪台物理机管，太方便了。\n\n接下来的问题，就是**物理机A上的容器如何访问到物理机B上的容器呢？**\n\n你是不是想到了熟悉的场景？虚拟机也需要跨物理机互通，往往通过Overlay的方式，容器是不是也可以这样做呢？\n\n**这里我要说Flannel使用UDP实现Overlay网络的方案。**\n\n![img](01ee306698c7dd6207e80fea0a8238c8-20230115031645129.jpg)\n\n在物理机A上的容器A里面，能看到的容器的IP地址是172.17.8.2/24，里面设置了默认的路由规则default via 172.17.8.1 dev eth0。\n\n如果容器A要访问172.17.9.2，就会发往这个默认的网关172.17.8.1。172.17.8.1就是物理机上面docker0网桥的IP地址，这台物理机上的所有容器都是连接到这个网桥的。\n\n在物理机上面，查看路由策略，会有这样一条172.17.0.0/24 via 172.17.0.0 dev flannel.1，也就是说发往172.17.9.2的网络包会被转发到flannel.1这个网卡。\n\n这个网卡是怎么出来的呢？在每台物理机上，都会跑一个flanneld进程，这个进程打开一个/dev/net/tun字符设备的时候，就出现了这个网卡。\n\n你有没有想起qemu-kvm，打开这个字符设备的时候，物理机上也会出现一个网卡，所有发到这个网卡上的网络包会被qemu-kvm接收进来，变成二进制串。只不过接下来qemu-kvm会模拟一个虚拟机里面的网卡，将二进制的串变成网络包，发给虚拟机里面的网卡。但是flanneld不用这样做，所有发到flannel.1这个网卡的包都会被flanneld进程读进去，接下来flanneld要对网络包进行处理。\n\n物理机A上的flanneld会将网络包封装在UDP包里面，然后外层加上物理机A和物理机B的IP地址，发送给物理机B上的flanneld。\n\n为什么是UDP呢？因为不想在flanneld之间建立两两连接，而UDP没有连接的概念，任何一台机器都能发给另一台。\n\n物理机B上的flanneld收到包之后，解开UDP的包，将里面的网络包拿出来，从物理机B的flannel.1网卡发出去。\n\n在物理机B上，有路由规则172.17.9.0/24 dev docker0 proto kernel scope link src 172.17.9.1。\n\n将包发给docker0，docker0将包转给容器B。通信成功。\n\n上面的过程连通性没有问题，但是由于全部在用户态，所以性能差了一些。\n\n跨物理机的连通性问题，在虚拟机那里有成熟的方案，就是VXLAN，那**能不能Flannel也用VXLAN呢**？\n\n当然可以了。如果使用VXLAN，就不需要打开一个TUN设备了，而是要建立一个VXLAN的VTEP。如何建立呢？可以通过netlink通知内核建立一个VTEP的网卡flannel.1。在我们讲OpenvSwitch的时候提过，netlink是一种用户态和内核态通信的机制。\n\n当网络包从物理机A上的容器A发送给物理机B上的容器B，在容器A里面通过默认路由到达物理机A上的docker0网卡，然后根据路由规则，在物理机A上，将包转发给flannel.1。这个时候flannel.1就是一个VXLAN的VTEP了，它将网络包进行封装。\n\n内部的MAC地址这样写：源为物理机A的flannel.1的MAC地址，目标为物理机B的flannel.1的MAC地址，在外面加上VXLAN的头。\n\n外层的IP地址这样写：源为物理机A的IP地址，目标为物理机B的IP地址，外面加上物理机的MAC地址。\n\n这样就能通过VXLAN将包转发到另一台机器，从物理机B的flannel.1上解包，变成内部的网络包，通过物理机B上的路由转发到docker0，然后转发到容器B里面。通信成功。\n\n![img](a568cb08c615b351e871bd981541a201-20230115031645154.jpg)\n\n## 小结\n\n好了，今天的内容就到这里，我来总结一下。\n\n- 基于NAT的容器网络模型在微服务架构下有两个问题，一个是IP重叠，一个是端口冲突，需要通过Overlay网络的机制保持跨节点的连通性。\n- Flannel是跨节点容器网络方案之一，它提供的Overlay方案主要有两种方式，一种是UDP在用户态封装，一种是VXLAN在内核态封装，而VXLAN的性能更好一些。\n\n最后，给你留两个问题：\n\n1. 通过Flannel的网络模型可以实现容器与容器直接跨主机的互相访问，那你知道如果容器内部访问外部的服务应该怎么融合到这个网络模型中吗？\n2. 基于Overlay的网络毕竟做了一次网络虚拟化，有没有更加高性能的方案呢？\n\n# 31 讲容器网络之Calico：为高效说出善意的谎言\n\n上一节我们讲了Flannel如何解决容器跨主机互通的问题，这个解决方式其实和虚拟机的网络互通模式是差不多的，都是通过隧道。但是Flannel有一个非常好的模式，就是给不同的物理机设置不同网段，这一点和虚拟机的Overlay的模式完全不一样。\n\n在虚拟机的场景下，整个网段在所有的物理机之间都是可以“飘来飘去”的。网段不同，就给了我们做路由策略的可能。\n\n## Calico网络模型的设计思路\n\n我们看图中的两台物理机。它们的物理网卡是同一个二层网络里面的。由于两台物理机的容器网段不同，我们完全可以将两台物理机配置成为路由器，并按照容器的网段配置路由表。\n\n![img](1e2420928488bdcf66ffd001393c3c50-20230115031645168.jpg)\n\n例如，在物理机A中，我们可以这样配置：要想访问网段172.17.9.0/24，下一跳是192.168.100.101，也即到物理机B上去。\n\n这样在容器A中访问容器B，当包到达物理机A的时候，就能够匹配到这条路由规则，并将包发给下一跳的路由器，也即发给物理机B。在物理机B上也有路由规则，要访问172.17.9.0/24，从docker0的网卡进去即可。\n\n当容器B返回结果的时候，在物理机B上，可以做类似的配置：要想访问网段172.17.8.0/24，下一跳是192.168.100.100，也即到物理机A上去。\n\n当包到达物理机B的时候，能够匹配到这条路由规则，将包发给下一跳的路由器，也即发给物理机A。在物理机A上也有路由规则，要访问172.17.8.0/24，从docker0的网卡进去即可。\n\n这就是**Calico网络的大概思路**，**即不走Overlay网络，不引入另外的网络性能损耗，而是将转发全部用三层网络的路由转发来实现**，只不过具体的实现和上面的过程稍有区别。\n\n首先，如果全部走三层的路由规则，没必要每台机器都用一个docker0，从而浪费了一个IP地址，而是可以直接用路由转发到veth pair在物理机这一端的网卡。同样，在容器内，路由规则也可以这样设定：把容器外面的veth pair网卡算作默认网关，下一跳就是外面的物理机。\n\n于是，整个拓扑结构就变成了这个图中的样子。\n\n![img](c3e999c033a0417df98c0bcc34c9349c-20230115031645192.jpg)\n\n## Calico网络的转发细节\n\n我们来看其中的一些细节。\n\n容器A1的IP地址为172.17.8.2/32，这里注意，不是/24，而是/32，将容器A1作为一个单点的局域网了。\n\n容器A1里面的默认路由，Calico配置得比较有技巧。\n\n```sql\ndefault via 169.254.1.1 dev eth0 \n169.254.1.1 dev eth0 scope link \n```\n\n这个IP地址169.254.1.1是默认的网关，但是整个拓扑图中没有一张网卡是这个地址。那如何到达这个地址呢？\n\n前面我们讲网关的原理的时候说过，当一台机器要访问网关的时候，首先会通过ARP获得网关的MAC地址，然后将目标MAC变为网关的MAC，而网关的IP地址不会在任何网络包头里面出现，也就是说，没有人在乎这个地址具体是什么，只要能找到对应的MAC，响应ARP就可以了。\n\nARP本地有缓存，通过ip neigh命令可以查看。\n\n```undefined\n169.254.1.1 dev eth0 lladdr ee:ee:ee:ee:ee:ee STALE\n```\n\n这个MAC地址是Calico硬塞进去的，但是没有关系，它能响应ARP，于是发出的包的目标MAC就是这个MAC地址。\n\n在物理机A上查看所有网卡的MAC地址的时候，我们会发现veth1就是这个MAC地址。所以容器A1里发出的网络包，第一跳就是这个veth1这个网卡，也就到达了物理机A这个路由器。\n\n在物理机A上有三条路由规则，分别是去两个本机的容器的路由，以及去172.17.9.0/24，下一跳为物理机B。\n\n```bash\n172.17.8.2 dev veth1 scope link \n172.17.8.3 dev veth2 scope link \n172.17.9.0/24 via 192.168.100.101 dev eth0 proto bird onlink\n```\n\n同理，物理机B上也有三条路由规则，分别是去两个本机的容器的路由，以及去172.17.8.0/24，下一跳为物理机A。\n\n```bash\n172.17.9.2 dev veth1 scope link \n172.17.9.3 dev veth2 scope link \n172.17.8.0/24 via 192.168.100.100 dev eth0 proto bird onlink\n```\n\n如果你觉得这些规则过于复杂，我将刚才的拓扑图转换为这个更加容易理解的图。\n\n![img](e59559ad7b46b9811553b6b0a85e8e7d-20230115031645228.jpg)\n\n在这里，物理机化身为路由器，通过路由器上的路由规则，将包转发到目的地。在这个过程中，没有隧道封装解封装，仅仅是单纯的路由转发，性能会好很多。但是，这种模式也有很多问题。\n\n## Calico的架构\n\n### 路由配置组件Felix\n\n如果只有两台机器，每台机器只有两个容器，而且保持不变。我手动配置一下，倒也没啥问题。但是如果容器不断地创建、删除，节点不断地加入、退出，情况就会变得非常复杂。\n\n![img](f29027cca71f3dfbba8c2f1a35c29331-20230115031645219.jpg)\n\n就像图中，有三台物理机，两两之间都需要配置路由，每台物理机上对外的路由就有两条。如果有六台物理机，则每台物理机上对外的路由就有五条。新加入一个节点，需要通知每一台物理机添加一条路由。\n\n这还是在物理机之间，一台物理机上，每创建一个容器，也需要多配置一条指向这个容器的路由。如此复杂，肯定不能手动配置，需要每台物理机上有一个agent，当创建和删除容器的时候，自动做这件事情。这个agent在Calico中称为Felix。\n\n### 路由广播组件BGP Speaker\n\n当Felix配置了路由之后，接下来的问题就是，如何将路由信息，也即将“如何到达我这个节点，访问我这个节点上的容器”这些信息，广播出去。\n\n能想起来吗？这其实就是路由协议啊！路由协议就是将“我能到哪里，如何能到我”的信息广播给全网传出去，从而客户端可以一跳一跳地访问目标地址的。路由协议有很多种，Calico使用的是BGP协议。\n\n在Calico中，每个Node上运行一个软件BIRD，作为BGP的客户端，或者叫作BGP Speaker，将“如何到达我这个Node，访问我这个Node上的容器”的路由信息广播出去。所有Node上的BGP Speaker 都互相建立连接，就形成了全互连的情况，这样每当路由有所变化的时候，所有节点就都能够收到了。\n\n### 安全策略组件\n\nCalico中还实现了灵活配置网络策略Network Policy，可以灵活配置两个容器通或者不通。这个怎么实现呢？\n\n![img](1a0ba797b9a0f0e32c9e561b97955917-1584286910692-20230115031645234.jpg)\n\n虚拟机中的安全组，是用iptables实现的。Calico中也是用iptables实现的。这个图里的内容是iptables在内核处理网络包的过程中可以嵌入的处理点。Calico也是在这些点上设置相应的规则。\n\n![img](d27a1bf22f9b70696ca13abb6a655d15-20230115031645259.jpg)\n\n当网络包进入物理机上的时候，进入PREOUTING规则，这里面有一个规则是cali-fip-dnat，这是实现浮动IP（Floating IP）的场景，主要将外网的IP地址dnat为容器内的IP地址。在虚拟机场景下，路由器的网络namespace里面有一个外网网卡上，也设置过这样一个DNAT规则。\n\n接下来可以根据路由判断，是到本地的，还是要转发出去的。\n\n如果是本地的，走INPUT规则，里面有个规则是cali-wl-to-host，wl的意思是workload，也即容器，也即这是用来判断从容器发到物理机的网络包是否符合规则的。这里面内嵌一个规则cali-from-wl-dispatch，也是匹配从容器来的包。如果有两个容器，则会有两个容器网卡，这里面内嵌有详细的规则“cali-fw-cali网卡1”和“cali-fw-cali网卡2”，fw就是from workload，也就是匹配从容器1来的网络包和从容器2来的网络包。\n\n如果是转发出去的，走FORWARD规则，里面有个规则cali-FORWARD。这里面分两种情况，一种是从容器里面发出来，转发到外面的；另一种是从外面发进来，转发到容器里面的。\n\n第一种情况匹配的规则仍然是cali-from-wl-dispatch，也即from workload。第二种情况匹配的规则是cali-to-wl-dispatch，也即to workload。如果有两个容器，则会有两个容器网卡，在这里面内嵌有详细的规则“cali-tw-cali网卡1”和“cali-tw-cali网卡2”，tw就是to workload，也就是匹配发往容器1的网络包和发送到容器2的网络包。\n\n接下来是匹配OUTPUT规则，里面有cali-OUTPUT。接下来是POSTROUTING规则，里面有一个规则是cali-fip-snat，也即发出去的时候，将容器网络IP转换为浮动IP地址。在虚拟机场景下，路由器的网络namespace里面有一个外网网卡上，也设置过这样一个SNAT规则。\n\n至此为止，Calico的所有组件基本凑齐。来看看我汇总的图。\n\n![img](df8d92d84af55369055738283339d6b2-20230115031645280.jpg)\n\n## 全连接复杂性与规模问题\n\n这里面还存在问题，就是BGP全连接的复杂性问题。\n\n你看刚才的例子里只有六个节点，BGP的互连已经如此复杂，如果节点数据再多，这种全互连的模式肯定不行，到时候都成蜘蛛网了。于是多出了一个组件BGP Route Reflector，它也是用BIRD实现的。有了它，BGP Speaker就不用全互连了，而是都直连它，它负责将全网的路由信息广播出去。\n\n可是问题来了，规模大了，大家都连它，它受得了吗？这个BGP Router Reflector会不会成为瓶颈呢？\n\n所以，肯定不能让一个BGP Router Reflector管理所有的路由分发，而是应该有多个BGP Router Reflector，每个BGP Router Reflector管一部分。\n\n多大算一部分呢？咱们讲述数据中心的时候，说服务器都是放在机架上的，每个机架上最顶端有个TOR交换机。那将机架上的机器连在一起，这样一个机架是不是可以作为一个单元，让一个BGP Router Reflector来管理呢？如果要跨机架，如何进行通信呢？这就需要BGP Router Reflector也直接进行路由交换。它们之间的交换和一个机架之间的交换有什么关系吗？\n\n有没有觉得在这个场景下，一个机架就像一个数据中心，可以把它设置为一个AS，而BGP Router Reflector有点儿像数据中心的边界路由器。在一个AS内部，也即服务器和BGP Router Reflector之间使用的是数据中心内部的路由协议iBGP，BGP Router Reflector之间使用的是数据中心之间的路由协议eBGP。\n\n![img](f7e9467901ccb4b7e8039c53314244ff-20230115031645949.jpg)\n\n这个图中，一个机架上有多台机器，每台机器上面启动多个容器，每台机器上都有可以到达这些容器的路由。每台机器上都启动一个BGP Speaker，然后将这些路由规则上报到这个Rack上接入交换机的BGP Route Reflector，将这些路由通过iBGP协议告知到接入交换机的三层路由功能。\n\n在接入交换机之间也建立BGP连接，相互告知路由，因而一个Rack里面的路由可以告知另一个Rack。有多个核心或者汇聚交换机将接入交换机连接起来，如果核心和汇聚起二层互通的作用，则接入和接入之间之间交换路由即可。如果核心和汇聚交换机起三层路由的作用，则路由需要通过核心或者汇聚交换机进行告知。\n\n## 跨网段访问问题\n\n上面的Calico模式还有一个问题，就是跨网段问题，这里的跨网段是指物理机跨网段。\n\n前面我们说的那些逻辑成立的条件，是我们假设物理机可以作为路由器进行使用。例如物理机A要告诉物理机B，你要访问172.17.8.0/24，下一跳是我192.168.100.100；同理，物理机B要告诉物理机A，你要访问172.17.9.0/24，下一跳是我192.168.100.101。\n\n之所以能够这样，是因为物理机A和物理机B是同一个网段的，是连接在同一个交换机上的。那如果物理机A和物理机B不是在同一个网段呢？\n\n![img](88a1817b32c3c364fbbdf50b05d49e84-20230115031645994.jpg)\n\n例如，物理机A的网段是192.168.100.100/24，物理机B的网段是192.168.200.101/24，这样两台机器就不能通过二层交换机连接起来了，需要在中间放一台路由器，做一次路由转发，才能跨网段访问。\n\n本来物理机A要告诉物理机B，你要访问172.17.8.0/24，下一跳是我192.168.100.100的，但是中间多了一台路由器，下一跳不是我了，而是中间的这台路由器了，这台路由器的再下一跳，才是我。这样之前的逻辑就不成立了。\n\n我们看刚才那张图的下半部分。物理机B上的容器要访问物理机A上的容器，第一跳就是物理机B，IP为192.168.200.101，第二跳是中间的物理路由器右面的网口，IP为192.168.200.1，第三跳才是物理机A，IP为192.168.100.100。\n\n这是咱们通过拓扑图看到的，关键问题是，在系统中物理机A如何告诉物理机B，怎么让它才能到我这里？物理机A根本不可能知道从物理机B出来之后的下一跳是谁，况且现在只是中间隔着一个路由器这种简单的情况，如果隔着多个路由器呢？谁能把这一串的路径告诉物理机B呢？\n\n我们能想到的第一种方式是，让中间所有的路由器都来适配Calico。本来它们互相告知路由，只互相告知物理机的，现在还要告知容器的网段。这在大部分情况下，是不可能的。\n\n第二种方式，还是在物理机A和物理机B之间打一个隧道，这个隧道有两个端点，在端点上进行封装，将容器的IP作为乘客协议放在隧道里面，而物理主机的IP放在外面作为承载协议。这样不管外层的IP通过传统的物理网络，走多少跳到达目标物理机，从隧道两端看起来，物理机A的下一跳就是物理机B，这样前面的逻辑才能成立。\n\n这就是Calico的**IPIP模式**。使用了IPIP模式之后，在物理机A上，我们能看到这样的路由表：\n\n```bash\n172.17.8.2 dev veth1 scope link \n172.17.8.3 dev veth2 scope link \n172.17.9.0/24 via 192.168.200.101 dev tun0 proto bird onlink\n```\n\n这和原来模式的区别在于，下一跳不再是同一个网段的物理机B了，IP为192.168.200.101，并且不是从eth0跳，而是建立一个隧道的端点tun0，从这里才是下一跳。\n\n如果我们在容器A1里面的172.17.8.2，去ping容器B1里面的172.17.9.2，首先会到物理机A。在物理机A上根据上面的规则，会转发给tun0，并在这里对包做封装：\n\n- 内层源IP为172.17.8.2；\n- 内层目标IP为172.17.9.2；\n- 外层源IP为192.168.100.100；\n- 外层目标IP为192.168.200.101。\n\n将这个包从eth0发出去，在物理网络上会使用外层的IP进行路由，最终到达物理机B。在物理机B上，tun0会解封装，将内层的源IP和目标IP拿出来，转发给相应的容器。\n\n## 小结\n\n好了，这一节就到这里，我们来总结一下。\n\n- Calico推荐使用物理机作为路由器的模式，这种模式没有虚拟化开销，性能比较高。\n- Calico的主要组件包括路由、iptables的配置组件Felix、路由广播组件BGP Speaker，以及大规模场景下的BGP Route Reflector。\n- 为解决跨网段的问题，Calico还有一种IPIP模式，也即通过打隧道的方式，从隧道端点来看，将本来不是邻居的两台机器，变成相邻的机器。\n\n最后，给你留两个思考题：\n\n1. 将Calico部署在公有云上的时候，经常会选择使用IPIP模式，你知道这是为什么吗？\n2. 容器是用来部署微服务的，微服务之间的通信，除了网络要互通，还需要高效的传输信息，例如下单的商品、价格、数量、支付的钱等等，这些要通过什么样的协议呢？\n\n# 32 讲RPC协议综述：远在天边，近在眼前\n\n前面我们讲了容器网络如何实现跨主机互通，以及微服务之间的相互调用。\n\n![img](06ba300a78aef37b9d190aba61c37865-1584286945413-20230115031645295.jpg)\n\n网络是打通了，那服务之间的互相调用，该怎么实现呢？你可能说，咱不是学过Socket。服务之间分调用方和被调用方，我们就建立一个TCP或者UDP的连接，不就可以通信了？\n\n![img](77d5eeb659d5347874bda5e8f711f692-1584286945430-20230115031645313.jpg)\n\n你仔细想一下，这事儿没这么简单。我们就拿最简单的场景，客户端调用一个加法函数，将两个整数加起来，返回它们的和。\n\n如果放在本地调用，那是简单的不能再简单了，只要稍微学过一种编程语言，三下五除二就搞定了。但是一旦变成了远程调用，门槛一下子就上去了。\n\n首先你要会Socket编程，至少先要把咱们这门网络协议课学一下，然后再看N本砖头厚的Socket程序设计的书，学会咱们学过的几种Socket程序设计的模型。这就使得本来大学毕业就能干的一项工作，变成了一件五年工作经验都不一定干好的工作，而且，搞定了Socket程序设计，才是万里长征的第一步。后面还有很多问题呢！\n\n## 如何解决这五个问题？\n\n### 问题一：如何规定远程调用的语法？\n\n客户端如何告诉服务端，我是一个加法，而另一个是乘法。我是用字符串“add”传给你，还是传给你一个整数，比如1表示加法，2表示乘法？服务端该如何告诉客户端，我的这个加法，目前只能加整数，不能加小数，不能加字符串；而另一个加法“add1”，它能实现小数和整数的混合加法。那返回值是什么？正确的时候返回什么，错误的时候又返回什么？\n\n### 问题二：如果传递参数？\n\n我是先传两个整数，后传一个操作符“add”，还是先传操作符，再传两个整数？是不是像咱们数据结构里一样，如果都是UDP，想要实现一个逆波兰表达式，放在一个报文里面还好，如果是TCP，是一个流，在这个流里面，如何将两次调用进行分界？什么时候是头，什么时候是尾？别这次的参数和上次的参数混了起来，TCP一端发送出去的数据，另外一端不一定能一下子全部读取出来。所以，怎么才算读完呢？\n\n### 问题三：如何表示数据？\n\n在这个简单的例子中，传递的就是一个固定长度的int值，这种情况还好，如果是变长的类型，是一个结构体，甚至是一个类，应该怎么办呢？如果是int，不同的平台上长度也不同，该怎么办呢？\n\n在网络上传输超过一个Byte的类型，还有大端Big Endian和小端Little Endian的问题。\n\n假设我们要在32位四个Byte的一个空间存放整数1，很显然只要一个Byte放1，其他三个Byte放0就可以了。那问题是，最后一个Byte放1呢，还是第一个Byte放1呢？或者说1作为最低位，应该是放在32位的最后一个位置呢，还是放在第一个位置呢？\n\n最低位放在最后一个位置，叫作Little Endian，最低位放在第一个位置，叫作Big Endian。TCP/IP协议栈是按照Big Endian来设计的，而X86机器多按照Little Endian来设计的，因而发出去的时候需要做一个转换。\n\n### 问题四：如何知道一个服务端都实现了哪些远程调用？从哪个端口可以访问这个远程调用？\n\n假设服务端实现了多个远程调用，每个可能实现在不同的进程中，监听的端口也不一样，而且由于服务端都是自己实现的，不可能使用一个大家都公认的端口，而且有可能多个进程部署在一台机器上，大家需要抢占端口，为了防止冲突，往往使用随机端口，那客户端如何找到这些监听的端口呢？\n\n### 问题五：发生了错误、重传、丢包、性能等问题怎么办？\n\n本地调用没有这个问题，但是一旦到网络上，这些问题都需要处理，因为网络是不可靠的，虽然在同一个连接中，我们还可通过TCP协议保证丢包、重传的问题，但是如果服务器崩溃了又重启，当前连接断开了，TCP就保证不了了，需要应用自己进行重新调用，重新传输会不会同样的操作做两遍，远程调用性能会不会受影响呢？\n\n## 协议约定问题\n\n看到这么多问题，你是不是想起了我[第一节]讲过的这张图。\n\n![img](984b421d4e13d42e2b0500d0427d94ab-20230115031645419.jpg)\n\n本地调用函数里有很多问题，比如词法分析、语法分析、语义分析等等，这些编译器本来都能帮你做了。但是在远程调用中，这些问题你都需要重新操心。\n\n很多公司的解决方法是，弄一个核心通信组，里面都是Socket编程的大牛，实现一个统一的库，让其他业务组的人来调用，业务的人不需要知道中间传输的细节。通信双方的语法、语义、格式、端口、错误处理等，都需要调用方和被调用方开会商量，双方达成一致。一旦有一方改变，要及时通知对方，否则通信就会有问题。\n\n可是不是每一个公司都有这种大牛团队，往往只有大公司才配得起，那有没有已经实现好的框架可以使用呢？\n\n当然有。一个大牛Bruce Jay Nelson写了一篇论文[Implementing Remote Procedure Calls](https://www.cs.cmu.edu/~dga/15-712/F07/papers/birrell842.pdf)，定义了RPC的调用标准。后面所有RPC框架，都是按照这个标准模式来的。\n\n![img](8534c52daf3682cd1cfe5a3375ec9525-20230115031645347.jpg)\n\n当客户端的应用想发起一个远程调用时，它实际是通过本地调用本地调用方的Stub。它负责将调用的接口、方法和参数，通过约定的协议规范进行编码，并通过本地的RPCRuntime进行传输，将调用网络包发送到服务器。\n\n服务器端的RPCRuntime收到请求后，交给提供方Stub进行解码，然后调用服务端的方法，服务端执行方法，返回结果，提供方Stub将返回结果编码后，发送给客户端，客户端的RPCRuntime收到结果，发给调用方Stub解码得到结果，返回给客户端。\n\n这里面分了三个层次，对于用户层和服务端，都像是本地调用一样，专注于业务逻辑的处理就可以了。对于Stub层，处理双方约定好的语法、语义、封装、解封装。对于RPCRuntime，主要处理高性能的传输，以及网络的错误和异常。\n\n最早的RPC的一种实现方式称为Sun RPC或ONC RPC。Sun公司是第一个提供商业化RPC库和 RPC编译器的公司。这个RPC框架是在NFS协议中使用的。\n\nNFS（Network File System）就是网络文件系统。要使NFS成功运行，要启动两个服务端，一个是mountd，用来挂载文件路径；一个是nfsd，用来读写文件。NFS可以在本地mount一个远程的目录到本地的一个目录，从而本地的用户在这个目录里面写入、读出任何文件的时候，其实操作的是远程另一台机器上的文件。\n\n操作远程和远程调用的思路是一样的，就像操作本地一样。所以NFS协议就是基于RPC实现的。当然无论是什么RPC，底层都是Socket编程。\n\n![img](2a0fd84c2d3dced623511e2a5226d0eb-20230115031645393.jpg)\n\nXDR（External Data Representation，外部数据表示法）是一个标准的数据压缩格式，可以表示基本的数据类型，也可以表示结构体。\n\n这里是几种基本的数据类型。\n\n![img](4a649954fea1cee22fcfa8bdb34c03af-20230115031645475.jpg)\n\n在RPC的调用过程中，所有的数据类型都要封装成类似的格式。而且RPC的调用和结果返回，也有严格的格式。\n\n- XID唯一标识一对请求和回复。请求为0，回复为1。\n- RPC有版本号，两端要匹配RPC协议的版本号。如果不匹配，就会返回Deny，原因就是RPC_MISMATCH。\n- 程序有编号。如果服务端找不到这个程序，就会返回PROG_UNAVAIL。\n- 程序有版本号。如果程序的版本号不匹配，就会返回PROG_MISMATCH。\n- 一个程序可以有多个方法，方法也有编号，如果找不到方法，就会返回PROC_UNAVAIL。\n- 调用需要认证鉴权，如果不通过，则Deny。\n- 最后是参数列表，如果参数无法解析，则返回GABAGE_ARGS。\n\n![img](c724675527afdbd43964bdf24684fa65-20230115031645508.jpg)\n\n为了可以成功调用RPC，在客户端和服务端实现RPC的时候，首先要定义一个双方都认可的程序、版本、方法、参数等。\n\n![img](5c3ebb31ac4415d7895247bf8758fa58-20230115031645644.jpg)\n\n如果还是上面的加法，则双方约定为一个协议定义文件，同理如果是NFS、mount和读写，也会有类似的定义。\n\n有了协议定义文件，ONC RPC会提供一个工具，根据这个文件生成客户端和服务器端的Stub程序。\n\n![img](27dc1ccd0481408055c87e0e5d8b02b9-20230115031645968.jpg)\n\n最下层的是XDR文件，用于编码和解码参数。这个文件是客户端和服务端共享的，因为只有双方一致才能成功通信。\n\n在客户端，会调用clnt_create创建一个连接，然后调用add_1，这是一个Stub函数，感觉是在调用本地一样。其实是这个函数发起了一个RPC调用，通过调用clnt_call来调用ONC RPC的类库，来真正发送请求。调用的过程非常复杂，一会儿我详细说这个。\n\n当然服务端也有一个Stub程序，监听客户端的请求，当调用到达的时候，判断如果是add，则调用真正的服务端逻辑，也即将两个数加起来。\n\n服务端将结果返回服务端的Stub，这个Stub程序发送结果给客户端，客户端的Stub程序正在等待结果，当结果到达客户端Stub，就将结果返回给客户端的应用程序，从而完成整个调用过程。\n\n有了这个RPC的框架，前面五个问题中的前三个“如何规定远程调用的语法？”“如何传递参数？”以及“如何表示数据？”基本解决了，这三个问题我们统称为**协议约定问题**。\n\n## 传输问题\n\n但是错误、重传、丢包、性能等问题还没有解决，这些问题我们统称为**传输问题**。这个就不用Stub操心了，而是由ONC RPC的类库来实现。这是大牛们实现的，我们只要调用就可以了。\n\n![img](33e1afe4a79e81096e09b850424930e4-20230115031645969.jpg)\n\n在这个类库中，为了解决传输问题，对于每一个客户端，都会创建一个传输管理层，而每一次RPC调用，都会是一个任务，在传输管理层，你可以看到熟悉的队列机制、拥塞窗口机制等。\n\n由于在网络传输的时候，经常需要等待，因而同步的方式往往效率比较低，因而也就有Socket的异步模型。为了能够异步处理，对于远程调用的处理，往往是通过状态机来实现的。只有当满足某个状态的时候，才进行下一步，如果不满足状态，不是在那里等，而是将资源留出来，用来处理其他的RPC调用。\n\n![img](0258775aac1126735504c9a6399745f5-20230115031645992.jpg)\n\n从这个图可以看出，这个状态转换图还是很复杂的。\n\n首先，进入起始状态，查看RPC的传输层队列中有没有空闲的位置，可以处理新的RPC任务。如果没有，说明太忙了，或直接结束或重试。如果申请成功，就可以分配内存，获取服务的端口号，然后连接服务器。\n\n连接的过程要有一段时间，因而要等待连接的结果，会有连接失败，或直接结束或重试。如果连接成功，则开始发送RPC请求，然后等待获取RPC结果，这个过程也需要一定的时间；如果发送出错，可以重新发送；如果连接断了，可以重新连接；如果超时，可以重新传输；如果获取到结果，就可以解码，正常结束。\n\n这里处理了连接失败、重试、发送失败、超时、重试等场景。不是大牛真写不出来，因而实现一个RPC的框架，其实很有难度。\n\n## 服务发现问题\n\n传输问题解决了，我们还遗留一个问题，就是问题四“如何找到RPC服务端的那个随机端口”。这个问题我们称为服务发现问题。在ONC RPC中，服务发现是通过portmapper实现的。\n\n![img](2aff190d1f878749d2a5bd73228ca37c-20230115031646103.jpg)\n\nportmapper会启动在一个众所周知的端口上，RPC程序由于是用户自己写的，会监听在一个随机端口上，但是RPC程序启动的时候，会向portmapper注册。客户端要访问RPC服务端这个程序的时候，首先查询portmapper，获取RPC服务端程序的随机端口，然后向这个随机端口建立连接，开始RPC调用。从图中可以看出，mount命令的RPC调用，就是这样实现的。\n\n## 小结\n\n好了，这一节就到这里，我们来总结一下。\n\n- 远程调用看起来用Socket编程就可以了，其实是很复杂的，要解决协议约定问题、传输问题和服务发现问题。\n- 大牛Bruce Jay Nelson的论文、早期ONC RPC框架，以及NFS的实现，给出了解决这三大问题的示范性实现，也即协议约定要公用协议描述文件，并通过这个文件生成Stub程序；RPC的传输一般需要一个状态机，需要另外一个进程专门做服务发现。\n\n最后，给你留两个思考题。\n\n1. 在这篇文章中，mount的过程是通过系统调用，最终调用到RPC层。一旦mount完毕之后，客户端就像写入本地文件一样写入NFS了，这个过程是如何触发RPC层的呢？\n2. ONC RPC是早期的RPC框架，你觉得它有哪些问题呢？\n\n# 33 讲基于XML的SOAP协议：不要说NBA，请说美国职业篮球联赛\n\n上一节我们讲了RPC的经典模型和设计要点，并用最早期的ONC RPC为例子，详述了具体的实现。\n\n## ONC RPC存在哪些问题？\n\nONC RPC将客户端要发送的参数，以及服务端要发送的回复，都压缩为一个二进制串，这样固然能够解决双方的协议约定问题，但是存在一定的不方便。\n\n首先，**需要双方的压缩格式完全一致**，一点都不能差。一旦有少许的差错，多一位，少一位或者错一位，都可能造成无法解压缩。当然，我们可以用传输层的可靠性以及加入校验值等方式，来减少传输过程中的差错。\n\n其次，**协议修改不灵活**。如果不是传输过程中造成的差错，而是客户端因为业务逻辑的改变，添加或者删除了字段，或者服务端添加或者删除了字段，而双方没有及时通知，或者线上系统没有及时升级，就会造成解压缩不成功。\n\n因而，当业务发生改变，需要多传输一些参数或者少传输一些参数的时候，都需要及时通知对方，并且根据约定好的协议文件重新生成双方的Stub程序。自然，这样灵活性比较差。\n\n如果仅仅是沟通的问题也还好解决，其实更难弄的还有**版本的问题**。比如在服务端提供一个服务，参数的格式是版本一的，已经有50个客户端在线上调用了。现在有一个客户端有个需求，要加一个字段，怎么办呢？这可是一个大工程，所有的客户端都要适配这个，需要重新写程序，加上这个字段，但是传输值是0，不需要这个字段的客户端很“冤”，本来没我啥事儿，为啥让我也忙活？\n\n最后，**ONC RPC的设计明显是面向函数的，而非面向对象**。而当前面向对象的业务逻辑设计与实现方式已经成为主流。\n\n这一切的根源就在于压缩。这就像平时我们爱用缩略语。如果是篮球爱好者，你直接说NBA，他马上就知道什么意思，但是如果你给一个大妈说NBA，她可能就不知所云。\n\n所以，这种RPC框架只能用于客户端和服务端全由一拨人开发的场景，或者至少客户端和服务端的开发人员要密切沟通，相互合作，有大量的共同语言，才能按照既定的协议顺畅地进行工作。\n\n## XML与SOAP\n\n但是，一般情况下，我们做一个服务，都是要提供给陌生人用的，你和客户不会经常沟通，也没有什么共同语言。就像你给别人介绍NBA，你要说美国职业篮球赛，这样不管他是干啥的，都能听得懂。\n\n放到我们的场景中，对应的就是用**文本类**的方式进行传输。无论哪个客户端获得这个文本，都能够知道它的意义。\n\n一种常见的文本类格式是XML。我们这里举个例子来看。\n\n```xml\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e\n\u003cgeek:purchaseOrder xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:geek=\"http://www.example.com/geek\"\u003e\n    \u003corder\u003e\n        \u003cdate\u003e2018-07-01\u003c/date\u003e\n        \u003cclassName\u003e趣谈网络协议\u003c/className\u003e\n        \u003cAuthor\u003e刘超\u003c/Author\u003e\n        \u003cprice\u003e68\u003c/price\u003e\n    \u003c/order\u003e\n\u003c/geek:purchaseOrder\u003e\n```\n\n我这里不准备详细讲述XML的语法规则，但是你相信我，看完下面的内容，即便你没有学过XML，也能一看就懂，这段XML描述的是什么，不像全面的二进制，你看到的都是010101，不知所云。\n\n有了这个，刚才我们说的那几个问题就都不是问题了。\n\n首先，**格式没必要完全一致**。比如如果我们把price和author换个位置，并不影响客户端和服务端解析这个文本，也根本不会误会，说这个作者的名字叫68。\n\n如果有的客户端想增加一个字段，例如添加一个推荐人字段，只需要在上面的文件中加一行：\n\n```xml\n\u003crecommended\u003e Gary \u003c/recommended\u003e \n```\n\n对于不需要这个字段的客户端，只要不解析这一行就是了。只要用简单的处理，就不会出现错误。\n\n另外，这种表述方式显然是描述一个订单对象的，是一种面向对象的、更加接近用户场景的表示方式。\n\n既然XML这么好，接下来我们来看看怎么把它用在RPC中。\n\n### 传输协议问题\n\n我们先解决第一个，传输协议的问题。\n\n基于XML的最著名的通信协议就是**SOAP**了，全称**简单对象访问协议**（Simple Object Access Protocol）。它使用XML编写简单的请求和回复消息，并用HTTP协议进行传输。\n\nSOAP将请求和回复放在一个信封里面，就像传递一个邮件一样。信封里面的信分**抬头**和**正文**。\n\n```xml\nPOST /purchaseOrder HTTP/1.1\nHost: www.geektime.com\nContent-Type: application/soap+xml; charset=utf-8\nContent-Length: nnn\n\u003c?xml version=\"1.0\"?\u003e\n\u003csoap:Envelope xmlns:soap=\"http://www.w3.org/2001/12/soap-envelope\"\nsoap:encodingStyle=\"http://www.w3.org/2001/12/soap-encoding\"\u003e\n    \u003csoap:Header\u003e\n        \u003cm:Trans xmlns:m=\"http://www.w3schools.com/transaction/\"\n          soap:mustUnderstand=\"1\"\u003e1234\n        \u003c/m:Trans\u003e\n    \u003c/soap:Header\u003e\n    \u003csoap:Body xmlns:m=\"http://www.geektime.com/perchaseOrder\"\u003e\n        \u003cm:purchaseOrder\"\u003e\n            \u003corder\u003e\n                \u003cdate\u003e2018-07-01\u003c/date\u003e\n                \u003cclassName\u003e趣谈网络协议\u003c/className\u003e\n                \u003cAuthor\u003e刘超\u003c/Author\u003e\n                \u003cprice\u003e68\u003c/price\u003e\n            \u003c/order\u003e\n        \u003c/m:purchaseOrder\u003e\n    \u003c/soap:Body\u003e\n\u003c/soap:Envelope\u003e\n```\n\nHTTP协议我们学过，这个请求使用POST方法，发送一个格式为 application/soap + xml 的XML正文给 [www.geektime.com](https://www.geektime.com/)，从而下一个单，这个订单封装在SOAP的信封里面，并且表明这是一笔交易（transaction），而且订单的详情都已经写明了。\n\n### 协议约定问题\n\n接下来我们解决第二个问题，就是双方的协议约定是什么样的？\n\n因为服务开发出来是给陌生人用的，就像上面下单的那个XML文件，对于客户端来说，它如何知道应该拼装成上面的格式呢？这就需要对于服务进行描述，因为调用的人不认识你，所以没办法找到你，问你的服务应该如何调用。\n\n当然你可以写文档，然后放在官方网站上，但是你的文档不一定更新得那么及时，而且你也写的文档也不一定那么严谨，所以常常会有调试不成功的情况。因而，我们需要一种相对比较严谨的**Web服务描述语言**，**WSDL**（Web Service Description Languages）。它也是一个XML文件。\n\n在这个文件中，要定义一个类型order，与上面的XML对应起来。\n\n```xml\n \u003cwsdl:types\u003e\n  \u003cxsd:schema targetNamespace=\"http://www.example.org/geektime\"\u003e\n   \u003cxsd:complexType name=\"order\"\u003e\n    \u003cxsd:element name=\"date\" type=\"xsd:string\"\u003e\u003c/xsd:element\u003e\n\u003cxsd:element name=\"className\" type=\"xsd:string\"\u003e\u003c/xsd:element\u003e\n\u003cxsd:element name=\"Author\" type=\"xsd:string\"\u003e\u003c/xsd:element\u003e\n    \u003cxsd:element name=\"price\" type=\"xsd:int\"\u003e\u003c/xsd:element\u003e\n   \u003c/xsd:complexType\u003e\n  \u003c/xsd:schema\u003e\n \u003c/wsdl:types\u003e\n```\n\n接下来，需要定义一个message的结构。\n\n```xml\n \u003cwsdl:message name=\"purchase\"\u003e\n  \u003cwsdl:part name=\"purchaseOrder\" element=\"tns:order\"\u003e\u003c/wsdl:part\u003e\n \u003c/wsdl:message\u003e\n```\n\n接下来，应该暴露一个端口。\n\n```xml\n \u003cwsdl:portType name=\"PurchaseOrderService\"\u003e\n  \u003cwsdl:operation name=\"purchase\"\u003e\n   \u003cwsdl:input message=\"tns:purchase\"\u003e\u003c/wsdl:input\u003e\n   \u003cwsdl:output message=\"......\"\u003e\u003c/wsdl:output\u003e\n  \u003c/wsdl:operation\u003e\n \u003c/wsdl:portType\u003e\n```\n\n然后，我们来编写一个binding，将上面定义的信息绑定到SOAP请求的body里面。\n\n```xml\n \u003cwsdl:binding name=\"purchaseOrderServiceSOAP\" type=\"tns:PurchaseOrderService\"\u003e\n  \u003csoap:binding style=\"rpc\"\n   transport=\"http://schemas.xmlsoap.org/soap/http\" /\u003e\n  \u003cwsdl:operation name=\"purchase\"\u003e\n   \u003cwsdl:input\u003e\n    \u003csoap:body use=\"literal\" /\u003e\n   \u003c/wsdl:input\u003e\n   \u003cwsdl:output\u003e\n    \u003csoap:body use=\"literal\" /\u003e\n   \u003c/wsdl:output\u003e\n  \u003c/wsdl:operation\u003e\n \u003c/wsdl:binding\u003e\n```\n\n最后，我们需要编写service。\n\n```xml\n \u003cwsdl:service name=\"PurchaseOrderServiceImplService\"\u003e\n  \u003cwsdl:port binding=\"tns:purchaseOrderServiceSOAP\" name=\"PurchaseOrderServiceImplPort\"\u003e\n   \u003csoap:address location=\"http://www.geektime.com:8080/purchaseOrder\" /\u003e\n  \u003c/wsdl:port\u003e\n \u003c/wsdl:service\u003e\n```\n\nWSDL还是有些复杂的，不过好在有工具可以生成。\n\n对于某个服务，哪怕是一个陌生人，都可以通过在服务地址后面加上“?wsdl”来获取到这个文件，但是这个文件还是比较复杂，比较难以看懂。不过好在也有工具可以根据WSDL生成客户端Stub，让客户端通过Stub进行远程调用，就跟调用本地的方法一样。\n\n### 服务发现问题\n\n最后解决第三个问题，服务发现问题。\n\n这里有一个**UDDI**（Universal Description, Discovery, and Integration），也即**统一描述、发现和集成协议**。它其实是一个注册中心，服务提供方可以将上面的WSDL描述文件，发布到这个注册中心，注册完毕后，服务使用方可以查找到服务的描述，封装为本地的客户端进行调用。\n\n## 小结\n\n好了，这一节就到这里了，我们来总结一下。\n\n- 原来的二进制RPC有很多缺点，格式要求严格，修改过于复杂，不面向对象，于是产生了基于文本的调用方式——基于XML的SOAP。\n- SOAP有三大要素：协议约定用WSDL、传输协议用HTTP、服务发现用UDDL。\n\n最后，给你留两个思考题：\n\n1. 对于HTTP协议来讲，有多种方法，但是SOAP只用了POST，这样会有什么问题吗？\n2. 基于文本的RPC虽然解决了二进制的问题，但是SOAP还是有点复杂，还有一种更便捷的接口规则，你知道是什么吗？\n\n# 34 讲基于JSON的RESTful接口协议：我不关心过程，请给我结果\n\n上一节我们讲了基于XML的SOAP协议，SOAP的S是啥意思来着？是Simple，但是好像一点儿都不简单啊！\n\n你会发现，对于SOAP来讲，无论XML中调用的是什么函数，多是通过HTTP的POST方法发送的。但是咱们原来学HTTP的时候，我们知道HTTP除了POST，还有PUT、DELETE、GET等方法，这些也可以代表一个个动作，而且基本满足增、删、查、改的需求，比如增是POST，删是DELETE，查是GET，改是PUT。\n\n## 传输协议问题\n\n对于SOAP来讲，比如我创建一个订单，用POST，在XML里面写明动作是CreateOrder；删除一个订单，还是用POST，在XML里面写明了动作是DeleteOrder。其实创建订单完全可以使用POST动作，然后在XML里面放一个订单的信息就可以了，而删除用DELETE动作，然后在XML里面放一个订单的ID就可以了。\n\n于是上面的那个SOAP就变成下面这个简单的模样。\n\n```xml\nPOST /purchaseOrder HTTP/1.1\nHost: www.geektime.com\nContent-Type: application/xml; charset=utf-8\nContent-Length: nnn\n\n\u003c?xml version=\"1.0\"?\u003e\n \u003corder\u003e\n     \u003cdate\u003e2018-07-01\u003c/date\u003e\n      \u003cclassName\u003e趣谈网络协议\u003c/className\u003e\n       \u003cAuthor\u003e刘超\u003c/Author\u003e\n       \u003cprice\u003e68\u003c/price\u003e\n  \u003c/order\u003e\n```\n\n而且XML的格式也可以改成另外一种简单的文本化的对象表示格式JSON。\n\n```makefile\nPOST /purchaseOrder HTTP/1.1\nHost: www.geektime.com\nContent-Type: application/json; charset=utf-8\nContent-Length: nnn\n\n{\n \"order\": {\n  \"date\": \"2018-07-01\",\n  \"className\": \"趣谈网络协议\",\n  \"Author\": \"刘超\",\n  \"price\": \"68\"\n }\n}\n```\n\n经常写Web应用的应该已经发现，这就是RESTful格式的API的样子。\n\n## 协议约定问题\n\n然而RESTful可不仅仅是指API，而是一种架构风格，全称Representational State Transfer，表述性状态转移，来自一篇重要的论文《架构风格与基于网络的软件架构设计》（Architectural Styles and the Design of Network-based Software Architectures）。\n\n这篇文章从深层次，更加抽象地论证了一个互联网应用应该有的设计要点，而这些设计要点，成为后来我们能看到的所有高并发应用设计都必须要考虑的问题，再加上REST API比较简单直接，所以后来几乎成为互联网应用的标准接口。\n\n因此，和SOAP不一样，REST不是一种严格规定的标准，它其实是一种设计风格。如果按这种风格进行设计，RESTful接口和SOAP接口都能做到，只不过后面的架构是REST倡导的，而SOAP相对比较关注前面的接口。\n\n而且由于能够通过WSDL生成客户端的Stub，因而SOAP常常被用于类似传统的RPC方式，也即调用远端和调用本地是一样的。\n\n然而本地调用和远程跨网络调用毕竟不一样，这里的不一样还不仅仅是因为有网络而导致的客户端和服务端的分离，从而带来的网络性能问题。更重要的问题是，客户端和服务端谁来维护状态。所谓的状态就是对某个数据当前处理到什么程度了。\n\n这里举几个例子，例如，我浏览到哪个目录了，我看到第几页了，我要买个东西，需要扣减一下库存，这些都是状态。本地调用其实没有人纠结这个问题，因为数据都在本地，谁处理都一样，而且一边处理了，另一边马上就能看到。\n\n当有了RPC之后，我们本来期望对上层透明，就像上一节说的“远在天边，尽在眼前”。于是使用RPC的时候，对于状态的问题也没有太多的考虑。\n\n就像NFS一样，客户端会告诉服务端，我要进入哪个目录，服务端必须要为某个客户端维护一个状态，就是当前这个客户端浏览到哪个目录了。例如，客户端输入cd hello，服务端要在某个地方记住，上次浏览到/root/liuchao了，因而客户的这次输入，应该给它显示/root/liuchao/hello下面的文件列表。而如果有另一个客户端，同样输入cd hello，服务端也在某个地方记住，上次浏览到/var/lib，因而要给客户显示的是/var/lib/hello。\n\n不光NFS，如果浏览翻页，我们经常要实现函数next()，在一个列表中取下一页，但是这就需要服务端记住，客户端A上次浏览到20～30页了，那它调用next()，应该显示30～40页，而客户端B上次浏览到100～110页了，调用next()应该显示110～120页。\n\n上面的例子都是在RPC场景下，由服务端来维护状态，很多SOAP接口设计的时候，也常常按这种模式。这种模式原来没有问题，是因为客户端和服务端之间的比例没有失衡。因为一般不会同时有太多的客户端同时连上来，所以NFS还能把每个客户端的状态都记住。\n\n公司内部使用的ERP系统，如果使用SOAP的方式实现，并且服务端为每个登录的用户维护浏览到报表那一页的状态，由于一个公司内部的人也不会太多，把ERP放在一个强大的物理机上，也能记得过来。\n\n但是互联网场景下，客户端和服务端就彻底失衡了。你可以想象“双十一”，多少人同时来购物，作为服务端，它能记得过来吗？当然不可能，只好多个服务端同时提供服务，大家分担一下。但是这就存在一个问题，服务端怎么把自己记住的客户端状态告诉另一个服务端呢？或者说，你让我给你分担工作，你也要把工作的前因后果给我说清楚啊！\n\n那服务端索性就要想了，既然这么多客户端，那大家就分分工吧。服务端就只记录资源的状态，例如文件的状态，报表的状态，库存的状态，而客户端自己维护自己的状态。比如，你访问到哪个目录了啊，报表的哪一页了啊，等等。\n\n这样对于API也有影响，也就是说，当客户端维护了自己的状态，就不能这样调用服务端了。例如客户端说，我想访问当前目录下的hello路径。服务端说，我怎么知道你的当前路径。所以客户端要先看看自己当前路径是/root/liuchao，然后告诉服务端说，我想访问/root/liuchao/hello路径。\n\n再比如，客户端说我想访问下一页，服务端说，我怎么知道你当前访问到哪一页了。所以客户端要先看看自己访问到了100～110页，然后告诉服务器说，我想访问110～120页。\n\n这就是服务端的无状态化。这样服务端就可以横向扩展了，一百个人一起服务，不用交接，每个人都能处理。\n\n所谓的无状态，其实是服务端维护资源的状态，客户端维护会话的状态。对于服务端来讲，只有资源的状态改变了，客户端才调用POST、PUT、DELETE方法来找我；如果资源的状态没变，只是客户端的状态变了，就不用告诉我了，对于我来说都是统一的GET。\n\n虽然这只改进了GET，但是已经带来了很大的进步。因为对于互联网应用，大多数是读多写少的。而且只要服务端的资源状态不变，就给了我们缓存的可能。例如可以将状态缓存到接入层，甚至缓存到CDN的边缘节点，这都是资源状态不变的好处。\n\n按照这种思路，对于API的设计，就慢慢变成了以资源为核心，而非以过程为核心。也就是说，客户端只要告诉服务端你想让资源状态最终变成什么样就可以了，而不用告诉我过程，不用告诉我动作。\n\n还是文件目录的例子。客户端应该访问哪个绝对路径，而非一个动作，我就要进入某个路径。再如，库存的调用，应该查看当前的库存数目，然后减去购买的数量，得到结果的库存数。这个时候应该设置为目标库存数（但是当前库存数要匹配），而非告知减去多少库存。\n\n这种API的设计需要实现幂等，因为网络不稳定，就会经常出错，因而需要重试，但是一旦重试，就会存在幂等的问题，也就是同一个调用，多次调用的结果应该一样，不能一次支付调用，因为调用三次变成了支付三次。不能进入cd a，做了三次，就变成了cd a/a/a。也不能扣减库存，调用了三次，就扣减三次库存。\n\n当然按照这种设计模式，无论RESTful API还是SOAP API都可以将架构实现成无状态的，面向资源的、幂等的、横向扩展的、可缓存的。\n\n但是SOAP的XML正文中，是可以放任何动作的。例如XML里面可以写\u003c ADD \u003e，\u003c MINUS \u003e等。这就方便使用SOAP的人，将大量的动作放在API里面。\n\nRESTful没这么复杂，也没给客户提供这么多的可能性，正文里的JSON基本描述的就是资源的状态，没办法描述动作，而且能够出发的动作只有CRUD，也即POST、GET、PUT、DELETE，也就是对于状态的改变。\n\n所以，从接口角度，就让你死了这条心。当然也有很多技巧的方法，在使用RESTful API的情况下，依然提供基于动作的有状态请求，这属于反模式了。\n\n## 服务发现问题\n\n对于RESTful API来讲，我们已经解决了传输协议的问题——基于HTTP，协议约定问题——基于JSON，最后要解决的是服务发现问题。\n\n有个著名的基于RESTful API的跨系统调用框架叫Spring Cloud。在Spring Cloud中有一个组件叫 Eureka。传说，阿基米德在洗澡时发现浮力原理，高兴得来不及穿上裤子，跑到街上大喊：“Eureka（我找到了）！”所以Eureka是用来实现注册中心的，负责维护注册的服务列表。\n\n服务分服务提供方，它向Eureka做服务注册、续约和下线等操作，注册的主要数据包括服务名、机器IP、端口号、域名等等。\n\n另外一方是服务消费方，向Eureka获取服务提供方的注册信息。为了实现负载均衡和容错，服务提供方可以注册多个。\n\n当消费方要调用服务的时候，会从注册中心读出多个服务来，那怎么调用呢？当然是RESTful方式了。\n\nSpring Cloud提供一个RestTemplate工具，用于将请求对象转换为JSON，并发起Rest调用，RestTemplate的调用也是分POST、PUT、GET、 DELETE的，当结果返回的时候，根据返回的JSON解析成对象。\n\n通过这样封装，调用起来也很方便。\n\n## 小结\n\n好了，这一节就到这里了，我们来总结一下。\n\n- SOAP过于复杂，而且设计是面向动作的，因而往往因为架构问题导致并发量上不去。\n- RESTful不仅仅是一个API，而且是一种架构模式，主要面向资源，提供无状态服务，有利于横向扩展应对高并发。\n\n最后，给你留两个思考题：\n\n1. 在讨论RESTful模型的时候，举了一个库存的例子，但是这种方法有很大问题，那你知道为什么要这样设计吗？\n2. 基于文本的RPC虽然解决了二进制的问题，但是它本身也有问题，你能举出一些例子吗？\n\n# 35 讲二进制类RPC协议：还是叫NBA吧，总说全称多费劲\n\n前面我们讲了两个常用文本类的RPC协议，对于陌生人之间的沟通，用NBA、CBA这样的缩略语，会使得协议约定非常不方便。\n\n在讲CDN和DNS的时候，我们讲过接入层的设计，对于静态资源或者动态资源静态化的部分都可以做缓存。但是对于下单、支付等交易场景，还是需要调用API。\n\n对于微服务的架构，API需要一个API网关统一的管理。API网关有多种实现方式，用Nginx或者OpenResty结合Lua脚本是常用的方式。在上一节讲过的Spring Cloud体系中，有个组件Zuul也是干这个的。\n\n## 数据中心内部是如何相互调用的？\n\nAPI网关用来管理API，但是API的实现一般在一个叫作**Controller层**的地方。这一层对外提供API。由于是让陌生人访问的，我们能看到目前业界主流的，基本都是RESTful的API，是面向大规模互联网应用的。\n\n![img](f08ef51889add2c26c57c9edd3db93b8-20230115031646413.jpg)\n\n在Controller之内，就是咱们互联网应用的业务逻辑实现。上节讲RESTful的时候，说过业务逻辑的实现最好是无状态的，从而可以横向扩展，但是资源的状态还需要服务端去维护。资源的状态不应该维护在业务逻辑层，而是在最底层的持久化层，一般会使用分布式数据库和ElasticSearch。\n\n这些服务端的状态，例如订单、库存、商品等，都是重中之重，都需要持久化到硬盘上，数据不能丢，但是由于硬盘读写性能差，因而持久化层往往吞吐量不能达到互联网应用要求的吞吐量，因而前面要有一层缓存层，使用Redis或者memcached将请求拦截一道，不能让所有的请求都进入数据库“中军大营”。\n\n缓存和持久化层之上一般是**基础服务层**，这里面提供一些原子化的接口。例如，对于用户、商品、订单、库存的增删查改，将缓存和数据库对再上层的业务逻辑屏蔽一道。有了这一层，上层业务逻辑看到的都是接口，而不会调用数据库和缓存。因而对于缓存层的扩容，数据库的分库分表，所有的改变，都截止到这一层，这样有利于将来对于缓存和数据库的运维。\n\n再往上就是**组合层**。因为基础服务层只是提供简单的接口，实现简单的业务逻辑，而复杂的业务逻辑，比如下单，要扣优惠券，扣减库存等，就要在组合服务层实现。\n\n这样，Controller层、组合服务层、基础服务层就会相互调用，这个调用是在数据中心内部的，量也会比较大，还是使用RPC的机制实现的。\n\n由于服务比较多，需要一个单独的注册中心来做服务发现。服务提供方会将自己提供哪些服务注册到注册中心中去，同时服务消费方订阅这个服务，从而可以对这个服务进行调用。\n\n调用的时候有一个问题，这里的RPC调用，应该用二进制还是文本类？其实文本的最大问题是，占用字节数目比较多。比如数字123，其实本来二进制8位就够了，但是如果变成文本，就成了字符串123。如果是UTF-8编码的话，就是三个字节；如果是UTF-16，就是六个字节。同样的信息，要多费好多的空间，传输起来也更加占带宽，时延也高。\n\n因而对于数据中心内部的相互调用，很多公司选型的时候，还是希望采用更加省空间和带宽的二进制的方案。\n\n这里一个著名的例子就是Dubbo服务化框架二进制的RPC方式。\n\n![img](c622af64f47e264453088e79c3e631c2-20230115031646039.jpg)\n\nDubbo会在客户端的本地启动一个Proxy，其实就是客户端的Stub，对于远程的调用都通过这个Stub进行封装。\n\n接下来，Dubbo会从注册中心获取服务端的列表，根据路由规则和负载均衡规则，在多个服务端中选择一个最合适的服务端进行调用。\n\n调用服务端的时候，首先要进行编码和序列化，形成Dubbo头和序列化的方法和参数。将编码好的数据，交给网络客户端进行发送，网络服务端收到消息后，进行解码。然后将任务分发给某个线程进行处理，在线程中会调用服务端的代码逻辑，然后返回结果。\n\n这个过程和经典的RPC模式何其相似啊！\n\n## 如何解决协议约定问题？\n\n接下来我们还是来看RPC的三大问题，其中注册发现问题已经通过注册中心解决了。我们下面就来看协议约定问题。\n\nDubbo中默认的RPC协议是Hessian2。为了保证传输的效率，Hessian2将远程调用序列化为二进制进行传输，并且可以进行一定的压缩。这个时候你可能会疑惑，同为二进制的序列化协议，Hessian2和前面的二进制的RPC有什么区别呢？这不绕了一圈又回来了吗？\n\nHessian2是解决了一些问题的。例如，原来要定义一个协议文件，然后通过这个文件生成客户端和服务端的Stub，才能进行相互调用，这样使得修改就会不方便。Hessian2不需要定义这个协议文件，而是自描述的。什么是自描述呢？\n\n所谓自描述就是，关于调用哪个函数，参数是什么，另一方不需要拿到某个协议文件、拿到二进制，靠它本身根据Hessian2的规则，就能解析出来。\n\n原来有协议文件的场景，有点儿像两个人事先约定好，0表示方法add，然后后面会传两个数。服务端把两个数加起来，这样一方发送012，另一方知道是将1和2加起来，但是不知道协议文件的，当它收到012的时候，完全不知道代表什么意思。\n\n而自描述的场景，就像两个人说的每句话都带前因后果。例如，传递的是“函数：add，第一个参数1，第二个参数2”。这样无论谁拿到这个表述，都知道是什么意思。但是只不过都是以二进制的形式编码的。这其实相当于综合了XML和二进制共同优势的一个协议。\n\nHessian2是如何做到这一点的呢？这就需要去看Hessian2的序列化的[语法描述文件](http://hessian.caucho.com/doc/hessian-serialization.html)。\n\n![img](618bad147f6933f61ef56cf73d671166-20230115031646108.jpg)\n\n看起来很复杂，编译原理里面是有这样的语法规则的。\n\n我们从Top看起，下一层是value，直到形成一棵树。这里面的有个思想，为了防止歧义，每一个类型的起始数字都设置成为独一无二的。这样，解析的时候，看到这个数字，就知道后面跟的是什么了。\n\n这里还是以加法为例子，“add(2,3)”被序列化之后是什么样的呢？\n\n```csharp\nH x02 x00     # Hessian 2.0\nC          # RPC call\n x03 add     # method \"add\"\n x92        # two arguments\n x92        # 2 - argument 1\n x93        # 3 - argument 2\n```\n\n- H开头，表示使用的协议是Hession，H的二进制是0x48。\n- C开头，表示这是一个RPC调用。\n- 0x03，表示方法名是三个字符。\n- 0x92，表示有两个参数。其实这里存的应该是2，之所以加上0x90，就是为了防止歧义，表示这里一定是一个int。\n- 第一个参数是2，编码为0x92，第二个参数是3，编码为0x93。\n\n这个就叫作**自描述**。\n\n另外，Hessian2是面向对象的，可以传输一个对象。\n\n```yaml\nclass Car {\n String color;\n String model;\n}\nout.writeObject(new Car(\"red\", \"corvette\"));\nout.writeObject(new Car(\"green\", \"civic\"));\n---\nC            # object definition (#0)\n x0b example.Car    # type is example.Car\n x92          # two fields\n x05 color       # color field name\n x05 model       # model field name\n\nO            # object def (long form)\n x90          # object definition #0\n x03 red        # color field value\n x08 corvette      # model field value\n\nx60           # object def #0 (short form)\n x05 green       # color field value\n x05 civic       # model field value\n```\n\n首先，定义这个类。对于类型的定义也传过去，因而也是自描述的。类名为example.Car，字符长11位，因而前面长度为0x0b。有两个成员变量，一个是color，一个是model，字符长5位，因而前面长度0x05,。\n\n然后，传输的对象引用这个类。由于类定义在位置0，因而对象会指向这个位置0，编码为0x90。后面red和corvette是两个成员变量的值，字符长分别为3和8。\n\n接着又传输一个属于相同类的对象。这时候就不保存对于类的引用了，只保存一个0x60，表示同上就可以了。\n\n可以看出，Hessian2真的是能压缩尽量压缩，多一个Byte都不传。\n\n## 如何解决RPC传输问题？\n\n接下来，我们再来看Dubbo的RPC传输问题。前面我们也说了，基于Socket实现一个高性能的服务端，是很复杂的一件事情，在Dubbo里面，使用了Netty的网络传输框架。\n\nNetty是一个非阻塞的基于事件的网络传输框架，在服务端启动的时候，会监听一个端口，并注册以下的事件。\n\n- **连接事件**：当收到客户端的连接事件时，会调用void connected(Channel channel) 方法。\n- 当**可写事件**触发时，会调用void sent(Channel channel, Object message)，服务端向客户端返回响应数据。\n- 当**可读事件**触发时，会调用void received(Channel channel, Object message) ，服务端在收到客户端的请求数据。\n- 当**发生异常**时，会调用void caught(Channel channel, Throwable exception)。\n\n当事件触发之后，服务端在这些函数中的逻辑，可以选择直接在这个函数里面进行操作，还是将请求分发到线程池去处理。一般异步的数据读写都需要另外的线程池参与，在线程池中会调用真正的服务端业务代码逻辑，返回结果。\n\nHessian2是Dubbo默认的RPC序列化方式，当然还有其他选择。例如，Dubbox从Spark那里借鉴Kryo，实现高性能的序列化。\n\n到这里，我们说了数据中心里面的相互调用。为了高性能，大家都愿意用二进制，但是为什么后期Spring Cloud又兴起了呢？这是因为，并发量越来越大，已经到了微服务的阶段。同原来的SOA不同，微服务粒度更细，模块之间的关系更加复杂。\n\n在上面的架构中，如果使用二进制的方式进行序列化，虽然不用协议文件来生成Stub，但是对于接口的定义，以及传的对象DTO，还是需要共享JAR。因为只有客户端和服务端都有这个JAR，才能成功地序列化和反序列化。\n\n但当关系复杂的时候，JAR的依赖也变得异常复杂，难以维护，而且如果在DTO里加一个字段，双方的JAR没有匹配好，也会导致序列化不成功，而且还有可能循环依赖。这个时候，一般有两种选择。\n\n第一种，建立严格的项目管理流程。\n\n- 不允许循环调用，不允许跨层调用，只准上层调用下层，不允许下层调用上层。\n- 接口要保持兼容性，不兼容的接口新添加而非改原来的，当接口通过监控，发现不用的时候，再下掉。\n- 升级的时候，先升级服务提供端，再升级服务消费端。\n\n第二种，改用RESTful的方式。\n\n- 使用Spring Cloud，消费端和提供端不用共享JAR，各声明各的，只要能变成JSON就行，而且JSON也是比较灵活的。\n- 使用RESTful的方式，性能会降低，所以需要通过横向扩展来抵消单机的性能损耗。\n\n这个时候，就看架构师的选择喽！\n\n## 小结\n\n好了，这节就到这里了，我们来总结一下。\n\n- RESTful API对于接入层和Controller层之外的调用，已基本形成事实标准，但是随着内部服务之间的调用越来越多，性能也越来越重要，于是Dubbo的RPC框架有了用武之地。\n- Dubbo通过注册中心解决服务发现问题，通过Hessian2序列化解决协议约定的问题，通过Netty解决网络传输的问题。\n- 在更加复杂的微服务场景下，Spring Cloud的RESTful方式在内部调用也会被考虑，主要是JAR包的依赖和管理问题。\n\n最后，给你留两个思考题。\n\n1. 对于微服务模式下的RPC框架的选择，Dubbo和SpringCloud各有优缺点，你能做个详细的对比吗？\n2. 到目前为止，我们讲过的RPC，还没有跨语言调用的场景，你知道如果跨语言应该怎么办吗？\n\n# 36 讲跨语言类RPC协议：交流之前，双方先来个专业术语表\n\n到目前为止，咱们讲了四种RPC，分别是ONC RPC、基于XML的SOAP、基于JSON的RESTful和Hessian2。\n\n通过学习，我们知道，二进制的传输性能好，文本类的传输性能差一些；二进制的难以跨语言，文本类的可以跨语言；要写协议文件的严谨一些，不写协议文件的灵活一些。虽然都有服务发现机制，有的可以进行服务治理，有的则没有。\n\n我们也看到了RPC从最初的客户端服务器模式，最终演进到微服务。对于RPC框架的要求越来越多了，具体有哪些要求呢？\n\n- 首先，传输性能很重要。因为服务之间的调用如此频繁了，还是二进制的越快越好。\n- 其次，跨语言很重要。因为服务多了，什么语言写成的都有，而且不同的场景适宜用不同的语言，不能一个语言走到底。\n- 最好既严谨又灵活，添加个字段不用重新编译和发布程序。\n- 最好既有服务发现，也有服务治理，就像Dubbo和Spring Cloud一样。\n\n## Protocol Buffers\n\n这是要多快好省的建设社会主义啊。理想还是要有的嘛，这里我就来介绍一个向“理想”迈进的GRPC。\n\nGRPC首先满足二进制和跨语言这两条，二进制说明压缩效率高，跨语言说明更灵活。但是又是二进制，又是跨语言，这就相当于两个人沟通，你不但说方言，还说缩略语，人家怎么听懂呢？所以，最好双方弄一个协议约定文件，里面规定好双方沟通的专业术语，这样沟通就顺畅多了。\n\n对于GRPC来讲，二进制序列化协议是Protocol Buffers。首先，需要定义一个协议文件.proto。\n\n我们还看买极客时间专栏的这个例子。\n\n```java\nsyntax = “proto3”;\npackage com.geektime.grpc\noption java_package = “com.geektime.grpc”;\nmessage Order {\n  required string date = 1;\n  required string classname = 2;\n  required string author = 3;\n  required int price = 4;\n}\n\nmessage OrderResponse {\n  required string message = 1;\n}\n\nservice PurchaseOrder {\n  rpc Purchase (Order) returns (OrderResponse) {}\n}\n```\n\n在这个协议文件中，我们首先指定使用proto3的语法，然后我们使用Protocol Buffers的语法，定义两个消息的类型，一个是发出去的参数，一个是返回的结果。里面的每一个字段，例如date、classname、author、price都有唯一的一个数字标识，这样在压缩的时候，就不用传输字段名称了，只传输这个数字标识就行了，能节省很多空间。\n\n最后定义一个Service，里面会有一个RPC调用的声明。\n\n无论使用什么语言，都有相应的工具生成客户端和服务端的Stub程序，这样客户端就可以像调用本地一样，调用远程的服务了。\n\n## 协议约定问题\n\nProtocol Buffers是一款压缩效率极高的序列化协议，有很多设计精巧的序列化方法。\n\n对于int类型32位的，一般都需要4个Byte进行存储。在Protocol Buffers中，使用的是变长整数的形式。对于每一个Byte的8位，最高位都有特殊的含义。\n\n如果该位为 1，表示这个数字没完，后续的Byte也属于这个数字；如果该位为 0，则这个数字到此结束。其他的7个Bit才是用来表示数字的内容。因此，小于128的数字都可以用一个Byte表示；大于128的数字，比如130，会用两个字节来表示。\n\n对于每一个字段，使用的是TLV（Tag，Length，Value）的存储办法。\n\n其中Tag = (field_num \u003c\u003c 3) | wire_type。field_num就是在proto文件中，给每个字段指定唯一的数字标识，而wire_type用于标识后面的数据类型。\n\n![img](a66aa9ca6c6575f4b335881ae786ba10-20230115031646100.jpg)\n\n例如，对于string author = 3，在这里field_num为3，string的wire_type为2，于是 (field_num \u003c\u003c 3) | wire_type = (11000) | 10 = 11010 = 26；接下来是Length，最后是Value为“liuchao”，如果使用UTF-8编码，长度为7个字符，因而Length为7。\n\n可见，在序列化效率方面，Protocol Buffers简直做到了极致。\n\n在灵活性方面，这种基于协议文件的二进制压缩协议往往存在更新不方便的问题。例如，客户端和服务器因为需求的改变需要添加或者删除字段。\n\n这一点上，Protocol Buffers考虑了兼容性。在上面的协议文件中，每一个字段都有修饰符。比如：\n\n- required：这个值不能为空，一定要有这么一个字段出现；\n- optional：可选字段，可以设置，也可以不设置，如果不设置，则使用默认值；\n- repeated：可以重复0到多次。\n\n如果我们想修改协议文件，对于赋给某个标签的数字，例如string author=3，这个就不要改变了，改变了就不认了；也不要添加或者删除required字段，因为解析的时候，发现没有这个字段就会报错。对于optional和repeated字段，可以删除，也可以添加。这就给了客户端和服务端升级的可能性。\n\n例如，我们在协议里面新增一个string recommended字段，表示这个课程是谁推荐的，就将这个字段设置为optional。我们可以先升级服务端，当客户端发过来消息的时候，是没有这个值的，将它设置为一个默认值。我们也可以先升级客户端，当客户端发过来消息的时候，是有这个值的，那它将被服务端忽略。\n\n至此，我们解决了协议约定的问题。\n\n## 网络传输问题\n\n接下来，我们来看网络传输的问题。\n\n如果是Java技术栈，GRPC的客户端和服务器之间通过Netty Channel作为数据通道，每个请求都被封装成HTTP 2.0的Stream。\n\nNetty是一个高效的基于异步IO的网络传输框架，这个上一节我们已经介绍过了。HTTP 2.0在[第14讲]，我们也介绍过。HTTP 2.0协议将一个TCP的连接，切分成多个流，每个流都有自己的ID，而且流是有优先级的。流可以是客户端发往服务端，也可以是服务端发往客户端。它其实只是一个虚拟的通道。\n\nHTTP 2.0还将所有的传输信息分割为更小的消息和帧，并对它们采用二进制格式编码。\n\n通过这两种机制，HTTP 2.0的客户端可以将多个请求分到不同的流中，然后将请求内容拆成帧，进行二进制传输。这些帧可以打散乱序发送， 然后根据每个帧首部的流标识符重新组装，并且可以根据优先级，决定优先处理哪个流的数据。\n\n![img](03d4a216c024a9e761ed43c6787bf7dd-1584287099474-20230115031646089.jpg)\n\n由于基于HTTP 2.0，GRPC和其他的RPC不同，可以定义四种服务方法。\n\n第一种，也是最常用的方式是**单向RPC**，即客户端发送一个请求给服务端，从服务端获取一个应答，就像一次普通的函数调用。\n\n```scss\nrpc SayHello(HelloRequest) returns (HelloResponse){}\n```\n\n第二种方式是**服务端流式RPC**，即服务端返回的不是一个结果，而是一批。客户端发送一个请求给服务端，可获取一个数据流用来读取一系列消息。客户端从返回的数据流里一直读取，直到没有更多消息为止。\n\n```scss\nrpc LotsOfReplies(HelloRequest) returns (stream HelloResponse){}\n```\n\n第三种方式为**客户端流式RPC**，也即客户端的请求不是一个，而是一批。客户端用提供的一个数据流写入并发送一系列消息给服务端。一旦客户端完成消息写入，就等待服务端读取这些消息并返回应答。\n\n```scss\nrpc LotsOfGreetings(stream HelloRequest) returns (HelloResponse) {}\n```\n\n第四种方式为**双向流式 RPC**，即两边都可以分别通过一个读写数据流来发送一系列消息。这两个数据流操作是相互独立的，所以客户端和服务端能按其希望的任意顺序读写，服务端可以在写应答前等待所有的客户端消息，或者它可以先读一个消息再写一个消息，或者读写相结合的其他方式。每个数据流里消息的顺序会被保持。\n\n```scss\nrpc BidiHello(stream HelloRequest) returns (stream HelloResponse){}\n```\n\n如果基于HTTP 2.0，客户端和服务器之间的交互方式要丰富得多，不仅可以单方向远程调用，还可以实现当服务端状态改变的时候，主动通知客户端。\n\n至此，传输问题得到了解决。\n\n## 服务发现与治理问题\n\n最后是服务发现与服务治理的问题。\n\nGRPC本身没有提供服务发现的机制，需要借助其他的组件，发现要访问的服务端，在多个服务端之间进行容错和负载均衡。\n\n其实负载均衡本身比较简单，LVS、HAProxy、Nginx都可以做，关键问题是如何发现服务端，并根据服务端的变化，动态修改负载均衡器的配置。\n\n在这里我们介绍一种对于GRPC支持比较好的负载均衡器Envoy。其实Envoy不仅仅是负载均衡器，它还是一个高性能的C++写的Proxy转发器，可以配置非常灵活的转发规则。\n\n这些规则可以是静态的，放在配置文件中的，在启动的时候加载。要想重新加载，一般需要重新启动，但是Envoy支持热加载和热重启，这在一定程度上缓解了这个问题。\n\n当然，最好的方式是将规则设置为动态的，放在统一的地方维护。这个统一的地方在Envoy眼中被称为服务发现（Discovery Service），过一段时间去这里拿一下配置，就修改了转发策略。\n\n无论是静态的，还是动态的，在配置里面往往会配置四个东西。\n\n第一个是listener。Envoy既然是Proxy，专门做转发，就得监听一个端口，接入请求，然后才能够根据策略转发，这个监听的端口就称为listener。\n\n第二个是endpoint，是目标的IP地址和端口。这个是Proxy最终将请求转发到的地方。\n\n第三个是cluster。一个cluster是具有完全相同行为的多个endpoint，也即如果有三个服务端在运行，就会有三个IP和端口，但是部署的是完全相同的三个服务，它们组成一个cluster，从cluster到endpoint的过程称为负载均衡，可以轮询。\n\n第四个是route。有时候多个cluster具有类似的功能，但是是不同的版本号，可以通过route规则，选择将请求路由到某一个版本号，也即某一个cluster。\n\n如果是静态的，则将后端的服务端的IP地址拿到，然后放在配置文件里面就可以了。\n\n如果是动态的，就需要配置一个服务发现中心，这个服务发现中心要实现Envoy的API，Envoy可以主动去服务发现中心拉取转发策略。\n\n![img](ef916f46dc293ac2d5739b496f0b27ce-20230115031646110.jpg)\n\n看来，Envoy进程和服务发现中心之间要经常相互通信，互相推送数据，所以Envoy在控制面和服务发现中心沟通的时候，就可以使用GRPC，也就天然具备在用户面支撑GRPC的能力。\n\nEnvoy如果复杂的配置，都能干什么事呢？\n\n一种常见的规则是**配置路由策略**。例如后端的服务有两个版本，可以通过配置Envoy的route，来设置两个版本之间，也即两个cluster之间的route规则，一个占99%的流量，一个占1%的流量。\n\n另一种常见的规则就是**负载均衡策略**。对于一个cluster下的多个endpoint，可以配置负载均衡机制和健康检查机制，当服务端新增了一个，或者挂了一个，都能够及时配置Envoy，进行负载均衡。\n\n![img](50443d6848f890e475e71be11489d33c-20230115031646166.jpg)\n\n所有这些节点的变化都会上传到注册中心，所有这些策略都可以通过注册中心进行下发，所以，更严格的意义上讲，注册中心可以称为**注册治理中心**。\n\nEnvoy这么牛，是不是能够将服务之间的相互调用全部由它代理？如果这样，服务也不用像Dubbo，或者Spring Cloud一样，自己感知到注册中心，自己注册，自己治理，对应用干预比较大。\n\n如果我们的应用能够意识不到服务治理的存在，就是直接进行GRPC的调用就可以了。\n\n这就是未来服务治理的趋势**Serivce Mesh**，也即应用之间的相互调用全部由Envoy进行代理，服务之间的治理也被Envoy进行代理，完全将服务治理抽象出来，到平台层解决。\n\n![img](15e254a8e92e031b20feb6ebdcc32402-20230115031646188.jpg)\n\n至此RPC框架中有治理功能的Dubbo、Spring Cloud、Service Mesh就聚齐了。\n\n## 小结\n\n好了，这一节就到这里了，我们来总结一下。\n\n- GRPC是一种二进制，性能好，跨语言，还灵活，同时可以进行服务治理的多快好省的RPC框架，唯一不足就是还是要写协议文件。\n- GRPC序列化使用Protocol Buffers，网络传输使用HTTP 2.0，服务治理可以使用基于Envoy的Service Mesh。\n\n# 37 讲知识串讲：用双十一的故事串起碎片的网络协议（上）\n\n基本的网络知识我们都讲完了，还记得最初举的那个“双十一”下单的例子吗？这一节开始，我们详细地讲解这个过程，用这个过程串起我们讲过的网络协议。\n\n我把这个过程分为十个阶段，从云平台中搭建一个电商开始，到BGP路由广播，再到DNS域名解析，从客户看商品图片，到最终下单的整个过程，每一步我都会详细讲解。这节我们先来看前三个阶段。\n\n## 1.部署一个高可用高并发的电商平台\n\n首先，咱们**要有个电商平台**。假设我们已经有了一个特别大的电商平台，这个平台应该部署在哪里呢？假设我们用公有云，一般公有云会有多个位置，比如在华东、华北、华南都有。毕竟咱们的电商是要服务全国的，当然到处都要部署了。我们把主站点放在华东。\n\n![img](eddde5929de2a72b197321e5ad87e120-20230115031646217.jpg)\n\n为了每个点都能“雨露均沾”，也为了高可用性，往往需要有多个机房，形成多个可用区（Available Zone）。由于咱们的应用是分布在两个可用区的，所以假如任何一个可用区挂了，都不会受影响。\n\n我们来回想[数据中心]那一节，每个可用区里有一片一片的机柜，每个机柜上有一排一排的服务器，每个机柜都有一个接入交换机，有一个汇聚交换机将多个机柜连在一起。\n\n这些服务器里面部署的都是计算节点，每台上面都有Open vSwitch创建的虚拟交换机，将来在这台机器上创建的虚拟机，都会连到Open vSwitch上。\n\n![img](d66c01c39e911e784525a118c37b50a7-20230115031646237.jpg)\n\n接下来，你**在云计算的界面上创建一个VPC**（Virtual Private Cloud，虚拟私有网络），指定一个IP段，这样以后你部署的所有应用都会在这个虚拟网络里，使用你分配的这个IP段。为了不同的VPC相互隔离，每个VPC都会被分配一个VXLAN的ID。尽管不同用户的虚拟机有可能在同一个物理机上，但是不同的VPC二层压根儿是不通的。\n\n由于有两个可用区，在这个VPC里面，要为每一个可用区分配一个Subnet，也就是在大的网段里分配两个小的网段。当两个可用区里面网段不同的时候，就可以配置路由策略，访问另外一个可用区，走某一条路由了。\n\n接下来，应该**创建数据库持久化层**。大部分云平台都会提供PaaS服务，也就是说，不需要你自己搭建数据库，而是采用直接提供数据库的服务，并且单机房的主备切换都是默认做好的，数据库也是部署在虚拟机里面的，只不过从界面上，你看不到数据库所在的虚拟机而已。\n\n云平台会给每个Subnet的数据库实例分配一个域名。创建数据库实例的时候，需要你指定可用区和Subnet，这样创建出来的数据库实例可以通过这个Subnet的私网IP进行访问。\n\n为了分库分表实现高并发的读写，在创建的多个数据库实例之上，会**创建一个分布式数据库的实例**，也需要指定可用区和Subnet，还会为分布式数据库分配一个私网IP和域名。\n\n对于数据库这种高可用性比较高的，需要进行跨机房高可用，因而两个可用区都要部署一套，但是只有一个是主，另外一个是备，云平台往往会提供数据库同步工具，将应用写入主的数据同步给备数据库集群。\n\n接下来是**创建缓存集**群。云平台也会提供PaaS服务，也需要每个可用区和Subnet创建一套，缓存的数据在内存中，由于读写性能要求高，一般不要求跨可用区读写。\n\n再往上层就是**部署咱们自己写的程序**了。基础服务层、组合服务层、Controller层，以及Nginx层、API网关等等，这些都是部署在虚拟机里面的。它们之间通过RPC相互调用，需要到注册中心进行注册。\n\n它们之间的网络通信是虚拟机和虚拟机之间的。如果是同一台物理机，则那台物理机上的OVS就能转发过去；如果是不同的物理机，这台物理机的OVS和另一台物理机的OVS中间有一个VXLAN的隧道，将请求转发过去。\n\n再往外就是**负载均衡**了，负载均衡也是云平台提供的PaaS服务，也是属于某个VPC的，部署在虚拟机里面的，但是负载均衡有个外网的IP，这个外网的IP地址就是在网关节点的外网网口上的。在网关节点上，会有NAT规则，将外网IP地址转换为VPC里面的私网IP地址，通过这些私网IP地址访问到虚拟机上的负载均衡节点，然后通过负载均衡节点转发到API网关的节点。\n\n网关节点的外网网口是带公网IP地址的，里面有一个虚拟网关转发模块，还会有一个OVS，将私网IP地址放到VXLAN隧道里面，转发到虚拟机上，从而实现外网和虚拟机网络之间的互通。\n\n不同的可用区之间，通过核心交换机连在一起，核心交换机之外是边界路由器。\n\n在华北、华东、华南同样也部署了一整套，每个地区都创建了VPC，这就需要有一种机制将VPC连接到一起。云平台一般会提供硬件的VPC互连的方式，当然也可以使用软件互连的方式，也就是使用VPN网关，通过IPsec VPN将不同地区的不同VPC通过VPN连接起来。\n\n对于不同地区和不同运营商的用户，我们希望他能够就近访问到网站，而且当一个点出了故障之后，我们希望能够在不同的地区之间切换，这就需要有智能DNS，这个也是云平台提供的。\n\n对于一些静态资源，可以保持在对象存储里面，通过CDN下发到边缘节点，这样客户端就能尽快加载出来。\n\n## 2.大声告诉全世界，可以到我这里买东西\n\n当电商应用搭建完毕之后，接下来需要将如何访问到这个电商网站广播给全网。\n\n刚才那张图画的是一个可用区的情况，对于多个可用区的情况，我们可以隐去计算节点的情况，将外网访问区域放大。\n\n![img](e132bc3ba500b1197139f30c02e20124-20230115031646228.jpg)\n\n外网IP是放在虚拟网关的外网网口上的，这个IP如何让全世界知道呢？当然是通过BGP路由协议了。\n\n每个可用区都有自己的汇聚交换机，如果机器数目比较多，可以直接用核心交换机，每个Region也有自己的核心交换区域。\n\n在核心交换外面是安全设备，然后就是边界路由器。边界路由器会和多个运营商连接，从而每个运营商都能够访问到这个网站。边界路由器可以通过BGP协议，将自己数据中心里面的外网IP向外广播，也就是告诉全世界，如果要访问这些外网IP，都来我这里。\n\n每个运营商也有很多的路由器、很多的点，于是就可以将如何到达这些IP地址的路由信息，广播到全国乃至全世界。\n\n## 3.打开手机来上网，域名解析得地址\n\n这个时候，不但你的这个网站的IP地址全世界都知道了，你打的广告可能大家也都看到了，于是有客户下载App来买东西了。\n\n![img](85c125c225faba29c0f374e18ea8c6fc-20230115031646251.jpg)\n\n客户的手机开机以后，在附近寻找基站eNodeB，发送请求，申请上网。基站将请求发给MME，MME对手机进行认证和鉴权，还会请求HSS看有没有钱，看看是在哪里上网。\n\n当MME通过了手机的认证之后，开始建立隧道，建设的数据通路分两段路，其实是两个隧道。一段是从eNodeB到SGW，第二段是从SGW到PGW，在PGW之外，就是互联网。\n\nPGW会为手机分配一个IP地址，手机上网都是带着这个IP地址的。\n\n当在手机上面打开一个App的时候，首先要做的事情就是解析这个网站的域名。\n\n在手机运营商所在的互联网区域里，有一个本地的DNS，手机会向这个DNS请求解析DNS。当这个DNS本地有缓存，则直接返回；如果没有缓存，本地DNS才需要递归地从根DNS服务器，查到.com的顶级域名服务器，最终查到权威DNS服务器。\n\n如果你使用云平台的时候，配置了智能DNS和全局负载均衡，在权威DNS服务中，一般是通过配置CNAME的方式，我们可以起一个别名，例如 [vip.yourcomany.com](http://vip.yourcomany.com/) ，然后告诉本地DNS服务器，让它请求GSLB解析这个域名，GSLB就可以在解析这个域名的过程中，通过自己的策略实现负载均衡。\n\nGSLB通过查看请求它的本地DNS服务器所在的运营商和地址，就知道用户所在的运营商和地址，然后将距离用户位置比较近的Region里面，三个负载均衡SLB的公网IP地址，返回给本地DNS服务器。本地DNS解析器将结果缓存后，返回给客户端。\n\n对于手机App来说，可以绕过刚才的传统DNS解析机制，直接只要HTTPDNS服务，通过直接调用HTTPDNS服务器，得到这三个SLB的公网IP地址。\n\n看，经过了如此复杂的过程，咱们的万里长征还没迈出第一步，刚刚得到IP地址，包还没发呢？话说手机App拿到了公网IP地址，接下来应该做什么呢？\n\n# 38 讲知识串讲：用双十一的故事串起碎片的网络协议（中）\n\n上一节我们讲到，手机App经过了一个复杂的过程，终于拿到了电商网站的SLB的IP地址，是不是该下单了？\n\n别忙，俗话说的好，买东西要货比三家。大部分客户在购物之前要看很多商品图片，比来比去，最后好不容易才下决心，点了下单按钮。下单按钮一按，就要开始建立连接。建立连接这个过程也挺复杂的，最终还要经过层层封装，才构建出一个完整的网络包。今天我们就来看这个过程。\n\n## 4.购物之前看图片，静态资源CDN\n\n客户想要在购物网站买一件东西的时候，一般是先去详情页看看图片，是不是想买的那一款。\n\n![img](7023762edeaf4d481bc90331f60db769-20230115031646270.jpg)\n\n我们部署电商应用的时候，一般会把静态资源保存在两个地方，一个是接入层nginx后面的varnish缓存里面，一般是静态页面；对于比较大的、不经常更新的静态图片，会保存在对象存储里面。这两个地方的静态资源都会配置CDN，将资源下发到边缘节点。\n\n配置了CDN之后，权威DNS服务器上，会为静态资源设置一个CNAME别名，指向另外一个域名 [cdn.com](http://cdn.com/) ，返回给本地DNS服务器。\n\n当本地DNS服务器拿到这个新的域名时，需要继续解析这个新的域名。这个时候，再访问的时候就不是原来的权威DNS服务器了，而是 [cdn.com](http://cdn.com/) 的权威DNS服务器。这是CDN自己的权威DNS服务器。\n\n在这个服务器上，还是会设置一个CNAME，指向另外一个域名，也即CDN网络的全局负载均衡器。\n\n本地DNS服务器去请求CDN的全局负载均衡器解析域名，全局负载均衡器会为用户选择一台合适的缓存服务器提供服务，将IP返回给客户端，客户端去访问这个边缘节点，下载资源。缓存服务器响应用户请求，将用户所需内容传送到用户终端。\n\n如果这台缓存服务器上并没有用户想要的内容，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器，将内容拉到本地。\n\n## 5.看上宝贝点下单，双方开始建连接\n\n当你浏览了很多图片，发现实在喜欢某个商品，于是决定下单购买。\n\n电商网站会对下单的情况提供RESTful的下单接口，而对于下单这种需要保密的操作，需要通过HTTPS协议进行请求。\n\n在所有这些操作之前，首先要做的事情是**建立连接**。\n\n![img](705e4cc5acc7b364bdf015f333d40783-20230115031646406.jpg)\n\nHTTPS协议是基于TCP协议的，因而要先**建立TCP的连接**。在这个例子中，TCP的连接是从手机上的App和负载均衡器SLB之间的。\n\n尽管中间要经过很多的路由器和交换机，但是TCP的连接是端到端的。TCP这一层和更上层的HTTPS无法看到中间的包的过程。尽管建立连接的时候，所有的包都逃不过在这些路由器和交换机之间的转发，转发的细节我们放到那个下单请求的发送过程中详细解读，这里只看端到端的行为。\n\n对于TCP连接来讲，需要通过三次握手建立连接，为了维护这个连接，双方都需要在TCP层维护一个连接的状态机。\n\n一开始，客户端和服务端都处于CLOSED状态。服务端先是主动监听某个端口，处于LISTEN状态。然后客户端主动发起连接SYN，之后处于SYN-SENT状态。服务端收到发起的连接，返回SYN，并且ACK客户端的SYN，之后处于SYN-RCVD状态。\n\n客户端收到服务端发送的SYN和ACK之后，发送ACK的ACK，之后处于ESTABLISHED状态。这是因为，它一发一收成功了。服务端收到ACK的ACK之后，处于ESTABLISHED状态，因为它的一发一收也成功了。\n\n当TCP层的连接建立完毕之后，接下来轮到**HTTPS层建立连接**了，在HTTPS的交换过程中，TCP层始终处于ESTABLISHED。\n\n对于HTTPS，客户端会发送Client Hello消息到服务器，用明文传输TLS版本信息、加密套件候选列表、压缩算法候选列表等信息。另外，还会有一个随机数，在协商对称密钥的时候使用。\n\n然后，服务器会返回Server Hello消息，告诉客户端，服务器选择使用的协议版本、加密套件、压缩算法等。这也有一个随机数，用于后续的密钥协商。\n\n然后，服务器会给你一个服务器端的证书，然后说：“Server Hello Done，我这里就这些信息了。”\n\n客户端当然不相信这个证书，于是从自己信任的CA仓库中，拿CA的证书里面的公钥去解密电商网站的证书。如果能够成功，则说明电商网站是可信的。这个过程中，你可能会不断往上追溯CA、CA的CA、CA的CA的CA，反正直到一个授信的CA，就可以了。\n\n证书验证完毕之后，觉得这个服务端是可信的，于是客户端计算产生随机数字Pre-master，发送Client Key Exchange，用证书中的公钥加密，再发送给服务器，服务器可以通过私钥解密出来。\n\n接下来，无论是客户端还是服务器，都有了三个随机数，分别是：自己的、对端的，以及刚生成的Pre-Master随机数。通过这三个随机数，可以在客户端和服务器产生相同的对称密钥。\n\n有了对称密钥，客户端就可以说：“Change Cipher Spec，咱们以后都采用协商的通信密钥和加密算法进行加密通信了。”\n\n然后客户端发送一个Encrypted Handshake Message，将已经商定好的参数等，采用协商密钥进行加密，发送给服务器用于数据与握手验证。\n\n同样，服务器也可以发送Change Cipher Spec，说：“没问题，咱们以后都采用协商的通信密钥和加密算法进行加密通信了”，并且也发送Encrypted Handshake Message的消息试试。\n\n当双方握手结束之后，就可以通过对称密钥进行加密传输了。\n\n真正的下单请求封装成网络包的发送过程，我们先放一放，我们来接着讲这个网络包的故事。\n\n## 6.发送下单请求网络包，西行需要出网关\n\n当客户端和服务端之间建立了连接后，接下来就要发送下单请求的网络包了。\n\n在用户层发送的是HTTP的网络包，因为服务端提供的是RESTful API，因而HTTP层发送的就是一个请求。\n\n```makefile\nPOST /purchaseOrder HTTP/1.1\nHost: www.geektime.com\nContent-Type: application/json; charset=utf-8\nContent-Length: nnn \n{\n \"order\": {\n  \"date\": \"2018-07-01\",\n  \"className\": \"趣谈网络协议\",\n  \"Author\": \"刘超\",\n  \"price\": \"68\"\n }\n}\n```\n\nHTTP的报文大概分为三大部分。第一部分是**请求行**，第二部分是**请求的首部**，第三部分才是**请求的正文实体**。\n\n在请求行中，URL就是 [www.geektime.com/purchaseOrder](https://www.geektime.com/purchaseOrder) ，版本为HTTP 1.1。\n\n请求的类型叫作POST，它需要主动告诉服务端一些信息，而非获取。需要告诉服务端什么呢？一般会放在正文里面。正文可以有各种各样的格式，常见的格式是JSON。\n\n请求行下面就是我们的首部字段。首部是key value，通过冒号分隔。\n\nContent-Type是指正文的格式。例如，我们进行POST的请求，如果正文是JSON，那么我们就应该将这个值设置为JSON。\n\n接下来是正文，这里是一个JSON字符串，里面通过文本的形式描述了，要买一个课程，作者是谁，多少钱。\n\n这样，HTTP请求的报文格式就拼凑好了。接下来浏览器或者移动App会把它交给下一层传输层。\n\n怎么交给传输层呢？也是用Socket进行程序设计。如果用的是浏览器，这些程序不需要你自己写，有人已经帮你写好了；如果在移动APP里面，一般会用一个HTTP的客户端工具来发送，并且帮你封装好。\n\nHTTP协议是基于TCP协议的，所以它使用面向连接的方式发送请求，通过Stream二进制流的方式传给对方。当然，到了TCP层，它会把二进制流变成一个的报文段发送给服务器。\n\n在TCP头里面，会有源端口号和目标端口号，目标端口号一般是服务端监听的端口号，源端口号在手机端，往往是随机分配一个端口号。这个端口号在客户端和服务端用于区分请求和返回，发给那个应用。\n\n在IP头里面，都需要加上自己的地址（即源地址）和它想要去的地方（即目标地址）。当一个手机上线的时候，PGW会给这个手机分配一个IP地址，这就是源地址，而目标地址则是云平台的负载均衡器的外网IP地址。\n\n在IP层，客户端需要查看目标地址和自己是否是在同一个局域网，计算是否是同一个网段，往往需要通过CIDR子网掩码来计算。\n\n对于这个下单场景，目标IP和源IP不会在同一个网段，因而需要发送到默认的网关。一般通过DHCP分配IP地址的时候，同时配置默认网关的IP地址。\n\n但是客户端不会直接使用默认网关的IP地址，而是发送ARP协议，来获取网关的MAC地址，然后将网关MAC作为目标MAC，自己的MAC作为源MAC，放入MAC头，发送出去。\n\n一个完整的网络包的格式是这样的。\n\n![img](99c282efaca15deb79c7821c9c577349-20230115031646334.jpg)\n\n真不容易啊，本来以为上篇就发送下单包了，结果到中篇这个包还没发送出去，只是封装了一个如此长的网络包。别着急，你可以自己先预想一下，接下来该做什么了？\n\n# 39 讲知识串讲：用双十一的故事串起碎片的网络协议（下）\n\n上一节，我们封装了一个长长的网络包，“大炮”准备完毕，开始发送。\n\n发送的时候可以说是重重关隘，从手机到移动网络、互联网，还要经过多个运营商才能到达数据中心，到了数据中心就进入第二个复杂的过程，从网关到VXLAN隧道，到负载均衡，到Controller层、组合服务层、基础服务层，最终才下单入库。今天，我们就来看这最后一段过程。\n\n## 7.一座座城池一道道关，流控拥塞与重传\n\n网络包已经组合完毕，接下来我们来看，如何经过一道道城关，到达目标公网IP。\n\n对于手机来讲，默认的网关在PGW上。在移动网络里面，从手机到SGW，到PGW是有一条隧道的。在这条隧道里面，会将上面的这个包作为隧道的乘客协议放在里面，外面SGW和PGW在核心网机房的IP地址。网络包直到PGW（PGW是隧道的另一端）才将里面的包解出来，转发到外部网络。\n\n所以，从手机发送出来的时候，网络包的结构为：\n\n- 源MAC：手机也即UE的MAC；\n- 目标MAC：网关PGW上面的隧道端点的MAC；\n- 源IP：UE的IP地址；\n- 目标IP：SLB的公网IP地址。\n\n进入隧道之后，要封装外层的网络地址，因而网络包的格式为：\n\n- 外层源MAC：E-NodeB的MAC；\n- 外层目标MAC：SGW的MAC；\n- 外层源IP：E-NodeB的IP；\n- 外层目标IP：SGW的IP；\n- 内层源MAC：手机也即UE的MAC；\n- 内层目标MAC：网关PGW上面的隧道端点的MAC；\n- 内层源IP：UE的IP地址；\n- 内层目标IP：SLB的公网IP地址。\n\n当隧道在SGW的时候，切换了一个隧道，会从SGW到PGW的隧道，因而网络包的格式为：\n\n- 外层源MAC：SGW的MAC；\n- 外层目标MAC：PGW的MAC；\n- 外层源IP：SGW的IP；\n- 外层目标IP：PGW的IP；\n- 内层源MAC：手机也即UE的MAC；\n- 内层目标MAC：网关PGW上面的隧道端点的MAC；\n- 内层源IP：UE的IP地址；\n- 内层目标IP：SLB的公网IP地址。\n\n在PGW的隧道端点将包解出来，转发出去的时候，一般在PGW出外部网络的路由器上，会部署NAT服务，将手机的IP地址转换为公网IP地址，当请求返回的时候，再NAT回来。\n\n因而在PGW之后，相当于做了一次[欧洲十国游型]的转发，网络包的格式为：\n\n- 源MAC：PGW出口的MAC；\n- 目标MAC：NAT网关的MAC；\n- 源IP：UE的IP地址；\n- 目标IP：SLB的公网IP地址。\n\n在NAT网关，相当于做了一次[玄奘西游型]的转发，网络包的格式变成：\n\n- 源MAC：NAT网关的MAC；\n- 目标MAC：A2路由器的MAC；\n- 源IP：UE的公网IP地址；\n- 目标IP：SLB的公网IP地址。\n\n![img](8edfb048b27aed3cc2eeb892ae22175a-20230115031646342.jpg)\n\n出了NAT网关，就从核心网到达了互联网。在网络世界，每一个运营商的网络成为自治系统AS。每个自治系统都有边界路由器，通过它和外面的世界建立联系。\n\n对于云平台来讲，它可以被称为Multihomed AS，有多个连接连到其他的AS，但是大多拒绝帮其他的AS传输包。例如一些大公司的网络。对于运营商来说，它可以被称为Transit AS，有多个连接连到其他的AS，并且可以帮助其他的AS传输包，比如主干网。\n\n如何从出口的运营商到达云平台的边界路由器？在路由器之间需要通过BGP协议实现，BGP又分为两类，eBGP和iBGP。自治系统之间、边界路由器之间使用eBGP广播路由。内部网络也需要访问其他的自治系统。\n\n边界路由器如何将BGP学习到的路由导入到内部网络呢？通过运行iBGP，使内部的路由器能够找到到达外网目的地最好的边界路由器。\n\n网站的SLB的公网IP地址早已经通过云平台的边界路由器，让全网都知道了。于是这个下单的网络包选择的下一跳是A2，也即将A2的MAC地址放在目标MAC地址中。\n\n到达A2之后，从路由表中找到下一跳是路由器C1，于是将目标MAC换成C1的MAC地址。到达C1之后，找到下一跳是C2，将目标MAC地址设置为C2的MAC。到达C2后，找到下一跳是云平台的边界路由器，于是将目标MAC设置为边界路由器的MAC地址。\n\n你会发现，这一路，都是只换MAC，不换目标IP地址。这就是所谓下一跳的概念。\n\n在云平台的边界路由器，会将下单的包转发进来，经过核心交换，汇聚交换，到达外网网关节点上的SLB的公网IP地址。\n\n我们可以看到，手机到SLB的公网IP，是一个端到端的连接，连接的过程发送了很多包。所有这些包，无论是TCP三次握手，还是HTTPS的密钥交换，都是要走如此复杂的过程到达SLB的，当然每个包走的路径不一定一致。\n\n网络包走在这个复杂的道路上，很可能一不小心就丢了，怎么办？这就需要借助TCP的机制重新发送。\n\n既然TCP要对包进行重传，就需要维护Sequence Number，看哪些包到了，哪些没到，哪些需要重传，传输的速度应该控制到多少，这就是**TCP的滑动窗口协议**。\n\n![img](4879c63596736cc91dbb696046e95024-20230115031646549.jpg)\n\n整个TCP的发送，一开始会协商一个Sequence Number，从这个Sequence Number开始，每个包都有编号。滑动窗口将接收方的网络包分成四个部分：\n\n- 已经接收，已经ACK，已经交给应用层的包；\n- 已经接收，已经ACK，未发送给应用层；\n- 已经接收，尚未发送ACK；\n- 未接收，尚有空闲的缓存区域。\n\n对于TCP层来讲，每一个包都有ACK。ACK需要从SLB回复到手机端，将上面的那个过程反向来一遍，当然路径不一定一致，可见ACK也不是那么轻松的事情。\n\n如果发送方超过一定的时间没有收到ACK，就会重新发送。只有TCP层ACK过的包，才会发给应用层，并且只会发送一份，对于下单的场景，应用层是HTTP层。\n\n你可能会问了，TCP老是重复发送，会不会导致一个单下了两遍？是否要求服务端实现幂等？从TCP的机制来看，是不会的。只有收不到ACK的包才会重复发，发到接收端，在窗口里面只保存一份，所以在同一个TCP连接中，不用担心重传导致二次下单。\n\n但是TCP连接会因为某种原因断了，例如手机信号不好，这个时候手机把所有的动作重新做一遍，建立一个新的TCP连接，在HTTP层调用两次RESTful API。这个时候可能会导致两遍下单的情况，因而RESTful API需要实现幂等。\n\n当ACK过的包发给应用层之后，TCP层的缓存就空了出来，这会导致上面图中的大三角，也即接收方能够容纳的总缓存，整体顺时针滑动。小的三角形，也即接收方告知发送方的窗口总大小，也即还没有完全确认收到的缓存大小，如果把这些填满了，就不能再发了，因为没确认收到，所以一个都不能扔。\n\n## 8.从数据中心进网关，公网NAT成私网\n\n包从手机端经历千难万险，终于到了SLB的公网IP所在的公网网口。由于匹配上了MAC地址和IP地址，因而将网络包收了进来。\n\n![img](ab5b03ebf7facf71721566165f921252-20230115031646365.jpg)\n\n在虚拟网关节点的外网网口上，会有一个NAT规则，将公网IP地址转换为VPC里面的私网IP地址，这个私网IP地址就是SLB的HAProxy所在的虚拟机的私网IP地址。\n\n当然为了承载比较大的吞吐量，虚拟网关节点会有多个，物理网络会将流量分发到不同的虚拟网关节点。同样HAProxy也会是一个大的集群，虚拟网关会选择某个负载均衡节点，将某个请求分发给它，负载均衡之后是Controller层，也是部署在虚拟机里面的。\n\n当网络包里面的目标IP变成私有IP地址之后，虚拟路由会查找路由规则，将网络包从下方的私网网口发出来。这个时候包的格式为：\n\n- 源MAC：网关MAC；\n- 目标MAC：HAProxy虚拟机的MAC；\n- 源IP：UE的公网IP；\n- 目标IP：HAProxy虚拟机的私网IP。\n\n## 9.进入隧道打标签，RPC远程调用下单\n\n在虚拟路由节点上，也会有OVS，将网络包封装在VXLAN隧道里面，VXLAN ID就是给你的租户创建VPC的时候分配的。包的格式为：\n\n- 外层源MAC：网关物理机MAC；\n- 外层目标MAC：物理机A的MAC；\n- 外层源IP：网关物理机IP；\n- 外层目标IP：物理机A的IP；\n- 内层源MAC：网关MAC；\n- 内层目标MAC：HAProxy虚拟机的MAC；\n- 内层源IP：UE的公网IP；\n- 内层目标IP：HAProxy虚拟机的私网IP。\n\n在物理机A上，OVS会将包从VXLAN隧道里面解出来，发给HAProxy所在的虚拟机。HAProxy所在的虚拟机发现MAC地址匹配，目标IP地址匹配，就根据TCP端口，将包发给HAProxy进程，因为HAProxy是在监听这个TCP端口的。因而HAProxy就是这个TCP连接的服务端，客户端是手机。对于TCP的连接状态、滑动窗口等，都是在HAProxy上维护的。\n\n在这里HAProxy是一个四层负载均衡，也即它只解析到TCP层，里面的HTTP协议它不关心，就将请求转发给后端的多个Controller层的一个。\n\nHAProxy发出去的网络包就认为HAProxy是客户端了，看不到手机端了。网络包格式如下：\n\n- 源MAC：HAProxy所在虚拟机的MAC；\n- 目标MAC：Controller层所在虚拟机的MAC；\n- 源IP：HAProxy所在虚拟机的私网IP；\n- 目标IP：Controller层所在虚拟机的私网IP。\n\n当然这个包发出去之后，还是会被物理机上的OVS放入VXLAN隧道里面，网络包格式为：\n\n- 外层源MAC：物理机A的MAC；\n- 外层目标MAC：物理机B的MAC；\n- 外层源IP：物理机A的IP；\n- 外层目标IP：物理机B的IP；\n- 内层源MAC：HAProxy所在虚拟机的MAC；\n- 内层目标MAC：Controller层所在虚拟机的MAC；\n- 内层源IP：HAProxy所在虚拟机的私网IP；\n- 内层目标IP：Controller层所在虚拟机的私网IP。\n\n在物理机B上，OVS会将包从VXLAN隧道里面解出来，发给Controller层所在的虚拟机。Controller层所在的虚拟机发现MAC地址匹配，目标IP地址匹配，就根据TCP端口，将包发给Controller层的进程，因为它在监听这个TCP端口。\n\n在HAProxy和Controller层之间，维护一个TCP的连接。\n\nController层收到包之后，它是关心HTTP里面是什么的，于是解开HTTP的包，发现是一个POST请求，内容是下单购买一个课程。\n\n10.下单扣减库存优惠券，数据入库返回成功\n\n下单是一个复杂的过程，因而往往在组合服务层会有一个专门管理下单的服务，Controller层会通过RPC调用这个组合服务层。\n\n假设我们使用的是Dubbo，则Controller层需要读取注册中心，将下单服务的进程列表拿出来，选出一个来调用。\n\nDubbo中默认的RPC协议是Hessian2。Hessian2将下单的远程调用序列化为二进制进行传输。\n\nNetty是一个非阻塞的基于事件的网络传输框架。Controller层和下单服务之间，使用了Netty的网络传输框架。有了Netty，就不用自己编写复杂的异步Socket程序了。Netty使用的方式，就是咱们讲[Socket编程]的时候，一个项目组支撑多个项目（IO多路复用，从派人盯着到有事通知）这种方式。\n\nNetty还是工作在Socket这一层的，发送的网络包还是基于TCP的。在TCP的下层，还是需要封装上IP头和MAC头。如果跨物理机通信，还是需要封装的外层的VXLAN隧道里面。当然底层的这些封装，Netty都不感知，它只要做好它的异步通信即可。\n\n在Netty的服务端，也即下单服务中，收到请求后，先用Hessian2的格式进行解压缩。然后将请求分发到线程中进行处理，在线程中，会调用下单的业务逻辑。\n\n下单的业务逻辑比较复杂，往往要调用基础服务层里面的库存服务、优惠券服务等，将多个服务调用完毕，才算下单成功。下单服务调用库存服务和优惠券服务，也是通过Dubbo的框架，通过注册中心拿到库存服务和优惠券服务的列表，然后选一个调用。\n\n调用的时候，统一使用Hessian2进行序列化，使用Netty进行传输，底层如果跨物理机，仍然需要通过VXLAN的封装和解封装。\n\n咱们以库存为例子的时候，讲述过幂等的接口实现的问题。因为如果扣减库存，仅仅是谁调用谁减一。这样存在的问题是，如果扣减库存因为一次调用失败，而多次调用，这里指的不是TCP多次重试，而是应用层调用的多次重试，就会存在库存扣减多次的情况。\n\n这里常用的方法是，使用乐观锁（Compare and Set，简称CAS）。CAS要考虑三个方面，当前的库存数、预期原来的库存数和版本，以及新的库存数。在操作之前，查询出原来的库存数和版本，真正扣减库存的时候，判断如果当前库存的值与预期原值和版本相匹配，则将库存值更新为新值，否则不做任何操作。\n\n这是一种基于状态而非基于动作的设计，符合RESTful的架构设计原则。这样的设计有利于高并发场景。当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。\n\n最终，当下单更新到分布式数据库中之后，整个下单过程才算真正告一段落。\n\n好了，经过了十个过程，下单终于成功了，你是否对这个过程了如指掌了呢？如果发现对哪些细节比较模糊，可以回去看一下相应的章节，相信会有更加深入的理解。\n\n到此，我带着你用下单过程把网络协议的知识都复习了一遍。授人以鱼不如授人以渔。下一节，我将会带你来搭建一个网络实验环境，配合实验来说明理论。\n\n# 40 讲放弃完美主义，执行力就是限时限量认真完成\n\n你好，我是刘超。\n\n从筹备、上线到今天专栏完结，过去了将近半年的时间。200多天，弹指一挥间。\n\n我原本计划写36篇，最后愣是写到了45篇。原本编辑让我一篇写两三千字，结果几乎每篇都是四五千字。这里面涉及图片张数我没具体数过，但是据说多到让编辑上传到吐。编辑一篇我的稿件的工作量相当于别的专栏的两倍。\n\n人常说，有多少付出，就有多少回报。但是，写这个“趣谈网络协议”专栏，我收获的东西远超过我的想象。我希望你的收获也是如此。为什么这么说呢？我们把时间放回到这个专栏最开始的时候，我慢慢跟你讲。\n\n## 我不是最懂的人，但我想尝试成为这样的人\n\n今年年初，极客时间来找我，希望我讲一些偏重基础的知识，比如网络协议。\n\n他们一提到这个主题，我就很兴奋，因为这也触动了我心中长期以来的想法，因为网络这个东西学起来实在是太痛苦。\n\n但是，说实话，接下这个重任，我心里其实是有点“怕”的。我怕自己不够专业，毕竟业内有这么多网络工程师和研究网络理论的教授。我讲这个课会不会贻笑大方啊？\n\n我知道，很多技术人员不敢写博客、写公众号，其实都有这种“怕”的心理：我又不牛，没啥要分享的，要是误导了别人怎么办？\n\n如果你想做出一些成绩，这个心理一定要克服。其实每个人都有自己的**相对优势**。对于某个东西，你研究的时间不一定是最长的，但是你可能有特殊的角度、表达方式和应用场景。坚定了这个想法之后，我就开始投入热火朝天的专栏写作了。\n\n一旦开始写，我发现，这个事情远没有看上去那么简单。它会花费你非常多的个人时间。写专栏这几个月，晚上两点之后睡，周末全在写专栏，基本成为我的生活常态。但是**我想挑战一下自己，我觉得，只要咬牙挺过去，自己的技术就会上升一个层次。**\n\n## 放弃完美主义，执行力就是限时限量认真完成\n\n技术人都有完美主义倾向，觉得什么事情都要钻研个底儿朝天，才拿出来见人。我也一样。\n\n我曾经答应某出版社写一本搜索引擎的书。这本书分为原理篇和实践篇。我总觉得我还没把原理篇写完，就不能写实践篇。但是，仅原理篇我就写了一年。搜索引擎就火了一两年，最后时间窗口过了，书稿没有完成，这件事儿也就这么搁浅了。\n\n所以，完美主义虽然是个很好听的词，但是它往往是和拖延症如影随形的，它常常会给拖延症披上一个华丽的外衣，说，我是因为追求完美嘛。但是，最终的结果往往是，理论研究半天还没动手，执行力很差。时间点过了，就心安理得地说，反正现在也不需要了，那就算了吧。久而久之，你就会发现，自己好像陷入了瓶颈。\n\n我慢慢明白过来，我们**不是为了做技术而做技术，做技术是为了满足人类需求的**。完美主义是好事儿，但是，坚持完美主义的同时要限时限量地完成，才能形成执行力。\n\n写这个专栏之后，我更加深刻地体会到这一点。每周都要写三篇文章，压力很大，根本容不得任何拖延。如果我还是坚持以前完美主义的做法，读完十本书，用三年时间把网络协议都研究透再来写，那现在就没有这个专栏了。\n\n如果我们要强调执行力，时间点这个因素就至关重要。在固定的时间点上，就要把控范围，不能顾虑太多，要勇于放弃。就像给产品做排期，先做最小闭环的功能集合，其他的放在以后再补充。在这个前提下，以自己最大的限度往完美的方向上努力。比如，我觉得每天2点睡是我的身体极限，努力到这个程度，我也就无愧于心了。\n\n所以说，我们做事情的目的并不是完美，而是在固定的时间点，以固定的数量和质量，尽可能认真地满足当时的客户需求，这才是最重要的。\n\n这样做肯定会有不满意的地方，比如很多同学在留言区指出我的错误，甚至有的同学提的问题，我原来都没思考过。但是，我觉得这些都不是事儿。我可以再查资料，再补充、再完善。所以，后来时间宽裕了，我还增加了5期答疑，回答了一下之前没来得及回答地问题。高手在民间，咱们一起来讨论和进步。这个过程已经让我受益良多。\n\n## 保持饥渴，不怕被“鄙视”，勇于脱离舒适区\n\n有人可能会问了，你看你既不是最专业的，还不追求完美，真的不怕被人“鄙视”吗？\n\n被“鄙视”，谁都怕，这也是为什么越大的会议，参加人数越多的演讲，越是没有人提出具体的问题。大家都怕丢人，看上去好像大家都听懂了，就我啥都不懂，我要是问，被大家笑话怎么办？我想很多人都有这样的经历吧？我也来给你讲讲我的亲身经历。\n\n我从Windows开发去做Linux的存储系统开发时，连Linux man都不会看；我在惠普从事OpenStack实施工作的时候，对于网络的了解一塌糊涂，一直被甲方骂；我在华为做云计算，支撑运营商项目的时候，面对一大堆核心网词汇，一脸懵；我在网易云对内支撑考拉的时候，在微服务架构方面也是小白……被“鄙视”了这么多次之后，我不怕了。因为这每一次“鄙视”都可以让我发现自己的短板，然后啃下这些东西，这不就是最大的收获吗？\n\n我就是这样一直被“鄙视”着成长起来的人，我就是常常在别人分享的时候坐第一排问很傻的那种问题的人，我就是常常一知半解还愿意和别人讨论的人……\n\n**怕被“鄙视”，说明你还不够饥渴，还没有勇气脱离你的舒适区。** 在你熟悉的领域里面，你是最最权威的，但是，天下之大，你真的只满足于眼前这一亩三分地吗？\n\n很多人因为怕被“鄙视”，不敢问、不敢做，因而与很多美好的东西都擦肩而过了。直到有一天你用到了，你才后悔，当时自己怎么没去多问一句。\n\n所以，当你看到一个特别好的、突破自己的学习机会，别犹豫，搭上这辆车。等过了十年，你会发现，当年那些嘲笑、轻视，甚至谩骂，都算不了什么，进步本身才是最最重要的。\n\n今天，咱们没有谈具体的知识，我只表达了一下我的观点。我就是那个你在直播里看到的，那个邋遢、搞笑、不装，同时做事认真，愿意和你一起进步的技术大叔。\n\n脱离舒适区吧，希望我们可以一起成长！\n","lastmodified":"2023-05-09T16:33:58.311366888Z","tags":[]},"/%E9%83%A8%E7%BD%B2%E6%80%BB%E7%BB%93":{"title":"部署总结","content":"\n## 利用 Obsidian 笔记创造个人知识库，自动生成个人网站数字花园\n","lastmodified":"2023-05-09T16:33:58.311366888Z","tags":["花园"]},"/%E9%9D%A2%E8%AF%95%E9%A2%98":{"title":"面试题","content":"\n[[redis]]\n\n[[Go]]\n\n[[操作系统]]\n\n[[网络]]\n\n![[前端面试题]]\n","lastmodified":"2023-05-09T16:33:58.311366888Z","tags":[]},"/1-MySQL%E7%9A%84%E6%95%B0%E6%8D%AE%E7%9B%AE%E5%BD%95":{"title":"1 MySQL的数据目录","content":"\n# 1 MySQL的数据目录\n\n## 1 MySQL8的主要目录结构\n\n1. 数据库文件目录\n\n```bash\nmysql\u003e show variables like 'datadir';\n+---------------+-----------------+\n| Variable_name | Value           |\n+---------------+-----------------+\n| datadir       | /var/lib/mysql/ |\n+---------------+-----------------+\n1 row in set (0.04 sec)\n```\n\n从结果中可以看出，在我的计算机上MySQL的数据目录就是 /var/lib/mysql/ 。\n\n2. 相关命令目录\n\n   相关命令目录：`/usr/bin`（mysqladmin、mysqlbinlog、mysqldump等命令）和`/usr/sbin`。\n\n3. 配置文件目录\n\n   配置文件目录：`/usr/share/mysql-8.0`（命令及配置文件），`/etc/mysql（如my.cnf）`\n\n## 2 数据库和文件系统的关系\n\n### 2.1 查看默认数据库  \n\n查看一下在我的计算机上当前有哪些数据库：`mysql\u003e SHOW DATABASES;`  \n可以看到有4个数据库是属于MySQL自带的系统数据库:\n\n1. mysql  \n   MySQL 系统自带的核心数据库，它存储了MySQL的用户账户和权限信息，一些存储过程、事件的定义信息，一些运行过程中产生的日志信息，一些帮助信息以及时区信息等。\n\n2. information_schema  \n   MySQL 系统自带的数据库，这个数据库保存着MySQL服务器 维护的所有其他数据库的信息 ，比如有哪些表、哪些视图、哪些触发器、哪些列、哪些索引。这些信息并不是真实的用户数据，而是一些描述性信息，有时候也称之为元数据 。在系统数据库 information_schema 中提供了一些以innodb_sys 开头的表，用于表示内部系统表。\n\n   ```bash\n   mysql\u003e SHOW TABLES LIKE 'innodb_sys%';\n   +--------------------------------------------+\n   | Tables_in_information_schema (innodb_sys%) |\n   +--------------------------------------------+\n   | INNODB_SYS_DATAFILES                       |\n   | INNODB_SYS_VIRTUAL                         |\n   | INNODB_SYS_INDEXES                         |\n   | INNODB_SYS_TABLES                          |\n   | INNODB_SYS_FIELDS                          |\n   | INNODB_SYS_TABLESPACES                     |\n   | INNODB_SYS_FOREIGN_COLS                    |\n   | INNODB_SYS_COLUMNS                         |\n   | INNODB_SYS_FOREIGN                         |\n   | INNODB_SYS_TABLESTATS                      |\n   +--------------------------------------------+\n   10 rows in set (0.00 sec)\n   ```\n\n3. performance_schema  \n   MySQL 系统自带的数据库，这个数据库里主要保存MySQL服务器运行过程中的一些状态信息，可以用来监控 MySQL 服务的各类性能指标 。包括统计最近执行了哪些语句，在执行过程的每个阶段都花费了多长时间，内存的使用情况等信息。\n\n4. sys  \n   MySQL 系统自带的数据库，这个数据库主要是通过 视图 的形式把 information_schema 和performance_schema 结合起来，帮助系统管理员和开发人员监控 MySQL 的技术性能。\n\n### 2.2 数据库在文件系统中的表示  \n\n   看一下我的计算机上的数据目录下的内容：\n\n  ```bash\n  [root@atguigu01 mysql]# cd /var/lib/mysql\n  [root@atguigu01 mysql]# ll\n  总用量 189980\n  -rw-r-----. 1 mysql mysql       56 7月  28 00:27 auto.cnf\n  -rw-r-----. 1 mysql mysql      179 7月  28 00:27 binlog.000001\n  -rw-r-----. 1 mysql mysql      820 7月  28 01:00 binlog.000002\n  -rw-r-----. 1 mysql mysql      179 7月  29 14:08 binlog.000003\n  -rw-r-----. 1 mysql mysql      582 7月  29 16:47 binlog.000004\n  -rw-r-----. 1 mysql mysql      179 7月  29 16:51 binlog.000005\n  -rw-r-----. 1 mysql mysql      179 7月  29 16:56 binlog.000006\n  -rw-r-----. 1 mysql mysql      179 7月  29 17:37 binlog.000007\n  -rw-r-----. 1 mysql mysql    24555 7月  30 00:28 binlog.000008\n  -rw-r-----. 1 mysql mysql      179 8月   1 11:57 binlog.000009\n  -rw-r-----. 1 mysql mysql      156 8月   1 23:21 binlog.000010\n  -rw-r-----. 1 mysql mysql      156 8月   2 09:25 binlog.000011\n  -rw-r-----. 1 mysql mysql     1469 8月   4 01:40 binlog.000012\n  -rw-r-----. 1 mysql mysql      156 8月   6 00:24 binlog.000013\n  -rw-r-----. 1 mysql mysql      179 8月   6 08:43 binlog.000014\n  -rw-r-----. 1 mysql mysql      156 8月   6 10:56 binlog.000015\n  -rw-r-----. 1 mysql mysql      240 8月   6 10:56 binlog.index\n  -rw-------. 1 mysql mysql     1676 7月  28 00:27 ca-key.pem\n  -rw-r--r--. 1 mysql mysql     1112 7月  28 00:27 ca.pem\n  -rw-r--r--. 1 mysql mysql     1112 7月  28 00:27 client-cert.pem\n  -rw-------. 1 mysql mysql     1676 7月  28 00:27 client-key.pem\n  drwxr-x---. 2 mysql mysql     4096 7月  29 16:34 dbtest\n  -rw-r-----. 1 mysql mysql   196608 8月   6 10:58 #ib_16384_0.dblwr\n  -rw-r-----. 1 mysql mysql  8585216 7月  28 00:27 #ib_16384_1.dblwr\n  -rw-r-----. 1 mysql mysql     3486 8月   6 08:43 ib_buffer_pool\n  -rw-r-----. 1 mysql mysql 12582912 8月   6 10:56 ibdata1\n  -rw-r-----. 1 mysql mysql 50331648 8月   6 10:58 ib_logfile0\n  \n  ```\n\n这个数据目录下的文件和子目录比较多，除了 information_schema 这个系统数据库外，**其他的数据库在数据目录下都有对应的子目录**。\n\n以 temp 数据库为例，在MySQL5.7 中打开：\n\n```bash\n[root@atguigu02 mysql]# cd ./temp\n[root@atguigu02 temp]# ll\n总用量 1144\n-rw-r-----. 1 mysql mysql   8658 8月  18 11:32 countries.frm\n-rw-r-----. 1 mysql mysql 114688 8月  18 11:32 countries.ibd\n-rw-r-----. 1 mysql mysql     61 8月  18 11:32 db.opt\n-rw-r-----. 1 mysql mysql   8716 8月  18 11:32 departments.frm\n-rw-r-----. 1 mysql mysql 147456 8月  18 11:32 departments.ibd\n-rw-r-----. 1 mysql mysql   3017 8月  18 11:32 emp_details_view.frm\n-rw-r-----. 1 mysql mysql   8982 8月  18 11:32 employees.frm\n-rw-r-----. 1 mysql mysql 180224 8月  18 11:32 employees.ibd\n-rw-r-----. 1 mysql mysql   8660 8月  18 11:32 job_grades.frm\n-rw-r-----. 1 mysql mysql  98304 8月  18 11:32 job_grades.ibd\n-rw-r-----. 1 mysql mysql   8736 8月  18 11:32 job_history.frm\n-rw-r-----. 1 mysql mysql 147456 8月  18 11:32 job_history.ibd\n-rw-r-----. 1 mysql mysql   8688 8月  18 11:32 jobs.frm\n-rw-r-----. 1 mysql mysql 114688 8月  18 11:32 jobs.ibd\n-rw-r-----. 1 mysql mysql   8790 8月  18 11:32 locations.frm\n-rw-r-----. 1 mysql mysql 131072 8月  18 11:32 locations.ibd\n-rw-r-----. 1 mysql mysql   8614 8月  18 11:32 regions.frm\n-rw-r-----. 1 mysql mysql 114688 8月  18 11:32 regions.ibd\n```\n\n在MySQL8.0中打开：\n\n```bash\n[root@atguigu01 mysql]# cd ./temp\n[root@atguigu01 temp]# ll\n总用量 1080\n-rw-r-----. 1 mysql mysql 131072 7月  29 23:10 countries.ibd\n-rw-r-----. 1 mysql mysql 163840 7月  29 23:10 departments.ibd\n-rw-r-----. 1 mysql mysql 196608 7月  29 23:10 employees.ibd\n-rw-r-----. 1 mysql mysql 114688 7月  29 23:10 job_grades.ibd\n-rw-r-----. 1 mysql mysql 163840 7月  29 23:10 job_history.ibd\n-rw-r-----. 1 mysql mysql 131072 7月  29 23:10 jobs.ibd\n-rw-r-----. 1 mysql mysql 147456 7月  29 23:10 locations.ibd\n-rw-r-----. 1 mysql mysql 131072 7月  29 23:10 regions.ib\n```\n\n### 2.3 表在文件系统中的表示\n\n#### 2.3.1 InnoDB存储引擎模式\n\n1. 表结构\n\n   为了保存表结构， InnoDB 在 数据目录 下对应的数据库子目录下创建了一个专门用于 描述表结构的文件 ，文件名是这样：\n\n   `表名.frm`\n\n   比方说我们在 atguigu 数据库下创建一个名为 test 的表：\n\n   ```bash\n   mysql\u003e USE atguigu;\n   Database changed\n   mysql\u003e CREATE TABLE test (\n       -\u003e     c1 INT\n       -\u003e );\n   Query OK, 0 rows affected (0.03 sec)\n   ```\n\n   那在数据库 atguigu 对应的子目录下就会创建一个名为 `test.frm` 的用于描述表结构的文件。`.frm`的格式在不同的平台上都是相同的。这个后缀名为``.frm`是以 二进制格式 存储的，我们直接打开是乱码的。\n\n2. 表中数据和索引\n   1. 系统表空间（system tablespace）\n\n      默认情况下，InnoDB会在数据目录下创建一个名为`ibdata1`大小为 12M 的文件，这个文件就是对应的**系统表空间**在文件系统上的表示。怎么才12M？注意这个文件是**自扩展文件**，当不够用的时候它会自己增加文件大小。\n\n      当然，如果你想让系统表空间对应文件系统上多个实际文件，或者仅仅觉得原来的 ibdata1 这个文件名难听，那可以在MySQL启动时配置对应的文件路径以及它们的大小，比如我们这样修改一下my.cnf 配置文件：  \n      `innodb_data_file_path=data1:512M;data2:512M:autoextend`\n\n#### 2.3.2 MyISAM存储引擎模式\n\n1. 表结构  \n   在存储表结构方面， MyISAM 和 InnoDB 一样，也是在 数据目录 下对应的数据库子目录下创建了一个专门用于描述表结构的文件：\n\n   `xx.frm`\n\n2. 表中数据和索引  \n   在MyISAM中的索引全部都是二级索引 ，**该存储引擎的数据和索引是分开存放的**。所以在文件系统中也是使用不同的文件来存储数据文件和索引文件，同时表数据都存放在对应的数据库子目录下。假如 test  \n   表使用MyISAM存储引擎的话，那么在它所在数据库对应的 atguigu 目录下会为 test 表创建这三个文  \n   件：`表名.ibd test.frm test.ibd`\n\n   举例：创建一个 MyISAM 表，使用 ENGINE 选项显式指定引擎。因为 InnoDB 是默认引擎。\n\n   ```bash\n   CREATE TABLE `student_myisam` (\n     `id` bigint NOT NULL AUTO_INCREMENT,\n     `name` varchar(64) DEFAULT NULL,\n     `age` int DEFAULT NULL,\n     `sex` varchar(2) DEFAULT NULL,\n     PRIMARY KEY (`id`)\n   )ENGINE=MYISAM AUTO_INCREMENT=0 DEFAULT CHARSET=utf8mb3;\n   ```\n\n### 2.4 小结\n\n举例： 数据库a ， 表b 。\n\n1. 如果表b采用 `InnoDB` ，``data\\a`中会产生1个或者2个文件：\n   - b.frm ：描述表结构文件，字段长度等\n   - 如果采用 系统表空间 模式的，数据信息和索引信息都存储在 ibdata1 中\n   - 如果采用 独立表空间 存储模式，data\\a中还会产生 b.ibd 文件（存储数据信息和索引信息）  \n   此外：\n   1. MySQL5.7 中会在data/a的目录下生成 db.opt 文件用于保存数据库的相关配置。比如：字符集、比较规则。而MySQL8.0不再提供db.opt文件。\n   2. MySQL8.0中不再单独提供b.frm，而是合并在b.ibd文件中。\n2. 如果表b采用 MyISAM ，data\\a中会产生3个文件：\n\n  - MySQL5.7 中： b.frm ：描述表结构文件，字段长度等。\n\n    MySQL8.0 中 b.xxx.sdi ：描述表结构文件，字段长度等\n\n  - b.MYD (MYData)：数据信息文件，存储数据信息(如果采用独立表存储模式)\n  - b.MYI (MYIndex)：存放索引信息文件\n","lastmodified":"2023-05-09T16:33:58.263365716Z","tags":[]},"/11-%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%E8%A7%84%E8%8C%83%E8%8C%83%E5%BC%8F":{"title":"11 数据库设计规范（范式）","content":"\n## 1 为什么需要数据库设计\n\n## 2 范 式\n\n### 2.1 范式简介\n\n在关系型数据库中，关于数据表设计的基本原则、规则就称为范式。可以理解为，一张数据表的设计结\n\n构需要满足的某种设计标准的 级别 。要想设计一个结构合理的关系型数据库，必须满足一定的范式。\n\n### 2.2 范式都包括哪些\n\n目前关系型数据库有六种常见范式，按照范式级别，从低到高分别是：第一范式（\n\n1NF）、第二范式\n\n（\n\n2NF）、第三范式（\n\n3NF）、巴斯-科德范式（\n\nBCNF）、第四范式(4NF）和第五范式（\n\n5NF，又称完美\n\n范式）。\n\n### 2.3 键和相关属性的概念\n\n举例：\n\n这里有两个表：\n\n球员表(player) ：球员编号 | 姓名 | 身份证号 | 年龄 | 球队编号\n\n球队表(team) ：球队编号 | 主教练 | 球队所在地字段名称\n\n字段类型\n\n是否是主键\n\n说明\n\nid\n\nINT\n\n是\n\n主键id\n\nusername\n\nVARCHAR(30)\n\n否\n\n用户名\n\npassword\n\nVARCHAR(50)\n\n否\n\n密码\n\nuser_info\n\nVARCHAR(255)\n\n否\n\n用户信息 (包含真实姓名、电话、住址)\n\n超键 ：对于球员表来说，超键就是包括球员编号或者身份证号的任意组合，比如（球员编号）\n\n（球员编号，姓名）（身份证号，年龄）等。\n\n候选键 ：就是最小的超键，对于球员表来说，候选键就是（球员编号）或者（身份证号）。\n\n主键 ：我们自己选定，也就是从候选键中选择一个，比如（球员编号）。\n\n外键 ：球员表中的球队编号。\n\n主属性 、 非主属性 ：在球员表中，主属性是（球员编号）（身份证号），其他的属性（姓名）\n\n（年龄）（球队编号）都是非主属性。\n\n### 2.4 第一范式(1st NF)\n\n举例1：\n\n假设一家公司要存储员工的姓名和联系方式。它创建一个如下表：\n\n该表不符合 1NF ，因为规则说“表的每个属性必须具有原子（单个）值”，lisi和zhaoliu员工的\n\nemp_mobile 值违反了该规则。为了使表符合 1NF ，我们应该有如下表数据：\n\n举例2：\n\nuser 表的设计不符合第一范式\n\n其中，user_info字段为用户信息，可以进一步拆分成更小粒度的字段，不符合数据库设计对第一范式的\n\n要求。将user_info拆分后如下：字段名称\n\n字段类型\n\n是否是主键\n\n说明\n\nid\n\nINT\n\n是\n\n主键id\n\nusername\n\nVARCHAR(30)\n\n否\n\n用户名\n\npassword\n\nVARCHAR(50)\n\n否\n\n密码\n\nreal_name\n\nVARCHAR(30)\n\n否\n\n真实姓名\n\nphone\n\nVARCHAR(12)\n\n否\n\n联系电话\n\naddress\n\nVARCHAR(100)\n\n否\n\n家庭住址\n\n姓名\n\n年龄\n\n地址\n\n张三\n\n20\n\n广东省广州市三元里78号\n\n李四\n\n24\n\n广东省深圳市龙华新区\n\n姓名\n\n年龄\n\n省\n\n市\n\n地址\n\n张三\n\n20\n\n广东\n\n广州\n\n三元里78号\n\n李四\n\n24\n\n广东\n\n深圳\n\n龙华新区\n\n举例3：\n\n属性的原子性是 主观的 。例如，Employees关系中雇员姓名应当使用1个（\n\nfullname）、2个（\n\nfirstname\n\n和lastname）还是3个（\n\nfirstname、middlename和lastname）属性表示呢？答案取决于应用程序。如果应\n\n用程序需要分别处理雇员的姓名部分（如：用于搜索目的），则有必要把它们分开。否则，不需要。\n\n表1：\n\n表2：\n\n### 2.5 第二范式(2nd NF)\n\n举例1：\n\n成绩表 （学号，课程号，成绩）关系中，（学号，课程号）可以决定成绩，但是学号不能决定成绩，课\n\n程号也不能决定成绩，所以“（学号，课程号）→成绩”就是 完全依赖关系 。\n\n举例2：\n\n比赛表 player_game ，里面包含球员编号、姓名、年龄、比赛编号、比赛时间和比赛场地等属性，这\n\n里候选键和主键都为（球员编号，比赛编号），我们可以通过候选键（或主键）来决定如下的关系：\n\n但是这个数据表不满足第二范式，因为数据表中的字段之间还存在着如下的对应关系：\n\n对于非主属性来说，并非完全依赖候选键。这样会产生怎样的问题呢？\n\n(球员编号, 比赛编号) → (姓名, 年龄, 比赛时间, 比赛场地，得分)\n\n(球员编号) → (姓名，年龄)\n\n(比赛编号) → (比赛时间, 比赛场地)表名\n\n属性（字段）\n\n球员 player 表\n\n球员编号、姓名和年龄等属性\n\n比赛 game 表\n\n比赛编号、比赛时间和比赛场地等属性\n\n球员比赛关系 player_game 表\n\n球员编号、比赛编号和得分等属性\n\n1. 数据冗余 ：如果一个球员可以参加 m 场比赛，那么球员的姓名和年龄就重复了 m-1 次。一个比赛\n\n也可能会有 n 个球员参加，比赛的时间和地点就重复了 n-1 次。\n\n2. 插入异常 ：如果我们想要添加一场新的比赛，但是这时还没有确定参加的球员都有谁，那么就没\n\n法插入。\n\n3. 删除异常 ：如果我要删除某个球员编号，如果没有单独保存比赛表的话，就会同时把比赛信息删\n\n除掉。\n\n4. 更新异常 ：如果我们调整了某个比赛的时间，那么数据表中所有这个比赛的时间都需要进行调\n\n整，否则就会出现一场比赛时间不同的情况。\n\n为了避免出现上述的情况，我们可以把球员比赛表设计为下面的三张表。\n\n这样的话，每张数据表都符合第二范式，也就避免了异常情况的发生。\n\n1NF 告诉我们字段属性需要是原子性的，而 2NF 告诉我们一张表就是一个独立的对象，一张表只\n\n表达一个意思。\n\n举例3：\n\n定义了一个名为 Orders 的关系，表示订单和订单行的信息：\n\n违反了第二范式，因为有非主键属性仅依赖于候选键（或主键）的一部分。例如，可以仅通过orderid找\n\n到订单的 orderdate，以及 customerid 和 companyname，而没有必要再去使用productid。\n\n修改：\n\nOrders表和OrderDetails表如下，此时符合第二范式。字段名称\n\n字段类型\n\n是否是主键\n\n说明\n\nid\n\nINT\n\n是\n\n商品主键id （主键）\n\ncategory_id\n\nINT\n\n否\n\n商品类别id\n\ncategory_name\n\nVARCHAR(30)\n\n否\n\n商品类别名称\n\ngoods_name\n\nVARCHAR(30)\n\n否\n\n商品名称\n\nprice\n\nDECIMAL(10,2)\n\n否\n\n商品价格\n\n字段名称\n\n字段类型\n\n是否是主键\n\n说明\n\nid\n\nINT\n\n是\n\n商品类别主键id\n\ncategory_name\n\nVARCHAR(30)\n\n否\n\n商品类别名称\n\n字段名称\n\n字段类型\n\n是否是主键\n\n说明\n\nid\n\nINT\n\n是\n\n商品主键id\n\ncategory_id\n\nVARCHAR(30)\n\n否\n\n商品类别id\n\ngoods_name\n\nVARCHAR(30)\n\n否\n\n商品名称\n\nprice\n\nDECIMAL(10,2)\n\n否\n\n商品价格\n\n### 2.6 第三范式(3rd NF)\n\n举例1：\n\n部门信息表 ：每个部门有部门编号（\n\ndept_id）、部门名称、部门简介等信息。\n\n员工信息表 ：每个员工有员工编号、姓名、部门编号。列出部门编号后就不能再将部门名称、部门简介\n\n等与部门有关的信息再加入员工信息表中。\n\n如果不存在部门信息表，则根据第三范式（\n\n3NF）也应该构建它，否则就会有大量的数据冗余。\n\n举例2：\n\n商品类别名称依赖于商品类别编号，不符合第三范式。\n\n修改：\n\n表1：符合第三范式的 商品类别表 的设计\n\n表2：符合第三范式的 商品表 的设计\n\n商品表goods通过商品类别id字段（\n\ncategory_id）与商品类别表goods_category进行关联。\n\n举例3：\n\n球员player表 ：球员编号、姓名、球队名称和球队主教练。现在，我们把属性之间的依赖关系画出\n\n来，如下图所示：表名\n\n属性（字段）\n\n球员表\n\n球员编号、姓名和球队名称\n\n球队表\n\n球队名称、球队主教练\n\n你能看到球员编号决定了球队名称，同时球队名称决定了球队主教练，非主属性球队主教练就会传递依\n\n赖于球员编号，因此不符合 3NF 的要求。\n\n如果要达到 3NF 的要求，需要把数据表拆成下面这样：\n\n举例4：\n\n修改第二范式中的举例3。\n\n此时的Orders关系包含 orderid、orderdate、customerid 和 companyname 属性，主键定义为 orderid。\n\ncustomerid 和companyname均依赖于主键——orderid。例如，你需要通过orderid主键来查找代表订单中\n\n客户的customerid，同样，你需要通过 orderid 主键查找订单中客户的公司名称（\n\ncompanyname）。然\n\n而， customerid和companyname也是互相依靠的。为满足第三范式，可以改写如下：\n\n符合3NF后的数据模型通俗地讲，2NF和3NF通常以这句话概括：“每个非键属性依赖于键，依赖于\n\n整个键，并且除了键别无他物”。\n\n## 3 反范式化字段名称\n\n字段类型\n\n是否是主键\n\n说明\n\nid\n\nINT\n\n是\n\n商品id （主键）\n\ncategory_id\n\nVARCHAR(30)A\n\n否\n\n商品类别id\n\ncategory_name\n\nVARCHAR(30)\n\n否\n\n商品类别名称\n\ngoods_name\n\nVARCHAR(30)\n\n否\n\n商品名称\n\nprice\n\nDECIMAL(10,2)\n\n否\n\n商品价格\n\n### 3.1 概述\n\n规范化 vs 性能\n\n1. 为满足某种商业目标 , 数据库性能比规范化数据库更重要\n2. 在数据规范化的同时 , 要综合考虑数据库的性能\n3. 通过在给定的表中添加额外的字段，以大量减少需要从中搜索信息所需的时间\n4. 通过在给定的表中插入计算列，以方便查询\n\n### 3.2 应用举例\n\n举例1：\n\n员工的信息存储在 employees 表 中，部门信息存储在 departments 表 中。通过 employees 表中的\n\ndepartment_id字段与 departments 表建立关联关系。如果要查询一个员工所在部门的名称：\n\n如果经常需要进行这个操作，连接查询就会浪费很多时间。可以在 employees 表中增加一个冗余字段\n\ndepartment_name，这样就不用每次都进行连接操作了。\n\n举例2：\n\n反范式化的 goods商品信息表 设计如下：\n\n举例3： 我们有 2 个表，分别是 商品流水表（atguigu.trans ）和 商品信息表\n\n（atguigu.goodsinfo） 。商品流水表里有 400 万条流水记录，商品信息表里有 2000 条商品记录。\n\n商品流水表：\n\n商品信息表：\n\nselect employee_id,department_name\n\nfrom employees e join departments d\n\non e.department_id = d.department_id;新的商品流水表如下所示：\n\n举例4：\n\n课程评论表 class_comment ，对应的字段名称及含义如下：\n\n学生表 student ，对应的字段名称及含义如下：\n\n在实际应用中，我们在显示课程评论的时候，通常会显示这个学生的昵称，而不是学生 ID，因此当我们\n\n想要查询某个课程的前 1000 条评论时，需要关联 class_comment 和 student这两张表来进行查询。\n\n实验数据：模拟两张百万量级的数据表\n\n为了更好地进行 SQL 优化实验，我们需要给学生表和课程评论表随机模拟出百万量级的数据。我们可以\n\n通过存储过程来实现模拟数据。\n\n反范式优化实验对比\n\n如果我们想要查询课程 ID 为 10001 的前 1000 条评论，需要写成下面这样：\n\n运行结果（\n\n1000 条数据行）：\n\nSELECT p.comment_text, p.comment_time, stu.stu_name\n\nFROM class_comment AS p LEFT JOIN student AS stu\n\nON p.stu_id = stu.stu_id\n\nWHERE p.class_id = 10001\n\nORDER BY p.comment_id DESC\n\nLIMIT 1000;运行时长为 0.395 秒，对于网站的响应来说，这已经很慢了，用户体验会非常差。\n\n如果我们想要提升查询的效率，可以允许适当的数据冗余，也就是在商品评论表中增加用户昵称字段，\n\n在 class_comment 数据表的基础上增加 stu_name 字段，就得到了 class_comment2 数据表。\n\n这样一来，只需单表查询就可以得到数据集结果：\n\n运行结果（\n\n1000 条数据）：\n\n优化之后只需要扫描一次聚集索引即可，运行时间为 0.039 秒，查询时间是之前的 1/10。 你能看到，\n\n在数据量大的情况下，查询效率会有显著的提升。\n\n3.3 反范式的新问题\n\n存储 空间变大 了\n\n一个表中字段做了修改，另一个表中冗余的字段也需要做同步修改，否则 数据不一致\n\n若采用存储过程来支持数据的更新、删除等额外操作，如果更新频繁，会非常 消耗系统资源\n\n在 数据量小 的情况下，反范式不能体现性能的优势，可能还会让数据库的设计更加 复杂\n\nSELECT comment_text, comment_time, stu_name\n\nFROM class_comment2\n\nWHERE class_id = 10001\n\nORDER BY class_id DESC LIMIT 1000;3.4 反范式的适用场景\n\n当冗余信息有价值或者能 大幅度提高查询效率 的时候，我们才会采取反范式的优化。\n\n1. 增加冗余字段的建议\n2. 历史快照、历史数据的需要\n\n在现实生活中，我们经常需要一些冗余信息，比如订单中的收货人信息，包括姓名、电话和地址等。每\n\n次发生的 订单收货信息 都属于 历史快照 ，需要进行保存，但用户可以随时修改自己的信息，这时保存这\n\n些冗余信息是非常有必要的。\n\n反范式优化也常用在 数据仓库 的设计中，因为数据仓库通常 存储历史数据 ，对增删改的实时性要求不\n\n强，对历史数据的分析需求强。这时适当允许数据的冗余度，更方便进行数据分析。\n\n## 4 BCNF(巴斯范式)\n\n1. 案例\n\n我们分析如下表的范式情况：\n\n在这个表中，一个仓库只有一个管理员，同时一个管理员也只管理一个仓库。我们先来梳理下这些属性\n\n之间的依赖关系。\n\n仓库名决定了管理员，管理员也决定了仓库名，同时（仓库名，物品名）的属性集合可以决定数量这个\n\n属性。这样，我们就可以找到数据表的候选键。\n\n候选键 ：是（管理员，物品名）和（仓库名，物品名），然后我们从候选键中选择一个作为 主键 ，比\n\n如（仓库名，物品名）。\n\n主属性 ：包含在任一候选键中的属性，也就是仓库名，管理员和物品名。\n\n非主属性 ：数量这个属性。\n\n2. 是否符合三范式\n\n如何判断一张表的范式呢？我们需要根据范式的等级，从低到高来进行判断。\n\n首先，数据表每个属性都是原子性的，符合 1NF 的要求；\n\n其次，数据表中非主属性”数量“都与候选键全部依赖，（仓库名，物品名）决定数量，（管理员，物品\n\n名）决定数量。因此，数据表符合 2NF 的要求；\n\n最后，数据表中的非主属性，不传递依赖于候选键。因此符合 3NF 的要求。StudentId\n\nMajor\n\nAdvisor\n\nMajGPA\n\n1\n\n人工智能\n\nEdward\n\n4.0\n\n2\n\n大数据\n\nWilliam\n\n3.8\n\n1\n\n大数据\n\nWilliam\n\n3.7\n\n3\n\n大数据\n\nJoseph\n\n4.0\n\nStudentId\n\nAdvisor\n\nMajGPA\n\n1\n\nEdward\n\n4.0\n\n2\n\nWilliam\n\n3.8\n\n1\n\nWilliam\n\n3.7\n\n3\n\nJoseph\n\n4.0\n\n3. 存在的问题\n\n既然数据表已经符合了 3NF 的要求，是不是就不存在问题了呢？我们来看下面的情况：\n\n1. 增加一个仓库，但是还没有存放任何物品。根据数据表实体完整性的要求，主键不能有空值，因\n\n此会出现 插入异常 ；\n\n2. 如果仓库更换了管理员，我们就可能会 修改数据表中的多条记录 ；\n3. 如果仓库里的商品都卖空了，那么此时仓库名称和相应的管理员名称也会随之被删除。\n\n你能看到，即便数据表符合 3NF 的要求，同样可能存在插入，更新和删除数据的异常情况。\n\n4. 问题解决\n\n首先我们需要确认造成异常的原因：主属性仓库名对于候选键（管理员，物品名）是部分依赖的关系，\n\n这样就有可能导致上面的异常情况。因此引入BCNF，它在 3NF 的基础上消除了主属性对候选键的部分依\n\n赖或者传递依赖关系。\n\n如果在关系R中，U为主键，A属性是主键的一个属性，若存在A-\u003eY，Y为主属性，则该关系不属于\n\nBCNF。\n\n根据 BCNF 的要求，我们需要把仓库管理关系 warehouse_keeper 表拆分成下面这样：\n\n仓库表 ：（仓库名，管理员）\n\n库存表 ：（仓库名，物品名，数量）\n\n这样就不存在主属性对于候选键的部分依赖或传递依赖，上面数据表的设计就符合 BCNF。\n\n再举例：\n\n有一个 学生导师表 ，其中包含字段：学生ID，专业，导师，专业GPA，这其中学生ID和专业是联合主\n\n键。\n\n这个表的设计满足三范式，但是这里存在另一个依赖关系，“专业”依赖于“导师”，也就是说每个导师只\n\n做一个专业方面的导师，只要知道了是哪个导师，我们自然就知道是哪个专业的了。\n\n所以这个表的部分主键Major依赖于非主键属性Advisor，那么我们可以进行以下的调整，拆分成2个表：\n\n学生导师表：\n\n导师表：Advisor\n\nMajor\n\nEdward\n\n人工智能\n\nWilliam\n\n大数据\n\nJoseph\n\n大数据\n\nCourse\n\nTeacher\n\nBook\n\n英语\n\nBill\n\n人教版英语\n\n英语\n\nBill\n\n美版英语\n\n英语\n\nJay\n\n美版英语\n\n高数\n\nWilliam\n\n人教版高数\n\n高数\n\nDave\n\n美版高数\n\nCourse\n\nTeacher\n\n英语\n\nBill\n\n英语\n\nJay\n\n高数\n\nWilliam\n\n高数\n\nDave\n\n## 5 第四范式\n\n举例1：职工表(职工编号，职工孩子姓名，职工选修课程)。\n\n在这个表中，同一个职工可能会有多个职工孩子姓名。同样，同一个职工也可能会有多个职工选修课\n\n程，即这里存在着多值事实，不符合第四范式。\n\n如果要符合第四范式，只需要将上表分为两个表，使它们只有一个多值事实，例如： 职工表一 (职工编\n\n号，职工孩子姓名)， 职工表二 (职工编号，职工选修课程)，两个表都只有一个多值事实，所以符合第四\n\n范式。\n\n举例2：\n\n比如我们建立课程、教师、教材的模型。我们规定，每门课程有对应的一组教师，每门课程也有对应的\n\n一组教材，一门课程使用的教材和教师没有关系。我们建立的关系表如下：\n\n课程ID，教师ID，教材ID；这三列作为联合主键。\n\n为了表述方便，我们用Name代替ID，这样更容易看懂：\n\n这个表除了主键，就没有其他字段了，所以肯定满足BC范式，但是却存在 多值依赖 导致的异常。\n\n假如我们下学期想采用一本新的英版高数教材，但是还没确定具体哪个老师来教，那么我们就无法在这\n\n个表中维护Course高数和Book英版高数教材的的关系。\n\n解决办法是我们把这个多值依赖的表拆解成2个表，分别建立关系。这是我们拆分后的表：Course\n\nBook\n\n英语\n\n人教版英语\n\n英语\n\n美版英语\n\n高数\n\n人教版高数\n\n高数\n\n美版高数\n\n以及\n\n## 6 第五范式、域键范式\n\n除了第四范式外，我们还有更高级的第五范式（又称完美范式）和域键范式（\n\nDKNF）。\n\n在满足第四范式（\n\n4NF）的基础上，消除不是由候选键所蕴含的连接依赖。如果关系模式R中的每一个连\n\n接依赖均由R的候选键所隐含，则称此关系模式符合第五范式。\n\n函数依赖是多值依赖的一种特殊的情况，而多值依赖实际上是连接依赖的一种特殊情况。但连接依赖不\n\n像函数依赖和多值依赖可以由 语义直接导出 ，而是在 关系连接运算 时才反映出来。存在连接依赖的关系\n\n模式仍可能遇到数据冗余及插入、修改、删除异常等问题。\n\n第五范式处理的是 无损连接问题 ，这个范式基本 没有实际意义 ，因为无损连接很少出现，而且难以察\n\n觉。而域键范式试图定义一个 终极范式 ，该范式考虑所有的依赖和约束类型，但是实用价值也是最小\n\n的，只存在理论研究中。\n\n## 7 实战案例\n\n[视频讲解](https://www.bilibili.com/video/BV1iq4y1u7vj?from=search\u0026seid=4297501441472622157\u0026spm_id_from=333.337.0.0)  \n![](Pasted%20image%2020221117142235.png)  \n![](Pasted%20image%2020221117142408.png)  \n![](Pasted%20image%2020221117142838.png)  \n![](Pasted%20image%2020221117142812.png)\n\n## 8 ER模型\n\nER 模型中有三个要素，分别是实体、属性和关系。\n\n实体 ，可以看做是数据对象，往往对应于现实生活中的真实存在的个体。在 ER 模型中，用 矩形 来表\n\n示。实体分为两类，分别是 强实体 和 弱实体 。强实体是指不依赖于其他实体的实体；弱实体是指对另\n\n一个实体有很强的依赖关系的实体。\n\n属性 ，则是指实体的特性。比如超市的地址、联系电话、员工数等。在 ER 模型中用 椭圆形 来表示。\n\n关系 ，则是指实体之间的联系。比如超市把商品卖给顾客，就是一种超市与顾客之间的联系。在 ER 模\n\n型中用 菱形 来表示。\n\n注意：实体和属性不容易区分。这里提供一个原则：我们要从系统整体的角度出发去看，可以独立存在\n\n的是实体，不可再分的是属性。也就是说，属性不能包含其他属性。8.2 关系的类型\n\n在 ER 模型的 3 个要素中，关系又可以分为 3 种类型，分别是 一对一、一对多、多对多。\n\n一对一 ：指实体之间的关系是一一对应的，比如个人与身份证信息之间的关系就是一对一的关系。一个\n\n人只能有一个身份证信息，一个身份证信息也只属于一个人。\n\n一对多 ：指一边的实体通过关系，可以对应多个另外一边的实体。相反，另外一边的实体通过这个关\n\n系，则只能对应唯一的一边的实体。比如说，我们新建一个班级表，而每个班级都有多个学生，每个学\n\n生则对应一个班级，班级对学生就是一对多的关系。\n\n多对多 ：指关系两边的实体都可以通过关系对应多个对方的实体。比如在进货模块中，供货商与超市之\n\n间的关系就是多对多的关系，一个供货商可以给多个超市供货，一个超市也可以从多个供货商那里采购\n\n商品。再比如一个选课表，有许多科目，每个科目有很多学生选，而每个学生又可以选择多个科目，这\n\n就是多对多的关系。\n\n8.3 建模分析\n\nER 模型看起来比较麻烦，但是对我们把控项目整体非常重要。如果你只是开发一个小应用，或许简单设\n\n计几个表够用了，一旦要设计有一定规模的应用，在项目的初始阶段，建立完整的 ER 模型就非常关键\n\n了。开发应用项目的实质，其实就是 建模 。\n\n我们设计的案例是 电商业务 ，由于电商业务太过庞大且复杂，所以我们做了业务简化，比如针对\n\nSKU（\n\nStockKeepingUnit，库存量单位）和SPU（\n\nStandard Product Unit，标准化产品单元）的含义上，我\n\n们直接使用了SKU，并没有提及SPU的概念。本次电商业务设计总共有8个实体，如下所示。\n\n地址实体\n\n用户实体\n\n购物车实体\n\n评论实体\n\n商品实体\n\n商品分类实体\n\n订单实体\n\n订单详情实体\n\n其中， 用户 和 商品分类 是强实体，因为它们不需要依赖其他任何实体。而其他属于弱实体，因为它们\n\n虽然都可以独立存在，但是它们都依赖用户这个实体，因此都是弱实体。知道了这些要素，我们就可以\n\n给电商业务创建 ER 模型了，如图：在这个图中，地址和用户之间的添加关系，是一对多的关系，而商品和商品详情示一对1的关系，商品和\n\n订单是多对多的关系。 这个 ER 模型，包括了 8个实体之间的 8种关系。\n\n（\n\n1）用户可以在电商平台添加多个地址；\n\n（\n\n2）用户只能拥有一个购物车；\n\n（\n\n3）用户可以生成多个订单；\n\n（\n\n4）用户可以发表多条评论；\n\n（\n\n5）一件商品可以有多条评论；\n\n（\n\n6）每一个商品分类包含多种商品；\n\n（\n\n7）一个订单可以包含多个商品，一个商品可以在多个订单里。\n\n（\n\n8）订单中又包含多个订单详情，因为一个订单中可能包含不同种类的商品\n\n8.4 ER 模型的细化\n\n有了这个 ER 模型，我们就可以从整体上 理解 电商的业务了。刚刚的 ER 模型展示了电商业务的框架，\n\n但是只包括了订单，地址，用户，购物车，评论，商品，商品分类和订单详情这八个实体，以及它们之\n\n间的关系，还不能对应到具体的表，以及表与表之间的关联。我们需要把 属性加上 ，用 椭圆 来表示，\n\n这样我们得到的 ER 模型就更加完整了。\n\n因此，我们需要进一步去设计一下这个 ER 模型的各个局部，也就是细化下电商的具体业务流程，然后把\n\n它们综合到一起，形成一个完整的 ER 模型。这样可以帮助我们理清数据库的设计思路。\n\n接下来，我们再分析一下各个实体都有哪些属性，如下所示。\n\n（\n\n1） 地址实体 包括用户编号、省、市、地区、收件人、联系电话、是否是默认地址。\n\n（\n\n2） 用户实体 包括用户编号、用户名称、昵称、用户密码、手机号、邮箱、头像、用户级别。\n\n（\n\n3） 购物车实体 包括购物车编号、用户编号、商品编号、商品数量、图片文件url。（\n\n4） 订单实体 包括订单编号、收货人、收件人电话、总金额、用户编号、付款方式、送货地址、下单\n\n时间。\n\n（\n\n5） 订单详情实体 包括订单详情编号、订单编号、商品名称、商品编号、商品数量。\n\n（\n\n6） 商品实体 包括商品编号、价格、商品名称、分类编号、是否销售，规格、颜色。\n\n（\n\n7） 评论实体 包括评论id、评论内容、评论时间、用户编号、商品编号\n\n（\n\n8） 商品分类实体 包括类别编号、类别名称、父类别编号\n\n这样细分之后，我们就可以重新设计电商业务了，ER 模型如图：\n\n8.5 ER 模型图转换成数据表\n\n通过绘制 ER 模型，我们已经理清了业务逻辑，现在，我们就要进行非常重要的一步了：把绘制好的 ER\n\n模型，转换成具体的数据表，下面介绍下转换的原则：\n\n（\n\n1）一个 实体 通常转换成一个 数据表 ；\n\n（\n\n2）一个 多对多的关系 ，通常也转换成一个 数据表 ；\n\n（\n\n3）一个 1 对 1 ，或者 1 对多 的关系，往往通过表的 外键 来表达，而不是设计一个新的数据表；\n\n（\n\n4） 属性 转换成表的 字段 。\n\n下面结合前面的ER模型，具体讲解一下怎么运用这些转换的原则，把 ER 模型转换成具体的数据表，从\n\n而把抽象出来的数据模型，落实到具体的数据库设计当中。\n\n详情见视频讲解（https://www.bilibili.com/video/BV1iq4y1u7vj?from=search\u0026seid=429750144147262215\n\n7\u0026spm_id_from=333.337.0.0）\n\n其实，任何一个基于数据库的应用项目，都可以通过这种 先建立 ER 模型 ，再 转换成数据表 的方式，\n\n完成数据库的设计工作。创建 ER 模型不是目的，目的是把业务逻辑梳理清楚，设计出优秀的数据库。我\n\n建议你不是为了建模而建模，要利用创建 ER 模型的过程来整理思路，这样创建 ER 模型才有意义。9. 数据表的设计原则\n\n综合以上内容，总结出数据表设计的一般原则：\"三少一多\"\n\n1. 数据表的个数越少越好\n2. 数据表中的字段个数越少越好\n3. 数据表中联合主键的字段个数越少越好\n4. 使用主键和外键越多越好\n\n注意：这个原则并不是绝对的，有时候我们需要牺牲数据的冗余度来换取数据处理的效率。\n\n10. 数据库对象编写建议\n\n10.1 关于库\n\n1. 【强制】库的名称必须控制在32个字符以内，只能使用英文字母、数字和下划线，建议以英文字\n\n母开头。\n\n2. 【强制】库名中英文 一律小写 ，不同单词采用 下划线 分割。须见名知意。\n3. 【强制】库的名称格式：业务系统名称_子系统名。\n4. 【强制】库名禁止使用关键字（如type,order等）。\n5. 【强制】创建数据库时必须 显式指定字符集 ，并且字符集只能是utf8或者utf8mb4。\n\n创建数据库SQL举例：CREATE DATABASE crm_fund DEFAULT CHARACTER SET 'utf8' ;\n\n6. 【建议】对于程序连接数据库账号，遵循 权限最小原则\n\n使用数据库账号只能在一个DB下使用，不准跨库。程序使用的账号 原则上不准有drop权限 。\n\n7. 【建议】临时库以 tmp_ 为前缀，并以日期为后缀；\n\n备份库以 bak_ 为前缀，并以日期为后缀。10.2 关于表、列\n\n1. 【强制】表和列的名称必须控制在32个字符以内，表名只能使用英文字母、数字和下划线，建议\n\n以 英文字母开头 。\n\n2. 【强制】 表名、列名一律小写 ，不同单词采用下划线分割。须见名知意。\n3. 【强制】表名要求有模块名强相关，同一模块的表名尽量使用 统一前缀 。比如：crm_fund_item\n4. 【强制】创建表时必须 显式指定字符集 为utf8或utf8mb4。\n5. 【强制】表名、列名禁止使用关键字（如type,order等）。\n6. 【强制】创建表时必须 显式指定表存储引擎 类型。如无特殊需求，一律为InnoDB。\n7. 【强制】建表必须有comment。\n8. 【强制】字段命名应尽可能使用表达实际含义的英文单词或 缩写 。如：公司 ID，不要使用\n\ncorporation_id, 而用corp_id 即可。\n\n9. 【强制】布尔值类型的字段命名为 is_描述 。如member表上表示是否为enabled的会员的字段命\n\n名为 is_enabled。\n\n10. 【强制】禁止在数据库中存储图片、文件等大的二进制数据\n\n通常文件很大，短时间内造成数据量快速增长，数据库进行数据库读取时，通常会进行大量的随\n\n机IO操作，文件很大时，IO操作很耗时。通常存储于文件服务器，数据库只存储文件地址信息。\n\n11. 【建议】建表时关于主键： 表必须有主键 (1)强制要求主键为id，类型为int或bigint，且为\n\nauto_increment 建议使用unsigned无符号型。 (2)标识表里每一行主体的字段不要设为主键，建议\n\n设为其他字段如user_id，order_id等，并建立unique key索引。因为如果设为主键且主键值为随机\n\n插入，则会导致innodb内部页分裂和大量随机I/O，性能下降。\n\n12. 【建议】核心表（如用户表）必须有行数据的 创建时间字段 （\n\ncreate_time）和 最后更新时间字段\n\n（\n\nupdate_time），便于查问题。\n\n13. 【建议】表中所有字段尽量都是 NOT NULL 属性，业务可以根据需要定义 DEFAULT值 。 因为使用\n\nNULL值会存在每一行都会占用额外存储空间、数据迁移容易出错、聚合函数计算结果偏差等问\n\n题。\n\n14. 【建议】所有存储相同数据的 列名和列类型必须一致 （一般作为关联列，如果查询时关联列类型\n\n不一致会自动进行数据类型隐式转换，会造成列上的索引失效，导致查询效率降低）。\n\n15. 【建议】中间表（或临时表）用于保留中间结果集，名称以 tmp_ 开头。\n\n备份表用于备份或抓取源表快照，名称以 bak_ 开头。中间表和备份表定期清理。\n\n16. 【示范】一个较为规范的建表语句：\n\nCREATE TABLE user_info (\n\n`id` int unsigned NOT NULL AUTO_INCREMENT COMMENT '自增主键',\n\n`user_id` bigint(11) NOT NULL COMMENT '用户id',\n\n`username` varchar(45) NOT NULL COMMENT '真实姓名',\n\n`email` varchar(30) NOT NULL COMMENT '用户邮箱',\n\n`nickname` varchar(45) NOT NULL COMMENT '昵称',\n\n`birthday` date NOT NULL COMMENT '生日',\n\n`sex` tinyint(4) DEFAULT '0' COMMENT '性别',\n\n`short_introduce` varchar(150) DEFAULT NULL COMMENT '一句话介绍自己，最多50个汉字',\n\n`user_resume` varchar(300) NOT NULL COMMENT '用户提交的简历存放地址',\n\n`user_register_ip` int NOT NULL COMMENT '用户注册时的源ip',\n\n`create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',\n\n`update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE\n\nCURRENT_TIMESTAMP COMMENT '修改时间',\n\n`user_review_status` tinyint NOT NULL COMMENT '用户资料审核状态，1为通过，2为审核中，3为未\n\n通过，4为还未提交审核',17. 【建议】创建表时，可以使用可视化工具。这样可以确保表、字段相关的约定都能设置上。\n\n实际上，我们通常很少自己写 DDL 语句，可以使用一些可视化工具来创建和操作数据库和数据表。\n\n可视化工具除了方便，还能直接帮我们将数据库的结构定义转化成 SQL 语言，方便数据库和数据表结构\n\n的导出和导入。\n\n10.3 关于索引\n\n1. 【强制】InnoDB表必须主键为id int/bigint auto_increment，且主键值 禁止被更新 。\n2. 【强制】InnoDB和MyISAM存储引擎表，索引类型必须为 BTREE 。\n3. 【建议】主键的名称以 pk_ 开头，唯一键以 uni_ 或 uk_ 开头，普通索引以 idx_ 开头，一律\n\n使用小写格式，以字段的名称或缩写作为后缀。\n\n4. 【建议】多单词组成的columnname，取前几个单词首字母，加末单词组成column_name。如:\n\nsample 表 member_id 上的索引：idx_sample_mid。\n\n5. 【建议】单个表上的索引个数 不能超过6个 。\n6. 【建议】在建立索引时，多考虑建立 联合索引 ，并把区分度最高的字段放在最前面。\n7. 【建议】在多表 JOIN 的SQL里，保证被驱动表的连接列上有索引，这样JOIN 执行效率最高。\n8. 【建议】建表或加索引时，保证表里互相不存在 冗余索引 。 比如：如果表里已经存在key(a,b)，\n\n则key(a)为冗余索引，需要删除。\n\n10.4 SQL编写\n\n1. 【强制】程序端SELECT语句必须指定具体字段名称，禁止写成 *。\n2. 【建议】程序端insert语句指定具体字段名称，不要写成INSERT INTO t1 VALUES(…)。\n3. 【建议】除静态表或小表（\n\n100行以内），DML语句必须有WHERE条件，且使用索引查找。\n\n4. 【建议】INSERT INTO…VALUES(XX),(XX),(XX).. 这里XX的值不要超过5000个。 值过多虽然上线很\n\n快，但会引起主从同步延迟。\n\n5. 【建议】SELECT语句不要使用UNION，推荐使用UNION ALL，并且UNION子句个数限制在5个以\n\n内。\n\n6. 【建议】线上环境，多表 JOIN 不要超过5个表。\n7. 【建议】减少使用ORDER BY，和业务沟通能不排序就不排序，或将排序放到程序端去做。ORDER\n\nBY、GROUP BY、DISTINCT 这些语句较为耗费CPU，数据库的CPU资源是极其宝贵的。\n\n8. 【建议】包含了ORDER BY、GROUP BY、DISTINCT 这些查询的语句，WHERE 条件过滤出来的结果\n\n集请保持在1000行以内，否则SQL会很慢。\n\n9. 【建议】对单表的多次alter操作必须合并为一次\n\n对于超过100W行的大表进行alter table，必须经过DBA审核，并在业务低峰期执行，多个alter需整\n\n合在一起。 因为alter table会产生 表锁 ，期间阻塞对于该表的所有写入，对于业务可能会产生极\n\n大影响。\n\n10. 【建议】批量操作数据时，需要控制事务处理间隔时间，进行必要的sleep。\n11. 【建议】事务里包含SQL不超过5个。\n\n因为过长的事务会导致锁数据较久，MySQL内部缓存、连接消耗过多等问题。\n\n12. 【建议】事务里更新语句尽量基于主键或UNIQUE KEY，如UPDATE… WHERE id=XX;\n\nPRIMARY KEY (`id`),\n\nUNIQUE KEY `uniq_user_id` (`user_id`),\n\nKEY `idx_username`(`username`),\n\nKEY `idx_create_time_status`(`create_time`,`user_review_status`)\n\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='网站用户基本信息'否则会产生间隙锁，内部扩大锁定范围，导致系统性能下降，产生死锁。\n\n## 11 PowerDesigner的使用\n\nPowerDesigner是一款开发人员常用的数据库建模工具，用户利用该软件可以方便地制作 数据流程图、概念数据模型、物理数据模型它几乎包括了数据库模型设计的全过程，是Sybase公司为企业建模和设计提供的一套完整的集成化企业级建模解决方案。\n\n### 11.1 开始界面\n\n当前使用的PowerDesigner版本是16.5的。打开软件即是此页面，可选择Create Model,也可以选择Do NotShow page Again,自行在打开软件后创建也可以！完全看个人的喜好，在此我在后面的学习中不在显示此\n\n页面。\n\n“Create Model”的作用类似于普通的一个文件，该文件可以单独存放也可以归类存放。\n\n“Create Project”的作用类似于文件夹，负责把有关联关系的文件集中归类存放。\n\n11.2 概念数据模型\n\n常用的模型有4种，分别是 概念模型(CDM Conceptual Data Model) ， 物理模型（PDM,Physical\n\nData Model） ， 面向对象的模型（OOM Objcet Oriented Model） 和 业务模型（BPM Business\n\nProcess Model） ，我们先创建概念数据模型。点击上面的ok，即可出现下图左边的概念模型1，可以自定义概念模型的名字，在概念模型中使用最多的\n\n就是如图所示的Entity(实体),Relationship(关系)Entity实体\n\n选中右边框中Entity这个功能，即可出现下面这个方框，需要注意的是书写name的时候，code自行补\n\n全，name可以是英文的也可以是中文的，但是code必须是英文的。\n\n填充实体字段\n\nGeneral中的name和code填好后，就可以点击Attributes（属性）来设置name（名字），code(在数据库中\n\n的字段名)，Data Type(数据类型) ，length(数据类型的长度)\n\nName: 实体名字一般为中文，如论坛用户\n\nCode: 实体代号，一般用英文，如XXXUser\n\nComment:注释，对此实体详细说明\n\nCode属性：代号，一般用英文UID DataType\n\nDomain域，表示属性取值范围如可以创建10个字符的地址域\n\nM:Mandatory强制属性，表示该属性必填。不能为空\n\nP:Primary Identifer是否是主标识符，表示实体唯一标识符\n\nD:Displayed显示出来，默认全部勾选在此上图说明name和code的起名方法\n\n设置主标识符\n\n如果不希望系统自动生成标识符而是手动设置的话，那么切换到Identifiers选项卡，添加一行Identifier，\n\n然后单击左上角的“属性”按钮，然后弹出的标识属性设置对话框中单击“添加行”按钮，选择该标识中使\n\n用的属性。例如将学号设置为学生实体的标识。放大模型\n\n创建好概念数据模型如图所示，但是创建好的字体很小，读者可以按着ctrl键同时滑动鼠标的可滑动按钮\n\n即可放大缩写字体，同时也可以看到主标识符有一个*号的标志，同时也显示出来了，name,Data type和\n\nlength这些可见的属性实体关系\n\n同理创建一个班级的实体（需要特别注意的是，点击完右边功能的按钮后需要点击鼠标指针状态的按钮\n\n或者右击鼠标即可，不然很容易乱操作，这点注意一下就可以了），然后使用Relationship（关系）这个\n\n按钮可以连接学生和班级之间的关系，发生一对多（班级对学生）或者多对一（学生对班级）的关系。\n\n如图所示\n\n需要注意的是点击Relationship这个按钮，就把班级和学生联系起来了，就是一条线，然后双击这条线进\n\n行编辑，在General这块起name和code上面的name和code起好后就可以在Cardinalities这块查看班级和学生的关系，可以看到班级的一端是一\n\n条线，学生的一端是三条，代表班级对学生是一对多的关系即one对many的关系，点击应用，然后确定\n\n即可一对多和多对一练习完还有多对多的练习，如下图操作所示，老师实体和上面介绍的一样，自己将\n\nname，data type等等修改成自己需要的即可，满足项目开发需求即可。（\n\ncomment是解释说明，自己可\n\n以写相关的介绍和说明）多对多需要注意的是自己可以手动点击按钮将关系调整称为多对多的关系many对many的关系，然后点\n\n击应用和确定即可综上即可完成最简单的学生，班级，教师这种概念数据模型的设计，需要考虑数据的类型和主标识码，\n\n是否为空。关系是一对一还是一对多还是多对多的关系，自己需要先规划好再设计，然后就ok了。11.3 物理数据模型\n\n上面是概念数据模型，下面介绍一下物理数据模型，以后 经常使用 的就是物理数据模型。打开\n\nPowerDesigner，然后点击File--\u003eNew Model然后选择如下图所示的物理数据模型，物理数据模型的名字\n\n自己起，然后选择自己所使用的数据库即可。\n\n创建好主页面如图所示，但是右边的按钮和概念模型略有差别，物理模型最常用的三个是\n\ntable(表) ， view(视图) ， reference(关系) ；鼠标先点击右边table这个按钮然后在新建的物理模型点一下，即可新建一个表，然后双击新建如下图所\n\n示，在General的name和code填上自己需要的，点击应用即可），如下图：\n\n然后点击Columns,如下图设置，非常简单，需要注意的就是P（\n\nprimary主键） , F （\n\nforeign key外键） ,\n\nM（mandatory强制性的，代表不可为空） 这三个。在此设置学号的自增（MYSQL里面的自增是这个AUTO_INCREMENT），班级编号同理，不多赘述！\n\n在下面的这个点上对号即可，就设置好了自增全部完成后如下图所示。\n\n班级物理模型同理如下图所示创建即可完成后如下图所示上面的设置好如上图所示，然后下面是关键的地方，点击右边按钮Reference这个按钮，因为是班级对学\n\n生是一对多的，所以鼠标从学生拉到班级如下图所示，学生表将发生变化，学生表里面增加了一行，这\n\n行是班级表的主键作为学生表的外键，将班级表和学生表联系起来。（仔细观察即可看到区别。）做完上面的操作，就可以双击中间的一条线，显示如下图，修改name和code即可\n\n但是需要注意的是，修改完毕后显示的结果却如下图所示，并没有办法直接像概念模型那样，修改过后\n\n显示在中间的那条线上面，自己明白即可。学习了多对一或者一对多的关系，接下来学习多对对的关系，同理自己建好老师表，这里不在叙述，记\n\n得老师编号自增，建好如下图所示\n\n下面是多对多关系的关键，由于物理模型多对多的关系需要一个中间表来连接，如下图，只设置一个字\n\n段，主键，自增点击应用，然后设置Columns，只添加一个字段\n\n这是设置字段递增，前面已经叙述过好几次设置好后如下图所示，需要注意的是有箭头的一方是一，无箭头的一方是多，即一对多的多对一的关系\n\n需要搞清楚，学生也可以有很多老师，老师也可以有很多学生，所以学生和老师都可以是主体；\n\n可以看到添加关系以后学生和教师的关系表前后发生的变化11.4 概念模型转为物理模型\n\n1：如下图所示先打开概念模型图，然后点击Tool,如下图所示\n\n点开的页面如下所示，name和code已经从概念模型1改成物理模型1了完成后如下图所示，将自行打开修改的物理模型，需要注意的是这些表的数据类型已经自行改变了，而\n\n且中间表出现两个主键，即双主键，\n\n11.5 物理模型转为概念模型\n\n上面介绍了概念模型转物理模型，下面介绍一下物理模型转概念模型（如下图点击操作即可）然后出现如下图所示界面，然后将物理修改为概念 ，点击应用确认即可点击确认后将自行打开如下图所示的页面，自己观察有何变化，如果转换为oracle的，数据类型会发生变\n\n化，比如Varchar2等等）；\n\n11.6 物理模型导出SQL语句\n\n下面介绍一下物理模型导出SQL语句（点击Database按钮的Generate Database或者按ctrl+G）\n\n打开之后如图所示，修改好存在sql语句的位置和生成文件的名称即可在Selection中选择需要导出的表，然后点击应用和确认即可\n\n完成以后出现如下图所示，可以点击Edit或者close按钮自此，就完成了导出sql语句，就可以到自己指定的位置查看导出的sql语句了；PowerDesigner在以后在\n\n项目开发过程中用来做需求分析和数据库的设计非常的方便和快捷。\n","lastmodified":"2023-05-09T16:33:58.263365716Z","tags":[]},"/13-%E4%BA%8B%E5%8A%A1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86":{"title":"13 事务基础知识","content":"\n## 1 数据库事务概述\n\n### 1.1 存储引擎支持情况\n\n`SHOW ENGINES` 命令来查看当前 MySQL 支持的存储引擎都有哪些，以及这些存储引擎是否支持事务。  \n![](Pasted%20image%2020221117151525.png)  \n能看出在 MySQL 中，只有InnoDB 是支持事务的。\n\n### 1.2 基本概念\n\n**事务**：一组逻辑操作单元，使数据从一种状态变换到另一种状态。  \n**事务处理的原则**：保证所有事务都作为**一个工作单元**来执行，即使出现了故障，都不能改变这种执行方式。当在一个事务中执行多个操作时，要么**所有的事务都被提交**( commit )，那么这些修改就永久地保存下来；要么数据库管理系统将**放弃所作的所有修改**，整个事务回滚(ollback )到最初状态。\n\n### 1.3 事务的ACID特性\n\n**原子性（atomicity）**：原子性是指事务是一个不可分割的工作单位，要么全部提交，要么全部失败回滚。  \n**一致性（consistency）**：（国内很多网站上对一致性的阐述有误，具体可以参考[Wikipedia](https://en.wikipedia.org/wiki/ACID)）根据定义，*一致性是指事务执行前后，数据从一个合法性状态变换到另外一个合法性状态*。这种状态是语义上的而不是语法上的，跟具体的业务有关。  \n那什么是合法的数据状态呢？满足预定的约束的状态就叫做合法的状态。通俗一点，这状态是由你自己来定义的（比如满足现实世界中的约束）。满足这个状态，数据就是一致的，不满足这个状态，数据就是不一致的！如果事务中的某个操作失败了，系统就会自动撤销当前正在执行的事务，返回到事务操作之前的状态。  \n**隔离型（isolation）**：事务的隔离性是指**一个事务的执行不能被其他事务干扰**，即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。  \n如果无法保证隔离性会怎么样？假设A账户有200元，B账户0元。A账户往B账户转账两次，每次金额为50元，分别在两个事务中执行。如果无法保证隔离性，会出现下面的情形：\n\n```mysql\nUPDATE accounts SET money = money - 50 WHERE NAME = 'AA';\nUPDATE accounts SET money = money + 50 WHERE NAME = 'BB';\n```\n\n![500](Pasted%20image%2020221117152043.png)  \n**持久性（durability）**：*持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来的其他操作和数据库故障不应该对其有任何影响*。持久性是通过事务日志来保证的。日志包括了重做日志和回滚日志 。当我们通过事务对数据进行修改的时候，首先会将数据库的变化信息记录到重做日志中，然后再对数据库中对应的行进行修改。这样做的好处是，即使数据库系统崩溃，数据库重启后也能找到没有更新到数据库系统中的重做日志，重新执行，从而使事务具有持久性。\n\n### 1.4 事务的状态\n\n我们现在知道事务是一个抽象的概念，它其实对应着一个或多个数据库操作，MySQL根据这些操作所执行的不同阶段把事务大致划分成几个状态：\n\n1. 活动的（active）  \n事务对应的数据库操作正在执行过程中时，我们就说该事务处在活动的状态。\n2. 部分提交的（partially committed）  \n当事务中的最后一个操作执行完成，但由于操作都在内存中执行，所造成的影响并没有刷新到磁盘时，我们就说该事务处在部分提交的状态。\n3. 失败的（failed）  \n当事务处在活动的或者部分提交的状态时，可能遇到了某些错误（数据库自身的错误、操作系统  \n错误或者直接断电等）而无法继续执行，或者人为的停止当前事务的执行，我们就说该事务处在 失败的状态。\n4. 中止的（aborted）  \n如果事务执行了一部分而变为失败的状态，那么就需要把已经修改的事务中的操作还原到事务执行前的状态。换句话说，就是要撤销失败事务对当前数据库造成的影响。我们把这个撤销的过程称之为回滚 。**当回滚操作执行完毕时，也就是数据库恢复到了执行事务之前的状态，我们就说该事务处在了中止的状态**。  \n举例：\n\n```mysql\nUPDATE accounts SET money = money - 50 WHERE NAME = 'AA'; \nUPDATE accounts SET money = money + 50 WHERE NAME = 'BB';\n```\n\n5. 提交的（committed）  \n当一个处在部分提交的状态的事务将修改过的数据都同步到磁盘上之后，我们就可以说该事务处在了提交的状态。  \n![500](Pasted%20image%2020221117152755.png)\n\n## 2 如何使用事务\n\n使用事务有两种方式，分别为显式事务和隐式事务。\n\n### 2.1 显式事务\n\n**步骤1**： START TRANSACTION 或者 BEGIN ，作用是显式开启一个事务。  \n`mysql\u003e BEGIN; 或者 mysql\u003e START TRANSACTION;\n\n\u003eSTART TRANSACTION 语句相较于 BEGIN 特别之处在于，后边能跟随几个修饰符:  \nREAD ONLY:标识当前事务是一个只读事务 ，也就是属于该事务的数据库操作只能读取数据，而不能修改数据。  \nREAD WRITE ：标识当前事务是一个读写事务，也就是属于该事务的数据库操作既可以读取数据，也可以修改数据。  \nWITH CONSISTENT SNAPSHOT ：启动一致性读。\n\n**步骤2**：一系列事务中的操作（主要是DML，不含DDL）\n\n**步骤3**：提交事务 或 中止事务（即回滚事务）\n\n```mysql\n# 提交事务。当提交事务后，对数据库的修改是永久性的。 \nmysql\u003e COMMIT;\n\n# 回滚事务。即撤销正在进行的所有没有提交的修改\nmysql\u003e ROLLBACK;\n\n# 将事务回滚到某个保存点。\nmysql\u003e ROLLBACK TO [SAVEPOINT]\n```\n\n### 2.2 隐式事务\n\nMySQL中有一个系统变量 autocommit ：\n\n```mysql\nmysql\u003e SHOW VARIABLES LIKE 'autocommit'; \n+---------------+-------+\n| Variable_name | Value | \n+---------------+-------+\n| autocommit | ON | \n+---------------+-------+\n1 row in set (0.01 sec)\n```\n\n当然，如果我们想关闭这种自动提交的功能，可以使用下边两种方法之一：\n\n1. 显式的的使用 START TRANSACTION 或者 BEGIN 语句开启一个事务。这样在本次事务提交或者回滚前会暂时关闭掉自动提交的功能。\n2. 把系统变量 autocommit 的值设置为 OFF ，就像这样：`SET autocommit = OFF; #或SET autocommit = 0;`\n\n### 2.3 隐式提交数据的情况\n\n- 数据定义语言（Data definition language，缩写为：DDL）  \n![500](Pasted%20image%2020221117165028.png)\n- 隐式使用或修改mysql数据库中的表  \n![500](Pasted%20image%2020221117165046.png)\n- 事务控制或关于锁定的语句\n    1. 当我们在一个事务还没提交或者回滚时就又使用 START TRANSACTION 或者 BEGIN 语句开启了另一个事务时，会 隐式的提交 上一/]个事务。即：\n    2. 当前的autocommit系统变量的值为OFF，我们手动把它调为ON时，也会隐式的提交前边语句所属的事务。\n    3. 使用LOCK TABLES 、 UNLOCK TABLES等关于锁定的语句也会隐式的提交前边语句所属的事务。  \n![600](Pasted%20image%2020221117165156.png)\n\n## 3 事务隔离级别\n\nMySQL是一个客户端／服务器架构 的软件，对于同一个服务器来说，可以有若干个客户端与之连接，每个客户端与服务器连接上之后，就可以称为一个会话（ Session ）。每个客户端都可以在自己的会话中向服务器发出请求语句，一个请求语句可能是某个事务的一部分，也就是对于服务器来说可能同时处理多个事务。事务有隔离性的特性，理论上在某个事务对某个数据进行访问 时，其他事务应该进行排队，当该事务提交之后，其他事务才可以继续访问这个数据。但是这样对性能影响太大 ，**我们既想保持事务的隔离性，又想让服务器在处理访问同一数据的多个事务时 性能尽量高些**，那就看二者如何权衡取舍了。\n\n### 3.1 数据准备\n\n我们需要创建一个表：\n\n```mysql\nCREATE TABLE student ( \nstudentno INT, \nname VARCHAR(20), \nclass varchar(20), \nPRIMARY KEY (studentno) \n) Engine=InnoDB CHARSET=utf8;\n```\n\n然后向这个表里插入一条数据：  \n`INSERT INTO student VALUES(1, '小谷', '1班');`\n\n### 3.2 数据并发问题\n\n针对事务的隔离性和并发性，我们怎么做取舍呢？先看一下访问相同数据的事务在 不保证串行执行（也就是执行完一个再执行另一个）的情况下可能会出现哪些问题：\n\n1. 脏写（ Dirty Write ）  \n对于两个事务 Session A、Session B，如果事务Session A修改了另一个未提交事务Session B修改过的数据，那就意味着发生了脏写。\n2. 脏读（ Dirty Read ）  \n对于两个事务 Session A、Session B，Session A 读取了已经被 Session B 更新但还 没有被提交 的字段。之后若 Session B回滚 ，Session A读取的内容就是临时且无效的。  \nSession A和Session B各开启了一个事务，Session B中的事务先将studentno列为1的记录的name列更新为'张三'，然后Session A中的事务再去查询这条studentno为1的记录，如果读到列name的值为'张三'，而Session B中的事务稍后进行了回滚，那么Session A中的事务相当于读到了一个不存在的数据，这种现象就称之为脏读 。\n3. 不可重复读（ Non-Repeatable Read ）  \n对于两个事务Session A、Session B，Session A 读取 了一个字段，然后 Session B 更新了该字段。 之后Session A 再次读取同一个字段，值就不同了。那就意味着发生了不可重复读。我们在Session B中提交了几个隐式事务 （注意是隐式事务，意味着语句结束事务就提交了），这些事务都修改了studentno列为1的记录的列name的值，每次事务提交之后，如果Session A中的事务都可以查看到最新的值，这种现象也被称之为不可重复读。\n4. 幻读（ Phantom ）  \n对于两个事务Session A、Session B, Session A 从一个表中读取了一个字段, 然后 Session B 在该表中插入了一些新的行。之后, 如果Session A再次读取同一个表, 就会多出几行。那就意味着发生了幻读。  \nSession A中的事务先根据条件 studentno \u003e 0这个条件查询表student，得到了name列值为'张三'的记录；之后Session B中提交了一个 隐式事务 ，该事务向表student中插入了一条新记录；之后Session A中的事务再根据相同的条件 studentno \u003e 0查询表student，得到的结果集中包含Session B中的事务新插入的那条记录，这种现象也被称之为 幻读 。我们把新插入的那些记录称之为幻影记录。\n\n### 3.3 SQL中的四种隔离级别\n\n上面介绍了几种并发事务执行过程中可能遇到的一些问题，这些问题有轻重缓急之分，我们给这些问题按照严重性来排一下序：`脏写 \u003e 脏读 \u003e 不可重复读 \u003e 幻读`\n\n我们愿意舍弃一部分隔离性来换取一部分性能在这里就体现在：设立一些隔离级别，隔离级别越低，并发问题发生的就越多。 SQL标准中设立了4个隔离级别 ：\n\n- READ UNCOMMITTED ：读未提交，在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。**不能避免脏读、不可重复读、幻读。**\n- READ COMMITTED ：读已提交，它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。**可以避免脏读，但不可重复读、幻读问题仍然存在。**\n- REPEATABLE READ ：可重复读，事务A在读到一条数据之后，此时事务B对该数据进行了修改并提交，那么事务A再读该数据，读到的还是原来的内容。**可以避免脏读、不可重复读，但幻读问题仍然存在。这是MySQL的默认隔离级别。**\n- SERIALIZABLE ：可串行化，确保事务可以从一个表中读取相同的行。在这个事务持续期间，禁止其他事务对该表执行插入、更新和删除操作。所有的并发问题都可以避免，但性能十分低下。**能避免脏读、不可重复读和幻读。**\n\nSQL标准 中规定，针对不同的隔离级别，并发事务可以发生不同严重程度的问题，具体情况如下：  \n![](Pasted%20image%2020221117163029.png)  \n脏写怎么没涉及到？因为脏写这个问题太严重了，不论是哪种隔离级别，都不允许脏写的情况发生。不同的隔离级别有不同的现象，并有不同的锁和并发机制，隔离级别越高，数据库的并发性能就越差，4种事务隔离级别与并发性能的关系如下：  \n![500](Pasted%20image%2020221117163131.png)\n\n### 3.4 MySQL支持的四种隔离级别\n\nMySQL的默认隔离级别为REPEATABLE READ，我们可以手动修改一下事务的隔离级别\n\n```mysql\n# 查看隔离级别，MySQL 5.7.20的版本之前：\nmysql\u003e SHOW VARIABLES LIKE 'tx_isolation';\n\n+---------------+-----------------+\n\n| Variable_name | Value |\n\n+---------------+-----------------+\n\n| tx_isolation | REPEATABLE-READ |\n\n+---------------+-----------------+\n\n1 row in set (0.00 sec)\n\n# MySQL 5.7.20版本之后，引入transaction_isolation来替换tx_isolation\n\n# 查看隔离级别，MySQL 5.7.20的版本及之后：\n\nmysql\u003e SHOW VARIABLES LIKE 'transaction_isolation';\n\n+-----------------------+-----------------+\n\n| Variable_name | Value |\n\n+-----------------------+-----------------+\n\n| transaction_isolation | REPEATABLE-READ |\n\n+-----------------------+-----------------+\n\n1 row in set (0.02 sec)\n\n#或者不同MySQL版本中都可以使用的：\n\nSELECT @@transaction_isolation;\n```\n\n### 3.5 如何设置事务的隔离级别\n\n通过下面的语句修改事务的隔离级别：\n\n```mysql\nSET [GLOBAL|SESSION] TRANSACTION ISOLATION LEVEL 隔离级别;\n\n#其中，隔离级别格式：\n\u003e READ UNCOMMITTED\n\u003e READ COMMITTED\n\u003e REPEATABLE READ\n\u003e SERIALIZABLE\n\nSET [GLOBAL|SESSION] TRANSACTION_ISOLATION = '隔离级别'\n#其中，隔离级别格式：\n\u003e READ-UNCOMMITTED\n\u003e READ-COMMITTED\n\u003e REPEATABLE-READ\n\u003e SERIALIZABLE\n```\n\n## 4. 事务的常见分类\n\n从事务理论的角度来看，可以把事务分为以下几种类型：  \n![](Pasted%20image%2020221117165738.png)  \n![](Pasted%20image%2020221117165800.png)  \n![](Pasted%20image%2020221117165717.png)\n","lastmodified":"2023-05-09T16:33:58.263365716Z","tags":[]},"/15-%E9%94%81":{"title":"15 锁","content":"\n事务的隔离性由这章讲述的**锁**来实现\n\n## 1 概述\n\n在数据库中，除传统的计算资源（如CPU、RAM、I/O等）的争用以外，数据也是一种供许多用户共享的资源。为保证数据的一致性，需要对并发操作进行控制 ，因此产生了锁 。同时锁机制 也为实现MySQL的各个隔离级别提供了保证。**锁冲突也是影响数据库并发访问性能的一个重要因素**。所以锁对数据库而言显得尤其重要，也更加复杂。\n\n## 2 MySQL并发事务访问相同记录\n\n并发事务访问相同记录的情况大致可以划分为3种：\n\n### 2.1 读-读情况\n\n读-读情况，即并发事务相继读取相同的记录。读取操作本身不会对记录有任何影响，并不会引起什么问题，所以允许这种情况的发生。\n\n### 2.2 写-写情况\n\n写-写情况，即并发事务相继对相同的记录做出改动。  \n在这种情况下会发生**脏写**的问题，任何一种隔离级别都不允许这种问题的发生。所以在多个未提交事务相继对一条记录做改动时，需要让它们排队执行，这个排队的过程其实是通过锁来实现的。这个所谓的锁其实是一个内存中的结构，在事务执行前本来是没有锁的，也就是说一开始是没有锁结构和记录进行关联的，如图所示：  \n![](Pasted%20image%2020221118132128.png)  \n当一个事务想对这条记录做改动时，首先会看看内存中有没有与这条记录关联的**锁结构**，当没有的时候就会在内存中生成一个锁结构与之关联。比如，事务T1要对这条记录做改动，就需要生成一个锁结构与之关联：  \n![](Pasted%20image%2020221118150029.png)  \n在锁结构里有很多信息，为了简化理解，只把两个比较重要的属性拿了出来：\n\n1. trx信息：代表这个锁结构是哪个事务生成的。\n2. is_waiting ：代表当前事务是否在等待。  \n当事务T1改动了这条记录后，就生成了一个锁结构与该记录关联，**因为之前没有别的事务为这条记录加锁，所以is-waiting 属性就是 fa1se，我们把这个场景就称之为获取锁成功，或者 加锁成功，然后就可以继续执行操作了**。  \n在事务T1提交之前，另一个事务T2也想对该记录做改动，那么先看看有没有锁结构与这条记录关联，发现有一个锁结构与之关联后，**然后也生成了一个锁结构与这条记录关联，不过锁结构的 is_waiting属性值为true**，表示当前事务需要等待，我们把这个场景就称之为 获取锁失败，或者 加锁失败，图示：  \n![](Pasted%20image%2020221118150153.png)  \n在事务丁1提交之后，就会把该事务生成的锁结构释放掉，然后看看还有没有别的事务在等待获取锁，发现了事务T2还在等待获取锁，所以把事务 T2 对应的锁结构的is_waiting 属性设置为false，然后把该事务对应的线程唤醒，让它继续执行，此时事务T2就算获取到锁了。效果图就是这样：![](Pasted%20image%2020221118150230.png)\n\n小结几种说法：\n\n- 不加锁  \n    意思就是不需要在内存中生成对应的 锁结构 ，可以直接执行操作。\n- 获取锁成功，或者加锁成功  \n    意思就是在内存中生成了对应的锁结构 ，而且锁结构的 is_waiting 属性为 false ，也就是事务可以继续执行操作。\n- 获取锁失败，或者加锁失败，或者没有获取到锁  \n    意思就是在内存中生成了对应的锁结构，不过锁结构的 is_waiting 属性为 true ，也就是事务需要等待，不可以继续执行操作。\n\n### 2.3 读-写或写-读情况\n\n读-写 或 写-读 ，即一个事务进行读取操作，另一个进行改动操作。这种情况下可能发生**脏读、不可重复读、 幻读**的问题。  \n各个数据库厂商对 SQL标准 的支持都可能不一样。比如MySQL在 REPEATABLE READ 隔离级别上就已经解决了幻读问题。\n\n### 2.4 并发问题的解决方案\n\n怎么解决脏读、不可重复读、幻读这些问题呢？其实有两种可选的解决方案：\n\n1. **读操作利用多版本并发控制（ MVCC ，下章讲解），写操作进行加锁**。  \n        所谓的 MVCC ，就是生成一个ReadView，通过Readview找到符合条件的记录版本（历史版本由 undo日志构建）。查询语句只能读到在生成ReadView之前 已提交事务所做的更改，在生成ReadView之前未提交的事务或者之后才开启的事务所做的更改是看不到的。而写操作 肯定针对的是 最新版本的记录，读记录的历史版本和改动记录的最新版本本身并不冲突，也就是采用MVCC时，读-写操作并不冲突。\n    - 普通的SELECT语句在READ COMMITTED和REPEATABLE READ隔离级别下会使用到MVCC读取记录。在 READ COMMITTED 隔离级别下，一个事务在执行过程中每次执行SELECT操作时都会生成一个ReadView，ReadView的存在本身就保证了 事务不可以读取到未提交的事务所做的更改 ，也就是**避免了脏读现象**； ^5d06c5\n    - 在 REPEATABLE READ 隔离级别下，一个事务在执行过程中只有 第一次执行SELECT操作 才会生成一个ReadView，之后的SELECT操作都 复用 这个ReadView，这样也就**避免了不可重复读和幻读的问题**。\n2. **读、写操作都采用加锁的方式**。  \n    如果我们的一些业务场景不允许读取记录的旧版本，而是每次都必须去读取记录的最新版本。比如，在银行存款的事务中，你需要先把账户的余额读出来，然后将其加上本次存款的数额，最后再写到数据库中。在将账户余额读取出来后，就不想让别的事务再访问该余额，直到本次存款事务执行完成，其他事务才可以访问账户的余额。这样**在读取记录的时候就需要对其进行 加锁操作，这样也就意味着 读操作和 写操作也像写-写 操作那样排队行**。\n    - 脏读 的产生是因为当前事务读取了另一个未提交事务写的一条记录，如果另一个事务在写记录的时候就给这条记录加锁，那么当前事务就无法继续读取该记录了，所以也就不会有脏读问题的产生了。\n    - 不可重复读 的产生是因为当前事务先读取一条记录，另外一个事务对该记录做了改动之后并提交之后，当前事务再次读取时会获得不同的值，如果在当前事务读取记录时就给该记录加锁，那么另一个事务就无法修改该记录，自然也不会发生不可重复读了。\n    - 幻读 问题的产生是因为当前事务读取了一个范围的记录，然后另外的事务向该范围内插入了新记录，当前事务再次读取该范围的记录时发现了新插入的新记录。采用加锁的方式解决幻读问题就有一些麻烦，因为当前事务在第一次读取记录时幻影记录并不存在，所以渎取的时候加锁就有点尴尬（因为你并不知道给谁加锁）。\n    \n\n小结对比发现：\n\n- 采用 MVCC 方式的话， 读-写 操作彼此并不冲突，性能更高。\n- 采用 加锁 方式的话， 读-写 操作彼此需要排队执行，影响性能。  \n一般情况下我们当然愿意采用 MVCC 来解决 读-写 操作并发执行的问题，但是业务在某些特殊情况下，要求必须采用 加锁 的方式执行。下面就讲解下MySQL中不同类别的锁。\n\n## 3 锁的不同角度分类\n\n![](Pasted%20image%2020221118133144.png)\n\n### 3.1 从数据操作的类型划分：读锁、写锁\n\n对于数据库中并发事务的 读-读情况并不会引起什么问题。对于 写-写、读-写 或写-读这些情况可能会引起一些问题，需要使用 MVCC 或者 加锁的方式来解决它们。在使用加锁的方式解决问题时，由于既要允许 读-读情况不受影响，又要使 写-写、读-写 或写-读情况中的操作 相互阻塞，所以MysQL实现一个由两种类型的锁组成的锁系统来解决。这两种类型的锁通常被称为 共享锁 (Shared Lock， s Lock）和排他锁 (ExClusive Lpck. x Lock)，  \n也叫 读锁 (readlock）和写锁 (write lock)。\n\n**读锁** ：也称为共享锁、英文用S表示。针对同一份数据，多个事务的读操作可以同时进行而不会  \n互相影响，相互不阻塞的。\n\n**写锁** ：也称为**排他锁**、英文用X表示。当前写操作没有完成前，它会阻断其他写锁和读锁。这样  \n就能确保在给定的时间里，只有一个事务能执行写入，并防止其他用户读取正在写入的同一资源。\n\n\u003e需要注意的是对于 InnoDB 引擎来说，读锁和写锁可以加在表上，也可以加在行上。\n\n![](Pasted%20image%2020221118151341.png)  \n![](Pasted%20image%2020221118151349.png)  \n![](Pasted%20image%2020221118151432.png)  \n![](Pasted%20image%2020221118151841.png)  \n![](Pasted%20image%2020221118152101.png)\n\n### 3.2 从数据操作的粒度划分：表级锁、页级锁、行锁\n\n为了尽可能提高数据库的并发度，每次锁定的数据范围越小越好，理论上每次只锁定当前操作的数据的方案会得到最大的并发度，**但是管理锁是很 耗资源的事情（涉及获取、检查、释放锁等动作）。因此数据库系统需要在高并发响应 和 系统性能 两方面进行平衡，这样就产生了“锁粒度 (Lock granularity)”的概念**  \n对一条记录加锁影响的也只是这条记录而已，我们就说这个锁的粒度比较细;其实一个事务也可以在 表级别进行加锁，自然就被称之为 表级锁 或者 表锁，对一个表加锁影响整个表中的记录，我们就说这个锁的粒度比较粗。锁的粒度主要分为表级锁、页级锁和行锁。\n\n#### 表锁（Table Lock）\n\n##### 表级别的S锁、X锁\n\n在对某个表执行SELECT、INSERT、DELETE、UPDATE语句时，InnoDB存储引擎是不会为这个表添加表级别的S锁或者X锁的。在对某个表执行一些诸如ALTER TABLE 、DROP TABLE这类的 DDL 语句时，其他事务对这个表并发执行诸如SELECT、INSERT、DELETE、UPDATE的语句会发生阻塞。同理，某个事务中对某个表执行SELECT、INSERT、DELETE、UPDATE语句时，在其他会话中对这个表执行DDL语句也会发生阻塞。**这个过程其实是通过在 server层使用一种称之为元数据锁（英文名： Metadata Locks ，简称 MDL ）结构来实现的**。  \n一般情况下，不会使用InnoDB存储引擎提供的表级别的 S锁 和 X锁 。只会在一些特殊情况下，比方说**崩溃恢复**过程中用到。比如，在系统变量 autocommit=0，innodb_table_locks = 1 时， 手动 获取InnoDB存储引擎提供的表t的S锁或者X锁可以这么写：`LOCK TABLES t READ` ：InnoDB存储引擎会对表 t 加表级别的 S锁 。`LOCK TABLES t WRITE` ：InnoDB存储引擎会对表t加表级别的X锁 。不过尽量避免在使用InnoDB存储引擎的表上使用 LOCK TABLES 这样的手动锁表语句，它们并不会提供什么额外的保护，只是会降低并发能力而已。InnoDB的厉害之处还是实现了更细粒度的行锁 ，关于InnoDB表级别的S锁和X锁大家了解一下就可以了。  \n        ![](Pasted%20image%2020221118152455.png)\n\n##### 意向锁 （intention lock）\n\nInnoDB 支持多粒度锁（multiple granularity locking），它允许行级锁与表级锁共存，而**意向锁就是其中的一种表锁**。\n\n- 意向锁的存在是为了**协调行锁和表锁的关系**，支持多粒度 （表锁与行锁）的锁并存。\n- 意向锁是一种 不与行级锁冲突表级锁，这一点非常重要。\n- 表明“某个事务正在某些行持有了锁或该事务准备去持有锁”\n\n意向锁分为两种：\n\n1. 意向共享锁（intention shared lock, IS）：事务有意向对表中的某些行加共享锁（S锁）\n2. 意向排他锁（intention exclusive lock, IX）：事务有意向对表中的某些行加排他锁（X锁）\n\n即：**意向锁是由存储引擎自己维护的，用户无法手动操作意向锁**，在为数据行加共享/排他锁之前，InooDB 会先获取该数据行所在数据表的对应意向锁。\n\n\u003e意向锁要解决的问题  \n现在有两个事务，分别是T1和72，其中T2试图在该表级别上应用共享或排它锁，如果没有意向锁存在，那么T2就需要去检查各个页或行是否存在锁;如果存在意向锁，那么此时就会受到由T1控制的表级别意向锁的阻塞。T2在锁定该表前不必检查各个页或行锁，而只需检查表上的意向锁。简单来说**就是给更大一级别的空间示意里面是否已经上过锁**。  \n在数据表的场景中，如果我们给某一行数据加上了排它锁，数据库会自动给更大一级的空间，比如数据页或数据表加上意向锁，告诉其他人这个数据页或数据表己经有人上过排它锁了，这样当其他人想要获取数据表排它锁的时候，只需要了解是否有人已经获取了这个数据表的意向排他锁即可。  \n如果事务想要获得数据表中某些记录的共享锁，就需要在数据表上添加意向共享锁。  \n如果事务想要获得数据表中某些记录的排他锁，就需要在数据表上添加意向排他锁。  \n这时，意向锁会告诉其他事务已经有人锁定了表中的某些记录。\n\n\u003e意向锁的并发性:  \n意向锁不会与行级的共享/排他锁互斥！正因为如此，意向锁并不会影响到多个事务对不同数据行加排他锁时的并发性。（不然我们直接用普通的表锁就行了）\n\n从上面的案例可以得到如下结论：  \n    1. InnoDB 支持 多粒度锁 ，特定场景下，行级锁可以与表级锁共存。  \n    2. 意向锁之间互不排斥，但**除了IS与S兼容外，意向锁会与共享锁/排他锁互斥**。  \n    3. IX，IS是表级锁，不会和行级的X，S锁发生冲突。只会和表级的X，S发生冲突。  \n    4. 意向锁在保证并发性的前提下，**实现了行锁和表锁共存且满足事务隔离性的要求**。\n\n##### 自增锁（AUTO-INC锁）\n\n在使用MySQL过程中，我们可以为表的某个列添加 AUTO_INCREMENT 属性。举例：\n\n```mysql\n        CREATE TABLE `teacher` (\n        `id` int NOT NULL AUTO_INCREMENT,\n        `name` varchar(255) NOT NULL,\n        PRIMARY KEY (`id`)\n        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;\n```\n\n由于这个表的id字段声明了AUTO_INCREMENT，意味着在书写插入语句时不需要为其赋值，SQL语句修改:`INSERT INTO teacher (name) VALUES ('zhangsan'), ('lisi');`\n\n上边的插入语句并没有为id列显式赋值，所以系统会自动为它赋上递增的值.现在我们看到的上面插入数据只是一种简单的插入模式，所有插入数据的方式总共分为三类，分别是“ Simple inserts ”，“ Bulk inserts ”和“ Mixed-mode inserts ”。\n\n1. “Simple inserts” （简单插入）  \n    可以 预先确定要插入的行数 （当语句被初始处理时）的语句。包括没有嵌套子查询的单行和多行INSERT…VALUES() 和 REPLACE 语句。比如我们上面举的例子就属于该类插入，已经确定要插入的行数。\n2. “Bulk inserts” （批量插入）  \n    事先不知道要插入的行数 （和所需自动递增值的数量）的语句。比如 INSERT … SELECT ， REPLACE … SELECT 和 LOAD DATA 语句，但不包括纯INSERT。 InnoDB在每处理一行，为AUTO_INCREMENT列分配一个新值。\n3. “Mixed-mode inserts” （混合模式插入）  \n    这些是“Simple inserts”语句但是指定部分新行的自动递增值。例如 INSERT INTO teacher (id,name) VALUES (1,'a'), (NULL,'b'), (5,'c'), (NULL,'d'); **只是指定了部分id的值**。另一种类型的“混合模式插入”是 INSERT … ON DUPLICATE KEY UPDATE 。  \n    **对于上面数据插入的案例，MysQL中采用了自增锁的方式来实现**，AUTO-INC锁是当向使用含有AUTO_INCREMENT列的表中插入数据时需要获取的一种特殊的**表级锁**，在执行插入语句时就在表级别加一个AUTO-NC锁，然后为每条待插入记录的AUTO_INCREMENT修饰的列分 配递增的值，在该语句执行结束后，再把AUTO-INC锁释放掉。一个事务在持有AUTO-INC锁的过程中，其他事务的插入语句都要被阻塞，**可以保证一个语句中分配的递增值是连续的**。也正因为此，其并发性显然并不高，当我们向一个有AUTO_INCREMENT关键字的主键插入值的时候，每条语句都要对这个表锁进行竞争，这样的并发潜力其实是很低下的，所以innodb通过 innodb_autoinc-lock_mode的不同取值来提供不同的锁定机制，来显著提高SQL语句的可伸缩性和性能。\n    \n\n    innodb_autoinc_lock_mode有三种取值，分别对应与不同锁定模式：\n\n    1. innodb_autoinc_lock_mode = 0(“传统”锁定模式)  \n        在此锁定模式下，所有类型的insert语句都会获得一个特殊的表级AUTO-INC锁，用于插入具有AUTO_INCREMENT列的表。这种模式其实就如我们上面的例子，即每当执行insert的时候，都会得到一个表级锁(AUTO-INC锁)，使得语句中生成的auto_increment为顺序，且在binlog中重放的时候，可以保证master与slave中数据的auto_increment是相同的。因为是表级锁，当在同一时间多个事务中执行insert的时候，对于AUTO-INC锁的争夺会 限制并发 能力。\n    2. innodb_autoinc_lock_mode = 1(“连续”锁定模式)  \n        在 MySQL 8.0 之前，连续锁定模式是默认的。在这个模式下，“bulk inserts”仍然使用AUTO-INC表级锁，并保持到语句结束。这适用于所有INSERT … SELECT，REPLACE … SELECT和LOAD DATA语句。同一时刻只有一个语句可以持有AUTO-INC锁。对于“Simple inserts”（要插入的行数事先已知），则通过在 **mutex（轻量锁） 的控制下获得所需数量的自动递增值来避免表级AUTO-INC锁**，它只在分配过程的持续时间内保持，而不是直到语句完成。不使用表级AUTO-INC锁，除非AUTO-INC锁由另一个事务保持。**如果另一个事务保持AUTO-INC锁，则“Simple inserts”等待AUTO-INC锁，如同它是一个“bulk inserts”**。\n    3. innodb_autoinc_lock_mode = 2(“交错”锁定模式)  \n        从 MySQL 8.0 开始，交错锁模式是 默认 设置。  \n        在这种锁定模式下，所有类INSERT语句都不会使用表级AUTO-INC 锁，并且可以同时执行多个语向。这是最快和最可扩展的锁定模式，但是当使用基于语句的复制或恢复方案时，从二进制日志重播SQL语句时，这是不安全的。\n        \n\n    在此锁定模式下，自动递增值保证在所有并发执行的所有类型的insert语向中是唯一旦单调递增的。但是，由于多个语句可以同时生成数字（即，跨语句交叉编号），为任何给定语句插入的行生成的值可能不是连续的。  \n    如果执行的语句是“simple inserts”，其中要插入的行数已提前知道，除了“Mixed-mode inserts°之外，为单个语句生成的数字不会有间隙。然而，当执行“bulk inserts\"时，在由任何给定语向分配的自动递增值中可能存在间隙。\n\n##### 元数据锁（MDL锁）\n\nMySQL5.5引入了meta data lock，简称MDL锁，属于表锁范畴。MDL 的作用是，保证读写的正确性。比如，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个 表结构做变更 ，增加了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。**因此，当对一个表做增删改查操作的时候，加 MDL读锁；当要对表做结构变更操作的时候，加 MDL 写锁**。  \n读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性，解决了DML和DDL操作之间的一致性问题。 不需要显式使用，在访问一个表的时候会被自动加上。\n\n#### 行锁 InnoDB\n\n行锁 （Row Lock） 也称为记录锁，顾名思义，就是锁住某一行(某条记录row）。需要的注意的是，MysQL 服务器层并没有实现行锁机制，行级锁只在存储引擎层实现。  \n优点：锁定力度小，发生锁冲突概率低，可以实现的并发度高。  \n缺点：对于锁的开销比较大，加锁会比较慢，容易出现死锁情况。  \n**InnoDB与MyISAM的最大不同有两点：一是支持事务 (TRANSACTION)；二是采用了行级锁。**\n\n##### 记录锁（Record Locks）\n\n记录锁也就是仅仅把一条记录锁上，官方的类型名称为： LOCK_REC_NOT_GAP 。比如我们把id值为8的那条记录加一个记录锁的示意图如图所示。仅仅是锁住了id值为8的记录，对周围的数据没有影响。  \n        ![](Pasted%20image%2020221118163401.png)  \n        ![](Pasted%20image%2020221118163413.png)  \n记录锁是有S锁和X锁之分的，称之为 S型记录锁 和 X型记录锁 。  \n当一个事务获取了一条记录的S型记录锁后，其他事务也可以继续获取该记录的S型记录锁，但不可以继续获取X型记录锁；  \n当一个事务获取了一条记录的X型记录锁后，其他事务既不可以继续获取该记录的S型记录锁，也不可以继续获取X型记录锁。\n\n##### 间隙锁（Gap Locks）\n\nMySQL 在 REPEATABLE READ 隔离级别下是可以解决幻读问题的，解决方案有两种，可以使用 MVCC 方案解决，也可以采用 加锁 方案解决。但是在使用加锁方案解决时有个大问题，就**是事务在第一次执行读取操作时，那些幻影记录尚不存在**，我们无法给这些 幻影记录 加上 记录锁 。InnoDB提出了一种称之为Gap Locks 的锁，官方的类型名称为： LOCK_GAP ，我们可以简称为 gap锁 。比如，把id值为8的那条记录加一个gap锁的示意图如下。  \n        ![](Pasted%20image%2020221118163606.png)  \n图中id值为8的记录加了gap锁，意味着 不允许别的事务在id值为8的记录前边的间隙插入新记录 ，其实就是id列的值(3, 8)这个区间的新记录是不允许立即插入的。比如，有另外一个事务再想插入一条id值为4的新记录，它定位到该条新记录的下一条记录的id值为8，而这条记录上又有一个gap锁，所以就会阻塞插入操作，直到拥有这个gap锁的事务提交了之后，id列的值在区间(3, 8)中的新记录才可以被插入。\n\n**gap锁的提出仅仅是为了防止插入幻影记录而提出的。**\n\n##### 临键锁（Next-Key Locks）\n\n**有时候我们既想锁住某条记录 ，又想阻止其他事务在该记录前边的间隙插入新记录** ，所以InnoDB就提出了一种称之为 Next-Key Locks 的锁，官方的类型名称为： LOCK_ORDINARY ，我们也可以简称为next-key锁 。Next-Key Locks是在存储引擎 innodb 、事务级别在 可重复读 的情况下使用的数据库锁，innodb默认的锁就是Next-Key locks。  \n        ![](Pasted%20image%2020221118165242.png)\n\nnext-key锁 的本质就是一个[记录锁（Record Locks）](#记录锁（Record%20Locks）)和一个[间隙锁（Gap Locks）](#间隙锁（Gap%20Locks）)的合体，它既能保护该条记录，又能阻止别的事务将新记录插入被保护记录前边的间隙。\n\n##### 插入意向锁（Insert Intention Locks）\n\n我们说一个事务在插入一条记录时需要判断一下插入位置是不是被别的事务加了gap锁（next-key锁也包含 gap锁 ），如果有的话，**插入操作需要等待，直到拥有gap锁的那个事务提交**。但是InnoDB规定**事务在等待的时候也需要在内存中生成一个锁结构**，表明有事务想在某个间隙 中插入新记录，但是现在在等待。InnoDB就把这种类型的锁命名为 Insert Intention Locks ，官方的类型名称为：LOCK_INSERT_INTENTION ，我们称为 插入意向锁 。插入意向锁是一种 Gap锁 ，不是意向锁，在insert操作时产生。\n\n插入意向锁是在插入一条记录行前，由 INSERT 操作产生的一种间陳锁。该锁用以表示插入意向，当多个事务在同一区间(gap） 插入位置不同的多条数据时，事务之间不需要互相等待。假设存在两条值分别为4和7的记录，两个不同的事务分别试图插入值为 5和6的两条记录，每个事务在获取插入行上独占的（排他）锁前，都会获取（4， 7）之间的间隙锁，但是因为数据行之间并 不冲突，所以两个事务之间并不会产生冲突(阻塞等待）。  \n总结来说，插入意向锁的特性可以分成两部分：\n\n- 插入意向锁是一种特殊的间隙锁 一—间隙锁可以锁定开区间内的部分记录。\n- 入意向锁之间 互不排斥，所以即使多个事务在同一区间插入多条记录，只要记录本身（主键、唯一索引不冲突，那么事务之间就不会出现冲突等待。  \n注意，**虽然插入意向锁中含有意向锁三个字，但是它并不属于意向锁而属于间隙锁，因为意向锁是表锁而插入意向锁是行锁**。  \n![](Pasted%20image%2020221118171107.png)  \n从图中可以看到，由于丁1持有gap锁，所以T2和丁3需要生成一个插入意向锁的锁结构并且处于等待状态。当T1提交后会把它获取到的锁都释放掉，这样T2和丁3就能获取到对应的插入意向锁了 （本质上就是把插入意向锁对应锁结构的is_ waiting属性改为false），T2和T3之间也并不会相互阻塞，它们可以同时获取到id值为s的插入意向锁，然后执行插入操作。\n\n插入意向锁是在插入一条记录行前，由 INSERT 操作产生的一种间隙锁。事实上插入意向锁并不会阻止别的事务继续获取该记录上任何类型的锁。\n\n#### 页锁\n\n页锁就是在页的粒度上进行锁定，锁定的数据资源比行锁要多，因为一个页中可以有多个行记录。当我们使用页锁的时候，会出现数据浪费的现象，但这样的浪费最多也就是一个页上的数据行。**页锁的开销介于表锁和行锁之间，会出现死锁。锁定粒度介于表锁和行锁之间，并发度一般**。  \n每个层级的锁数量是有限制的，因为锁会占用内存空间，锁空间的大小是有限的。当某个层级的锁数量超过了这个层级的阈值时，就会进行锁升级。锁升级就是用更大粒度的锁替代多个更小粒度的锁，比如InnoDB 中行锁升级为表锁，这样做的好处是占用的锁空间降低了，但同时数据的并发度也下降了。\n\n### 3.3 从对待锁的态度划分:乐观锁、悲观锁\n\n从对待锁的态度来看锁的话，可以将锁分成乐观锁和悲观锁，从名字中也可以看出这两种锁是两种看待数据并发的思维方式。需要注意的是，**乐观锁和悲观锁并不是锁，而是锁的设计思想**。\n\n#### 1 悲观锁（Pessimistic Locking）\n\n悲观锁是一种思想，顾名思义，就是很悲观，对数据被其他事务的修改持保守态度，会通过数据库自身的锁机制来实现，从而保证数据操作的排它性。**悲观锁总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会 阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）**。比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁，当其他线程想要访问数据时，都需要阻塞挂起。Java中 synchronized 和 ReentrantLock 等独占锁就是悲观锁思想的实现。\n\n![](Pasted%20image%2020221118171652.png)  \n![](Pasted%20image%2020221118171708.png)  \n![](Pasted%20image%2020221118171740.png)\n\n#### 2 乐观锁（Optimistic Locking）\n\n乐观锁认为对同一数据的并发操作不会总发生，属于小概率事件，不用每次都对数据上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，也就是不采用数据库自身的锁机制，而是通过程序来实现。在程序上，**我们可以采用版本号机制或者CAS机制实现**。乐观锁适用于多读的应用类型，这样可以提高吞吐量。在Java中java.util.concurrent.atomic包下的原子变量类就是使用了乐观锁的CAS实现的。\n\n![](Pasted%20image%2020221118172152.png)  \n![](Pasted%20image%2020221118172223.png)  \n![](Pasted%20image%2020221118172310.png)  \nVersion:0.9 StartHTML:0000000105 EndHTML:0000003877 StartFragment:0000000141 EndFragment:0000003837\n\n#### 3 两种锁的适用场景\n\n从这两种锁的设计思想中，我们总结一下乐观锁和悲观锁的适用场景：\n\n1. 乐观锁 适合 读操作多 的场景，相对来说写的操作比较少。它的优点在于 程序实现 ， 不存在死锁问题，不过适用场景也会相对乐观，因为它阻止不了除了程序以外的数据库操作。\n2. 悲观锁 适合 写操作多 的场景，因为写的操作具有 排它性 。采用悲观锁的方式，可以在数据库层面阻止其他事务对该数据的操作权限，防止 读 - 写 和 写 - 写 的冲突。\n\n### 3.4 按加锁的方式划分：显式锁、隐式锁\n\n#### 1 隐式锁\n\n![](Pasted%20image%2020221119111250.png)  \n（也就是为自己也创建一个锁结构， is_waiting 属性是 true ）。\n\n情景二：对于二级索引记录来说，本身并没有 trx_id 隐藏列，但是在二级索引页面的 Page Header 部分有一个 PAGE_MAX_TRX_ID 属性，该属性代表对该页面做改动的最大的 事务id ，如果 PAGE_MAX_TRX_ID 属性值小于当前最小的活跃 事务id ，那么说明对该页面做修改的事务都已经提交了，否则就需要在页面中定位到对应的二级索引记录，然后回表找到它对应的聚簇索引记录，然后再重复 情景一 的做法。\n\n![](Pasted%20image%2020221119111433.png)\n\n隐式锁的逻辑过程如下：  \nA. InnoDB的每条记录中都一个隐含的trx_id字段，这个字段存在于聚簇索引的B+Tree中。  \nB. 在操作一条记录前，首先根据记录中的trx_id检查该事务是否是活动的事务(未提交或回滚)。如果是活动的事务，首先将 隐式锁 转换为 显式锁 (就是为该事务添加一个锁)。  \nC. 检查是否有锁冲突，如果有冲突，创建锁，并设置为waiting状态。如果没有冲突不加锁，跳到E。  \nD. 等待加锁成功，被唤醒，或者超时。  \nE. 写数据，并将自己的trx_id写入trx_id字段。\n\n#### 2 显式锁\n\n通过特定的语句进行加锁，我们一般称之为显示加锁，例如：  \n显示加共享锁：  \n`select …. lock in share mode`  \n显示加排它锁：  \n`select …. for update`\n\n### 3.5 其它锁之：全局锁\n\n全局锁就是对 整个数据库实例 加锁。当你需要让整个库处于 只读状态 的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。全局锁的典型使用 场景 是：做 **全库逻辑备份** 。  \n全局锁的命令：`Flush tables with read lock`\n\n### 占坑 死锁\n\n## 4 锁的内存结构\n\n![](Pasted%20image%2020221120003952.png)  \nInnoDB存储引擎中的锁结构如下：  \n![](Pasted%20image%2020221120003933.png)  \n**结构解析：**\n\n1. 锁所在的事务信息：不论是表锁还是行锁，都是在事务执行过程中生成的，哪个事务生成了这个锁结构 ，这里就记录这个事务的信息。此锁所在的事务信息在内存结构中只是一个指针，通过指针可以找到内存中关于该事务的更多信息，比方说事务id等。\n2. 索引信息：对于行锁来说，需要记录一下加锁的记录是属于哪个索引的。这里也是一个指针。\n3. 表锁／行锁信息 ：表锁结构和行锁结构在这个位置的内容是不同的：  \n    表锁：记载着是对哪个表加的锁，还有其他的一些信息。  \n    行锁：记载了三个重要的信息：\n    - Space ID ：记录所在表空间。\n    - Page Number ：记录所在页号。\n    - n_bits ：对于行锁来说，一条记录就对应着一个比特位，一个页面中包含很多记录，用不同的比特位来区分到底是哪一条记录加了锁。为此在行锁结构的末尾放置了一堆比特位，这个n_bits 属性代表使用了多少比特位。**n_bits的值一般都比页面中记录条数多一些。主要是为了之后在页面中插入了新记录后也不至于重新分配锁结构**\n4. type_mode：这是一个32位的数，被分成了 lock_mode 、 lock_type 和 rec_lock_type 三个部分，如图所示：  \n    ![](Pasted%20image%2020221120005201.png)\n    - 锁的模式（ lock_mode ），占用低4位，可选的值如下：  \n        LOCK_IS （十进制的 0 ）：表示共享意向锁，也就是 IS锁 。  \n        LOCK_IX （十进制的 1 ）：表示独占意向锁，也就是 IX锁 。  \n        LOCK_S （十进制的 2 ）：表示共享锁，也就是 S锁 。  \n        LOCK_X （十进制的 3 ）：表示独占锁，也就是 X锁 。  \n        LOCK_AUTO_INC （十进制的 4 ）：表示 AUTO-INC锁 。  \n        在InnoDB存储引擎中，LOCK_IS，LOCK_IX，LOCK_AUTO_INC都算是表级锁的模式，LOCK_S和 LOCK_X既可以算是表级锁的模式，也可以是行级锁的模式。\n    - 锁的类型（ lock_type ），占用第5～8位，不过现阶段只有第5位和第6位被使用:  \n        LOCK_TABLE （十进制的 16 ），也就是当第5个比特位置为1时，表示表级锁。LOCK_REC （十进制的 32 ），也就是当第6个比特位置为1时，表示行级锁。\n    - 行锁的具体类型（ rec_lock_type ），使用其余的位来表示。只有在 lock_type 的值为LOCK_REC 时，也就是只有在该锁为行级锁时，才会被细分为更多的类型：  \n        LOCK_ORDINARY （十进制的 0 ）：表示 next-key锁 。  \n        LOCK_GAP （十进制的 512 ）：也就是当第10个比特位置为1时，表示 gap锁 。 LOCK_REC_NOT_GAP （十进制的 1024 ）：也就是当第11个比特位置为1时，表示正经 记录 锁 。  \n        LOCK_INSERT_INTENTION （十进制的 2048 ）：也就是当第12个比特位置为1时，表示插入意向锁。其他的类型：还有一些不常用的类型我们就不多说了。\n    - is_waiting 属性呢？基于内存空间的节省，所以把 is_waiting 属性放到了 type_mode 这个32位的数字中：  \n        LOCK_WAIT （十进制的 256 ） ：当第9个比特位置为 1 时，表示is_waiting 为 true ，也就是当前事务尚未获取到锁，处在等待状态；当这个比特位为 0 时，表示 is_waiting 为 false ，也就是当前事务获取锁成功。\n5. 其他信息：  \n    为了更好的管理系统运行过程中生成的各种锁结构而设计了各种哈希表和链表。\n6. 一堆比特位：  \n    如果是行锁结构的话，在该结构末尾还放置了一堆比特位，比特位的数量是由上边提到的 n_bits 属性表示的。InnoDB数据页中的每条记录在 记录头信息 中都包含一个 heap_no属性，伪记录 Infimum 的 heap_no 值为 0 ， Supremum 的 heap_no 值为 1 ，之后每插入一条记录， heap_no 值就增1。 锁结 构 最后的一堆比特位就对应着一个页面中的录，一个比特位映射一个 heap_no ，即一个比特位映射到页内的一条记录。\n\n## 5 锁监控\n\n关于MySQL锁的监控，我们一般可以通过检查 InnoDB_row_lock 等状态变量来分析系统上的行锁的争夺情况:`show status like 'innodb_row_lock%';`\n\n对各个状态量的说明如下：  \nInnodb_row_lock_current_waits：当前正在等待锁定的数量；  \n**Innodb_row_lock_time** ：从系统启动到现在锁定总时间长度；（等待总时长）  \n**Innodb_row_lock_time_avg** ：每次等待所花平均时间；（等待平均时长）  \nInnodb_row_lock_time_max：从系统启动到现在等待最常的一次所花的时间；  \n**Innodb_row_lock_waits** ：系统启动后到现在总共等待的次数；（等待总次数）\n\n其他监控方法：  \nMySQL把事务和锁的信息记录在了 information_schema 库中，涉及到的三张表分别是  \nINNODB_TRX 、 INNODB_LOCKS 和 INNODB_LOCK_WAITS 。  \nMySQL5.7及之前 ，可以通过information_schema.INNODB_LOCKS查看事务的锁情况，但只能看到阻塞事务的锁；如果事务并未被阻塞，则在该表中看不到该事务的锁情况。  \nMySQL8.0删除了information_schema.INNODB_LOCKS，添加了performance_schema.data_locks ，可以通过performance_schema.data_locks查看事务的锁情况，和MySQL5.7及之前不同，  \nperformance_schema.data_locks不但可以看到阻塞该事务的锁，还可以看到该事务所持有的锁。  \n同时，information_schema.INNODB_LOCK_WAITS也被performance_schema.data_lock_waits 所代替。\n","lastmodified":"2023-05-09T16:33:58.263365716Z","tags":[]},"/16-%E5%A4%9A%E7%89%88%E6%9C%AC%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6":{"title":"16 多版本并发控制","content":"\n## 1 什么是 MVCC\n\nMVCC（Multiversion Concurrency Control），多版本并发控制。顾名思义，MVCC 是通过数据行的多个版本管理来实现数据库的并发控制。这项技术使得在InnoDB的事务隔离级别下执行一致性读 操作有了保证。换言之，就是为了查询一些正在被另一个事务更新的行，并且可以看到它们被更新之前的值，这样在做查询的时候就不用等待另一个事务释放锁。\n\n## 2 快照读与当前读\n\nMVCC在MySQL InnoDB中的实现主要是为了提高数据库并发性能，用**更好的方式去处理读-写冲突**，做到即使有读写冲突时，也能做到不加锁，非阻塞并发读,而这个读指的就是快照读, 而非 当前读。**当前读实际上是一种加锁的操作，是悲观锁的实现,而MVCC本质是采用乐观锁思想的一种方式**。\n\n### 2.1 快照读\n\n快照读又叫一致性读，读取的是快照数据。**不加锁的简单的 SELECT 都属于快照读**，即不加锁的非阻塞读,比如这样：`SELECT * FROM player WHERE …`,之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是基于MVCC，它在很多情况下，避免了加锁操作，降低了开销。  \n既然是基于多版本，那么**快照读可能读到的并不一定是数据的最新版本**，而有可能是之前的历史版本。快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读。\n\n### 2.2 当前读\n\n当前读读取的是记录的最新版本（最新数据，而不是历史版本的数据），读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。**加锁的SELECT，或者对数据进行增删改都会进行当前读**。比如：\n\n```mysql\nSELECT * FROM student LOCK IN SHARE MODE; # 共享锁\n\nSELECT * FROM student FOR UPDATE; # 排他锁\n\nINSERT INTO student values ... # 排他锁\n\nDELETE FROM student WHERE ... # 排他锁\n\nUPDATE student SET ... # 排他锁\n```\n\n## 3 复习\n\n### 3.1 再谈隔离级别\n\n传送门：[3.3 SQL中的四种隔离级别](13%20事务基础知识.md#3.3%20SQL中的四种隔离级别)  \n我们知道事务有 4 个隔离级别，可能存在三种并发问题：  \n![500](Pasted%20image%2020221120182931.png)  \n在MysQL 中，默认的隔离级别是可重复读，可以解决脏读和不可重复读的问题，如果仅从定义的角度来看，它并不能解决幻读问题。如果我们想要解决幻读问题，就需要采用串行化的方式，也就是将隔离级别提升到最高，但这样一来就会大幅降低数据库的事务并发能力。  \nMvCC 可以不采用锁机制，而是通过乐观锁的方式来解决不可重复读和幻读问题！它可以在大多数情况下替代行级锁，降低系统的开销。\n\n### 3.2 隐藏字段、Undo Log版本链\n\n回顾一下undo日志的版本链，对于使用InnoDB存储引擎的表来说，它的聚簇索引记录中都包含两个必要的隐藏列。\n\n- trx_id ：每次一个事务对某条聚簇索引记录进行jieye.ericx@gmail.com改动时，都会把该事务的 事务id 赋值给  \ntrx_id 隐藏列。\n- roll_pointer ：每次对某条聚簇索引记录进行改动时，都会把旧的版本写入到 undo日志 中，然后这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息。  \n![](Pasted%20image%2020221128154946.png)  \n![](Pasted%20image%2020221128155018.png)  \n![](Pasted%20image%2020221128155034.png)\n\n## 4. MVCC 实现原理之 ReadView\n\nMVCC 的实现依赖于：**隐藏字段、Undo Log、Read View**。\n\n### 4.1 什么是ReadView\n\n在MvCC 机制中，多个事务对同一个行记录进行更新会产生多个历史快照，这些历史快照保存在 Undo Log里。如果一个事务想要查询这个行记录，需要读取哪个版本的行记录呢？这时就需要用到 ReadView 了，**它帮我们解決了行的可见性问题**。  \nReadVew 就是事务在使用MVCC机制进行快照读操作时产生的读视图。当事务启动时，会生成数据库系统当前的一个快照，InnoDB 为每个事务构造了一个数组，用来记录并维护系统当前 活跃事务的1D（“活跃\"指的就是，启动了但还没提交）。\n\n### 4.2 设计思路\n\n- 使用 READ UNCOMMITTED 隔离级别的事务，由于可以读到未提交事务修改过的记录，所以直接读取记录的最新版本就好了。\n- 使用 SERIALIZABLE 隔离级别的事务，InnoDB规定**使用加锁的方式来访问记录**。\n- 使用 READ COMMITTED 和 REPEATABLE READ 隔离级别的事务，都必须保证读到 已经提交了的 事务修改过的记录。假如另一个事务已经修改了记录但是尚未提交，是不能直接读取最新版本的记录的，核心问题就是需要判断一下版本链中的哪个版本是当前事务可见的，这是ReadView要解决的主要问题。  \n    这个ReadView中主要包含4个比较重要的内容，分别如下：\n    1. creator_trx_id ，创建这个 Read View 的事务 ID。\n\n    \u003e 说明：只有在对表中的记录做改动时（执行INSERT、DELETE、UPDATE这些语句时）才会为事务分配事务id，否则在一个只读事务中的事务id值都默认为0。\n    2. trx_ids ，表示在生成ReadView时当前系统中活跃的读写事务的事务id列表 。\n    3. up_limit_id ，活跃的事务中最小的事务 ID。\n    4. low_limit_id ，表示生成 ReadView 时系统中应该分配给下一个事务的 id 值。low_limit_id 是系统最大的事务 id 值，这里要注意是系统中的事务 id，需要区别于正在活跃的事务 ID。\n\n    \u003e 注意：low_limit_id并不是trx_ids中的最大值，事务id是递增分配的。比如，现在有id为1， 2，3这三个事务，之后id为3的事务提交了。那么一个新的读事务在生成ReadView时，trx_ids就包括1和2，up_limit_id的值就是1，low_limit_id的值就是4。  \n    ![](Pasted%20image%2020221128155825.png)\n\n### 4.3 ReadView的规则\n\n有了这个ReadView，这样在访问某条记录时，只需要按照下边的步骤判断记录的某个版本是否可见。\n\n- 如果被访问版本的trx_id属性值与ReadView中的 creator_trx_id 值相同，意味着当前事务在访问它自己修改过的记录，所以该版本**可以**被当前事务访问。\n- 如果被访问版本的trx_id属性值小于ReadView中的 up_limit_id 值，表明生成该版本的事务在当前事务生成ReadView前已经提交，所以该版本**可以**被当前事务访问。\n- 如果被访问版本的trx_id属性值大于或等于ReadView中的 low_limit_id 值，表明生成该版本的事务在当前事务生成ReadView后才开启，所以该版本**不可以**被当前事务访问。\n- 如果被访问版本的 trx_id 属性值在 ReadView 的 up_limit_id 和 low_limit_id 之间，那就需要判断一下 trx_id 属性值**是不是在 trx_ids 列表中**。如果在，说明创建 ReadView 时生成该版本的事务还是活跃的，该版本不可以被访问。如果不在，说明创建 ReadView 时生成该版本的事务已经被提交，该版本可以被访问。\n\n### 4.4 MVCC整体操作流程\n\n了解了这些概念之后，我们来看下当查询一条记录的时候，系统如何通过MVCC找到它：\n\n1. 首先获取事务自己的版本号，也就是事务 ID；\n2. 获取 ReadView；\n3. 查询得到的数据，然后与 ReadView 中的事务版本号进行比较；\n4. 如果不符合 ReadView 规则，就需要从 Undo Log 中获取历史快照；\n5. 最后返回符合规则的数据。  \n**如果某个版本的数据对当前事务不可见的话，那就顺着版本链找到下一个版本的数据，继续按照上边的步骤判断可见性**，依此类推，直到版本链中的最后一个版本。如果最后一个版本也不可见的话，那么就意味着该条记录对该事务完全不可见，查询结果就不包含该记录。\n\n\u003e InnoDB中，MVCC 是通过 Undo Log + Read View 进行数据读取，Undo Log 保存了历史快照，而Read View 规则帮我们判断当前版本的数据是否可见。\n\n在**隔离级别为读已提交**（Read Committed）时，一个事务中的每一次 SELECT 查询都会重新获取一次Read View。  \n![](Pasted%20image%2020221128160607.png)\n\n\u003e 注意，此时同样的查询语句都会重新获取一次 Read View，这时如果 Read View 不同，就可能产生不可重复读或者幻读的情况。\n\n当**隔离级别为可重复读**的时候，就避免了不可重复读，这是因为一个事务只在第一次 SELECT 的时候会获取一次 Read View，而后面所有的 SELECT 都会复用这个 Read View:  \n![](Pasted%20image%2020221128160653.png)\n\n## 5 举例说明\n\n## 6 总结\n\n![](Pasted%20image%2020221128164534.png)  \n![](Pasted%20image%2020221128164542.png)\n","lastmodified":"2023-05-09T16:33:58.263365716Z","tags":[]},"/17-%E5%85%B6%E4%BB%96%E6%95%B0%E6%8D%AE%E5%BA%93%E6%97%A5%E5%BF%97":{"title":"17 其他数据库日志","content":"\n我们在讲解数据库事务时，讲过两种日志：重做日志、回滚日志。  \n对于线上数据库应用系统，突然遭遇数据库宕机,怎么办？在这种情况下，定位宕机的原因 就非常关键。我们可以查看数据库的错误日志。因为日志中记录了数据库运行中的诊断信息，包括了错误、警告和注释等信息。比如：从日志中发现某个连接中的 SQL操作发生了死循环，导致内存不足，被系统强行终止了。明确了原因，处理起来也就轻松了，系统很快就恢复了运行。  \n除了发现错误，日志在数据复制、数据恢复、操作审计，以及确保数据的永久性和一致性等方面，都有着不可替代的作用。\n\nMysQL8.0官网日志地址：“https://dev.mysql.com/doc/refman/8.0/en/server-logs.html”\n\n## 1 MySQL支持的日志\n\n### 1.1 日志类型\n\nMySQL有不同类型的日志文件，用来存储不同类型的日志，分为 二进制日志 、 错误日志 、 通用查询日志和 慢查询日志 ，这也是常用的4种。MySQL 8又新增两种支持的日志： 中继日志 和 数据定义语句日志 。使用这些日志文件，可以查看MySQL内部发生的事情。\n\n这6类日志分别为：\n\n1. 慢查询日志：记录所有执行时间超过long_query_time的所有查询，方便我们对查询进行优化。\n2. 通用查询日志：记录所有连接的起始时间和终止时间，以及连接发送给数据库服务器的所有指令，对我们复原操作的实际场景、发现问题，甚至是对数据库操作的审计都有很大的帮助。\n3. 错误日志：记录MySQL服务的启动、运行或停止MySQL服务时出现的问题，方便我们了解服务器的状态，从而对服务器进行维护。\n4. 二进制日志：记录所有更改数据的语句，可以用于主从服务器之间的数据同步，以及服务器遇到故障时数据的无损失恢复。\n5. 中继日志：用于主从服务器架构中，从服务器用来存放主服务器二进制日志内容的一个中间文件。从服务器通过读取中继日志的内容，来同步主服务器上的操作。\n6. 数据定义语句日志：记录数据定义语句执行的元数据操作。\n\n除二进制日志外，其他日志都是 **文本文件** 。默认情况下，所有日志创建于 MySQL数据目录 中。\n\n### 1.2 日志的弊端\n\n日志功能会降低MySQL数据库的性能。  \n日志会占用大量的磁盘空间。\n\n## 2 慢查询日志(slow query log)\n\n前面章节[9 性能分析工具的使用](9%20性能分析工具的使用.md)已经详细讲述。\n\n## 3 通用查询日志(general query log)\n\n通用查询日志用来 记录用户的所有操作 ，包括启动和关闭MySQL服务、所有用户的连接开始时间和截止时间、发给 MySQL 数据库服务器的所有 SQL 指令等。当我们的数据发生异常时，查看通用查询日志，还原操作时的具体场景，可以帮助我们准确定位问题\n\n### 3.1 问题场景\n\n![](Pasted%20image%2020221128165521.png)\n\n### 3.2 查看当前状态\n\n```mysql\nVersion:0.9 StartHTML:0000000105 EndHTML:0000003470 StartFragment:0000000141 EndFragment:0000003430\n\nmysql\u003e SHOW VARIABLES LIKE '%general%';\n\n+------------------+------------------------------+\n\n| Variable_name | Value |\n\n+------------------+------------------------------+\n\n| general_log | OFF | #通用查询日志处于关闭状态\n\n| general_log_file | /var/lib/mysql/atguigu01.log | #通用查询日志文件的名称是atguigu01.log\n\n+------------------+------------------------------+\n\n2 rows in set (0.03 sec)\n```\n\n### 3.3 启动日志\n\n方式1：永久性方式  \n修改my.cnf或者my.ini配置文件来设置。在[mysqld]组下加入log选项，并重启MySQL服务。格式如下：  \n`[mysqld] general_log=ON general_log_file=[path[filename]] #日志文件所在目录路径，filename为日志文件名`  \n如果不指定目录和文件名，通用查询日志将默认存储在MySQL数据目录中的hostname.log文件中，hostname表示主机名。  \n方式2：临时性方式  \n![](Pasted%20image%2020221128165926.png)\n\n### 3.5 停止日志\n\n![](Pasted%20image%2020221128165958.png)\n\n### 3.6 删除刷新日志\n\n如果数据的使用非常频繁，那么通用查询日志会占用服务器非常大的磁盘空间。数据管理员可以删除很长时间之前的查询日志，以保证MySQL服务器上的硬盘空间。  \n手动删除文件\n\n## 4 错误日志(error log\n\n错误日志记录了 MysQL 服务器启动、停止运行的时间，以及系统启动、运行和停止过程中的诊断信息，包括错误、警告和提示等。  \n通过错误日志可以查看系统的运行状态，便于即时发现故障、修复故障。如果MySQL服务 出现异常，错误日志是发现问题、解决故障的 首选。\n\n### 4.1 启动日志\n\n在MySQL数据库中，错误日志功能是默认开启的。而且，错误日志无法被禁止。  \n默认情况下，错误日志存储在MySQL数据库的数据文件夹下，名称默认为 mysqld.log （Linux系统）或hostname.err（mac系统）。如果需要制定文件名，则需要在my.cnf或者my.ini中做如下配置：  \n`[mysqld] log-error=[path/[filename]] #path为日志文件所在的目录路径，filename为日志文件名`\n\n### 4.2 查看日志\n\n![](Pasted%20image%2020221128170227.png)\n\n### 4.3 删除刷新日志\n\n![](Pasted%20image%2020221128170253.png)\n\n### ![](Pasted%20image%2020221128170301.png)4.4 8.0新特性\n\n![](Pasted%20image%2020221128170751.png)\n\n## 5 二进制日志(bin log)\n\nbinlog可以说是MySQL中比较 重要 的日志了，在日常开发及运维过程中，经常会遇到。  \nbinlog即binary log，二进制日志文件，也叫作变更日志（update log）。它记录了数据库所有执行的  \nDDL 和 DML 等数据库更新事件的语句，但是不包含没有修改任何数据的语句（如数据查询语句select、 show等）。  \n![](Pasted%20image%2020221128172804.png)  \n可以说MySQL数据库的数据备份、主备、主主、主从都离不开binlog，需要依靠binlog来同步数据，保证数据一致性。  \n![](Pasted%20image%2020221128172832.png)\n\n### 5.1 查看默认情况\n\n查看记录二进制日志是否开启：在MySQL8中默认情况下，二进制文件是开启的\n\n```mysql\nVersion:0.9 StartHTML:0000000105 EndHTML:0000003977 StartFragment:0000000141 EndFragment:0000003937\n\nmysql\u003e show variables like '%log_bin%';\n\n+---------------------------------+----------------------------------+\n\n| Variable_name | Value |\n\n+---------------------------------+----------------------------------+\n\n| log_bin | ON |\n\n| log_bin_basename | /var/lib/mysql/binlog |\n\n| log_bin_index | /var/lib/mysql/binlog.index |\n\n| log_bin_trust_function_creators | OFF |\n\n| log_bin_use_v1_row_events | OFF |\n\n| sql_log_bin | ON |\n\n+---------------------------------+----------------------------------+\n\n6 rows in set (0.00 sec)\n```\n\n### 5.2 日志参数设置\n\n![](Pasted%20image%2020221128185835.png)  \n![](Pasted%20image%2020221128185843.png)\n\n### 5.3 查看日志\n\n![](Pasted%20image%2020221128190131.png)\n","lastmodified":"2023-05-09T16:33:58.263365716Z","tags":[]},"/2-%E7%94%A8%E6%88%B7%E4%B8%8E%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86":{"title":"2 用户与权限管理","content":"\n# 2 用户与权限管理\n\n## 1 用户管理\n\n### 1.1 登录MySQL服务器\n\n`mysql –h hostname|hostIP –P port –u username –p DatabaseName –e \"SQL语句\"`\n\n参数：\n\n- -h参数 后面接主机名或者主机IP，hostname为主机，hostIP为主机IP。\n- -P参数 后面接MySQL服务的端口，通过该参数连接到指定的端口。MySQL服务的默认端口是3306，  \n  不使用该参数时自动连接到3306端口，port为连接的端口号。\n- -u参数 后面接用户名，username为用户名。\n- -p参数 会提示输入密码。\n- DatabaseName参数 指明登录到哪一个数据库中。如果没有该参数，就会直接登录到MySQL数据库中，然后可以使用USE命令来选择数据库。\n- -e参数 后面可以直接加SQL语句。登录MySQL服务器以后即可执行这个SQL语句，然后退出MySQL服务器。\n\n举例：`mysql -uroot -p -hlocalhost -P3306 mysql -e \"select host,user from user\"`\n\n### 1.2 创建用户\n\nCREATE USER语句的基本语法形式如下：\n\n`CREATE USER 用户名 [IDENTIFIED BY '密码'][,用户名 [IDENTIFIED BY '密码']];`\n\n- 用户名参数表示新建用户的账户，由 用户（User） 和 主机名（Host） 构成；\n- “[ ]”表示可选，也就是说，可以指定用户登录时需要密码验证，也可以不指定密码验证，这样用户可以直接登录。不过，不指定密码的方式不安全，不推荐使用。如果指定密码值，这里需要使用  \n  IDENTIFIED BY指定明文密码值。\n- CREATE USER语句可以同时创建多个用户。\n\n```bash\nCREATE USER zhang3 IDENTIFIED BY '123123';  # 默认host是 %\nCREATE USER 'kangshifu'@'localhost' IDENTIFIED BY '123456';\n### 1.3 修改用户\n\n```bash\nUPDATE mysql.user SET USER='li4' WHERE USER='wang5';\nFLUSH PRIVILEGES;\n```\n\n### 1.4 删除用户  \n\n1. 使用DROP方式删除（推荐）  \n   使用DROP USER语句来删除用户时，必须用于DROP USER权限。DROP USER语句的基本语法形式如下：\n\n   `DROP USER user[,user]…;`\n\n   ```bash\n   DROP USER li4 ;  # 默认删除host为%的用户\n   DROP USER 'kangshifu'@'localhost';\n   ```\n\n2. 使用DELETE方式删除:\n\n   `DELETE FROM mysql.user WHERE Host=’hostname’ AND User=’username’;`\n\n   执行完DELETE命令后要使用FLUSH命令来使用户生效`FLUSH PRIVILEGES;`\n\n   举例：\n\n   ```bash\n   DELETE FROM mysql.user WHERE Host='localhost' AND User='Emily';\n   FLUSH PRIVILEGES;\n   ```\n\n   \u003e 注意：不推荐通过 `DELETE FROM USER u WHERE USER='li4' `进行删除，系统会有残留信息保留。而drop user命令会删除用户以及对应的权限，执行命令后你会发现mysql.user表和mysql.db表的相应记录都消失了。\n\n### 1.5 设置当前用户密码\n\n旧的写法如下 :\n\n```bash\n # 修改当前用户的密码:(MySQL5.7测试有效) \n SET PASSWORD = PASSWORD('123456');\n```\n\n这里介绍**推荐的写法** :\n\n1. **使用ALTER USER命令来修改当前用户密码** 用户可以使用ALTER命令来修改自身密码，如下语句代表修改当前登录用户的密码。基本语法如下:`ALTER USER USER() IDENTIFIED BY 'new_password';`\n2. **使用SET语句来修改当前用户密码** 使用root用户登录MySQL后，可以使用SET语句来修改密码，具体SQL语句如下:  \n   `SET PASSWORD='new_password';`该语句会自动将密码加密后再赋给当前用户。\n\n### 1.6 修改其它用户密码\n\n1. **使用ALTER语句来修改普通用户的密码** 可以使用ALTER USER语句来修改普通用户的密码。基本语法形式如下:\n\n   ```bash\n    ALTER USER user [IDENTIFIED BY '新密码'] [,user[IDENTIFIED BY '新密码']]...;\n   ```\n\n2. **使用SET命令来修改普通用户的密码** 使用root用户登录到MySQL服务器后，可以使用SET语句来修改普 通用户的密码。SET语句的代码如下:`SET PASSWORD FOR 'username'@'hostname'='new_password'; `\n3. **使用UPDATE语句修改普通用户的密码(不推荐)**\n\n   ```bash\n   UPDATE MySQL.user SET authentication_string=PASSWORD(\"123456\") WHERE User = \"username\" AND Host = \"hostname\";\n   ```\n\n### 1.7 MySQL8密码管理(了解)\n\n1. 密码过期策略\n   - 在MySQL中，数据库管理员可以手动设置账号密码过期，也可以建立一个**自动密码过期策略**。\n   - 过期策略可以是全局的 ，也可以为设置单独的过期策略。  \n`ALTER USER user PASSWORD EXPIRE;`  \n练习:`ALTER USER 'kangshifu'@'localhost' PASSWORD EXPIRE;`\n1. 使用**SQL**语句更改该变量的值并持久化  \n   `SET PERSIST default_password_lifetime = 180; `建立全局策略，设置密码每隔180天过期\n2. 配置文件**my.cnf**中进行维护  \n   `[mysqld]default_password_lifetime=180 #建立全局策略，设置密码每隔180天过期`  \n**手动设置指定时间过期方式**2:单独设置  \n每个账号既可延用全局密码过期策略，也可单独设置策略。在 CREATE USER 和 ALTER USER 语句上加入 PASSWORD EXPIRE 选项可实现单独设置策略。下面是一些语句示例：\n\n```sql\n#设置kangshifu账号密码每90天过期:\nCREATE USER 'kangshifu'@'localhost' PASSWORD EXPIRE INTERVAL 90 DAY; ALTER USER 'kangshifu'@'localhost' PASSWORD EXPIRE INTERVAL 90 DAY;\n#设置密码永不过期:\nCREATE USER 'kangshifu'@'localhost' PASSWORD EXPIRE NEVER; ALTER USER 'kangshifu'@'localhost' PASSWORD EXPIRE NEVER;\n#延用全局密码过期策略:\nCREATE USER 'kangshifu'@'localhost' PASSWORD EXPIRE DEFAULT; ALTER USER 'kangshifu'@'localhost' PASSWORD EXPIRE DEFAULT;\n```\n\n2. 密码重用策略  \n   1 手动设置密码重用方式**1**:全局  \n   方式1: 使用SQL\n\n   ```bash\n   SET PERSIST password_history = 6; #设置不能选择最近使用过的6个密码\n    SET PERSIST password_reuse_interval = 365; #设置不能选择最近一年内的密码\n   ```\n\n   方式2:my.cnf**配置文件\n\n   ```ini\n   [mysqld]\n   password_history=6\n   password_reuse_interval=365\n\n手动设置密码重用方式2:单独设置\n\n```\n","lastmodified":"2023-05-09T16:33:58.263365716Z","tags":[]},"/3-%E9%80%BB%E8%BE%91%E6%9E%B6%E6%9E%84":{"title":"3 逻辑架构","content":"\n## 1 逻辑架构剖析\n\n### 1.1 服务器处理客户端请求\n\n服务器进程对客户端进程发送的请求做了什么处理，才能产生最后的处理结果呢?这里以查询请求为例展示:\n\n![image-20220915111525491](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915111525491.png?lastModify=1668423115)\n\n![image-20220915111543027](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915111543027.png)\n\n### 1.2 Connectors\n\n### 1.3 第1层:连接层\n\n系统(客户端)访问 MySQL 服务器前，做的第一件事就是建立 TCP 连接。 经过三次握手建立连接成功后， MySQL 服务器对 TCP 传输过来的账号密码做身份认证、权限获取。\n\n**用户名或密码不对，会收到一个Access denied for user错误，客户端程序结束执行 用户名密码认证通过，会从权限表查出账号拥有的权限与连接关联，之后的权限判断逻辑，都将依 赖于此时读到的权限**\n\nTCP 连接收到请求后，必须要分配给一个线程专门与这个客户端的交互。所以还会有个线程池，去走后 面的流程。每一个连接从线程池中获取线程，省去了创建和销毁线程的开销。\n\n### 1.4 第2层:服务层\n\n- SQL Interface: SQL接口\n    \n\n    接收用户的SQL命令，并且返回用户需要查询的结果。比如SELECT … FROM就是调用SQL Interface MySQL支持DML(数据操作语言)、DDL(数据定义语言)、存储过程、视图、触发器、自定 义函数等多种SQL语言接口\n\n    \n- Parser: 解析器\n    \n\n    在解析器中对 SQL 语句进行语法分析、语义分析。将SQL语句**分解成数据结构**，并将这个结构传递到后续步骤，以后SQL语句的传递和处理就是基于这个结构的。如果在分解构成中遇到错误，那么就说明这个SQL语句是不合理的。\n\n    \n\n    在SQL命令传递到解析器的时候会被解析器验证和解析，并为其**创建语法树** ，并根据数据字典丰富查询语法树，会验证该客户端是否具有执行该查询的权限 。创建好语法树后，MySQL还会对SQl查询进行**语法上的优化，进行查询重写**。\n\n    \n- Optimizer: **查询优化器**\n    \n\n    SQL语句在语法解析之后、查询之前会使用查询优化器确定 SQL 语句的执行路径，生成一个 执行计划 。\n\n    \n\n    这个执行计划表明应该使用哪些索引进行查询(全表检索还是使用索引检索)，表之间的连接顺序如何，最后会按照执行计划中的步骤调用存储引擎提供的方法来真正的执行查询，并将查询结果返回给用户。\n\n    \n\n    它使用“ 选取-投影-连接 ”策略进行查询。例如:SELECT id,name FROM student WHERE gender = '女';这个SELECT查询先根据WHERE语句进行选取 ，而不是将表全部查询出来以后再进行gender过滤。 这个SELECT查询先根据id和name进行属性投影 ，而不是将属性全部取出以后再进行过 滤，将这两个查询条件连接起来生成最终查询结果。\n\n    \n- Caches \u0026 Buffers**: 查询缓存组件**\n    \n- MySQL内部维持着一些Cache和Buffer，比如Query Cache用来缓存一条SELECT语句的执行结 果，如果能够在其中找到对应的查询结果，那么就不必再进行查询解析、优化和执行的整个过 程了，直接将结果反馈给客户端。\n    \n\n    这个缓存机制是由一系列小缓存组成的。比如表缓存，记录缓存，key缓存，权限缓存等 。\n\n    \n\n    这个查询缓存可以在不同客户端之间共享 。\n\n    \n\n    **从MySQL 5.7.20开始，不推荐使用查询缓存，并在 MySQL 8.0中删除 。**\n\n### 1.5 第3层:引擎层\n\n   \n\n插件式存储引擎层( Storage Engines)，**真正的负责了MySQL中数据的存储和提取，对物理服务器级别 维护的底层数据执行操作** ，服务器通过API与存储引擎进行通信。不同的存储引擎具有的功能不同，这样 我们可以根据自己的实际需要进行选取。\n\nMySQL 8.0.25默认支持的存储引擎如下:\n\n![image-20220915112616863](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915112616863.png?lastModify=1668423115)\n\n### 1.6 存储层\n\n所有的数据，数据库、表的定义，表的每一行的内容，索引，都是存在**文件系统**上，以文件的方式存 在的，并完成与存储引擎的交互。当然有些存储引擎比如InnoDB，也支持不使用文件系统直接管理裸设备，但现代文件系统的实现使得这样做没有必要了。在文件系统之下，可以使用本地磁盘，可以使用 DAS、NAS、SAN等各种存储系统。\n\n### 1.7 小结\n\n![image-20220915112713156](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915112713156.png?lastModify=1668423115)\n\n \n\n## 2 SQL执行流程\n\n![image-20220915112749246](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915112749246.png?lastModify=1668423115)\n\n \n\nMySQL**查询流程:**\n\n1. **查询缓存**  \n  Server 如果在查询缓存中发现了这条 SQL 语句，就会直接将结果返回给客户端;如果没有，就进入到解析器阶段。需要说明的是，因为查询缓存往往效率不高，所以**在 MySQL8.0 之后就抛弃了这个功能。**\n\n大多数情况查询缓存就是个鸡肋，为什么呢?\n\n \u003e 查询缓存是提前把查询结果缓存起来，这样下次不需要执行就可以直接拿到结果。需要说明是，在 MySQL 中的查询缓存，不是缓存查询计划，而是查询对应的结果。这就意味着查询匹配的**鲁棒性降低**只有。两个查询请求**在任何字符上的不同(例如:空格、注释、 大小写)**，都会导致缓存不会命中。因此 MySQL 的 查询缓存命中率不高。\n \u003e\n \u003e 同时，如果查询请求中包含某些系统函数、用户自定义变量和函数、一些系统表，如 mysql 、 information_schema、 performance_schema 数据库中的表，那这个请求就不会被缓存。以某些系统函数举例，可能同样的函数的两次调用会产生不一样的结果，比如函数 NOW ，每次调用都会产生最新的当前时间，如果在一个查询请求中调用了这个函数，那即使查询请求的文本信息都一样，那不同时间的两次 查询也应该得到不同的结果，如果在第一次查询时就缓存了，那第二次查询的时候直接使用第一次查询 的结果就是错误的!\n \u003e\n \u003e 此外，既然是缓存，那就有它**缓存失效的时间**， mysql的缓存系统会监测涉及到的每张表，只要该表的结构或者数据被修改那使用该表的所有高速缓存查询都将变为无效，并从高速缓存中删除，对于更新压力大的数据库来说查询缓存的命中率非常低。\n\n2. **解析器** 在解析器中对 SQL 语句进行语法分析、语义分析。  \n![image-20220915113320707](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915113320707.png?lastModify=1668423115)\n \n\n分析器先做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面 的字符串分别**是什么，代表什么**。 MySQL从你输入的\"select\"这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID”。\n\n接着，要做“ 语法分析”。根据词法分析的结果，语法分析器(比如:Bison)会根据语法规则，判断你输 入的这个SQL语句是否满足 MySQL 语法。  \n`select department_id,job_id,avg(salary) from employees group by department_id;`  \n![image-20220915113450605](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915113450605.png?lastModify=1668423115)  \n3. **优化器**  \n在优化器中会确定 SQL 语句的执行路径，比如是根据全表检索 ，还是根据索引检索等。 举例:如下语句是执行两个表的 join:  \n`select * from test1 join test2 using(ID) where test1.name='zhangwei' and test2.name='mysql高级课程';`\n\n方案1:可以先从表 test1 里面取出 name='zhangwei'的记录的 ID 值，再根据 ID 值关联到表 test2，再判断 test2 里面 name的值是否等于 'mysql高级课程'。 方案2:可以先从表 test2 里面取出 name='mysql高级课程' 的记录的 ID 值，再根据 ID 值关联到 test1， 再判断 test1 里面 name的值是否等于 zhangwei。\n\n这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。如果你还有一些疑问，比如优化器是怎么选择索引的，有没有可能选择错等。后面讲到索引我们再谈。\n\n在查询优化器中，可以分为 **逻辑查询 优化阶段和 物理查询 优化阶段**。\n\n4. 执行器\n\n截止到现在，还没有真正去读写真实的表，仅仅只是产出了一个执行计划。于是就进入了 执行器阶段 。  \n![image-20220915114115407](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915114115407.png?lastModify=1668423115)  \n在执行之前需要判断该用户是否 具备权限 。如果没有，就会返回权限错误。如果具备权限，就执行 SQL 查询并返回结果。在 MySQL8.0 以下的版本，如果设置了查询缓存，这时会将查询结果进行缓存。  \n`select * from test where id=1;`  \n比如:表test中，ID字段没有索引，那么执行器的执行流程是这样的:  \n调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是1，如果不是则跳过，如果是则将这行存在结果集中; 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。  \n至此，这个语句就执行完成了。对于有索引的表，执行的逻辑也差不多。 SQL 语句在 MySQL 中的流程是: SQL语句→查询缓存→解析器→优化器→执行器 。  \n![image-20220915114505957](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915114505957.png?lastModify=1668423115)\n\n### 2.2 MySQL8中SQL执行原理\n\n1. 确认profiling 是否开启  \n`mysql\u003e select @@profiling; mysql\u003e show variables like 'profiling';`  \n![[Pasted image 20221114222819.png|300]]  \nprofiling=0 代表关闭，我们需要把 profiling 打开，即设置为 1：  \n`mysql\u003e set profiling=1;`\n2. 多次执行相同SQL查询,然后我们执行一个 SQL 查询(你可以执行任何一个 SQL 查询):`mysql\u003e select * from employ`\n3. 查看当前会话所产生的所有 profiles:  \n`mysql\u003e show profiles; # 显示最近的几次查询`  \n![image-20220915115000321](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915115000321.png?lastModify=1668423115)\n4. **查看**profile 显示执行计划，查看程序的执行步骤:  \n`mysql\u003e show profile;`  \n![image-20220915115033150](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915115033150.png?lastModify=1668423115)  \n当然你也可以查询指定的 Query ID，比如: `mysql\u003e show profile for query 7;`  \n查询 SQL 的执行时间结果和上面是一样的,此外，还可以查询更丰富的内容:`mysql\u003e show profile cpu,block io for query 6;`  \n![image-20220915115120346](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915115120346.png?lastModify=1668423115)  \n继续:`mysql\u003e show profile cpu,block io for query 7;`  \n![image-20220915115139323](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915115139323.png?lastModify=1668423115)\n\n### 2.4 SQL语法顺序\n\n随着Mysql版本的更新换代，其优化器也在不断的升级，优化器会分析不同执行顺序产生的性能消耗不同而动态调整执行顺序。 需求:查询每个部门年龄高于20岁的人数且高于20岁人数不能少于2人，显示人数最多的第一名部门信息 下面是经常出现的查询顺序:  \n![image-20220915120603828](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915120603828.png?lastModify=1668423115)\n\n## 3 数据库缓冲池(buffer pool)\n\nInnodb存储引擎是以页为单位来管理存储空间的，我们进行的增删改查操作其实本质上都是在访问页面(包括读页面、写页面、创建新页面等操作)。而磁盘 I/O 需要消耗的时间很多，而在内存中进行操作，效率则会高很多，为了能让数据表或者索引中的数据随时被我们所用，DBMS会申请占用，内存来作为缓冲池，在真正访问页面之前，需要把在磁盘上的页缓存到内存中的**缓冲池**之后才可以访问。  \n这样做的好处是可以让磁盘活动最小化，**从而减小磁盘IO的时间**。要知道，这种策略对提 升SQL语句的查询性能来说至关重要。如果索引的数据在缓冲池里，那么访问的成本就会降低很多。\n\n### 3.1 缓冲池 vs 查询缓存\n\n缓冲池和查询缓存是一个东西吗？不是。\n\n1. 缓冲池（Buffer Pool）  \n首先我们需要了解在 InnoDB 存储引擎中，缓冲池都包括了哪些。 在InnoDB存储引擎中有一部分数据会放到内存中，缓冲池则占了这部分内存的大部分，它用来存储各种数据的缓存，如下图所示：![image-20220915151003679](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915151003679.png?lastModify=1668423115)\n\n\u003e缓存池的重要性：对于使用InnoDB作为存储引擎的表来说，不管是用于存储用户数据的索引(包括聚簇索引和二级索引)，还是各种系统数据，都是以页的形式存放在表空间中的，而所谓的表空间只不过是InnoDB对文件系统上一个或几个实际文件的抽象，也就是说我们的数据说到底还是存储在磁盘上的。但是各位也都知道，磁盘的速度慢的跟乌龟一样,怎么能配得上“快如风，疾如电”的CPU呢?这里，缓冲池可以帮助我们消除CPU和磁盘之间的鸿沟。所以InnoDB存储引擎在处理客户端的请求时，当需要访问某个页的数据时，就会把完整的页的数据全部加载到内存中,也就是说即使我们只需要访问一个页的一条记录,那也需要先把整个页的数据加载到内存中。将整个页加载到内存中后就可以进行读写访问了，在进行完读写访问之后并不着急把该页对应的内存空间释放掉,而是将其缓存起来,这样将来有请求再次访问该页面时，就可以省去磁盘I0的开销了。\n\n\u003e缓存原则：“ 位置 * 频次 ” 这个原则，可以帮我们对 I/O 访问效率进行优化。首先，位置决定效率，提供缓冲池就是为了在内存中可以直接访问数据。 其次，频次决定优先级顺序。因为缓冲池的大小是有限的，比如磁盘有 200G，但是内存只有 16G，缓冲池大小只有 1G，就无法将所有数据都加载到缓冲池里，这时就涉及到优先级顺序，会优先对使用频次高 的热数据进行加载 。\n\n\u003e缓冲池的预读特性:了解了缓冲池的作用之后，我们还需要了解缓冲池的另一个特性:预读。缓冲池的作用就是提升I/0效率,而我们进行读取数据的时候存在一个“ **局部性原理**”，也就是说我们使用了一些数据，大概率还会使用它周围的一些数据 ，因此采用‘预读’’的机制提前加载，可以减少未来可能的磁盘/0操作。\n\n2. 查询缓存  \n查询缓存是提前把查询结果缓存起来，这样下次不需要执行就可以直接拿到结果。需要说明的是，在MySQL中的查询缓存，不是缓存查询计划，而是查询对应的结果。因为命中条件苛刻，而且**只要数据表发生变化，查询缓存就会失效，因此命中率低**。\n\n### 3.2 缓冲池如何读取数据\n\n缓冲池管理器会尽量将经常使用的数据保存起来，在数据库进行页面读操作的时候，首先会判断该页面 是否在缓冲池中，如果存在就直接读取，**如果不存在，就会通过内存或磁盘将页面存放到缓冲池中再进行读取**。  \n![image-20220915151640224](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915151640224.png?lastModify=1668423115)\n\n如果我们执行 SQL 语句的时候更新了缓存池中的数据，那么这些数据会马上同步到磁盘上吗？  \n实际上，当我们对数据库中的记录进行修改的时候，首先会修改缓冲池中页里面的记录信息，然后数据库会以一定的频率刷新到磁盘上。注意并不是每次发生更新操作，都会立刻进行磁盘回写。缓冲池会采用一种叫做checkpoint的机制将数据回写到磁盘上,这样做的好处就是提升了数据库的整体性能。 比如，当缓冲池不够用时，需要释放掉一些不常用的页，此时就可以强行采用checkpoint的方式，**将不常用的脏页回写到磁盘上,然后再从缓冲池中将这些页释放掉。这里脏页(dirty page)指的是缓冲池中被修改过的页，与磁盘上的数据页不一致。**\n\n### 3.3 查看/设置缓冲池的大小\n\n如果你使用的是 InnoDB 存储引擎，可以通过查看`innodb_buffer_pool_size`变量来查看缓冲池的大 小。  \n![image-20220915152021622](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915152021622.png?lastModify=1668423115)\n\n你能看到此时 InnoDB 的缓冲池大小只有 134217728/1024/1024=128MB。我们可以修改缓冲池大小，比如 改为256MB:`set global innodb_buffer_pool_size = 268435456;`\n\n### 3.4 多个Buffer Pool实例\n\n[server]  \ninnodb_buffer_pool_instances = 2\n\n这样就表明我们要创建2个 Buffer Pool 实例。 我们看下如何查看缓冲池的个数，使用命令：`show variables like 'innodb_buffer_pool_instances';`\n\n那每个 Buffer Pool 实例实际占多少内存空间呢？其实使用这个公式算出来的：`innodb_buffer_pool_size/innodb_buffer_pool_instances`也就是总共的大小除以实例的个数，结果就是每个 Buffer Pool 实例占用的大小。\n\n不过也不是说Buffer Pool实例创建的越多越好，分别管理各个Buffer Pool也是 需要性能开销的，InnoDB规定: 当`innodb_buffer_pool_size`的值小于1G的时候设置多个实例是无效的，InnoDB会默认把`innodb_buffer_pool_instances` 的值修改为1。而我们鼓励在Buffer Pool大于或等于1G的时候设置多个Buffer Pool实例。\n\n3.5 引申问题  \nBuffer Pool是MySQL内存结构中十分核心的一个组成，你可以先把它想象成一个黑盒子。 黑盒下的更新数据流程：\n\n当我们查询数据的时候，会先去Buffer Pool中查询。如果Buffer Pool中不存在，存储弓|擎会先将数据从磁盘加载到Buffer Pool中，然后将数据返回给客户端;同理，当我们更新某个数据的时候，如果这个数据不存在于Buffer Pool,同样会先数据加载进来,然后修改修改内存的数据。被修改过的数据会在之后统一刷入磁盘。\n\n![image-20220915154858925](file:///Users/ericx/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/TechIsAll/pics/image-20220915154858925.png?lastModify=1668423115)\n\n这个过程看似没啥问题，实则是有问题的。假设我们修改Buffer Pool中的数据成功，但是还没来得及将数据刷入磁盘MySQL就挂了怎么办?按照上图的逻辑，此时更新之后的数据只存在于Buffer Pool中，如果此时MySQL宕机了，这部分数据将会永久地丢失;\n\n我更新到一半突然发生错误了，想要回滚到更新之前的版本，该怎么办？连数据持久化的保证、事务回滚都做不到还谈什么崩溃恢复？ 答案：Redo Log \u0026 Undo Log\n","lastmodified":"2023-05-09T16:33:58.263365716Z","tags":[]},"/6-%E7%B4%A2%E5%BC%95%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84":{"title":"6 索引的数据结构","content":"\n# 6 索引的数据结构\n\n## 1. 为什么使用索引\n\n索弓是存储弓|擎用于快速找到数据记录的一种数据结构，就好比一本教课书的目录部分， 通过目录中找到对应文章的页码，便可快速定位到需要的文章。MySQL 中也是一样的道理， 进行数据查找时，首先查看查询条件是否命中某条索引，符合则通过索引查找相关数据，如果不符合则需要全表扫描，即需要一条一条地查找记录，直到找到与条件符合的记录。\n\n![image-20220915190545240](image-20220915190545240.png)\n\n假如给数据使用 二叉树 这样的数据结构进行存储，如下图所示\n\n![image-20220915191041693](image-20220915191041693.png)\n\n二叉搜索树， 采用这种存储结构搜索数据的时间，复杂度是 log2N。\n\n## 2. 索引及其优缺点\n\n### 2.1 索引概述\n\nMySQL 官方对索引的定义为：**索引（Index）是帮助 MySQL 高效获取数据的数据结构**。\n\n索引的本质：索引是数据结构。你可以简单理解为“**排好序的快速查找数据结构**”，满足特定查找算法。 这些数据结构以某种方式指向数据， 这样就可以在这些数据结构的基础上实现**高级查找算法** 。\n\n索引是在存储引擎中实现的，因此每种存储引擎的索引不一定完全相同，并且每种存储引擎不一定支持所有索引类型。同时,存储引擎可以定义每个表的最大索引数和最大索引长度。所有存储引擎支持每个表至少 16 个索引，总索引长度至少为 256 字节。有些存储引擎支持更多的索引数和更大的索引长度。\n\n### 2.2 优点\n\n1. 类似大学图书馆建书目索引，提高数据检索的效率，降低数据库的 IO 成本 ，这也是创建索引最主 要的原因。\n2. 通过创建唯一索引，可以保证数据库表中每一行 数据的唯一性 。\n3. 在实现数据的参考完整性方面，可以加速表和表之间的连接。换句话说，对于有依赖关系的子表和父表联合查询时， 可以提高查询速度。\n4. 在使用分组和排序子句进行数据查询时，可以显著减少查询中分组和排序的时间 ，降低了 CPU 的消耗。\n\n### 2.3 缺点\n\n增加索引也有许多不利的方面，主要表现在如下几个方面：\n\n1. 创建索引和维护索引要**耗费时间** ，并且随着数据量的增加，所耗费的时间也会增加。\n2. 索引需要占磁盘空间 ，除了数据表占数据空间之外，每一个索引还要占一定的物理空间， 存储在磁盘上 ，如果有大量的索引，索引文件就可能比数据文件更快达到最大文件尺寸。\n3. 虽然索引大大提高了查询速度，同时却会降低更新表的速度 。当对表中的数据进行增加、删除和修改的时候，索引也要动态地维护，这样就降低了数据的维护速度。\n\n因此，选择使用索引时，需要综合考虑索引的优点和缺点。\n\n\u003e 提示:  \n\u003e 索引可以提高查询的速度，但是会影响插入记录的速度。这种情况下，最好的办法是先删除表中的索引，然后插入数据，插入完成后再创建索引。\n\n## 3. InnoDB 中索引的推演\n\n### 3.1 索引之前的查找\n\n先来看一个精确匹配的例子： `SELECT [列名列表] FROM 表名 WHERE 列名 = xxx;`\n\n1. 在一个页中的查找\n\n   假设目前表中的记录比较少，所有的记录都可以被存放到一个页中，在查找记录的时候可以根据搜索条件的不同分为两种情况:\n\n   - 以主键为搜索条件  \n     可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录。\n   - 以其他列作为搜索条件  \n     因为在数据页中并没有对非主键列建立所谓的页目录，所以我们无法通过二分法快速定位相应的槽。这种情况下只能从最小记录、开始依次遍历单链表中的每条记录,然后对比每条记录是不是符合搜索条件。很显然，这种查找的效率是非常低的。\n\n2. 在很多页中查找\n\n   大部分情况下我们表中存放的记录都是非常多的，需要好多的数据页来存储这些记录。在很多页中查找记录的话可以分为两个步骤: 1.定位到记录所在的页。 2.从所在的页内中查找相应的记录。\n\n   在没有索引的情况下，不论是根据主键列或者其他列的值进行查找，由于我们并不能快速的定位到记录所在的页，所以只能从第一个页沿着双向链表一直往下找，在每一个页中根据我们上面的查找方式去查 找指定的记录。因为要遍历所有的数据页，所以这种方式显然是超级耗时的。如果一个表有一亿条记录 呢？此时索引应运而生。\n\n### 3.2 设计索引\n\n```bash\nmysql\u003e CREATE TABLE index_demo(\n-\u003e c1 INT,\n-\u003e c2 INT,\n-\u003e c3 CHAR(1),\n-\u003e PRIMARY KEY(c1)\n-\u003e ) ROW_FORMAT = Compact;\n```\n\n这个新建的 index_demo 表中有 2 个 INT 类型的列，1 个 CHAR(1)类型的列，而且我们规定了 c1 列为主键， 这个表使用 Compact 行格式来实际存储记录的。这里我们简化了 index_demo 表的行格式示意图：![image-20220916191805510](image-20220916191805510.png)\n\n我们只在示意图里展示记录的这几个部分:\n\n- record_type ：记录头信息的一项属性，表示记录的类型， 0 表示普通记录、 2 表示最小记 录、 3 表示最大记录、 1 暂时还没用过，下面讲。\n- next_record ：记录头信息的一项属性，表示下一条地址相对于本条记录的地址偏移量，我们用 箭头来表明下一条记录是谁。\n- 各个列的值 ：这里只记录在 index_demo 表中的三个列，分别是 c1 、 c2 和 c3 。\n- 其他信息 ：除了上述 3 种信息以外的所有信息，包括其他隐藏列的值以及记录的额外信息。\n\n将记录格式示意图的其他信息项暂时去掉并把它竖起来的效果就是这样：\n\n![image-20220916191926077](image-20220916191926077.png)\n\n把一些记录放到页里的示意图就是：\n\n![image-20220916191942056](image-20220916191942056.png)\n\n#### 1 一个简单的索引设计方案\n\n我们在根据某个搜索条件查找一些记录时为什么要遍历所有的数据页呢？因为各个页中的记录并没有规 律，我们并不知道我们的搜索条件匹配哪些页中的记录，所以不得不依次遍历所有的数据页。所以如果 我们 想快速的定位到需要查找的记录在哪些数据页 中该咋办？我们可以为快速定位记录所在的数据页而 建 立一个目录 ，建这个目录必须完成下边这些事：\n\n- 下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。\n- 给所有的页建立一个目录项。\n\n所以我们为上边几个页做好的目录就像这样子：\n\n![image-20220916215005870](image-20220916215005870.png)\n\n以 页 28 为例，它对应目录项 2 ，这个目录项中包含着该页的页号 28 以及该页中用户记录的最小主 键值 5 。我们只需要把几个目录项在物理存储器上连续存储（比如：数组），就可以实现根据主键 值快速查找某条记录的功能了。比如：查找主键值为 20 的记录，具体查找过程分两步： 1. 先从目录项中根据 二分法 快速确定出主键值为 20 的记录在 目录项 3 中（因为 12 \u003c 20 \u003c 209 ），它对应的页是 页 9 。 2. 再根据前边说的在页中查找记录的方式去 页 9 中定位具体的记录。\n\n至此，针对数据页做的简易目录就搞定了。这个目录有一个别名，称为 **索引** 。\n\n#### 2 InnoDB 中的索引方案\n\n1. 迭代 1 次：目录项纪录的页\n\n   ![image-20220916220239279](image-20220916220239279.png)\n\n   从图中可以看出来，我们新分配了一个编号为 30 的页来专门存储目录项记录。这里再次强调 目录项记录 和普通的 用户记录 的不同点：\n\n   - 目录项记录 的 record_type 值是 1，而 普通用户记录 的 record_type 值是 0\n   - 目录项记录**只有 主键值和页的编号 两个列**，而普通的用户记录的列是用户自己定义的，可能包含很多列 ，另外还有 InnoDB 自己添加的隐藏列。\n   - 了解：记录头信息里还有一个叫 `min_rec_mask` 的属性，只有在存储 目录项记录 的页中的主键值 最小的 目录项记录 的 min_rec_mask 值为 1 ，其他别的记录的 min_rec_mask 值都是 0 。\n\n相同点：两者用的是一样的数据页，都会为主键值生成 Page Directory （页目录），从而在按照主键 值进行查找时可以使用 二分法 来加快查询速度。\n\n现在以查找主键为 20 的记录为例，根据某个主键值去查找记录的步骤就可以大致拆分成下边两步： 先到存储 目录项记录 的页，也就是页 30 中通过 二分法 快速定位到对应目录项，因为 12 \u003c 20 \u003c 209 ，所以定位到对应的记录所在的页就是页 9。 再到存储用户记录的页 9 中根据 二分法 快速定位到主键值为 20 的用户记录。\n\n2. **迭代 2 次：多个目录项纪录的页**\n\n![image-20220916220613162](image-20220916220613162.png)\n\n从图中可以看出，我们插入了一条主键值为 320 的用户记录之后需要两个新的数据页：\n\n- 为存储该用户记录而新生成了 页 31 。\n- 因为原先存储目录项记录的 页 30 的容量已满 （我们前边假设只能存储 4 条目录项记录），所以不得 不需要一个新的 页 32 来存放 页 31 对应的目录项。\n\n现在因为存储目录项记录的页不止一个，所以如果我们想根据主键值查找一条用户记录大致需要 3 个步 骤，以查找主键值为 20 的记录为例：\n\n- 确定 目录项记录页\n- 我们现在的存储目录项记录的页有两个，即 页 30 和 页 32 ，又因为页 30 表示的目录项的主键值的 范围是 [1, 320) ，页 32 表示的目录项的主键值不小于 320 ，所以主键值为 20 的记录对应的目 录项记录在 页 30 中。\n- 通过目录项记录页 确定用户记录真实所在的页 。 在一个存储 目录项记录 的页中通过主键值定位一条目录项记录的方式说过了。\n- 在真实存储用户记录的页中定位到具体的记录。\n\n3. **迭代 3 次：目录项记录页的目录页**\n\n   ![image-20220916221316006](image-20220916221316006.png)\n\n   如图，我们生成了一个存储更高级目录项的 页 33 ，这个页中的两条记录分别代表页 30 和页 32，如果用 户记录的主键值在 [1, 320) 之间，则到页 30 中查找更详细的目录项记录，如果主键值 不小于 320 的 话，就到页 32 中查找更详细的目录项记录。\n\n   我们可以用下边这个图来描述它：\n\n   ![image-20220916221606364](image-20220916221606364.png)\n\n4. B+Tree\n\n   一个 B+树的节点其实可以分成好多层，规定最下边的那层，也就是存放我们用户记录的那层为第 0 层， 之后依次往上加。之前我们做了一个非常极端的假设：存放用户记录的页 最多存放 3 条记录 ，存放目录项 记录的页 最多存放 4 条记录 。其实真实环境中一个页存放的记录数量是非常大的，假设所有存放用户记录 的叶子节点代表的数据页可以存放 100 条用户记录 ，所有存放目录项记录的内节点代表的数据页可以存 放 1000 条目录项记录 ，那么：\n\n   - 如果 B+树只有 1 层，也就是只有 1 个用于存放用户记录的节点，最多能存放 100 条记录。\n   - 如果 B+树有 2 层，最多能存放 1000×100=10,0000 条记录。\n   - 如果 B+树有 3 层，最多能存放 1000×1000×100=1,0000,0000 条记录。\n   - 如果 B+树有 4 层，最多能存放 1000×1000×1000×100=1000,0000,0000 条记录。相当多的记 录！！！\n\n     你的表里能存放 100000000000 条记录吗？所以一般情况下，我们用到的 B+树都不会超过 4 层 ，那我们 通过主键值去查找某条记录最多只需要做 4 个页面内的查找（查找 3 个目录项页和一个用户记录页），又因为在每个页面内有所谓的 Page Directory （页目录），所以在**页面内也可以通过二分法实现快速 定位记录。**\n\n### 3.3 常见索引概念\n\n1. 聚簇索引\n\n   特点：\n\n   1. 使用记录主键值的大小进行记录和页的排序，这包括三个方面的含义：\n\n      - 页内的记录是**按照主键的大小顺序排成一个单向链表** 。\n      - 各个存放用户记录的页也是**根据页中用户记录的主键大小顺序排成一个双向链表**。\n      - 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成一个 双向链表 。\n\n   2. B+树的叶子节点存储的是完整的用户记录。\n\n      所谓完整的用户记录，就是指这个记录中存储了所有列的值（包括隐藏列）。\n\n   优点：\n\n   - 数据访问更快 ，因为聚簇索引**将索引和数据保存在同一个 B+树中**，因此从聚簇索引中获取数据比非 聚簇索引更快。\n   - 聚簇索引对于**主键的排序查找和范围查找速度非常快**\n   - 按照聚簇索引排列顺序，查询显示一定范围数据的时候，由于数据都是紧密相连，数据库不用从多个数据块中提取数据，所以节省了大量的 io 操作 。\n\n   缺点：\n\n   - 插入速度严重依赖于插入顺序 ，按照主键的顺序插入是最快的方式，否则将会出现页分裂，严重影响性能。因此，对于 InnoDB 表，我们一般都会定义一个自增的 ID 列为主键\n   - 更新主键的代价很高 ，因为将会导致被更新的行移动。因此，对于 InnoDB 表，我们一般定义**主键为不可更新**\n   - 二级索引访问需要两次索引查找 ，第一次找到主键值，第二次根据主键值找到行数\n\n2. 二级索引（辅助索引、非聚簇索引）\n\n![image-20220917151332531](image-20220917151332531.png)\n\n   **回表**：我们根据这个以 c2 列大小排序的 B+树只能确定我们要查找记录的主键值，所以如果我们想根据 c2 列的值查找到完整的用户记录的话，**仍然需要到聚簇索引中再查一遍**，这个过程称为回表，也就是根据 c2 列的值查询一条完整的用户记录需要使用到 2 棵 B+树！\n\n3. 联合索引\n\n   我们也可以同时以多个列的大小作为排序规则，也就是同时为多个列建立索引，比方说我们想让 B+树按 照 c2 和 c3 列 的大小进行排序，这个包含两层含义：\n\n   - 先把各个记录和页按照 c2 列进行排序\n   - 在记录的 c2 列相同的情况下，采用 c3 列进行排序\n\n   注意一点，以 c2 和 c3 列的大小为排序规则建立的 B+树称为**联合索引** ，本质上也是一个二级索引。它的意思与分别为 c2 和 c3 列分别建立索引的表述是不同的，不同点如下： 建立联合索引只会建立如上图一样的 1 棵 B+树。 为 c2 和 c3 列分别建立索引会分别以 c2 和 c3 列的大小为排序规则建立 2 棵 B+树。\n\n### 3.4 InnoDB 的 B+树索引的注意事项\n\n1. 根页面位置万年不动\n2. 内节点中目录项记录的唯一性\n3. 一个页面最少存储 2 条记录\n\n## 4.MyISAM 中的索引方案\n\n即使多个存储引擎支持同一种类型的索引，但是他们的实现原理也是不同的。Innodb 和 MyISAM 默认的索引是 Btree 索引；而 Memory 默认的索引是 Hash 索引。\n\nMyISAM 引擎使用 B+Tree 作为索引结构，叶子节点的 data 域存放的是 数据记录的地址 。  \n![image-20220917151332531](1ed78e86a4a4539d660cbc7088062cde.png)\n\n如果我们在 Col2 上建立一个二级索引，则此索引的结构如下图所示：  \n![image-20220917151332531](7124b14c8f4ceda3a22367da6b495817.png)\n\n### 4.3 MyISAM 与 InnoDB 对比\n\nMyISAM 的索引方式都是“非聚簇”的，与 InnoDB 包含 1 个聚簇索引是不同的。小结两种引擎中索引的区别：\n\n1. 在 InnoDB 存储引擎中，我们只需要**根据主键值对聚簇索引进行一次查找就能找到对应的记录**，而在 MyISAM 中却需要进行一次回表操作，**意味着 MyISAM 中建立的索引相当于全部都是二级索引** 。\n2. InnoDB 的数据文件本身就是索引文件，而 MyISAM 索引文件和数据文件是分离的 ，索引文件仅保存数据记录的地址。\n3. InnoDB 的非聚簇索引 data 域存储相应记录主键的值 ，而 MyISAM 索引记录的是地址 。换句话说， **InnoDB 的所有非聚簇索引都引用主键作为 data 域**。\n4. MyISAM 的回表操作是十分快速的，因为是拿着地址偏移量直接到文件中取数据的，反观 InnoDB 是通过获取主键之后再去聚簇索引里找记录，虽然说也不慢，但还是比不上直接用地址去访问。\n5. InnoDB 要求表必须有主键 （ MyISAM 可以没有 ）。如果没有显式指定，则 MySQL 系统会自动选择一个 可以非空且唯一标识数据记录的列作为主键。如果不存在这种列，则 MySQL 自动为 InnoDB 表生成一个隐 含字段作为主键，这个字段长度为 6 个字节，类型为长整型。  \n![image-20220917151332531](63abc199acbb4281a194563f38a773c1.png)\n\n## 5.索引的代价\n\n索引是个好东西，可不能乱建，它在空间和时间上都会有消耗：\n\n1. 空间上的代价:  \n每建立一个索引都要为它建立一棵B+树，每一棵B+树的每一个节点都是一个数据页，一个页默认会占用16KB的存储空间，一棵很大的B+树由许多数据页组成，那就是很大的一片存储空间。\n2. 时间上的代价:  \n每次对表中的数据进行增、删、改操作时，都需要去修改各个B+树索引。而且我们讲过,B+树每层节点都是按照索引列的值从小到大的顺序排序而组成了双向链表。不论是叶子节点中的记录，还是内节点中的记录（也就是不论是用户记录还是目录项记录）都是按照索引列的值从小到大的顺序而形成了一个单向链表。而增、删、改操作可能会对节点和记录的排序造成破坏，**所以存储引擎需要额外的时间进行一些记录移位页面分裂、页面回收等操作来维护好节点和记录的排序**。如果我们建了许多索引，每个索引对应的B+树都要进行相关的维护操作，会给性能拖后腿。\n","lastmodified":"2023-05-09T16:33:58.263365716Z","tags":[]},"/9-%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8":{"title":"9 性能分析工具的使用","content":"\n# 9 性能分析工具的使用\n\n## 1.数据库服务器的优化步骤\n\n当我们遇到数据库调优问题的时候，该如何思考呢？这里把思考的流程整理成下面这张图。 整个流程划分成了 观察（Show status） 和 行动（Action） 两个部分。字母 S 的部分代表观察（会使用相应的分析工具），字母 A 代表的部分是行动（对应分析可以采取的行动）。\n\n![image-20220926131318659](image-20220926131318659.png)\n\n我们可以通过观察了解数据库整体的运行状态，通过性能分析工具可以让我们了解执行慢的SQL都有哪些，查看具体的SQL执行计划，甚至是SQL执行中的每一步的成本代价， 这样才能定位问题所在，找到了问题，再采取相应的行动。  \n详细解释一下这张图:\n\n- 首先在S1部分，我们需要观察服务器的状态是否存在周期性的波动。如果存在周期性波动，**有可能是周期性节点的原因，比如双十一、促销活动**等。这样的话，我们可以通过A1这一步骤解决，也就是加缓存，或者更改缓存失效策略。\n- 如果缓存策略没有解决，或者不是周期性波动的原因，我们就需要进一步分析查询延迟和卡顿的原因。接下来进入S2这一步,我们需要开启慢查询。慢查询可以帮我们定位执行慢的SQL语句。我们可以**通过设置long_ query_ time参数定义“慢”的阈值**，如果SQL执行时间超过了long_query_time, 则会认为是慢查询。当收集上来这些慢查询之后，我们就可以通过分析工具对慢查询日志进行分析。\n- 在S3这一步骤中，我们就知道了执行慢的SQL,这样就可以针对性地用EXPLAIN查看对应SQL语句的执行计划，或者使用show profile 查看SQL中每个步骤的时间成本。这样我们就可以了解SQL查询慢是因为执行时间长，还是等待时间长。\n- 如果是SQL等待时间长，我们进入A2步骤。在这一步骤中， 我们可以调优服务器的参数，比如适当增加数据库缓冲池等。\n- 如果是SQL执行时间长，就进入A3步骤,这一步中我们需要考虑是索引设计的问题?还是查询关联的数据表过多?还是因为数据表的字段设计问题导致了这一现象。 然后在这些维度上进行对应的调整。\n- 如果A2和A3都不能解决问题，我们需要考虑数据库自身的SQL查询性能是否已经达到了瓶颈，如果确认没有达到性能瓶颈，就需要重新检查，重复以上的步骤。如果已经达到了性能瓶颈，进入A4阶段，需要考虑增加服务器，采用读写分离的架构，或者考虑对数据库进行分库分表，比如垂直分库、垂直分表和水平分表等。\n\n以上就是数据库调优的流程思路。如果我们发现执行SQL时存在不规则延迟或卡顿的时候，就可以采用分析工具帮我们定位有问题的SQL,这三种分析工具你可以理解是SQL调优的三个步骤:慢查询、EXPLAIN 和SHOW PROFILING。  \n![image-20220926131318659](image-20220926132332062.png)\n\n## 2. 查看系统性能参数\n\n在MySQL中，可以使用 SHOW STATUS 语句查询一些MySQL数据库服务器的性能参数 、 执行频率 。 SHOW STATUS语句语法如下：\n\n`SHOW [GLOBAL|SESSION] STATUS LIKE '参数';`\n\n一些常用的性能参数如下：\n\n- Connections：连接MySQL服务器的次数。\n- Uptime：MySQL服务器的上线时间。\n- Slow_queries：慢查询的次数。\n- Innodb_rows_read：Select查询返回的行数\n- Innodb_rows_inserted：执行INSERT操作插入的行数\n- Innodb_rows_updated：执行UPDATE操作更新的 行数\n- Innodb_rows_deleted：执行DELETE操作删除的行数\n- Com_select：查询操作的次数。\n- Com_insert：插入操作的次数。对于批量插入的 INSERT 操作，只累加一次。\n- Com_update：更新操作 的次数。\n- Com_delete：删除操作的次数。\n","lastmodified":"2023-05-09T16:33:58.263365716Z","tags":[]},"/CDN":{"title":"CDN","content":"\n## **一、什么是 CDN**\n\nCDN 的全称是(Content Delivery Network)，即内容分发网络。其目的是通过在现有的Internet中增加一层新的CACHE(缓存)层，将网站的内容发布到最接近用户的网络”边缘“的节点，使用户可以就近取得所需的内容，提高用户访问网站的响应速度。从技术上全面解决由于网络带宽小、用户访问量大、网点分布不均等原因，提高用户访问网站的响应速度。\n\n简单的说，CDN 的工作原理就是将您源站的资源缓存到位于全球各地的 CDN 节点上，用户请求资源时，就近返回节点上缓存的资源，而不需要每个用户的请求都回您的源站获取，避免网络拥塞、缓解源站压力，保证用户访问资源的速度和体验。\n\n![image-20210430000011104](image-20210430000011104.png)\n\nCDN 对网络的优化作用主要体现在如下几个方面\n\n- 解决服务器端的“第一公里”问题\n- 缓解甚至消除了不同运营商之间互联的瓶颈造成的影响\n- 减轻了各省的出口带宽压力\n- 缓解了骨干网的压力\n- 优化了网上热点内容的分布\n\n## **二、CDN工作原理**\n\n**传统访问过程**\n\n![image-20210430000056435](image-20210430000056435.png)\n\n由上图可见，用户访问未使用CDN缓存网站的过程为：\n\n1. 用户输入访问的域名，操作系统向 LocalDns 查询域名的 ip 地址\n2. LocalDns向 ROOT DNS 查询域名的授权服务器（这里假设LocalDns缓存过期）\n3. ROOT DNS将域名授权 dns记录回应给 LocalDns\n4. LocalDns 得到域名的授权 dns 记录后,继续向域名授权 dns 查询域名的 ip 地址\n5. 域名授权 dns 查询域名记录后，回应给 LocalDns\n6. LocalDns 将得到的域名ip地址，回应给 用户端\n7. 用户得到域名 ip 地址后，访问站点服务器\n8. 站点服务器应答请求，将内容返回给客户端\n\n**CDN 访问过程**\n\n![image-20210430000141459](image-20210430000141459.png)\n\n通过上图，我们可以了解到，使用了CDN缓存后的网站的访问过程变为：\n\n1. 用户输入访问的域名,操作系统向 LocalDns 查询域名的ip地址.\n2. LocalDns向 ROOT DNS 查询域名的授权服务器（这里假设LocalDns缓存过期）\n3. ROOT DNS将域名授权dns记录回应给 LocalDns\n4. LocalDns得到域名的授权dns记录后,继续向域名授权dns查询域名的ip地址\n5. 域名授权dns 查询域名记录后（一般是CNAME），回应给 LocalDns\n6. LocalDns 得到域名记录后，向智能调度DNS查询域名的ip地址\n7. 智能调度DNS 根据一定的算法和策略（比如静态拓扑，容量等），将最适合的CDN节点ip地址回应给 LocalDns\n8. LocalDns 将得到的域名ip地址，回应给 用户端\n9. 用户得到域名ip地址后，访问站点服务器\n10. CDN 节点服务器应答请求，将内容返回给客户端。（缓存服务器一方面在本地进行保存，以备以后使用，二方面把获取的数据返回给客户端，完成数据服务过程）\n\n通过以上的分析我们可以得到，**为了实现对普通用户透明（使用缓存后用户客户端无需进行任何设置）访问，需要使用 DNS（域名解析）来引导用户来访问 Cache 服务器**，以实现透明的加速服务。由于用户访问网站的第一步就是域名解析，所以通过修改dns来引导用户访问是最简单有效的方式。\n\n**CDN网络的组成要素**\n\n对于普通的 Internet 用户，每个 CDN 节点就相当于一个放置在它周围的网站服务器。\n\n通过对 DNS 的接管，用户的请求被透明地指向离他最近的节点，节点中 CDN 服务器会像网站的原始服务器一样，响应用户的请求。 由于它离用户更近，因而响应时间必然更快。\n\n从上面图中虚线圈起来的那块，就是 CDN 层，这层是位于用户端和站点服务器之间。\n\n智能调度 DNS（比如 f5 的 3DNS）\n\n- 智能调度DNS是CDN服务中的关键系统.当用户访问加入CDN服务的网站时，域名解析请求将最终由 “智能调度DNS”负责处理。\n- 它通过一组预先定义好的策略，将当时最接近用户的节点地址提供给用户，使用户可以得到快速的服务。\n- 同时它需要与分布在各地的CDN节点保持通信，跟踪各节点的健康状态、容量等信息，确保将用户的请求分配到就近可用的节点上.\n\n**缓存功能服务**\n\n- 负载均衡设备(如lvs,F5的BIG/IP)\n- 内容Cache服务器(如squid）\n- 共享存储\n\n## **三、名词解释**\n\n### **CNAME记录（CNAME record）**\n\nCNAME即别名( Canonical Name )；可以用来把一个域名解析到另一个域名，当 DNS 系统在查询 CNAME 左面的名称的时候，都会转向 CNAME 右面的名称再进行查询，一直追踪到最后的 PTR 或 A 名称，成功查询后才会做出回应，否则失败。\n\n\u003e 例如，你有一台服务器上存放了很多资料，你使用`docs.example.com`去访问这些资源，但又希望通过`documents.example.com`也能访问到这些资源，那么你就可以在您的DNS解析服务商添加一条CNAME记录，将`documents.example.com`指向`docs.example.com`，添加该条CNAME记录后，所有访问`documents.example.com`的请求都会被转到`docs.example.com`，获得相同的内容。\n\n### **CNAME域名**\n\n接入CDN时，在CDN提供商控制台添加完加速域名后，您会得到一个CDN给您分配的CNAME域名， 您需要在您的DNS解析服务商添加CNAME记录，将自己的加速域名指向这个CNAME域名，这样该域名所有的请求才会都将转向CDN的节点，达到加速效果。\n\n### **DNS**\n\nDNS 即 Domain Name System，是域名解析服务的意思。它在互联网的作用是：把域名转换成为网络可以识别的 IP 地址。人们习惯记忆域名，但机器间互相只认IP地址，域名与IP地址之间是一一对应的，它们之间的转换工作称为域名解析，域名解析需要由专门的域名解析服务器来完成，整个过程是自动进行的。比如：上网时输入的www.baidu.com 会自动转换成为 220.181.112.143。\n\n常见的DNS解析服务商有：阿里云解析，万网解析，DNSPod，新网解析，Route53（AWS），Dyn，Cloudflare等。\n\n### **回源 host**\n\n回源host：回源 host 决定回源请求访问到源站上的具体某个站点。\n\n\u003e 例子1：源站是域名源站为`www.a.com`，回源host为`www.b.com`,那么实际回源是请求到`www.a.com解析到的IP,对应的主机上的站点www.b.com\n\u003e\n\u003e 例子2：源站是IP源站为1.1.1.1, 回源host为www.b.com,那么实际回源的是1.1.1.1对应的主机上的站点www.b.com\n\n### **协议回源**\n\n指回源时使用的协议和客户端访问资源时的协议保持一致，即如果客户端使用 HTTPS 方式请求资源，当 CDN 节点上未缓存该资源时，节点会使用相同的 HTTPS 方式回源获取资源；同理如果客户端使用 HTTP 协议的请求，CDN 节点回源时也使用 HTTP 协议。\n","lastmodified":"2023-05-09T16:33:58.263365716Z","tags":[]},"/CSRFXSS":{"title":"CSRFXSS","content":"\n## **CSRF**\n\n跨站请求伪造（Cross Site Request Forgery），是指黑客诱导用户打开黑客的网站，在黑客的网站中，利用用户的登陆状态发起的跨站请求。CSRF攻击就是利用了用户的登陆状态，并通过第三方的站点来做一个坏事。\n\n要完成一次CSRF攻击,受害者依次完成两个步骤:\n\n1. 登录受信任网站A，并在本地生成Cookie\n2. 在不登出A的情况，访问危险网站B\n\n![img](Web%E5%AE%89%E5%85%A8.assets/640.jpeg)CSRF攻击\n\n在`a.com`登陆后种下cookie, 然后有个支付的页面，支付页面有个诱导点击的按钮或者图片，第三方网站域名为 `b.com`，中的页面请求 `a.com`的接口，`b.com` 其实拿不到cookie，请求 `a.com`会把Cookie自动带上（因为Cookie种在 `a.com`域下）。这就是为什么在服务端要判断请求的来源，及限制跨域（只允许信任的域名访问），然后除了这些还有一些方法来防止 CSRF 攻击，下面会通过几个简单的例子来详细介绍 CSRF 攻击的表现及如何防御。\n\n下面会通过一个例子来讲解 CSRF 攻击的表现是什么样子的。实现的例子：在前后端同域的情况下，前后端的域名都为 `http://127.0.0.1:3200`, 第三方网站的域名为 `http://127.0.0.1:3100`，钓鱼网站页面为 `http://127.0.0.1:3100/bad.html`。\n\n\u003e 平时自己写例子中会用到下面这两个工具，非常方便好用：\n\n- **http-server**[1]: 是基于node.js的HTTP 服务器，它最大的好处就是：可以使用任意一个目录成为服务器的目录，完全抛开后端的沉重工程，直接运行想要的js代码;\n- **nodemon**[2]: nodemon是一种工具，通过在检测到目录中的文件更改时自动重新启动节点应用程序来帮助开发基于node.js的应用程序\n\n前端页面：client.html\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"en\"\u003e\n\n\u003chead\u003e\n    \u003cmeta charset=\"UTF-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\n    \u003cmeta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"\u003e\n    \u003ctitle\u003eCSRF-demo\u003c/title\u003e\n    \u003cstyle\u003e\n        .wrap {\n            height: 500px;\n            width: 300px;\n            border: 1px solid #ccc;\n            padding: 20px;\n            margin-bottom: 20px;\n        }\n        input {\n            width: 300px;\n        }\n        .payInfo {\n            display: none;\n        }\n        .money {\n            font-size: 16px;\n        }\n    \u003c/style\u003e\n\u003c/head\u003e\n\n\u003cbody\u003e\n    \u003cdiv class=\"wrap\"\u003e\n        \u003cdiv class=\"loginInfo\"\u003e\n            \u003ch3\u003e登陆\u003c/h3\u003e\n            \u003cinput type=\"text\" placeholder=\"用户名\" class=\"userName\"\u003e\n            \u003cbr\u003e\n            \u003cinput type=\"password\" placeholder=\"密码\" class=\"password\"\u003e\n            \u003cbr\u003e\n            \u003cbr\u003e\n            \u003cbutton class=\"btn\"\u003e登陆\u003c/button\u003e\n        \u003c/div\u003e\n        \n        \n        \u003cdiv class=\"payInfo\"\u003e\n            \u003ch3\u003e转账信息\u003c/h3\u003e\n            \u003cp \u003e当前账户余额为 \u003cspan class=\"money\"\u003e0\u003c/span\u003e元\u003c/p\u003e\n            \u003c!-- \u003cinput type=\"text\" placeholder=\"收款方\" class=\"account\"\u003e --\u003e\n            \u003cbutton class=\"pay\"\u003e支付10元\u003c/button\u003e\n            \u003cbr\u003e\n            \u003cbr\u003e\n            \u003ca href=\"http://127.0.0.1:3100/bad.html\" target=\"_blank\"\u003e\n                听说点击这个链接的人都赚大钱了，你还不来看一下么\n            \u003c/a\u003e\n        \u003c/div\u003e\n    \u003c/div\u003e\n\u003c/body\u003e\n\u003cscript\u003e\n    const btn = document.querySelector('.btn');\n    const loginInfo = document.querySelector('.loginInfo');\n    const payInfo = document.querySelector('.payInfo');\n    const money = document.querySelector('.money');\n    let currentName = '';\n    // 第一次进入判断是否已经登陆\n    Fetch('http://127.0.0.1:3200/isLogin', 'POST', {})\n    .then((res) =\u003e {\n        if(res.data) {\n            payInfo.style.display = \"block\"\n            loginInfo.style.display = 'none';\n            Fetch('http://127.0.0.1:3200/pay', 'POST', {userName: currentName, money: 0})\n            .then((res) =\u003e {\n                money.innerHTML = res.data.money;\n            })\n        } else {\n            payInfo.style.display = \"none\"\n            loginInfo.style.display = 'block';\n        }\n        \n    })\n    // 点击登陆\n    btn.onclick = function () {\n        var userName = document.querySelector('.userName').value;\n        currentName = userName;\n        var password = document.querySelector('.password').value;\n        Fetch('http://127.0.0.1:3200/login', 'POST', {userName, password})\n        .then((res) =\u003e {\n            payInfo.style.display = \"block\";\n            loginInfo.style.display = 'none';\n            money.innerHTML = res.data.money;\n        })\n    }\n    // 点击支付10元\n    const pay = document.querySelector('.pay');\n    pay.onclick = function () {\n        Fetch('http://127.0.0.1:3200/pay', 'POST', {userName: currentName, money: 10})\n        .then((res) =\u003e {\n            console.log(res);\n            money.innerHTML = res.data.money;\n        })\n    }\n    // 封装的请求方法\n    function Fetch(url, method = 'POST', data) {\n        return new Promise((resolve, reject) =\u003e {\n            let options = {};\n            if (method !== 'GET') {\n                options = {\n                    headers: {\n                        'Content-Type': 'application/json',\n                    },\n                    body: JSON.stringify(data),\n                }\n            }\n            fetch(url, {\n                mode: 'cors', // no-cors, cors, *same-origin\n                method,\n                ...options,\n                credentials: 'include',\n            }).then((res) =\u003e {\n                return res.json();\n            }).then(res =\u003e {\n                resolve(res);\n            }).catch(err =\u003e {\n                reject(err);\n            });\n        })\n    }\n    \n\u003c/script\u003e\n\n\u003c/html\u003e\n```\n\n实现一个简单的支付功能：\n\n1. 会首先判断有没有登录，如果已经登陆过，就直接展示转账信息，未登录，展示登陆信息\n2. 登陆完成之后，会展示转账信息，点击支付，可以实现金额的扣减\n\n后端服务：server.js\n\n```javascript\nconst Koa = require(\"koa\");\nconst app = new Koa();\nconst route = require('koa-route');\nconst bodyParser = require('koa-bodyparser');\nconst cors = require('@koa/cors');\nconst KoaStatic = require('koa-static');\n\nlet currentUserName = '';\n\n// 使用  koa-static  使得前后端都在同一个服务下\napp.use(KoaStatic(__dirname));\n\napp.use(bodyParser()); // 处理post请求的参数\n\n// 初始金额为 1000\nlet money = 1000;\n\n// 调用登陆的接口\nconst login = ctx =\u003e {\n    const req = ctx.request.body;\n    const userName = req.userName;\n    currentUserName = userName;\n    // 简单设置一个cookie\n    ctx.cookies.set(\n        'name', \n        userName,\n        {\n          domain: '127.0.0.1', // 写cookie所在的域名\n          path: '/',       // 写cookie所在的路径\n          maxAge: 10 * 60 * 1000, // cookie有效时长\n          expires: new Date('2021-02-15'),  // cookie失效时间\n          overwrite: false,  // 是否允许重写\n          SameSite: 'None',\n        }\n      )\n    ctx.response.body = {\n        data: {\n            money,\n        },\n        msg: '登陆成功'\n    };\n}\n// 调用支付的接口\nconst pay = ctx =\u003e {\n    if(ctx.method === 'GET') {\n        money = money - Number(ctx.request.query.money);\n    } else {\n        money = money - Number(ctx.request.body.money);\n    }\n    ctx.set('Access-Control-Allow-Credentials', 'true');\n    // 根据有没有 cookie 来简单判断是否登录\n    if(ctx.cookies.get('name')){\n        ctx.response.body = {\n            data: {\n                money: money,\n            },\n            msg: '支付成功'\n        };\n    }else{\n        ctx.body = '未登录';\n    }\n}\n\n// 判断是否登陆\nconst isLogin = ctx =\u003e {\n    ctx.set('Access-Control-Allow-Credentials', 'true');\n\n    if(ctx.cookies.get('name')){\n        ctx.response.body = {\n            data: true,\n            msg: '登陆成功'\n        };\n\n    }else{\n        ctx.response.body = {\n            data: false,\n            msg: '未登录'\n        };\n    }\n}\n// 处理 options 请求\napp.use((ctx, next)=\u003e {\n    const headers = ctx.request.headers;\n    if(ctx.method === 'OPTIONS') {\n        ctx.set('Access-Control-Allow-Origin', headers.origin);\n        ctx.set('Access-Control-Allow-Headers', 'Content-Type');\n        ctx.set('Access-Control-Allow-Credentials', 'true');\n        ctx.status = 204;\n    } else {\n        next();\n    }\n})\n\napp.use(cors());\napp.use(route.post('/login', login));\napp.use(route.post('/pay', pay));\napp.use(route.get('/pay', pay));\napp.use(route.post('/isLogin', isLogin));\n\napp.listen(3200, () =\u003e {\n    console.log('启动成功');\n});\n```\n\n执行 `nodemon server.js`，访问页面 `http://127.0.0.1:3200/client.html`\n\n![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)CSRF-demo\n\n登陆完成之后，可以看到Cookie是种到 `http://127.0.0.1:3200` 这个域下面的。\n\n第三方页面 bad.html\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n    \u003cmeta charset=\"UTF-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\n    \u003ctitle\u003e第三方网站\u003c/title\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n    \u003cdiv\u003e\n        哈哈，小样儿，哪有赚大钱的方法，还是踏实努力工作吧！\n        \u003c!-- form 表单的提交会伴随着跳转到action中指定 的url 链接，为了阻止这一行为，可以通过设置一个隐藏的iframe 页面，并将form 的target 属性指向这个iframe，当前页面iframe则不会刷新页面 --\u003e\n        \u003cform action=\"http://127.0.0.1:3200/pay\" method=\"POST\" class=\"form\" target=\"targetIfr\" style=\"display: none\"\u003e\n            \u003cinput type=\"text\" name=\"userName\" value=\"xiaoming\"\u003e\n            \u003cinput type=\"text\" name=\"money\" value=\"100\"\u003e\n        \u003c/form\u003e\n        \u003ciframe name=\"targetIfr\" style=\"display:none\"\u003e\u003c/iframe\u003e\n    \u003c/div\u003e\n\u003c/body\u003e\n\u003cscript\u003e\n    document.querySelector('.form').submit();\n\u003c/script\u003e\n\u003c/html\u003e\n```\n\n使用 HTTP-server 起一个 本地端口为 3100的服务，就可以通过 `http://127.0.0.1:3100/bad.html` 这个链接来访问，CSRF攻击需要做的就是在正常的页面上诱导用户点击链接进入这个页面![img](Web%E5%AE%89%E5%85%A8.assets/640.gif)\n\n点击诱导链接，跳转到第三方的页面，第三方页面自动发了一个扣款的请求，所以在回到正常页面的时候，刷新，发现钱变少了。我们可以看到在第三方页面调用 `http://127.0.0.1:3200/pay` 这个接口的时候，Cookie自动加在了请求头上，这就是为什么 `http://127.0.0.1:3100/bad.html` 这个页面拿不到 Cookie，但是却能正常请求 `http://127.0.0.1:3200/pay`这个接口的原因。\n\nCSRF攻击大致可以分为三种情况，自动发起Get请求， 自动发起POST请求，引导用户点击链接。下面会分别对上面例子进行简单的改造来说明这三种情况\n\n### 三种攻击情形\n\n#### 自动发起Get请求\n\n在上面的 bad.html中，我们把代码改成下面这样\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n  \u003cbody\u003e\n    \u003cimg src=\"http://127.0.0.1:3200/payMoney?money=1000\"\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n```\n\n当用户访问含有这个img的页面后，浏览器会自动向自动发起 img 的资源请求，如果服务器没有对该请求做判断的话，那么会认为这是一个正常的链接。\n\n#### 自动发起POST请求\n\n上面例子中演示的就是这种情况。\n\n```html\n\u003cbody\u003e\n    \u003cdiv\u003e\n        哈哈，小样儿，哪有赚大钱的方法，还是踏实努力工作吧！\n        \u003c!-- form 表单的提交会伴随着跳转到action中指定 的url 链接，为了阻止这一行为，可以通过设置一个隐藏的iframe 页面，并将form 的target 属性指向这个iframe，当前页面iframe则不会刷新页面 --\u003e\n        \u003cform action=\"http://127.0.0.1:3200/pay\" method=\"POST\" class=\"form\" target=\"targetIfr\"\u003e\n            \u003cinput type=\"text\" name=\"userName\" value=\"xiaoming\"\u003e\n            \u003cinput type=\"text\" name=\"money\" value=\"100\"\u003e\n        \u003c/form\u003e\n        \u003ciframe name=\"targetIfr\" style=\"display:none\"\u003e\u003c/iframe\u003e\n    \u003c/div\u003e\n\u003c/body\u003e\n\u003cscript\u003e\n    document.querySelector('.form').submit();\n\u003c/script\u003e\n```\n\n上面这段代码中构建了一个隐藏的表单，表单的内容就是自动发起支付的接口请求。当用户打开该页面时，这个表单会被自动执行提交。当表单被提交之后，服务器就会执行转账操作。因此使用构建自动提交表单这种方式，就可以自动实现跨站点 POST 数据提交。\n\n#### 引导用户点击链接\n\n诱惑用户点击链接跳转到黑客自己的网站，示例代码如图所示\n\n```html\n\u003ca href=\"http://127.0.0.1:3100/bad.html\"\u003e听说点击这个链接的人都赚大钱了，你还不来看一下么\u003c/a\u003e\n```\n\n用户点击这个地址就会跳到黑客的网站，黑客的网站可能会自动发送一些请求，比如上面提到的自动发起Get或Post请求。\n\n### 如何防御CSRF\n\n#### 利用cookie的SameSite\n\nSameSite有3个值：Strict, Lax和None\n\n1. Strict。浏览器会完全禁止第三方cookie。比如a.com的页面中访问 b.com 的资源，那么a.com中的cookie不会被发送到 b.com服务器，只有从b.com的站点去请求b.com的资源，才会带上这些Cookie\n2. Lax。相对宽松一些，在跨站点的情况下，从第三方站点链接打开和从第三方站点提交 Get方式的表单这两种方式都会携带Cookie。但如果在第三方站点中使用POST方法或者通过 img、Iframe等标签加载的URL，这些场景都不会携带Cookie。\n3. None。任何情况下都会发送 Cookie数据\n\n我们可以根据实际情况将一些关键的Cookie设置 Stirct或者 Lax模式，这样在跨站点请求的时候，这些关键的Cookie就不会被发送到服务器，从而使得CSRF攻击失败。\n\n#### 验证请求的来源点\n\n由于CSRF攻击大多来自第三方站点，可以在服务器端验证请求来源的站点，禁止第三方站点的请求。可以通过HTTP请求头中的 Referer和Origin属性。\n\n![img](Web%E5%AE%89%E5%85%A8.assets/640.png)HTTP请求头\n\n但是这种 Referer和Origin属性是可以被伪造的，碰上黑客高手，这种判断就是不安全的了。\n\n#### CSRF Token\n\n1. 最开始浏览器向服务器发起请求时，服务器生成一个CSRF Token。CSRF Token其实就是服务器生成的字符串，然后将该字符串种植到返回的页面中(可以通过Cookie)\n2. 浏览器之后再发起请求的时候，需要带上页面中的 `CSRF Token`（在request中要带上之前获取到的Token，比如 `x-csrf-token：xxxx`）, 然后服务器会验证该Token是否合法。第三方网站发出去的请求是无法获取到 `CSRF Token`的值的。\n\n### 其他知识点补充\n\n#### 1. 第三方cookie\n\nCookie是种在服务端的域名下的，比如客户端域名是 a.com，服务端的域名是 b.com， Cookie是种在 b.com域名下的，在 Chrome的 Application下是看到的是 a.com下面的Cookie，是没有的，之后，在a.com下发送b.com的接口请求会自动带上Cookie(因为Cookie是种在b.com下的)\n\n#### 2. 简单请求和复杂请求\n\n复杂请求需要处理option请求。\n\n之前写过一篇特别详细的文章 [**CORS原理及@koa/cors源码解析**[3\\]](http://mp.weixin.qq.com/s?__biz=MzI0MzU5Nzg0Ng==\u0026mid=2247483694\u0026idx=1\u0026sn=f8a735dcbdbdf8a84fb7d9ebee99ae24\u0026chksm=e96bd657de1c5f41ea454eb24c8f606308a6d98c0f055da2e63297ec35e5467d1d4c15c8d812\u0026scene=21#wechat_redirect)，有空可以看一下。\n\n#### 3. Fetch的 credentials 参数\n\n如果没有配置credential 这个参数，fetch是不会发送Cookie的\n\ncredential的参数如下\n\n- include：不论是不是跨域的请求，总是发送请求资源域在本地的Cookies、HTTP Basic anthentication等验证信息\n- same-origin：只有当URL与响应脚本同源才发送 cookies、 HTTP Basic authentication 等验证信息\n- omit：从不发送cookies.\n\n平常写一些简单的例子，从很多细节问题上也能补充自己的一些知识盲点。\n","lastmodified":"2023-05-09T16:33:58.275366009Z","tags":[]},"/CSS%E5%9F%BA%E7%A1%80":{"title":"长度单位","content":"\n# 长度单位\n\n- 像素 px  \n  像素是我们在网页中使用的最多的一个单位，一个像素就相当于我们屏幕中的一个小点，我们的屏幕实际上就是由这些像素点构成的但是这些像素点，是不能直接看见。不同显示器一个像素的大小也不相同，显示效果越好越清晰，像素就越小，反之像素越大。\n\n- 百分比 %  \n  也可以将单位设置为一个百分比的形式，这样浏览器将会根据其父元素的样式来计算该值使用百分比的好处是，当父元素的属性值发生变化时，子元素也会按照比例发生改变。在我们创建一个自适应的页面时，经常使用百分比作为单位。\n\n- em\n\n  em和百分比类似，它是相对于当前元素的字体大小来计算的\n\n  - 1em = 1font-size\n  - 使用em时，当字体大小发生改变时，em也会随之改变\n  - 当设置字体相关的样式时，经常会使用em\n\n# 颜色单位\n\n1. 在CSS可以直接使用颜色的单词来表示不同的颜色  \n   红色：red；蓝色：blue；绿色：green\n\n2. RGB值：指的是通过Red Green Blue三元色，通过这三种颜色的不同的浓度，来表示出不同的颜色  \n   `例子：rgb(红色的浓度,绿色的浓度,蓝色的浓度)`\n\n   \u003e 颜色的浓度需要一个0-255之间的值，255表示最大，0表示没有浓度也可以采用一个百分数来设置，需要一个0% - 100%之间的数字使用百分数最终也会转换为0-255之间的数0%表示0,100%表示255.\n\n- 十六进制的rgb值，原理和上边RGB原理一样，只不过使用十六进制数来代替，使用三组两位的十六进制数组来表示一个颜色.每组表示一个颜色.\n\n  第一组表示红色的浓度，范围00-ff  \n  第二组表示绿色的浓度，范围是00-ff  \n  第三组表示蓝色的浓度，范围00-ff  \n  语法：#红色绿色蓝色  \n  00表示没有，相当于rgb中的0  \n  ff表示最大，相当于rgb中255\n\n  \u003e 红色： ff0000 ，像这种两位两位重复的颜色，可以简写： ff0000 可以写成 f00 ，以及 abc可以写成 aabbcc\n\n  ```css\n  background-color: rgb(161,187,215);\t\t\t\t\n  background-color: rgb(100%,50%,50%);\t\t\t\n  background-color: #00f;\t\t\t\n  background-color: #abc; /*#aabbcc*/\n  background-color: #084098;\n  ```\n\n# 字体\n\n```css\n/*\n设置文字的大小,浏览器中一般默认的文字大小都是16px\nfont-size设置的并不是文字本身的大小，在页面中，每个文字都是处在一个看不见的框中的我们设置的font-size实际上是设置格的高度，并不是字体的大小.一般情况下文字都要比这个格要小一些，也有时会比格大，根据字体的不同，显示效果也不能\t\n * */\nfont-size: 30px;\n\t\t\t\t\n/*\n * 通过font-family可以指定文字的字体\n * 当采用某种字体时，如果浏览器支持则使用该字体，\n * 如果字体不支持，则使用默认字体\n * 该样式可以同时指定多个字体，多个字体之间使用,分开\n * 当采用多个字体时，浏览器会优先使用前边的字体，\n * 如果前边没有在尝试下一个\n */\nfont-family: arial , 微软雅黑;\n\t\t\t\t\n/*\n * 浏览器使用的字体默认就是计算机中的字体，\n * \t如果计算机中有，则使用，如果没有就不用\n * 在开发中，如果字体太奇怪，用的太少了，尽量不要使用，\n * \t有可能用户的电脑没有，就不能达到想要的效果。\n */\t\nfont-family: \"curlz mt\";\n\n/*\n * font-style可以用来设置文字的斜体\n * \t- 可选值：\n * \tnormal，默认值，文字正常显示\n * \titalic 文字会以斜体显示\n * \toblique 文字会以倾斜的效果显示\n * \t- 大部分浏览器都不会对倾斜和斜体做区分，\n * \t也就是说我们设置italic和oblique它们的效果往往是一样的\n *  - 一般我们只会使用italic\n */\nfont-style: italic;\n\t\t\t\t\n/*\n * font-weight可以用来设置文本的加粗效果：\n * \t可选值：\n * \t\tnormal，默认值，文字正常显示\n * \t\tbold，文字加粗显示\n * \t该样式也可以指定100-900之间的9个值，\n * \t但是由于用户的计算机往往没有这么多级别的字体，所以达到\n * \t也就是200有可能比100粗，300有可能比200粗\n */\nfont-weight: bold;\n\t\t\t\t\n/*\n * font-variant可以用来设置小型大写字母\n * \t可选值：\n * \t\tnormal，默认值，文字正常显示\n * \t\tsmall-caps 文本以小型大写字母显示\n * 小型大写字母：\n * \t\t将所有的字母都以大写形式显示，但是小写字母的大写，\n * \t\t要比大写字母的大小小一些。\n */\nfont-variant: small-caps ;\n\n/*\n * 在CSS并没有为我们提供一个直接设置行间距的方式，\n * 我们只能通过设置行高来间接的设置行间距，行高越大行间距越大\n * 使用line-height来设置行高 \n * 行高类似于我们上学单线本，单线本是一行一行，线与线之间的距离就是行高，\n * 网页中的文字实际上也是写在一个看不见的线中的，而文字会默认在行高中垂直居中显示 \n * 行间距 = 行高 - 字体大小\n */\n.p1{\n\tfont-size: 20px;\n\t/*\n\t * 通过设置line-height可以间接的设置行高，\n\t * \t可以接收的值：\n\t * \t\t1.直接就收一个大小\n\t * \t\t2.可以指定一个百分数，则会相对于字体去计算行高\n\t * \t\t3.可以直接传一个数值，则行高会设置字体大小相应的倍数\n\t */\n\t/*line-height: 200%;*/\n\tline-height: 2;\n}\n\n/*\n * 对于单行文本来说，可以将行高设置为和父元素的高度一致，\n * \t这样可以是单行文本在父元素中垂直居中\n */\nline-height: 200px;\n\n/*\n * 在font中也可以指定行高\n * \t在字体大小后可以添加/行高，来指定行高，该值是可选的，如果不指定则会使用默认值\n */\nfont: 30px \"微软雅黑\";\nline-height: 50px;\n```\n\n# 字体图标\n\n字体图标使用场景： 主要用于显示网页中通用、常用的一些小图标。\n\n精灵图是有诸多优点的，但是缺点很明显。\n\n1. 图片文件还是比较大的。\n2. 图片本身放大和缩小会失真。\n3. 一旦图片制作完毕想要更换非常复杂。\n\n    此时，有一种技术的出现很好的解决了以上问题，就是字体图标 iconfont。 字体图标可以为前端工程师提供一种方便高效的图标使用方式，展示的是图标，本质属于字体。\n\n### 使用\n\n#### 下载\n\n![image-20200213183206610](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/css/20210410171954.png)\n\n#### 引入\n\n![image-20200213183333538](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/css/20210410171955.png)\n\n![image-20200213183402149](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/css/20210410171956.png)\n\n![image-20200213183416936](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/css/20210410171957.png)\n\n![image-20200213183429768](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/css/20210410171958.png)\n\n![image-20200213183441546](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/css/20210410171959.png)\n\n# 文本\n\n```css\n/*\n * text-transform可以用来设置文本的大小写\n * \t可选值：\n * \t\tnone 默认值，该怎么显示就怎么显示，不做任何处理\n * \t\tcapitalize 单词的首字母大写，通过空格来识别单词\n * \t\tuppercase 所有的字母都大写\n * \t\tlowercase 所有的字母都小写\n */\ntext-transform: lowercase;\n\n/*\n * text-decoration可以用来设置文本的修饰\n * \t可选值：\n * \t\tnone：默认值，不添加任何修饰，正常显示\n * \t\tunderline 为文本添加下划线\n * \t\toverline 为文本添加上划线\n * \t\tline-through 为文本添加删除线\n */\ntext-decoration: line-through;\n\na {\n/*超链接会默认添加下划线，也就是超链接的text-decoration的默认值是underline如果需要去除超链接的下划线则需要将该样式设置为none* */\n\ttext-decoration: none;\n}\n\n/**\n * letter-spacing可以指定字符间距\n */\nletter-spacing: 10px;\n\t\t\t\t\n/*\n * word-spacing可以设置单词之间的距离\n * \t实际上就是设置词与词之间空格的大小\n */\nword-spacing: 120px;\n\n/*\n * text-align用于设置文本的对齐方式\n * \t可选值：\n * \t\tleft 默认值，文本靠左对齐\n * \t\tright 文本靠右对齐\n * \t\tcenter 文本居中对齐\n * \t\tjustify 两端对齐\n * \t- 通过调整文本之间的空格的大小，来达到一个两端对齐的目的\n */\ntext-align: justify ;\n\n/*\n * text-indent用来设置首行缩进\n * \t当给它指定一个正值时，会自动向右侧缩进指定的像素\n * \t如果为它指定一个负值，则会向左移动指定的像素,\n * \t通过这种方式可以将一些不想显示的文字隐藏起来\n *  这个值一般都会使用em作为单位\n */\ntext-indent: -99999px;\n\ntext-overflow : clip | ellipsis\n　　参数：\n　　clip : 　不显示省略标记（...），而是简单的裁切\n　　（clip这个参数是不常用的！）\n　　ellipsis : 　当对象内文本溢出时显示省略标记（...）\n```\n\n**css3**新增\n\n`text-shadow: h-shadow v-shadow blur color;`\n\n![image-20200213181730916](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/css/20210410172000.png)\n\n# 选择器\n\n### 元素选择器\n\n作用：通过元素选择器可以选则页面中的所有指定元素\n\n语法：标签名 {}\n\n```css\np{\n\tcolor: red;\n}\n```\n\n### id选择器\n\n通过元素的id属性值选中唯一的一个元素\n\n语法：#id属性值 {}\n\n```css\n#p1{\n\tfont-size: 20px;\n}\n```\n\n### 类选择器\n\n通过元素的class属性值选中一组元素\n\n语法：.class属性值{}\n\n```css\n.classname{\n    color:red;\n}\n```\n\n### 选择器分组(并集选择器)\n\n通过选择器分组可以同时选中多个选择器对应的元素\n\n语法：选择器1,选择器2,选择器N{}\n\n```css\n\\#p1 , .p2 , h1{\n\tbackground-color: yellow;\n}\n```\n\n### 通配选择器\n\n他可以用来选中页面中的所有的元素  \n语法：*{}\n\n### 复合选择器（交集选择器）\n\n作用：可以选中同时满足多个选择器的元素\n\n语法：选择器1选择器2选择器N{}\n\n```css\nspan .p3 {\n\tbackground-color: yellow;\n}\n```\n\n\u003e 对于id选择器来说，不建议使用复合选择器\n\n### 伪类选择器\n\n伪类专门用来表示元素的一种的特殊的状态，  \n比如：访问过的超链接，比如普通的超链接，比如获取焦点的文本框.当我们需要为处在这些特殊状态的元素设置样式时，就可以使用伪类\n\n```css\n/*\n * 为没访问过的链接设置一个颜色为绿色\n * \t:link\n * \t\t- 表示普通的链接（没访问过的链接）\n */\na:link{\n\tcolor: yellowgreen;\n}\n```\n\n```css\n/*\n * 为访问过的链接设置一个颜色为红色\n * \t:visited\n * \t\t- 表示访问过的链接\n * \n * 浏览器是通过历史记录来判断一个链接是否访问过,\n * 由于涉及到用户的隐私问题，所以使用visited伪类只能设置字体的颜色\n * \n */\na:visited{\n\tcolor: red;\n}\n```\n\n```css\n/*\n * :hover伪类表示鼠标移入的状态\n */\na:hover{\n\tcolor: skyblue;\n}\n```\n\n```css\n/*\n * :active表示的是超链接被点击的状态\n */\na:active{\n\tcolor: black;\n}\n```\n\n```css\n/*\n * 文本框获取焦点以后，修改背景颜色为黄色\n */\ninput:focus{\n\tbackground-color: yellow;\n}\n```\n\n```css\n/**\n * 为p标签中选中的内容使用样式\n * \t可以使用::selection为类\n * \t注意：这个伪类在火狐中需要采用另一种方式编写::-moz-select\n */\n\t\t\t\n/**\n * 兼容火狐的\n */\np::-moz-selection{\n\tbackground-color: orange;\n}\n\t\t\t\n/**\n * 兼容大部分浏览器的\n */\np::selection{\n\tbackground-color: orange;\n}\n\t\t\t\n```\n\n### 伪元素选择器\n\n*表示元素中的一些特殊的位置*\n\n```css\n/*\n * 为p中第一个字符来设置一个特殊的样式\n */\t\t\np:first-letter {\n\tcolor: red;\n\tfont-size: 20px;\n}\n```\n\n```css\n/*\n * 为p中的第一行设置一个背景颜色为黄色\n */\t\t\np:first-line {\n\tbackground-color: yellow;\n}\n```\n\n```css\n/*\n * :before表示元素最前边的部分\n * \t一般before都需要结合content这个样式一起使用，\n * \t通过content可以向before或after的位置添加一些内容\n * \n * :after表示元素的最后边的部分\n */\np:before{\n\tcontent: \"我会出现在整个段落的最前边\";\n\tcolor: red;\n}\t\t\t\np:after{\n\tcontent: \"我会出现在整个段落的最后边\";\n\tcolor: orange;\n}\n```\n\n### 属性选择器\n\n作用：可以根据元素中的属性或属性值来选取指定元素\n\n语法：  \n[属性名] 选取含有指定属性的元素  \n[属性名=\"属性值\"] 选取含有指定属性值的元素  \n[属性名^=\"属性值\"] 选取属性值以指定内容开头的元素  \n[属性名$=\"属性值\"] 选取属性值以指定内容结尾的元素  \n[属性名*=\"属性值\"] 选取属性值以包含指定内容的元素\n\n```css\n/*\n * 为title属性值以ab开头的元素设置一个背景颜色为黄色\n */\np[title^=\"ab\"]{\n\tbackground-color: yellow;\n}\t\n```\n\n### 结构伪类选择器\\子元素选择器\n\n```css\n/*\n * 为第一个p标签设置一个背景颜色为黄色\n * \t:first-child 可以选中第一个子元素\n *  :last-child 可以选中最后一个子元素\n */\nbody \u003e p:first-child{\n\tbackground-color: yellow;\n}\np:last-child{\n\tbackground-color: yellow;\n}\n\t\t\t\n/*\n * :nth-child 可以选中任意位置的子元素\n * \t\t该选择器后边可以指定一个参数，指定要选中\n * \t\teven 表示偶数位置的子元素\n * \t\todd 表示奇数位置的子元素\n * \t\t\n */\np:nth-child(odd){\n\tbackground-color: yellow;\n}\n\t\t\t\n/*\n * :first-of-type\n * :last-of-type\n * :nth-of-type\n * \t和:first-child这些非常的类似，\n * \t只不过child，是在所有的子元素中排列\n * \t而type，是在当前类型的子元素中排列\n */\np:first-of-type{\n\tbackground-color: yellow;\n}\np:last-of-type{\n\tbackground-color: yellow;\n}\n```\n\n### 兄弟元素选择器\n\n```css\n/*\n * 为span后的一个p元素设置一个背景颜色为黄色\n * 后一个兄弟元素选择器\n * \t作用：可以选中一个元素后紧挨着的指定的兄弟元素\n * \t语法：前一个 + 后一个\n * \n */\nspan + p{\n\tbackground-color: yellow;\n}\n\t\t\t\n/*\n * 选中后边的所有兄弟元素\n * \t语法：前一个 ~ 后边所有\t\n */\nspan ~ p{\n\tbackground-color: yellow;\n}\n```\n\n### 否定伪类\n\n```css\n/*\n * 为所有的p元素设置一个背景颜色为黄色，除了class值为hello的\n * \n * 否定伪类：\n * \t作用：可以从已选中的元素中剔除出某些元素\n * \t语法：\n * \t\t:not(选择器)\n */\np:not(.hello){\n\tbackground-color: yellow;\n}\n\t\t\t\n```\n\n### 选择器的优先级\n\n当使用不同的选择器，选中同一个元素时并且设置相同的样式时，这时样式之间产生了冲突，最终到底采用哪个选择器定义的样式，由选择器的优先级（权重）决定, 优先级高的优先显示。\n\n- 优先级的规则\n  - 内联样式 1000\n  - id选择器 100\n  - 类和伪类 10\n  - 元素选择器 1\n  - \\* 0\n\n  继承的样式，没有优先级\n\n  当选择器中包含多种选择器时，需要将多种选择器的优先级相加然后在比较，但是注意，选择器优先级计算不会超过他的最大的数量级，如果选择器的优先级一样，则使用靠后的样式。\n\n   - 并集选择器的优先级是单独计算\n\n    div , p , \\#p1 , .hello{}  \n          * 可以在样式的最后，添加一个!important，则此时该样式将会获得一个最高的优先级，将会优先于所有的样式显示甚至超过内联样式，但是在开发中尽量避免使用!important\n\n# 盒子模型\n\n盒子可见框的大小由内容区，内边距和边框共同决定\n\n盒子模型：就是把 HTML 页面中的布局元素看作是一个矩形的盒子，也就是一个盛装内容的容器。 CSS 盒子模型本质上是一个盒子，封装周围的 HTML 元素，它包括：边框、外边距、内边距、和实际内容\n\n![image-20200213175806790](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/css/20210410172001.png)\n\n```css\n/*\n * 使用width来设置盒子内容区的宽度\n * 使用height来设置盒子内容区的高度\n * \n * width和height只是设置的盒子内容区的大小，而不是盒子的整个大小，\n * \t!!!盒子可见框的大小由内容区，内边距和边框共同决定\n */\nwidth: 300px;\nheight: 300px;\n\n```\n\n### 边框\n\n```css\n/*\n * 为元素设置边框\n * \t要为一个元素设置边框必须指定三个样式\n * \t\tborder-width:边框的宽度\n * \t\tborder-color:边框颜色\n * \t\tborder-style:边框的样式\n */\t\t\t\n/*\n * 设置边框的宽度\n */\nborder-width:10px ;\t\t\t\n/*\n使用border-width可以分别指定四个边框的宽度\n如果在border-width指定了四个值，\n则四个值会分别设置给 上 右 下 左，按照顺时针的方向设置的\n如果指定三个值，\n \t则三个值会分别设置给\t上  左右 下\n如果指定两个值，\n \t则两个值会分别设置给 上下 左右\t\n如果指定一个值，则四边全都是该值\t\n除了border-width，CSS中还提供了四个border-xxx-width\nxxx的值可能是top right bottom left专门用来设置指定边的宽度\t\n * */\nborder-width:10px 20px 30px 40px ;\nborder-width:10px 20px 30px ;\nborder-width: 10px 20px ;\nborder-width: 10px;\n\t\t\t\t\nborder-left-width:100px ;\t\t\t\n/*\n * 设置边框的颜色\n * 和宽度一样，color也提供四个方向的样式，可以分别指定颜色\n * border-xxx-color\n */\nborder-color: red;\nborder-color: red yellow orange blue;\nborder-color: red yellow orange;\nborder-color: red yellow;\n\t\t\t\t\n/*\n * 设置边框的样式\n * 可选值：\n * \t\tnone，默认值，没有边框\n * \t\tsolid 实线\n * \t\tdotted 点状边框\n * \t\tdashed 虚线\n * \t\tdouble 双线\n * \n * style也可以分别指定四个边的边框样式，规则和width一致，\n * \t同时它也提供border-xxx-style四个样式，来分别设置四个边\n */\nborder-style: double;\nborder-style: solid dotted dashed double; \n\n/*设置边框\n大部分的浏览器中，边框的宽度和颜色都是有默认值，而边框的样式默认值都是none\n * */\nborder-width:10px ;\nborder-color: red;\nborder-style: solid;\n\t\t\t\t\n/*\n * border\n * 边框的简写样式，通过它可以同时设置四个边框的样式，宽度，颜色\n * 而且没有任何的顺序要求\n * border一指定就是同时指定四个边不能分别指定\n * border-top border-right border-bottom border-left\n * 可以单独设置四个边的样式，规则和border一样，只不过它只对一个边生效\nborder : border-width || border-style || border-color \n */\nborder: red solid 10px   ;\nborder-left: red solid 10px   ;\t\t\t\nborder-top: red solid 10px;\nborder-bottom: red solid 10px;\nborder-left: red solid 10px;\t\t\t\nborder: red solid 10px;\nborder-right: none;\n\n/*border-collapse 属性控制浏览器绘制表格边框的方式。它控制相邻单元格的边框。\n语法：border-collapse:collapse; \ncollapse 单词是合并的意思\nborder-collapse: collapse; 表示相邻边框合并在一起\n\n边框会额外增加盒子的实际大小。因此我们有两种方案解决:\n1. 测量盒子大小的时候,不量边框.\n2. 如果测量的时候包含了边框,则需要 width/height 减去边框宽度*/\n```\n\n#### CSS3新增的关于边框\n\nborder-radius属性用于设置元素的外边框圆角。\n\n`border-radius:length; `\n\n- 参数值可以为数值或百分比的形式\n- 如果是正方形，想要设置为一个圆，把数值修改为高度或者宽度的一半即可，或者直接写为 50%\n- 该属性是一个简写属性，可以跟四个值，分别代表左上角、右上角、右下角、左下角\n- 分开写：border-top-left-radius、border-top-right-radius、border-bottom-right-radius 和 border-bottom-left-radius\n- 兼容性 ie9+ 浏览器支持, 但是不会影响页面布局,可以放心使用.\n\n`box-shadow: h-shadow v-shadow blur spread color inset; `\n\n![image-20200213181541903](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/css/20210410172002.png)\n\n- 默认的是外阴影(outset), 但是不可以写这个单词,否则造成阴影无效\n- 盒子阴影不占用空间，不会影响其他盒子排列。\n\n### 内边距（padding）\n\n```css\n/*\n * 内边距（padding），指的是盒子的内容区与盒子边框之间的距离\n * \t一共有四个方向的内边距，可以通过：\n * \t\tpadding-top\n * \t\tpadding-right\n * \t\tpadding-bottom\n * \t\tpadding-left\n * \t\t\t来设置四个方向的内边距\n * \n * 内边距会影响盒子的可见框的大小，元素的背景会延伸到内边距,\n * \t盒子的大小由内容区、内边距和边框共同决定\n * \t盒子可见框的宽度 = border-left-width + padding-left + width + padding-right + border-\n *  可见宽的高度 = border-top-width + padding-top + height + padding-bottom + border-bott\n\n当我们给盒子指定 padding 值之后，发生了 2 件事情：\n1. 内容和边框有了距离，添加了内边距。\n2. padding影响了盒子实际大小。\n也就是说，如果盒子已经有了宽度和高度，此时再指定内边框，会撑大盒子。\n解决方案：\n如果保证盒子跟效果图大小保持一致，则让 width/height 减去多出来的内边距大小即可。\n */\t\t\t\t\n/*设置上内边距*/\npadding-top: 100px;\n/*设置右内边距*/\npadding-right: 100px;\npadding-bottom: 100px;\npadding-left: 100px;\n\t\t\t\t\n/*\n * 使用padding可以同时设置四个边框的样式，规则和border-width一致\n */\npadding: 100px;\t\t\t\npadding: 100px 200px;\t\t\t\t\npadding: 100px 200px 300px;\t\t\npadding: 100px 200px 300px 400px;\n```\n\n### 外边距\n\n```css\n/*\n * 外边距指的是当前盒子与其他盒子之间的距离，\n * \t他不会影响可见框的大小，而是会影响到盒子的位置。\n * 盒子有四个方向的外边距：\n * \tmargin-top\n * \tmargin-right\n * \tmargin-bottom\n * \tmargin-left\n * 由于页面中的元素都是靠左靠上摆放的，\n * 所以注意当我们设置上和左外边距时，会导致盒子自身的位置发生改变，而如果是设置右和下外边距会改变其他盒子的位置\n */\n/*\n * 设置box1的上外边距，盒子上边框和其他的盒子的距离\n */\nmargin-top: 100px;\n\t\t\t\t\n/*\n * 左外边距\n */\nmargin-left: 100px;\n\t\t\t\t\n/*设置右和下外边距*/\nmargin-right: 100px;\nmargin-bottom: 100px;\n\t\t\t\t\n/*\n * 外边距也可以指定为一个负值，\n * \t如果外边距设置的是负值，则元素会向反方向移动\n */\nmargin-left: -150px;\nmargin-top: -100px;\nmargin-bottom: -100px;\nmargin-bottom: -100px;\n\t\t\t\t\n/*\n * margin还可以设置为auto，auto一般只设置给水平方向的margin\n * \t如果只指定，左外边距或右外边距的margin为auto则会将外边距设置为最大值\n * \t垂直方向外边距如果设置为auto，则外边距默认就是0\n * 如果将left和right同时设置为auto，则会将两侧的外边距设置为相同的值，\n * \t就可以使元素自动在父元素中居中，所以我们经常将左右外边距设置为auto\n * \t以使子元素在父元素中水平居中\n */\nmargin-left: auto;\nmargin-right: auto;\n\t\t\t\t\n/*\n * 外边距同样可以使用简写属性 margin，可以同时设置四个方向的外边距,\n * \t规则和padding一样\n */\nmargin: 0 auto;\n```\n\n#### 垂直外边距的重叠\n\n简单来说就是： 相邻块元素垂直外边距的合并\n\n- 当上下相邻的两个块元素（兄弟关系）相遇时，如果上面的元素有下外边距 **margin-bottom**，下面的元素有上外边距 **margin-top**，则他们之间的垂直间距不是 margin-bottom 与 margin-top 之和,取两个值中的**较大者**。这种现象被称为相邻块元素垂直外边距的合并。\n\n  ![image-20200213181053165](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/css/20210410172003.png)\n\n  解决方案： 尽量只给一个盒子添加 margin 值\n\n- 嵌套块元素垂直外边距的塌陷\n\n  对于两个嵌套关系（父子关系）的块元素，父元素有上外边距同时子元素也有上外边距，此时父元素会塌陷较大的**外边距值**。\n\n  ![image-20200213181039478](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/css/20210410172004.png)\n\n  解决方案：\n\n  - 可以为父元素定义上边框。\n  - 以为父元素定义上内边距。\n  - 可以为父元素添加 overflow:hidden\n  - 还有其他方法，比如浮动、固定，绝对定位的盒子不会有塌陷问题\n\n### 内联元素的盒子\n\n```css\n/*\n \t内容区、内边距 、边框 、外边距\n * */\t\t\t\n/*\n * 内联元素不能设置width和height\n */\nwidth: 200px;\nheight: 200px;\t\t\t\n/*\n * 设置水平内边距,内联元素可以设置水平方向的内边距\n */\npadding-left: 100px ;\npadding-right: 100px ;\t\t\t\t\n/*\n * 垂直方向内边距，内联元素可以设置垂直方向内边距，但是不会影响页面的布局\n */\npadding-top: 50px;\npadding-bottom: 50px;\n\t\t\t\t\n/*\n * 为元素设置边框,\n * \t内联元素可以设置边框，但是垂直的边框不会影响到页面的布局\n */\nborder: 1px blue solid;\n\t\t\t\t\n/*\n * 水平外边距\n * \t内联元素支持水平方向的外边距\n */\nmargin-left:100px ;\nmargin-right: 100px;\n\t\t\t\t\n/*\n * 内联元素不支持垂直外边距\n */\nmargin-top: 200px;\nmargin-bottom: 200px;\n\t\t\t\t\n/*\n * 设置一个左外边距\n * 水平方向的相邻外边距不会重叠，而是求和\n */\nmargin-left: 100px;\n```\n\n# display\u0026visibility\u0026opacity\n\n```css\n/*\n * 将一个内联元素变成块元素，\n * \t通过display样式可以修改元素的类型\n * \t可选值：\n * \t\tinline：可以将一个元素作为内联元素显示\n * \t\tblock: 可以将一个元素设置块元素显示\n * \t\tinline-block：将一个元素转换为行内块元素\n * \t\t- 可以使一个元素既有行内元素的特点又有块元素的特点\n * \t\t\t既可以设置宽高，又不会独占一行\n * \t\tnone: 不显示元素，并且元素不会在页面中继续占有位置\n */\ndisplay: none;\n```\n\n```css\n/*\n * visibility\n * \t- 可以用来设置元素的隐藏和显示的状态\n * \t- 可选值：\n * \t\tvisible 默认值，元素默认会在页面显示\n * \t\thidden 元素会隐藏不显示\t\n * 使用 visibility:hidden;隐藏的元素虽然不会在页面中显示，但是它的位置会依然保持\n */\nvisibility:hidden ;\n```\n\n```css\n/*\n * 设置元素的透明背景\n * opacity可以用来设置元素背景的透明，\n * \t它需要一个0-1之间的值\n * \t\t0 表示完全透明\n * \t\t1 表示完全不透明\n * \t\t0.5 表示半透明\n */\nopacity: 0.5;\n\t\t\t\t\n/*\n * opacity属性在IE8及以下的浏览器中不支持\n * IE8及以下的浏览器需要使用如下属性代替\n * \talpha(opacity=透明度)\n * 透明度，需要一个0-100之间的值\n * \t0 表示完全透明\n * \t100 表示完全不透明\n * \t50 半透明\n * 这种方式支持IE6，但是这种效果在IE Tester中无法测试\n */\nfilter: alpha(opacity=50);\n```\n\n# overflow\n\n子元素默认是存在于父元素的内容区中,理论上讲子元素的最大可以等于父元素内容区大小,如果子元素的大小超过了父元素的内容区，则**超过的大小会在父元素以外的位置显示**。\n\n超出父元素的内容，我们称为溢出的内容，父元素默认是将溢出内容，在父元素外边显示，通过overflow可以设置父元素如何处理溢出内容：\n\n - `visible` 默认值，不会对溢出内容做处理，元素会在父元素以外的位置显示\n - `hidden` 溢出的内容，会被修剪，不会显示\n - `scroll` 会为父元素添加滚动条，通过拖动滚动条来查看完整内容,该属性不论内容是否溢出，都会添加水平和垂直双方向的滚动条\n - `auto` 会根据需求自动添加滚动条，需要水平就添加水平，需要垂直就添加垂直，都不需要就都不加\n\n```css\noverflow: auto;\n```\n\n# 文档流\n\n文档流处在网页的最底层，它表示的是一个页面中的位置，我们所**创建的元素默认都处在文档流中**。元素在文档流中的特点:\n\n1. 块元素\n   - 块元素在文档流中会**独占一行**，块元素会自上向下排列\n   - 块元素在文档流中**默认宽度是父元素的100%**\n   - 块元素在文档流中的**高度默认被内容撑开**\n2. 内联元素\n   - 内联元素在文档流中只占自身的大小，会**默认从左向右排列**，如果一行中不足以容纳所有的内联元素，则换到下一行，继续自左向右。\n   - 在文档流中，内联元素的**宽度和高度默认都被内容撑开**\n\n\u003e 当元素的宽度的值为auto时，此时指定内边距padding不会影响可见框的大小，而是会自动修改宽度，以适应内边距。\n\n```html\n\u003cdiv style=\"background-color: #bfa;\"\u003e\n\t\u003cdiv style=\"height: 50px;\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv style=\"width: 100px; height: 100px; background-color: #ff0;\"\u003e\u003c/div\u003e\t\n\u003cspan style=\"background-color: yellowgreen;\"\u003e我是一个span\u003c/span\u003e\n\u003cspan style=\"background-color: yellowgreen;\"\u003e我是一个span\u003c/span\u003e\n\u003cspan style=\"background-color: yellowgreen;\"\u003e我是一个span\u003c/span\u003e\n\u003cspan style=\"background-color: yellowgreen;\"\u003e我是一个span\u003c/span\u003e\n\u003cspan style=\"background-color: yellowgreen;\"\u003e我是一个span\u003c/span\u003e\n\u003cspan style=\"background-color: yellowgreen;\"\u003e我是一个span\u003c/span\u003e\n```\n\n## 定位\n\n定位指的就是将指定的元素摆放到页面的任意位置，通过定位可以任意的摆放元素。\n\n当**开启了元素的定位（position属性值是一个非static的值）**时，可以通过一下四个属性来设置元素的偏移量：\n\n- left：元素相对于其定位位置的左侧偏移量\n- right：元素相对于其定位位置的右侧偏移量\n- top：元素相对于其定位位置的上边的偏移量\n- bottom：元素相对于其定位位置下边的偏移量\n\n\u003e 通常偏移量只需要使用两个就可以对一个元素进行定位，一般选择水平方向的一个偏移量和垂直方向的偏移量来为一个元素进行定位\n\n通过position属性来设置元素的定位\n\n```css\nposition:\n\t/*static：默认值，元素没有开启定位\n\trelative：开启元素的相对定位\n\tabsolute：开启元素的绝对定位\n\tfixed：开启元素的固定定位（也是绝对定位的一种）*/\n```\n\n### 1 相对定位\n\n当元素的position属性设置为relative时，则开启了元素的相对定位:\n\n1. 当开启了元素的相对定位以后，而不设置偏移量时，元素不会发生任何变化\n2. 相对定位是**相对于元素在文档流中原来的位置**进行定位\n3. 相对定位的元素**不会脱离文档流**\n4. 相对定位会使元素**提升一个层级**\n5. 相对定位不会改变元素的性质，**块还是块，内联还是内联**\n\n### 2 绝对定位\n\n当position属性值设置为absolute时，则开启了元素的绝对定位:\n\n1. 开启绝对定位，会使**元素脱离文档流**\n2. 开启绝对定位以后，如果不设置偏移量，则元素的位置不会发生变化\n3. 绝对定位是**相对于离他最近的开启了定位的祖先元素进行定位**的（一般情况，开启了子元素的绝对定位都如果所有的祖先元素都没有开启定位，则会相对于**浏览器窗口**进行定位）\n4. 绝对定位会使**元素提升一个层级**\n5. 绝对定位会改变元素的性质，**内联元素变成块元素，块元素的宽度和高度默认都被内容撑开**。\n\n### 3 固定定位\n\n当元素的position属性设置fixed时，则开启了元素的固定定位:\n\n\u003e 固定定位也是一种绝对定位，它的大部分特点都和绝对定位一样，不同的是：  \n\u003e **固定定位永远都会相对于浏览器窗口进行定位**  \n\u003e **固定定位会固定在浏览器窗口某个位置，不会随滚动条滚动**  \n\u003e IE6不支持固定定位\n\n### 4 层级\n\n如果定位元素的层级是一样，则下边的元素会盖住上边的,可以为z-index指定一个正整数作为值，该值将会作为当前元素的层级,层级越高，越优先显示。\n\n\u003e 对于没有开启定位的元素不能使用z-index\n\n```css\nz-index: 25;\t\n```\n\n## 浮动\n\n网页布局的本质——用 CSS 来摆放盒子。 把盒子摆放到相应位置.\n\nCSS 提供了三种传统布局方式(简单说,就是盒子如何进行排列顺序)：普通流（标准流）浮动 定位\n\n为什么需要浮动？\n\n![image-20200213182032983](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/css/20210410172005.png)\n\n虽然转换为行内块元素可以实现一行显示，但是他们之间会有大的空白缝隙，很难控制。\n\n有很多的布局效果，标准流没有办法完成，此时就可以利用浮动完成布局。 因为浮动可以改变元素标 签默认的排列方式. 浮动最典型的应用：**可以让多个块级元素一行内排列显示**。 网页布局第一准则：多个块级元素纵向排列找标准流，多个块级元素横向排列找浮动。\n\n浮动特性:\n\n1. 浮动元素会**脱离标准流**(脱标)\n2. 浮动的元素会一行内显示并且元素顶部对齐\n3. 浮动的元素会具有**行内块元素**的特性.\n\n设置了浮动（float）的元素最重要特性：\n\n1. 脱离标准普通流的控制（浮） 移动到指定位置（动）, （俗称脱标）\n2. 浮动的盒子不再保留原先的位置\n\n![image-20200213182413775](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/css/20210410172006.png)\n\n![image-20200213182359241](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/css/20210410172007.png)\n\n**任何元素都可以浮动。不管原先是什么模式的元素，添加浮动之后具有行内块元素相似的特性**。\n\n- 如果块级盒子没有设置宽度，默认宽度和父级一样宽，但是添加浮动后，它的大小根据内容来决定\n- 浮动的盒子中间是没有缝隙的，是紧挨着一起的\n- 行内元素同理\n\n```css\n/*\n * 块元素在文档流中默认垂直排列，所以这个三个div自上至下依次排开，\n * 如果希望块元素在页面中水平排列，可以使块元素脱离文档流\n * 使用float来使元素浮动，从而脱离文档流\n * 可选值：\n * \tnone，默认值，元素默认在文档流中排列\n * \tleft，元素会立即脱离文档流，向页面的左侧浮动\n * \tright，元素会立即脱离文档流，向页面的右侧浮动\n * 当为一个元素设置浮动以后（float属性是一个非none的值），\n */\nfloat: left;\n```\n\n浮动的元素不会盖住文字，文字会自动环绕在浮动元素的周围，所以我们可以通过浮动来设置文字环绕图片的效果。\n\n内联元素脱离文档流以后会变成块元素。\n\n### 清除浮动、高度塌陷\n\n由于父级盒子很多情况下，不方便给高度，但是子盒子浮动又不占有位置，最后父级盒子高度为 0 时，就会 影响下面的标准流盒子。\n\n![image-20200213182620335](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/css/20210410172008.png)\n\n由于浮动元素不再占用原文档流的位置，所以它会对后面的元素排版产生影响。\n\n清除浮动的本质\n\n- 是清除浮动元素造成的影响\n- 如果父盒子本身有高度，则不需要清除浮动\n- 清除浮动之后，父级就会根据浮动的子盒子自动检测高度。父级有了高度，就不会影响下面的标准流了\n\n选择器{clear:属性值;}\n\n![image-20200213182845069](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/css/20210410172009.png)\n\n我们实际工作中， 几乎只用 clear: both; 清除浮动的策略是: 闭合浮动.\n\n### 其他方法\n\n在文档流中，父元素的高度默认是被子元素撑开的，也就是子元素多高，父元素就多高。**但是当为子元素设置浮动以后，子元素会完全脱离文档流，此时将会导致子元素无法撑起父元素的高度，导致父元素的高度塌陷。由于父元素的高度塌陷了，则父元素下的所有元素都会向上移动，这样将会导致页面布局混乱。**  \n所以在开发中一定要避免出现高度塌陷的问题,我们可以将父元素的高度写死，以避免塌陷的问题出现，但是一旦高度写死，父元素的高度将不能自动适应子元素的高度，所以这种方案是不推荐使用的。\n\n根据W3C的标准，在页面中元素都一个隐含的属性叫做Block Formatting Context,简称BFC，该属性可以设置打开或者关闭，默认是关闭的。\n\n当开启元素的BFC以后，元素将会具有如下的特性：\n\n1. 父元素的垂直外边距不会和子元素重叠\n2. 开启BFC的元素不会被浮动元素所覆盖\n3. 开启BFC的元素可以包含浮动的子元素\n\n如何开启元素的BFC:\n\n1. 设置元素浮动使用这种方式开启，虽然可以撑开父元素，但是会导致父元素的宽度丢失而且使用这种方式也会导致下边的元素上移，不能解决问题\n2. 设置元素绝对定位\n3. 设置元素为inline-block 可以解决问题，但是会导致宽度丢失，不推荐使用这种方式\n4. 将元素的overflow设置为一个非visible的值\n\n   \u003e 将overflow设置为hidden是副作用最小的开启BFC的方式。\n\n但是在IE6及以下的浏览器中并不支持BFC，所以使用这种方式不能兼容IE6。在IE6中虽然没有BFC，但是具有另一个隐含的属性叫做hasLayout，该属性的作用和BFC类似，所在IE6浏览器可以通过开hasLayout来解决该问题开启方式很多，我们直接使用一种副作用最小的：直接将元素的zoom设置为1即可。\n\nzoom表示放大的意思，后边跟着一个数值，写几就将元素放大几倍。zoom:1表示不放大元素，但是通过该样式可以开启hasLayout。zoom这个样式，只在IE中支持，其他浏览器都不支持。\n\n```css\nzoom:1;\noverflow: hidden;\n```\n\n- 最终办法\n\n```css\n/*通过after伪类，选中box1的后边*/\n/*\n * 可以通过after伪类向元素的最后添加一个空白的块元素，然后对其清除浮动，\n * \t这样做和添加一个div的原理一样，可以达到一个相同的效果，\n * \t而且不会在页面中添加多余的div，这是我们最推荐使用的方式，几乎没有副作用\n */\n.clearfix:after{\n\t/*添加一个内容*/\n\tcontent: \"\";\n\t/*转换为一个块元素*/\n\tdisplay: block;\n\t/*清除两侧的浮动*/\n\tclear: both;\n}\n\t\t\t\n/*\n * 在IE6中不支持after伪类,\n * \t所以在IE6中还需要使用hasLayout来处理\n */\n.clearfix{\n\tzoom:1;\n}\n\n/*\n * 经过修改后的clearfix是一个多功能的\n * \t既可以解决高度塌陷，又可以确保父元素和子元素的垂直外边距不会重叠\n */\n.clearfix:before,\n.clearfix:after{\n\tcontent: \"\";\n\tdisplay: table;\n\tclear: both;\n}\t\t\n.clearfix{\n\tzoom: 1;\n}\nstyle\u003e\n```\n\n# 框架集\n\n```html\n\u003c!-- \n\t框架集和内联框架的作用类似，都是用于在一个页面中引入其他的外部的页面，\n\t\t框架集可以同时引入多个页面，而内联框架只能引入一个，\n\t\t在h5标准中，推荐使用框架集，而不使用内联框架\n\t使用frameset来创建一个框架集，注意frameset不能和body出现在同一个页面中\n\t\t所以要使用框架集，页面中就不可以使用body标签\n\t属性：\n\t\trows，指定框架集中的所有的框架，一行一行的排列\n\t\tcols， 指定框架集中的所有的页面，一列一列的排列\t\n\t\t这两个属性frameset必须选择一个，并且需要在属性中指定每一部分所占的大小\n\t\tframeset中也可以再嵌套frameset\n\tframeset和iframe一样，它里边的内容都不会被搜索引擎所检索，\n\t所以如果搜索引擎检索到的页面是一个框架页的话，它是不能去判断里边的内容的\n\t使用框架集则意味着页面中不能有自己的内容，只能引入其他的页面，而我们每单独加载一个页面\n\t浏览器都需要重新发送一次请求，引入几个页面就需要发送几次请求，用户的体验比较差\n\t如果非得用建议使用frameset而不使用iframe\t\n--\u003e\n\u003cframeset cols=\"30% , * , 30%\"\u003e\n\t\u003c!-- 在frameset中使用frame子标签来指定要引入的页面 \n\t\t引入几个页面就写几个frame\n\t--\u003e\t\n\t\u003cframe src=\"01.表格.html\" /\u003e\n\t\u003cframe src=\"02.表格.html\" /\u003e\n\t\u003c!-- 嵌套一个frameset --\u003e\n\t\u003cframeset rows=\"30%,50%,*\"\u003e\n\t\t\u003cframe src=\"04.表格的布局.html\" /\u003e\n\t\t\u003cframe src=\"05.完善clearfix.html\" /\u003e\n\t\t\u003cframe src=\"06.表单.html\" /\u003e\n\t\u003c/frameset\u003e\n\u003c/frameset\u003e\n```\n\n# background\n\n```css\n/*\n * 使用background-image来设置背景图片\n * \t- 语法：background-image:url(相对路径);\n * \n * \t- 如果背景图片大于元素，默认会显示图片的左上角\n * \t- 如果背景图片和元素一样大，则会将背景图片全部显示\n * \t- 如果背景图片小于元素大小，则会默认将背景图片平铺以充满元素\n * \n * 可以同时为一个元素指定背景颜色和背景图片，\n * \t这样背景颜色将会作为背景图片的底色\n * \t一般情况下设置背景图片时都会同时指定一个背景颜色\n */\nbackground-image:url(img/1.png);\t\t\n/*\n * background-repeat用于设置背景图片的重复方式\n * \t可选值：\n * \t\trepeat，默认值，背景图片会双方向重复（平铺）\n * \t\tno-repeat ，背景图片不会重复，有多大就显示多大\n * \t\trepeat-x， 背景图片沿水平方向重复\n * \t\trepeat-y，背景图片沿垂直方向重复\n */\nbackground-repeat: repeat-y;\n/*background-size \n *设置背景图片大小。图片可以保有其原有的尺寸，或者拉伸到新的尺寸，  \n *或者在保持其原有比例的同时缩放到元素的可用空间的尺寸。\n *控制显示范围，可以解决图片太大只显示左上角\n */\nbackground-size: cover\nbackground-size: contain\n/* 一个值：这个值指定图片的宽度，图片的高度隐式的为 auto */\nbackground-size: 50%\nbackground-size: 3em\nbackground-size: 12px\nbackground-size: auto\n/* 第一个值指定图片的宽度，第二个值指定图片的高度 */\nbackground-size: 50% auto\nbackground-size: 3em 25%\nbackground-size: auto 6px\nbackground-size: auto auto\n/* 全局属性 */\nbackground-size: inherit;\nbackground-size: initial;\nbackground-size: unset;\n\n/*\n * 设置一个背景颜色\n */\nbackground-color: #bfa;\n/*\n * 背景图片默认是贴着元素的左上角显示\n * 通过background-position可以调整背景图片在元素中的位置\n * 可选值：\n * \t\t该属性可以使用 top right left bottom center中的两个值\n * \t\t\t来指定一个背景图片的位置\n * \t\t\ttop left 左上\n * \t\t\tbottom right 右下\n * \t\t\t如果只给出一个值，则第二个值默认是center\n * \n * \t\t也可以直接指定两个偏移量，\n * \t\t\t第一个值是水平偏移量\n * \t\t\t\t- 如果指定的是一个正值，则图片会向右移动指定的像素\n * \t\t\t\t- 如果指定的是一个负值，则图片会向左移动指定的像素\n * \t\t\t第二个是垂直偏移量\t\n * \t\t\t\t- 如果指定的是一个正值，则图片会向下移动指定的像素\n * \t\t\t\t- 如果指定的是一个负值，则图片会向上移动指定的像素\n * \t\t\n */\nbackground-position: -80px -40px;\n\n/*\n * 当背景图片的background-attachment设置为fixed时，\n * \t背景图片的定位永远相对于浏览器的窗口\n */\nbackground-attachment: fixed;\n\n/*\n * background\n * \t- 通过该属性可以同时设置所有背景相关的样式\n * \t- 没有顺序的要求，谁在前睡在后都行\n * \t\t也没有数量的要求，不写的样式就使用默认值\n */\nbackground: #bfa url(img/3.png) center center no-repeat fixed;\n```\n\n# 布局\n\n## 表格布局\n\n表格在日常生活中使用的非常的多，比如excel就是专门用来创建表格的工具，表格就是用来表示一些格式化的数据的，比如：课程表、银行对账单。在网页中也可以来创建出不同的表格。\n\n```html\n\u003c!--\n\t在HTML中，使用table标签来创建一个表格\n--\u003e\n\u003ctable border=\"1\" width=\"40%\" align=\"center\"\u003e\n\u003c!-- \n\t在table标签中使用tr来表示表格中的一行，有几行就有几个tr\n--\u003e\n\u003ctr\u003e\n\t\u003c!-- 在tr中需要使用td来创建一个单元格，有几个单元格就有几个td --\u003e\n\t\u003ctd\u003eA1\u003c/td\u003e\n\t\u003ctd\u003eA2\u003c/td\u003e\n\t\u003ctd\u003eA3\u003c/td\u003e\n\t\u003ctd\u003eA4\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\t\u003c!--\n\t\t可以使用th标签来表示表头中的内容，\n\t\t\t它的用法和td一样，不同的是它会有一些默认效果\n\t--\u003e\n\t\u003cth\u003e学号\u003c/th\u003e\n\t\u003cth\u003e姓名\u003c/th\u003e\n\t\u003cth\u003e性别\u003c/th\u003e\n\t\u003cth\u003e住址\u003c/th\u003e\n\u003c/tr\u003e\t\t\t\n\u003ctr\u003e\n\t\u003ctd\u003eB1\u003c/td\u003e\n\t\u003ctd\u003eB2\u003c/td\u003e\n    \u003ctd\u003eB3\u003c/td\u003e\n\t\u003c!-- \n\t\trowspan用来设置纵向的合并单元格\n\t--\u003e\n\t\u003ctd rowspan=\"2\"\u003eB4\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\t\u003ctd\u003eC1\u003c/td\u003e\n\t\u003ctd\u003eC2\u003c/td\u003e\n\t\u003ctd\u003eC3\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\t\u003ctd\u003eD1\u003c/td\u003e\n\t\u003ctd\u003eD2\u003c/td\u003e\n\t\u003c!-- \n\t\tcolspan横向的合并单元格\n\t--\u003e\n\t\u003ctd colspan=\"2\"\u003eD3\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/table\u003e\n```\n\n```css\n/*\n * 设置表格的宽度\n */\ntable{\n\twidth: 300px;\n\t/*居中*/\n\tmargin: 0 auto;\n\t/*边框*/\n\t/*border:1px solid black;*/\n\t/*\n\t * table和td边框之间默认有一个距离\n\t * \t通过border-spacing属性可以设置这个距离\n\t */\n\t/*border-spacing:0px ;*/\n\t\n\t/*\n\t * border-collapse可以用来设置表格的边框合并\n\t * 如果设置了边框合并，则border-spacing自动失效\n\t */\n\tborder-collapse: collapse;\n\t/*设置背景样式*/\n\t/*background-color: #bfa;*/\n}\n\t\t\t\n/*\n * 设置边框\n */\ntd , th{\n\tborder: 1px solid black;\n}\n\t\t\t\n/*\n * 设置隔行变色\n */\ntr:nth-child(even){\n\tbackground-color: #bfa;\n}\n\t\t\t\n/*\n * 鼠标移入到tr以后，改变颜色\n */\ntr:hover{\n\tbackground-color: #ff0;\n}\n```\n\n长表格\n\n```html\n\u003ctable\u003e\n\t\u003c!-- \n\t\t有一些情况下表格是非常的长的，\n\t\t\t这时就需要将表格分为三个部分，表\n\t\t在HTML中为我们提供了三个标签：\n\t\t\tthead 表头\n\t\t\ttbody 表格主体\n\t\t\ttfoot 表格底部\n\t\t\t\n\t\t这三个标签的作用，就来区分表格的不\n\t\t\t都需要直接写到table中，tr需要写\n\t\t\t\n\t\tthead中的内容，永远会显示在表格的\n\t\ttfoot中的内容，永远都会显示表格的\n\t\ttbody中的内容，永远都会显示表格的\n\t\t\n\t\t如果表格中没有写tbody，浏览器会自\n\t\t并且将所有的tr都放到tbody中，所以\n\t\t通过table \u003e tr 无法选中行 需要通过\n\t--\u003e\n\t\u003cthead\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003cth\u003e日期\u003c/th\u003e\n\t\t\t\u003cth\u003e收入\u003c/th\u003e\n\t\t\t\u003cth\u003e支出\u003c/th\u003e\n\t\t\t\u003cth\u003e合计\u003c/th\u003e\n\t\t\u003c/tr\u003e\n\t\u003c/thead\u003e\n\t\n\t\u003ctfoot\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e\u003c/td\u003e\n\t\t\t\u003ctd\u003e合计\u003c/td\u003e\n\t\t\t\u003ctd\u003e100\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\u003c/tfoot\u003e\n\t\n\t\u003ctbody\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e10.24\u003c/td\u003e\n\t\t\t\u003ctd\u003e500\u003c/td\u003e\n\t\t\t\u003ctd\u003e300\u003c/td\u003e\n\t\t\t\u003ctd\u003e200\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\u003ctr\u003e\n\t\t\t\u003ctd\u003e10.24\u003c/td\u003e\n\t\t\t\u003ctd\u003e500\u003c/td\u003e\n\t\t\t\u003ctd\u003e300\u003c/td\u003e\n\t\t\t\u003ctd\u003e200\u003c/td\u003e\n\t\t\u003c/tr\u003e\n\t\t\n\t\u003c/tbody\u003e\t\n\u003c/table\u003e\n```\n\n**表格布局**\n\n```html\n\u003c!-- \n\t以前表格更多的情况实际上是用来对页面进行布局的，但是这种方式早已被CSS所淘汰了\n\t表格的列数由td最多的那行决定\n\t表格是可以嵌套，可以在td中在放置一个表格\n--\u003e\n\t\t\n\u003ctable border=\"1\" width=\"100%\"\u003e\n\t\u003ctr height=\"100px\"\u003e\n\t\t\u003ctd colspan=\"2\"\u003e\u003c/td\u003e\n\t\u003c/tr\u003e\n\t\u003ctr height=\"400px\"\u003e\n\t\t\u003ctd width=\"20%\"\u003e\u003c/td\u003e\n\t\t\u003ctd width=\"80%\"\u003e\n\t\t\t\u003ctable border=\"1\" width=\"100%\" height=\"100%\"\u003e\n\t\t\t\t\u003ctr\u003e\n\t\t\t\t\t\u003ctd\u003e\u003c/td\u003e\n\t\t\t\t\u003c/tr\u003e\n\t\t\t\t\u003ctr\u003e\n\t\t\t\t\t\u003ctd\u003e\u003c/td\u003e\n\t\t\t\t\u003c/tr\u003e\n\t\t\t\u003c/table\u003e\n\t\t\u003c/td\u003e\n\t\u003c/tr\u003e\n\t\u003ctr height=\"100px\"\u003e\n\t\t\u003ctd colspan=\"2\"\u003e\u003c/td\u003e\n\t\u003c/tr\u003e\n\u003c/table\u003e\n```\n\n## Flex布局\n\nW3C 在 2009 年提出了弹性盒，截止目前浏览器对 `FlexBox` 的支持已经相对完善，下面是 Can I use FlexBox 完整的兼容性情况，\n\n![image-20210503152739126](image-20210503152739126.png)\n\n关于弹性盒模型推荐阅读这篇文章[菜鸟](https://www.runoob.com/w3cnote/flex-grammar.html)。\n\n假设你已经阅读完并了解了弹性盒模型，响应式布局中我们需要关注 `FlexBox` 里的两个角色：**容器和子元素**。\n\n### 容器属性\n\n指定 `display` 属性为 `flex`，就可以将一个元素设置为 `FlexBox` 容器，**注意：设为Flex布局以后，子元素的float、clear和vertical-align属性将失效。**\n\n![image-20210503153109647](image-20210503153109647.png)\n\n我们可以通过定义它的属性，决定子元素的排列方式，属性可选值有 6 种，\n\n- flex-direction，主轴方向，也就是子元素排列的方向\n\n  ` flex-direction: row | row-reverse | column | column-reverse`\n\n  \u003cimg src=\"../../pics/image-20210503153130019.png\" alt=\"image-20210503153130019\" style=\"zoom:33%;\" /\u003e\n\n- flex-wrap，子元素能否换行展示及换行方式\n\n  `flex-wrap: nowrap | wrap | wrap-reverse`\n\n  \u003cimg src=\"../../pics/image-20210503153155215.png\" alt=\"image-20210503153155215\" style=\"zoom: 50%;\" /\u003e\n\n- flex-flow，flex-direction 和 flex-wrap 的简写形式\n\n  `flex-flow: \u003cflex-direction\u003e \u003cflex-wrap\u003e`\n\n- justify-content，子元素在主轴上的对齐方式\n\n  `justify-content: flex-start | flex-end | center | space-between | space-around`\n\n  \u003cimg src=\"../../pics/image-20210503153250995.png\" alt=\"image-20210503153250995\" style=\"zoom:50%;\" /\u003e\n\n- align-items，子元素在垂直于主轴的交叉轴上的排列方式\n\n  `align-items: flex-start | flex-end | center | baseline | stretch`\n\n  \u003cimg src=\"../../pics/image-20210503153313494.png\" alt=\"image-20210503153313494\" style=\"zoom:50%;\" /\u003e\n\n- align-content，子元素在多条轴线上的对齐方式\n\n  `align-content: flex-start | flex-end | center | space-between | space-around | stretch`\n\n  \u003cimg src=\"../../pics/image-20210503153333359.png\" alt=\"image-20210503153333359\" style=\"zoom:50%;\" /\u003e\n\n### 元素属性\n\n子元素也支持 6 个属性可选值，\n\n- order，子元素在主轴上的排列顺序\n\n  `order: \u003cinteger\u003e`\n\n  \u003cimg src=\"../../pics/image-20210503153458923.png\" alt=\"image-20210503153458923\" style=\"zoom:50%;\" /\u003e\n\n- flex-grow，子元素的放大比例，默认 0\n\n  `flex-grow: \u003cnumber\u003e; /* default 0 */`\n\n  \u003cimg src=\"../../pics/image-20210503153518220.png\" alt=\"image-20210503153518220\" style=\"zoom:50%;\" /\u003e\n\n- flex-shrink，子元素的缩小比例，默认 1\n\n  ` flex-shrink: \u003cnumber\u003e; /* default 1 */`\n\n  ![image-20210503153530647](image-20210503153530647.png)\n\n- flex-basis，分配剩余空间时，子元素的默认大小，默认 auto\n\n  `flex-basis: \u003clength\u003e | auto; /* default auto */`\n\n  它可以设为跟width或height属性一样的值（比如350px），则项目将占据固定空间。\n\n- flex：flex-grow，flex-shrink，flex-basis 的简写\n\n  `flex: none | [ \u003c'flex-grow'\u003e \u003c'flex-shrink'\u003e? || \u003c'flex-basis'\u003e ]`\n\n  该属性有两个快捷值：auto (1 1 auto) 和 none (0 0 auto)。\n\n  建议优先使用这个属性，而不是单独写三个分离的属性，因为浏览器会推算相关值。\n\n- align-self，覆盖容器的 align-items 属性\n\n  `align-self: auto | flex-start | flex-end | center | baseline | stretch;`\n\n  默认值为auto，表示继承父元素的align-items属性，如果没有父元素，则等同于stretch。\n\n  \u003cimg src=\"../../pics/image-20210503153700612.png\" alt=\"image-20210503153700612\" style=\"zoom:50%;\" /\u003e\n\n弹性盒模型布局非常灵活，属性值也足够应对大部分复杂的场景，但 `FlexBox` 基于轴线，**只能解决一维场景下的布局**，作为补充，W3C 在后续提出了**网格布局**（CSS Grid Layout），网格将容器再度划分为 “行” 和 “列”，产生单元格，项目（子元素）可以在单元格内组合定位，所以网格可以看作二维布局。\n\n## Grid布局\n\n网格布局（Grid）是最强大的 CSS 布局方案,它将网页划分成一个个网格，可以任意组合不同的网格，做出各种各样的布局。以前，只能通过复杂的 CSS 框架达到的效果，现在浏览器内置了。\n\n![image-20210503153842819](image-20210503153842819.png)\n\nGrid 布局与 [Flex 布局](http://www.ruanyifeng.com/blog/2015/07/flex-grammar.html)有一定的相似性，都可以指定容器内部多个项目的位置。但是，它们也存在重大区别。\n\nFlex 布局是轴线布局，只能指定\"项目\"针对轴线的位置，可以看作是**一维布局**。Grid 布局则是将容器划分成\"行\"和\"列\"，产生单元格，然后指定\"项目所在\"的单元格，可以看作是**二维布局**。Grid 布局远比 Flex 布局强大。\n\n### 基本概念\n\n#### 容器和项目\n\n采用网格布局的区域，称为\"容器\"（container）。容器内部采用网格定位的子元素，称为\"项目\"（item）。\n\n\u003e ```html\n\u003e \u003cdiv\u003e\n\u003e   \u003cdiv\u003e\u003cp\u003e1\u003c/p\u003e\u003c/div\u003e\n\u003e   \u003cdiv\u003e\u003cp\u003e2\u003c/p\u003e\u003c/div\u003e\n\u003e   \u003cdiv\u003e\u003cp\u003e3\u003c/p\u003e\u003c/div\u003e\n\u003e \u003c/div\u003e\n\u003e ```\n\n上面代码中，最外层的`\u003cdiv\u003e`元素就是容器，内层的三个`\u003cdiv\u003e`元素就是项目。\n\n注意：**项目只能是容器的顶层子元素**，不包含项目的子元素，比如上面代码的`\u003cp\u003e`元素就不是项目。Grid 布局只对项目生效。\n\n#### 行和列\n\n容器里面的水平区域称为\"行\"（row），垂直区域称为\"列\"（column）。\n\n![image-20210503154753737](image-20210503154753737.png)\n\n#### 单元格\n\n行和列的交叉区域，称为\"单元格\"（cell）。\n\n正常情况下，`n`行和`m`列会产生`n x m`个单元格。比如，3行3列会产生9个单元格。\n\n#### 网格线\n\n划分网格的线，称为\"网格线\"（grid line）。水平网格线划分出行，垂直网格线划分出列。\n\n正常情况下，`n`行有`n + 1`根水平网格线，`m`列有`m + 1`根垂直网格线，比如三行就有四根水平网格线。\n\n![img](1_bg2019032503.png)\n\n\u003e 上图是一个 4 x 4 的网格，共有5根水平网格线和5根垂直网格线。\n\n### 容器属性\n\n#### `display:grid|inline-grid`\n\n\u003cimg src=\"../../pics/image-20210503155044825.png\" alt=\"image-20210503155044825\" style=\"zoom:50%;\" /\u003e\n\n\u003cimg src=\"../../pics/image-20210503155054868.png\" alt=\"image-20210503155054868\" style=\"zoom:50%;\" /\u003e\n\n设为网格布局以后，容器子元素（项目）的`float`、`display: inline-block`、`display: table-cell`、`vertical-align`和`column-*`等设置都将**失效**。\n\n#### `grid-template-columns ,grid-template-rows`\n\n容器指定了网格布局以后，接着就要划分行和列。`grid-template-columns`属性定义每一列的列宽，`grid-template-rows`属性定义每一行的行高。\n\n```css\n.container {\n  display: grid;\n  grid-template-columns: 100px 100px 100px;\n  /*指定了一个三行三列的网格，列宽和行高都是100px。*/\n  /*grid-template-columns: 33.33% 33.33% 33.33%;*/\n  grid-template-rows: 100px 100px 100px;\n}\n```\n\n##### repeat()\n\n```css\n.container {\n  display: grid;\n  grid-template-columns: repeat(3, 33.33%);\n  grid-template-rows: repeat(3, 33.33%);\n}\n```\n\n`repeat()`重复某种模式也是可以的:`grid-template-columns: repeat(2, 100px 20px 80px);`\n\n\u003cimg src=\"../../pics/image-20210503155445644.png\" alt=\"image-20210503155445644\" style=\"zoom: 67%;\" /\u003e\n\n##### auto-fill 关键字\n\n有时，单元格的大小是固定的，但是容器的大小不确定。如果希望每一行（或每一列）容纳尽可能多的单元格，这时可以使用`auto-fill`关键字表示自动填充。\n\n```css\n.container {\n  display: grid;\n  grid-template-columns: repeat(auto-fill, 100px);\n}\n```\n\n\u003cimg src=\"../../pics/image-20210503155551356.png\" alt=\"image-20210503155551356\" style=\"zoom:67%;\" /\u003e\n\n##### fr 关键字\n\n为了方便表示比例关系，网格布局提供了`fr`关键字（fraction 的缩写，意为\"片段\"）。如果两列的宽度分别为`1fr`和`2fr`，就表示后者是前者的两倍。\n\n```css\n.container {\n  display: grid;\n  grid-template-columns: 1fr 1fr;\n}\n```\n\n\u003cimg src=\"../../pics/image-20210503155643834.png\" alt=\"image-20210503155643834\" style=\"zoom:50%;\" /\u003e\n\n`fr`可以与绝对长度的单位结合使用，这时会非常方便。\n\n```css\n.container {\n  display: grid;\n  grid-template-columns: 150px 1fr 2fr;\n}\n```\n\n\u003cimg src=\"../../pics/image-20210503155723770.png\" alt=\"image-20210503155723770\" style=\"zoom:67%;\" /\u003e\n\n##### minmax()\n\n`minmax()`函数产生一个长度范围，表示长度就在这个范围之中。它接受两个参数，分别为最小值和最大值。\n\n`grid-template-columns: 1fr 1fr minmax(100px, 1fr);`\n\n上面代码中，`minmax(100px, 1fr)`表示列宽不小于`100px`，不大于`1fr`。\n\n##### auto 关键字\n\n`auto`关键字表示由浏览器自己决定长度。\n\n```css\ngrid-template-columns: 100px auto 100px;\n```\n\n上面代码中，第二列的宽度，基本上等于该列单元格的最大宽度，除非单元格内容设置了`min-width`，且这个值大于最大宽度。\n\n##### 网格线的名称\n\n`grid-template-columns`属性和`grid-template-rows`属性里面，还可以使用方括号，指定每一根网格线的名字，方便以后的引用。\n\n\u003e ```css\n\u003e .container {\n\u003e   display: grid;\n\u003e   grid-template-columns: [c1] 100px [c2] 100px [c3] auto [c4];\n\u003e   grid-template-rows: [r1] 100px [r2] 100px [r3] auto [r4];\n\u003e }\n\u003e ```\n\n上面代码指定网格布局为3行 x 3列，因此有4根垂直网格线和4根水平网格线。方括号里面依次是这八根线的名字。\n\n网格布局允许同一根线有多个名字，比如`[fifth-line row-5]`。\n\n**（7）布局实例**\n\n`grid-template-columns`属性对于网页布局非常有用。两栏式布局只需要一行代码。\n\n\u003e ```css\n\u003e .wrapper {\n\u003e   display: grid;\n\u003e   grid-template-columns: 70% 30%;\n\u003e }\n\u003e ```\n\n上面代码将左边栏设为70%，右边栏设为30%。\n\n传统的十二网格布局，写起来也很容易。\n\n\u003e ```css\n\u003e grid-template-columns: repeat(12, 1fr);\n\u003e ```\n\n#### `row-gap ， column-gap ， gap`\n\n`grid-row-gap`属性设置行与行的间隔（行间距），`grid-column-gap`属性设置列与列的间隔（列间距）。\n\n```css\n.container {\n  grid-row-gap: 20px;\n  grid-column-gap: 20px;\n}\n```\n\n\u003cimg src=\"../../pics/image-20210503161523893.png\" alt=\"image-20210503161523893\" style=\"zoom:50%;\" /\u003e\n\n`grid-gap`属性是`grid-column-gap`和`grid-row-gap`的合并简写形式，语法如下。\n\n`grid-gap: \u003cgrid-row-gap\u003e \u003cgrid-column-gap\u003e;`\n\n因此，上面一段 CSS 代码等同于下面的代码。\n\n\u003e ```css\n\u003e .container {\n\u003e   grid-gap: 20px 20px;\n\u003e }\n\u003e ```\n\n如果`grid-gap`省略了第二个值，浏览器认为第二个值等于第一个值。\n\n\u003e 根据最新标准，上面三个属性名的`grid-`前缀已经删除，`grid-column-gap`和`grid-row-gap`写成`column-gap`和`row-gap`，`grid-gap`写成`gap`。\n\n#### `grid-template-areas`\n\n网格布局允许指定\"区域\"（area），一个区域由单个或多个单元格组成。`grid-template-areas`属性用于定义区域。\n\n```css\n.container {\n  display: grid;\n  grid-template-columns: 100px 100px 100px;\n  grid-template-rows: 100px 100px 100px;\n  grid-template-areas: 'a b c'\n                       'd e f'\n                       'g h i';\n}\n```\n\n上面代码先划分出9个单元格，然后将其定名为`a`到`i`的九个区域，分别对应这九个单元格。\n\n多个单元格合并成一个区域的写法如下,将9个单元格分成`a`、`b`、`c`三个区域。\n\n```css\ngrid-template-areas: 'a a a'\n                     'b b b'\n                     'c c c';\n```\n\n下面是一个布局实例。\n\n```css\ngrid-template-areas: \"header header header\"\n                     \"main main sidebar\"\n                     \"footer footer footer\";\n```\n\n上面代码中，顶部是页眉区域`header`，底部是页脚区域`footer`，中间部分则为`main`和`sidebar`。**如果某些区域不需要利用，则使用\"点\"（`.`）表示。**\n\n```css\ngrid-template-areas: 'a . c'\n                     'd . f'\n                     'g . i';\n```\n\n上面代码中，中间一列为点，表示没有用到该单元格，或者该单元格不属于任何区域。\n\n\u003e 注意，区域的命名会影响到网格线。每个区域的起始网格线，会自动命名为`区域名-start`，终止网格线自动命名为`区域名-end`。\n\u003e\n\u003e 比如，区域名为`header`，则起始位置的水平网格线和垂直网格线叫做`header-start`，终止位置的水平网格线和垂直网格线叫做`header-end`。\n\n#### grid-auto-flow\n\n划分网格以后，容器的子元素会按照顺序，自动放置在每一个网格。默认的放置顺序是\"先行后列\"，即先填满第一行，再开始放入第二行，即下图数字的顺序。\n\n\u003cimg src=\"../../pics/image-20210503162017139.png\" alt=\"image-20210503162017139\" style=\"zoom:50%;\" /\u003e\n\n这个顺序由`grid-auto-flow`属性决定，默认值是`row`，即\"先行后列\"。也可以将它设成`column`，变成\"先列后行\"。\n\n```css\ngrid-auto-flow: column;\n```\n\n![image-20210503162044134](image-20210503162044134.png)\n\n`grid-auto-flow`属性除了设置成`row`和`column`，还可以设成`row dense`和`column dense`。这两个值主要用于，某些项目指定位置以后，剩下的项目怎么自动放置。\n\n\u003cimg src=\"../../pics/image-20210503162137112.png\" alt=\"image-20210503162137112\" style=\"zoom:50%;\" /\u003e\n\n上图中，1号项目后面的位置是空的，这是因为3号项目默认跟着2号项目，所以会排在2号项目后面。\n\n现在修改设置，设为`row dense`，表示\"先行后列\"，并且尽可能紧密填满，尽量不出现空格。\n\n```css\ngrid-auto-flow: row dense;\n```\n\n\u003cimg src=\"../../pics/image-20210503162157420.png\" alt=\"image-20210503162157420\" style=\"zoom:50%;\" /\u003e\n\n上图会先填满第一行，再填满第二行，所以3号项目就会紧跟在1号项目的后面。8号项目和9号项目就会排到第四行。\n\n如果将设置改为`column dense`，表示\"先列后行\"，并且尽量填满空格。\n\n```css\ngrid-auto-flow: column dense;\n```\n\n\u003cimg src=\"../../pics/image-20210503162215954.png\" alt=\"image-20210503162215954\" style=\"zoom:50%;\" /\u003e\n\n上图会先填满第一列，再填满第2列，所以3号项目在第一列，4号项目在第二列。8号项目和9号项目被挤到了第四列。\n\n#### `justify-items,align-items,place-items`\n\n`justify-items`属性设置单元格内容的水平位置（左中右），`align-items`属性设置单元格内容的垂直位置（上中下）。\n\n```css\n.container {\n  justify-items: start | end | center | stretch;\n  align-items: start | end | center | stretch;\n}\n```\n\n这两个属性的写法完全相同，都可以取下面这些值。\n\n- start：对齐单元格的起始边缘。\n- end：对齐单元格的结束边缘。\n- center：单元格内部居中。\n- stretch：拉伸，占满单元格的整个宽度（默认值）。\n\n` justify-items: start;`\n\n\u003cimg src=\"../../pics/image-20210503162348517.png\" alt=\"image-20210503162348517\" style=\"zoom:50%;\" /\u003e\n\n`align-items: start;`\n\n\u003cimg src=\"../../pics/image-20210503162409442.png\" alt=\"image-20210503162409442\" style=\"zoom:50%;\" /\u003e\n\n`place-items`属性是`align-items`属性和`justify-items`属性的合并简写形式。\n\n\u003e ```css\n\u003e place-items: \u003calign-items\u003e \u003cjustify-items\u003e;\n\u003e ```\n\n下面是一个例子。\n\n\u003e ```css\n\u003e place-items: start end;\n\u003e ```\n\n如果省略第二个值，则浏览器认为与第一个值相等。\n\n#### `justify-content,align-content,place-content`\n\n`justify-content`属性是**整个内容区**域在容器里面的水平位置（左中右），`align-content`属性是整个内容区域的垂直位置（上中下）。\n\n\u003e ```css\n\u003e .container {\n\u003e   justify-content: start | end | center | stretch | space-around | space-between | space-evenly;\n\u003e   align-content: start | end | center | stretch | space-around | space-between | space-evenly; \n\u003e }\n\u003e ```\n\n这两个属性的写法完全相同，都可以取下面这些值。（下面的图都以`justify-content`属性为例，`align-content`属性的图完全一样，只是将水平方向改成垂直方向。）\n\n- start - 对齐容器的起始边框。\n- end - 对齐容器的结束边框。\n- center - 容器内部居中。\n\n  \u003cimg src=\"../../pics/image-20210503162825499.png\" alt=\"image-20210503162825499\" style=\"zoom:50%;\" /\u003e\n\n- stretch - 项目大小没有指定时，拉伸占据整个网格容器。\n\n  \u003cimg src=\"../../pics/image-20210503162813703.png\" alt=\"image-20210503162813703\" style=\"zoom:50%;\" /\u003e\n\n- space-around - 每个项目两侧的间隔相等。所以，项目之间的间隔比项目与容器边框的间隔大一倍。\n- space-between - 项目与项目的间隔相等，项目与容器边框之间没有间隔。\n- space-evenly - 项目与项目的间隔相等，项目与容器边框之间也是同样长度的间隔。\n\n  \u003cimg src=\"../../pics/image-20210503162757507.png\" alt=\"image-20210503162757507\" style=\"zoom:50%;\" /\u003e\n\n`place-content`属性是`align-content`属性和`justify-content`属性的合并简写形式。\n\n\u003e ```css\n\u003e place-content: \u003calign-content\u003e \u003cjustify-content\u003e\n\u003e ```\n\n下面是一个例子。\n\n\u003e ```css\n\u003e place-content: space-around space-evenly;\n\u003e ```\n\n如果省略第二个值，浏览器就会假定第二个值等于第一个值。\n\n#### `grid-auto-columns,grid-auto-rows`\n\n有时候，一些项目的指定位置，在现有网格的外部。比如网格只有3列，但是某一个项目指定在第5行。这时，浏览器会自动生成多余的网格，以便放置项目。\n\n`grid-auto-columns`属性和`grid-auto-rows`属性用来设置，浏览器自动创建的多余网格的列宽和行高。它们的写法与`grid-template-columns`和`grid-template-rows`完全相同。如果不指定这两个属性，浏览器完全根据单元格内容的大小，决定新增网格的列宽和行高。\n\n[下面的例子](https://jsbin.com/sayuric/edit?css,output)里面，划分好的网格是3行 x 3列，但是，8号项目指定在第4行，9号项目指定在第5行。\n\n```css\n.container {\n  display: grid;\n  grid-template-columns: 100px 100px 100px;\n  grid-template-rows: 100px 100px 100px;\n  grid-auto-rows: 50px; \n}\n```\n\n\u003cimg src=\"../../pics/image-20210503162934346.png\" alt=\"image-20210503162934346\" style=\"zoom:67%;\" /\u003e\n\n`grid-template,grid`\n\n`grid-template`属性是`grid-template-columns`、`grid-template-rows`和`grid-template-areas`这三个属性的合并简写形式。\n\n`grid`属性是`grid-template-rows`、`grid-template-columns`、`grid-template-areas`、 `grid-auto-rows`、`grid-auto-columns`、`grid-auto-flow`这六个属性的合并简写形式。\n\n从易读易写的角度考虑，还是建议不要合并属性，所以这里就不详细介绍这两个属性了。\n\n### 项目属性\n\n##### `grid-column-start,grid-column-end,grid-row-start,grid-row-end`\n\n项目的位置是可以指定的，具体方法就是指定项目的四个边框，分别定位在哪根网格线。\n\n- `grid-column-start`属性：左边框所在的垂直网格线\n- `grid-column-end`属性：右边框所在的垂直网格线\n- `grid-row-start`属性：上边框所在的水平网格线\n- `grid-row-end`属性：下边框所在的水平网格线\n\n```css\n.item-1 {\n  grid-column-start: 2;\n  grid-column-end: 4;\n}\n```\n\n\u003cimg src=\"../../pics/image-20210503163145662.png\" alt=\"image-20210503163145662\" style=\"zoom:50%;\" /\u003e\n\n上图中，只指定了1号项目的左右边框，没有指定上下边框，所以会采用默认位置，即上边框是第一根水平网格线，下边框是第二根水平网格线。\n\n除了1号项目以外，其他项目都没有指定位置，由浏览器自动布局，这时它们的位置由容器的`grid-auto-flow`属性决定，这个属性的默认值是`row`，因此会\"先行后列\"进行排列。读者可以把这个属性的值分别改成`column`、`row dense`和`column dense`，看看其他项目的位置发生了怎样的变化。\n\n```css\n.item-1 {\n  grid-column-start: 1;\n  grid-column-end: 3;\n  grid-row-start: 2;\n  grid-row-end: 4;\n}\n```\n\n\u003cimg src=\"../../pics/image-20210503163232079.png\" alt=\"image-20210503163232079\" style=\"zoom:50%;\" /\u003e\n\n这四个属性的值，除了指定为第几个网格线，还可以指定为网格线的名字。\n\n```css\n.item-1 {\n  grid-column-start: header-start;\n  grid-column-end: header-end;\n}\n```\n\n上面代码中，左边框和右边框的位置，都指定为网格线的名字。\n\n这四个属性的值还可以使用`span`关键字，表示\"跨越\"，即左右边框（上下边框）之间跨越多少个网格。\n\n```css\n.item-1 {\n  grid-column-start: span 2;\n}\n或\n.item-1 {\n  grid-column-end: span 2;\n}\n```\n\n\u003cimg src=\"../../pics/image-20210503163306372.png\" alt=\"image-20210503163306372\" style=\"zoom:50%;\" /\u003e\n\n使用这四个属性，如果产生了项目的重叠，则使用`z-index`属性指定项目的重叠顺序。\n\n##### `grid-column ,grid-row`\n\n`grid-column`属性是`grid-column-start`和`grid-column-end`的合并简写形式，`grid-row`属性是`grid-row-start`属性和`grid-row-end`的合并简写形式。\n\n```css\n.item {\n  grid-column: \u003cstart-line\u003e / \u003cend-line\u003e;\n  grid-row: \u003cstart-line\u003e / \u003cend-line\u003e;\n}\n```\n\n```css\n.item-1 {\n  grid-column: 1 / 3;\n  grid-row: 1 / 2;\n}\n/* 等同于 */\n.item-1 {\n  grid-column-start: 1;\n  grid-column-end: 3;\n  grid-row-start: 1;\n  grid-row-end: 2;\n}\n```\n\n上面代码中，项目`item-1`占据第一行，从第一根列线到第三根列线。\n\n这两个属性之中，也可以使用`span`关键字，表示跨越多少个网格。\n\n```css\n.item-1 {\n  background: #b03532;\n  grid-column: 1 / 3;\n  grid-row: 1 / 3;\n}\n/* 等同于 */\n.item-1 {\n  background: #b03532;\n  grid-column: 1 / span 2;\n  grid-row: 1 / span 2;\n}\n```\n\n\u003cimg src=\"../../pics/image-20210503163539996.png\" alt=\"image-20210503163539996\" style=\"zoom:50%;\" /\u003e\n\n斜杠以及后面的部分可以省略，默认跨越一个网格。\n\n```css\n.item-1 {\n  grid-column: 1;\n  grid-row: 1;\n}\n```\n\n上面代码中，项目`item-1`占据左上角第一个网格。\n\n##### `grid-area`\n\n`grid-area`属性指定项目放在哪一个区域。\n\n```css\n.item-1 {\n  grid-area: e;\n}\n```\n\n\u003cimg src=\"../../pics/image-20210503163631819.png\" alt=\"image-20210503163631819\" style=\"zoom:50%;\" /\u003e\n\n`grid-area`属性还可用作`grid-row-start`、`grid-column-start`、`grid-row-end`、`grid-column-end`的合并简写形式，直接指定项目的位置。\n\n```css\n.item {\n  grid-area: \u003crow-start\u003e / \u003ccolumn-start\u003e / \u003crow-end\u003e / \u003ccolumn-end\u003e;\n}\n```\n\n```css\n.item-1 {\n  grid-area: 1 / 1 / 3 / 3;\n}\n```\n\n##### `justify-self,align-self,place-self`\n\n`justify-self`属性设置单元格内容的水平位置（左中右），跟`justify-items`属性的用法完全一致，但只作用于单个项目。\n\n`align-self`属性设置单元格内容的垂直位置（上中下），跟`align-items`属性的用法完全一致，也是只作用于单个项目。\n\n```css\n.item {\n  justify-self: start | end | center | stretch;\n  align-self: start | end | center | stretch;\n}\n```\n\n- start：对齐单元格的起始边缘。\n- end：对齐单元格的结束边缘。\n- center：单元格内部居中。\n- stretch：拉伸，占满单元格的整个宽度（默认值）。\n\n```css\n.item-1  {\n  justify-self: start;\n}\n```\n\n![image-20210503163759111](image-20210503163759111.png)\n\n`place-self`属性是`align-self`属性和`justify-self`属性的合并简写形式。\n\n```css\nplace-self: \u003calign-self\u003e \u003cjustify-self\u003e;\n```\n\n```css\nplace-self: center center;\n```\n\n如果省略第二个值，`place-self`属性会认为这两个值相等。\n","lastmodified":"2023-05-09T16:33:58.275366009Z","tags":[]},"/CSS3-no":{"title":"CSS3","content":"\n# CSS3\n\n**优势：**\n\n- 使用CSS3技术来控制页面元素CSS属性的变化\n- 不需要js也能写\n- 由浏览器执行，更加流畅\n- 减少代码量\n\n**应用场景：**\n\n- 网页特效\n- 用户交互\n- 抽奖动画\n- 网页小游戏\n\n## transition\n\n**`transition`** [CSS](https://developer.mozilla.org/en/CSS) 属性是 [`transition-property`](https://developer.mozilla.org/zh-CN/docs/Web/CSS/transition-property)，[`transition-duration`](https://developer.mozilla.org/zh-CN/docs/Web/CSS/transition-duration)，[`transition-timing-function`](https://developer.mozilla.org/zh-CN/docs/Web/CSS/transition-timing-function) 和 [`transition-delay`](https://developer.mozilla.org/zh-CN/docs/Web/CSS/transition-delay) 的一个[简写属性](https://developer.mozilla.org/en-US/docs/Web/CSS/Shorthand_properties)。\n\n![image-20200213184433362](20210410172017.png)\n\n过渡可以为一个元素在不同状态之间切换的时候定义不同的过渡效果。比如在不同的伪元素之间切换，像是 [`:hover`](https://developer.mozilla.org/zh-CN/docs/Web/CSS/:hover)，[`:active`](https://developer.mozilla.org/zh-CN/docs/Web/CSS/:active) 或者通过 JavaScript 实现的状态变化。\n\n```css\n/* Apply to 1 property */\n/* property name | duration */\ntransition: margin-right 4s;\n\n/* property name | duration | delay */\ntransition: margin-right 4s 1s;\n\n/* property name | duration | timing function */\ntransition: margin-right 4s ease-in-out;\n\n/* property name | duration | timing function | delay */\ntransition: margin-right 4s ease-in-out 1s;\n\n/* Apply to 2 properties */\ntransition: margin-right 4s, color 1s;\n\n/* Apply to all changed properties */\ntransition: all 0.5s ease-out;\n\n/* Global values */\ntransition: inherit;\ntransition: initial;\ntransition: unset;\n```\n\n- ### transition-timing-function\n\n  transition的状态变化速度（又称timing function），默认不是匀速的，而是逐渐放慢，这叫做ease。除了ease以外，其他模式还包括\n\n  1. linear：匀速\n  2. ease-in：加速\n  3. ease-out：减速\n  4. cubic-bezier函数：自定义速度模式\n\n  最后那个cubic-bezier，可以使用[工具网站](http://cubic-bezier.com/)来定制。\n\n  ```css\n  img{\n      transition: 1s height cubic-bezier(.83,.97,.05,1.44);\n  }\n  ```\n\ntransition的优点在于简单易用，但是它有几个很大的局限。\n\n- transition需要事件触发，所以没法在网页加载时自动发生。\n- transition是一次性的，不能重复发生，除非一再触发。\n- transition只能定义开始状态和结束状态，不能定义中间状态，也就是说只有两个状态。\n- 一条transition规则，只能定义一个属性的变化，不能涉及多个属性。\n\n## transform\n\n转换`transform`是CSS3中具有颠覆性的特征之一，可以实现元素的位移、旋转、缩放等效果。\n\n移动：translate\n\n旋转：rotate\n\n缩放：scale\n\n### transform 2D\n\n![image-20200213201443181](20210410172018.png)\n\n**transform-origin**:\n\n转换中心点\n\n`transform-origin: x y;`\n\n- 注意后面的参数 x 和 y 用空格隔开\n- **x y 默认转换的中心点是元素的中心点 (50% 50%)**\n- 还可以给x y 设置 像素 或者 方位名词 （top bottom left right center）\n\n**translate：**\n\n`transform: translate(x,y);` 或者分开写\n\n`transform: translateX(n);`\n\n`transform: translateY(n);`\n\n- 定义 2D 转换中的移动，沿着 X 和 Y 轴移动元素\n- **translate最大的优点：不会影响到其他元素的位置**\n- translate中的**百分比单位是相对于自身元素**的 translate:(50%,50%);\n- 对行内标签没有效果\n\n**rotate**：\n\n`transform:rotate(度数)`\n\n- rotate里面跟度数， 单位是 deg 比如 rotate(45deg)\n- 角度为正时，顺时针，负时，为逆时针\n- 默认旋转的中心点是元素的中心点\n\n![image-20200213201803232](20210410172019.png)\n\n**scale**：\n\n`transform:scale(x,y);`\n\n- 注意其中的x和y用逗号分隔\n- transform:scale(1,1) ：宽和高都放大一倍，相对于没有放大\n- transform:scale(2,2) ：宽和高都放大了2倍\n- transform:scale(2) ：只写一个参数，第二个参数则和第一个参数一样，相当于 scale(2,2)\n- transform:scale(0.5,0.5)：缩小\n- **sacle缩放最大的优势：可以设置转换中心点缩放，默认以中心点缩放的，而且不影响其他盒子**\n\n**注意：**\n\n1. 同时使用多个转换，其格式为：`transform: translate() rotate() scale() …`等，\n2. **其顺序会影转换的效果（先旋转会改变坐标轴方向）**\n3. 当我们同时有位移和其他属性的时候，记得要将位移放到最前\n\n### transform 3D\n\n![image-20200213202206022](20210410172020.png)\n\n**translate3d**:\n\n3D移动在2D移动的基础上多加了一个可以移动的方向，就是z轴方向。\n\n- `translform:translateX(100px)`：仅仅是在x轴上移\n- `translform:translateY(100px)`：仅仅是在Y轴上移动\n- `translform:translateZ(100px)`：仅仅是在Z轴上移动（注意：translateZ一般用px单位）\n- `transform:translate3d(x,y,z)`：其中 x、y、z 分别指要移动的轴的方向的距离\n\n因为z轴是垂直屏幕，由里指向外面，所以默认是看不到元素在z轴的方向上移动。\n\n**perspective**:\n\n![image-20200213202357620](20210410172021.png)\n\n`translform:translateZ(100px)`：仅仅是在Z轴上移动。有了透视，就能看到translateZ 引起的变化了  \n\n**rotate3d**:\n\n3D旋转指可以让元素在三维平面内沿着 x轴，y轴，z轴或者自定义轴进行旋转。\n\n- `transform:rotateX(45deg)`：沿着x轴正方向旋转 45度\n- `transform:rotateY(45deg)` ：沿着y轴正方向旋转 45deg\n- `transform:rotateZ(45deg)` ：沿着Z轴正方向旋转 45deg\n- `transform:rotate3d(x,y,z,deg)`： 沿着自定义轴旋转 deg为角度（了解即可）xyz是表示旋转轴的矢量，是标示你是否希望沿着该轴旋转，最后一个标示旋转的角度。`transform:rotate3d(1,1,0,45deg)` 就是沿着对角线旋转 45deg\n\n**transfrom-style**:\n\n控制子元素是否开启三维立体环境。\n\n- `transform-style: flat;` 子元素不开启3d立体空间 默认的\n- `ltransform-style: preserve-3d;` 子元素开启立体空间\n\n代码写给父级，但是影响的是子盒子。这个属性很重要，后面必用。\n\n## Animation\n\n```css\n/*一个周期持续的时间，以及动画效果的名称\n当鼠标悬停在div元素上时，会产生名为‘动画名称’的动画效果，持续时间为1秒。为此，我们还需要用keyframes关键字，定义‘动画名称’效果。\n*/\ndiv:hover {\n  animation: 1s 动画名称;\n}\n\n@keyframes 动画名称 {\n   0% { background: #c00; }\n  50% { background: orange; }\n  100% { background: yellowgreen; }\n}\n```\n\n- 0% 是动画的开始，100% 是动画的完成。这样的规则就是动画序列。\n- 在 @keyframes 中规定某项 CSS 样式，就能创建由当前样式逐渐改为新样式的动画效果。\n- 动画是使元素从一种样式逐渐变化为另一种样式的效果。您可以改变任意多的样式任意多的次数。\n- 请用百分比来规定变化发生的时间，或用关键词 \"from\" 和 \"to\"，等同于 0% 和 100%。\n\n### 具体属性\n\n`animation：动画名称 持续时间 运动曲线 何时开始 播放次数 是否反方向 动画起始或者结束的状态;`\n\n![image-20200213203101735](20210410172022.png)\n\n\u003e 简写属性里面不包含 animation-play-state  \n\u003e 暂停动画：animation-play-state: puased; 经常和鼠标经过等其他配合使用  \n\u003e 想要动画走回来 ，而不是直接跳回来：`animation-direction:alternate;`  \n\u003e 盒子动画结束后，停在结束位置： `animation-fill-mode:forwards;`\n\n- animation-timing-function：规定动画的速度曲线，默认是“ease”\n\n![image-20200213203243443](20210410172023.png)\n\n### 旋转木马案例\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n  \u003cmeta charset=\"UTF-8\"\u003e\n  \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\n  \u003cmeta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"\u003e\n  \u003ctitle\u003eDocument\u003c/title\u003e\n  \u003cstyle\u003e\n    body{\n      perspective: 1000px;\n    }\n    section{\n      position: relative;\n      height: 200px;\n      width: 300px;\n      margin: 200px auto;\n      transform-style: preserve-3d;\n      animation: rota 0.5s linear infinite;\n    }\n\n    section:hover{\n      animation-play-state: paused;\n    }\n\n    @keyframes  rota {\n      0%{\n        transform:rotateY(0) ;\n      }\n      100%{\n        transform: rotateY(360deg);\n      }\n    }\n    section div{\n      position: absolute;\n      top: 0;\n      left: 0;\n      width: 100%;\n      height: 100%;\n      background: url('http://pic0.iqiyipic.com/common/lego/20200209/ccc614312b7048f19408192c91a8ecf9.jpg') no-repeat ;\n    }\n    section div:nth-child(1){\n      transform: translateZ(300px);\n    }\n    section div:nth-child(2){\n      transform: rotateY(60deg) translateZ(300px);\n    }\n    section div:nth-child(3){\n      transform: rotateY(120deg) translateZ(300px);\n    }\n    section div:nth-child(4){\n      transform: rotateY(180deg) translateZ(300px);\n    }\n    section div:nth-child(5){\n      transform: rotateY(240deg) translateZ(300px);\n    }\n    section div:nth-child(6){\n      transform: rotateY(300deg) translateZ(300px);\n    }\n  \u003c/style\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n  \u003csection\u003e\n    \u003cdiv\u003e\u003c/div\u003e\n    \u003cdiv\u003e\u003c/div\u003e\n    \u003cdiv\u003e\u003c/div\u003e\n    \u003cdiv\u003e\u003c/div\u003e\n    \u003cdiv\u003e\u003c/div\u003e\n    \u003cdiv\u003e\u003c/div\u003e\n  \u003c/section\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n```\n","lastmodified":"2023-05-09T16:33:58.275366009Z","tags":[]},"/Canvas":{"title":"canvas","content":"\n# canvas\n\n`\u003ccanvas\u003e` 是 HTML5 新增的元素，可用于通过使用JavaScript中的脚本来绘制图形，创建动画。`\u003ccanvas\u003e `最早由Apple引入WebKit.  \n我们可以使用\\\u003ccanvas\u003e标签来定义一个canvas元素。\n\n\u003e canvas元素默认具有高宽  \n\u003e \twidth： 300px  \n\u003e \theight：150px\n\n## 1.整体API\n\n```javascript\n1.注意点\n---canvas图像的渲染有别于html图像的渲染，\n\t\tcanvas的渲染极快，不会出现代码覆盖后延迟渲染的问题\n\t\t写canvas代码一定要具有同步思想\n---在获取上下文时，一定要先判断\n---画布高宽的问题\n\t画布默认高宽300*150\n\t切记一定要使用html的attribute的形式来定义画布的宽高\n\t通过css形式定义会缩放画布内的图像\n---绘制矩形的问题\n\ta.边框宽度的问题，边框宽度是在偏移量上下分别渲染一半，可能会出现小数边框，\n\t\t一旦出现小数边框都会向上取整\n\tb.canvas的api只支持一种图像的直接渲染：矩形\n---我们没法使用选择器来选到canvas中的图像\n2.画布api\n\toc.getContext(\"2d\"):获取画布的2d上下文\n\toc.width:画布在横向上css像素的个数\n\toc.height:画布在纵向上css像素的个数\n\toc.toDataUrl():拿到画布的图片地址\n3.上下文api\n\tctx.fillRect(x,y,w,h):填充矩形\n\tctx.strokeRect(x,ymwmh):带边框的矩形\n\tctx.clearRect(0,0,oc.width,oc.height):清除整个画布,注意原点的位置\n\tctx.fillStyle:填充颜色\n\t\t背景fillStyle的值可以是createPattern(image, repetition)返回的对象\n\t\t线性渐变fillStyle的值可以是createLinearGradient(x1, y1, x2, y2))返回的对象\n\t\t\t\taddColorStop(position, color)\n\t\t\t径向渐变fillStyle的值可以是createRadialGradient(x1, y1, r1, x2, y2, r2)返回的对象\n\t\t\t\taddColorStop(position, color)\n\tctx.strokeStyle:线条颜色\n\tctx.lineWidth：线条宽度\n\tctx.lineCap：线条两端的展现形式\n\tctx.lineJoin：线条连接处的展现形式\n\tctx.moveTo(x,y):将画笔抬起点到x，y处\n\tctx.lineTo(x,y):将画笔移到x，y处\n\tctx.rect(x,y,w,h)\n\tctx.arc(x,y,r,degS,degE,dir)\n\tctx.arcTo(x1,y1,x2,y2,r):2个坐标，一个半径\n\t\t结合moveTo(x,y)方法使用，\n\t\tx,y:起始点\n\t\tx1,y1：控制点\n\t\tx2,y2：结束点\n\tctx.quadraticCurveTo(x1,y1,x2,y2)\n\t\t结合moveTo(x,y)方法使用，\n\t\tx,y:起始点\n\t\tx1,y1：控制点\n\t\tx2,y2：结束点\n\t\t必须经过起点和终点\n\tctx.bezierCurveTo(x1, y1, x2, y2, x3, y3)\n\t\t结合moveTo(x,y)方法使用，\n\t\tx,y:起始点\n\t\tx1,y1：控制点\n\t\tx2,y2：控制点\n\t\tx3，y3：结束点\n\t\t必须经过起点和终点\n\tctx.fill()\n\tctx.stroke()\n\tctx.beginpath():清除路径容器\n\tctx.closepath():闭合路径\n\t\tfill自动闭合\n\t\tstroke需要手动闭合\n\tctx.save()\n\t\t将画布当前状态(样式相关 变换相关)压入到样式栈中\n\tctx.restore()\n\t\t将样式栈中栈顶的元素弹到样式容器中\n\t\t图像最终渲染依赖于样式容器\n\tctx.translate(x,y):将原点按当前坐标轴位移x，y个单位\n\tctx.rotate(弧度):将坐标轴按顺时针方向进行旋转\n\tctx.scale(因子):\n\t\t放大：放大画布，画布中的一个css像素所占据的物理面积变大，画布中包含的css像素的个数变少\n\t\t\t画布中图像所包含的css像素的个数不变\n\t\t缩小：缩小画布，画布中的一个css像素所占据的物理面积变小，画布中包含的css像素的个数变多\n\t\t\t画布中图像所包含的css像素的个数不变\n\tctx.drawImage(img,x,y,w,h):在canvas中引入图片一定在图片加载完成之后再去操作\n\tctx.measureText(\"文本\"):返回一个持有文本渲染宽度的对象\n\tctx.fillText()\n\tctx.strokeText()\n\tctx.font\n\tctx.textAlign\n\tctx.textBaseline\n\tshadowOffsetX = float\n\tshadowOffsetY = float\n\tshadowBlur = float\n\tshadowColor = color(必需项)\n\tctx.getImageData(x,y,w,h)\n\t\tImageData对象\n\t\t\twidth：选中区域在横向上css像素的个数\n\t\t\theight：选中区域在纵向上css像素的个数\n\t\t\tdata:数组\n\t\t\t\t选中区域所有像素点的rgba信息，rgba的取值从0到255\n\tctx.putImageData(imgdata,x,y)\n\tctx.createImageData(w,h)\n\t\t返回的是imgdata对象 默认像素点的信息是rgba(0,0,0,0)\n\tctx.globalAlpha\n\t\t取值为0到1\n\tctx.globalCompositeOperation\n\t\tsource-over(默认值):源在上面,新的图像层级比较高\n\t\tsource-in  :只留下源与目标的重叠部分(源的那一部分)\n\t\tsource-out :只留下源超过目标的部分\n\t\tsource-atop:砍掉源溢出的部分\n\t\tdestination-over:目标在上面,旧的图像层级比较高\n\t\tdestination-in:只留下源与目标的重叠部分(目标的那一部分)\n\t\tdestination-out:只留下目标超过源的部分\n\t\tdestination-atop:砍掉目标溢出的部分\t\n\tctx.ispointinpath(x,y)\n\t\tx,y这个点是否在路径上\n4.实例\n\t时钟动画：结合了所有基础api\n\t飞鸟动画：结合图片创建动画\n\t马赛克：像素操作\n\t刮刮卡：合成+像素操作\t\n```\n\n## 2.不支持的浏览器\n\n\\\u003ccanvas\u003e很容易定义一些替代内容。由于某些较老的浏览器（尤其是IE9之前的IE浏览器）  \n\t不支持HTML元素\"canvas\"，  \n\t但在这些浏览器上你应该要给用户展示些替代内容。  \n\t这非常简单：我们只需要在\u003ccanvas\u003e标签中提供替换内容就可以。  \n\t---\u003e支持\u003ccanvas\u003e的浏览器将会忽略在容器中包含的内容，并且只是正常渲染canvas。  \n\t--\u003e不支持\u003ccanvas\u003e的浏览器会显示代替内容\n\n## \\\u003ccanvas\u003e属性\n\n\\\u003ccanvas\u003e 看起来和\\\u003cimg\u003e元素很相像，唯一的不同就是它并没有 src 和 alt 属性。实际上，\\\u003ccanvas\u003e 标签只有两个属性—— width和height。这些都是可选的。当没有设置宽度和高度的时候，canvas会初始化宽度为300像素和高度为150像素。\n\n\u003e画布的高宽  \n\u003ehtml属性设置width height时只影响画布本身不影画布内容  \n\u003ecss属性设置width height时不但会影响画布本身的高宽，  \n\u003e还会使画布中的内容等比例缩放（缩放参照于画布默认的尺寸）\n\n## 渲染上下文\n\n\\\u003ccanvas\u003e 元素只是创造了一个固定大小的画布，要想在它上面去绘制内容，我们需要找到它的渲染上下文。\\\u003ccanvas\u003e 元素有一个叫做 getContext() 的方法，这个方法是用来获得渲染上下文和它的绘画功能。\n\n```javascript\nvar canvas = document.getElementById('box');\nvar ctx = canvas.getContext('2d');\n```\n\n检查支持性\n\n```javascript\nvar canvas =document.getElementById('tutorial');\nif (canvas.getContext){\n\tvar ctx = canvas.getContext('2d');\n} \n```\n\n## 绘制矩形\n\nHTML中的元素canvas只支持一种原生的图形绘制：矩形。所有其他的图形的绘制都至少需要生成一条路径.\n\ncanvas提供了三种方法绘制矩形：\n\n1. 绘制一个填充的矩形（填充色默认为黑色）\n\n   `fillRect(x, y, width, height)`\n\n2. 绘制一个矩形的边框（默认边框为:一像素实心黑色）\n\n   `strokeRect(x, y, width, height)`\n\n3. 清除指定矩形区域，让清除部分完全透明。\n\n   `clearRect(x, y, width, height)`\n\nx与y指定了在canvas画布上所绘制的矩形的左上角（相对于原点）的坐标。width和height设置矩形的尺寸。（存在边框的话，边框会在width上占据一个边框的宽度，height同理）\n\n#### strokeRect时，边框像素渲染问题\n\n按理渲染出的边框应该是1px的，  \ncanvas在渲染矩形边框时，边框宽度是平均分在偏移位置的两侧。  \ncontext.strokeRect(10,10,50,50):边框会渲染在10.5 和 9.5之间,浏览器是不会让一个像素只用自己的一半的  \n相当于边框会渲染在9到11之间  \ncontext.strokeRect(10.5,10.5,50,50):边框会渲染在10到11之间\n\n#### 添加样式和颜色\n\nfillStyle :设置图形的填充颜色。  \nstrokeStyle :设置图形轮廓的颜色。  \n默认情况下，线条和填充颜色都是黑色（CSS 颜色#000000）  \nineWidth : 这个属性设置当前绘线的粗细。属性值必须为正数。  \n描述线段宽度的数字。 0、 负数、 Infinity 和 NaN 会被忽略。  \n默认值是1.0。\n\n#### lineJoin\n\n设定线条与线条间接合处的样式（默认是 miter）  \n\tround : 圆角  \n\tbevel : 斜角  \n\tmiter : 直角\n\n#### 绘制路径基础api\n\n图形的基本元素是路径。路径是通过不同颜色和宽度的线段或曲线相连形成的不同形状的点的集合。  \n1.首先，你需要创建路径起始点。  \n2.然后你使用画图命令去画出路径  \n3.之后你把路径封闭。  \n4.一旦路径生成，你就能通过描边或填充路径区域来渲染图形。\n\n##### `beginPath()`\n\n新建一条路径，生成之后，图形绘制命令被指向到路径上准备生成路径。本质上，路径是由很多子路径构成，这些子路径都是在一个列表中，所有的子路径（线、弧形、等等）构成图形。而每次这个方法调用之后，列表清空重置，然后我们就可以重新绘制新的图形。\n\n##### `moveTo(x, y)`\n\n将笔触移动到指定的坐标x以及y上,当canvas初始化或者beginPath()调用后，你通常会使用moveTo()函数设置起点.\n\n##### `lineTo(x, y)`\n\n将笔触移动到指定的坐标x以及y上,绘制一条从当前位置到指定x以及y位置的直线。\n\n##### `closePath()`\n\n闭合路径之后图形绘制命令又重新指向到上下文中。闭合路径closePath(),不是必需的。这个方法会通过绘制一条从当前点到开始点的直线来闭合图形。如果图形是已经闭合了的，即当前点为开始点，该函数什么也不做  \n当你调用fill()函数时，所有没有闭合的形状都会自动闭合，所以你不需要调用closePath()函数。  \n但是调用stroke()时不会自动闭合.\n\n##### `stroke()`\n\n通过线条来绘制图形轮廓。不会自动调用closePath().\n\n##### `fill()`\n\n通过填充路径的内容区域生成实心的图形。自动调用closePath().\n\n##### `rect(x, y, width, height)`\n\n绘制一个左上角坐标为（x,y），宽高为width以及height的矩形。当该方法执行的时候，moveTo()方法自动设置坐标参数（0,0）。也就是说，当前笔触自动重置会默认坐标.\n\n##### `save()`\n\n是 Canvas 2D API 通过将当前状态放入栈中，保存 canvas 全部状态的方法.\n\n##### `restore()`\n\n是 Canvas 2D API 通过在绘图状态栈中弹出顶端的状态，将 canvas 恢复到最近的保存状态的方法。 如果没有保存状态，此方法不做任何改变。\n\n##### `lineCap`\n\nlineCap 是 Canvas 2D API 指定如何绘制每一条线段末端的属性。  \n有3个可能的值，分别是：  \n\tlineCap=\"butt\" :线段末端以方形结束。  \n\tround :线段末端以圆形结束  \n\tsquare:线段末端以方形结束，但是增加了一个宽度和线段相同，高度是线段厚度一半的矩形区域  \n\t默认值是 butt。\n\n\u003e 保存到栈中的绘制状态有下面部分组成：  \n\u003e \t当前的变换矩阵。  \n\u003e \t当前的剪切区域。  \n\u003e \t当前的虚线列表.  \n\u003e \t以下属性当前的值： strokeStyle,  \n\u003e \t\t\t\t fillStyle,  \n\u003e \t\t\t\t lineWidth,  \n\u003e \t\t\t\t lineCap,  \n\u003e \t\t\t\t lineJoin…\n\n![image-20200310011745357](20210410190325.png)\n\n`beginPath\\closePath`很重要，不然两个三角形第二次都会生成，后生成的会覆盖前面的。\n\n![image-20200310011830836](20210410190328.png)\n\n![image-20200310012804358](20210410190341.png)\n\n注意，颜色取决于save restore的嵌套层级，如下图save压栈，restore弹栈\n\n![image-20200310012930179](20210410190336.png)\n\n##### 基础模板\n\n```javascript\nwindow.onload=function(){\n\t/*\n\t1.路径容器\n\t\t每次调用路径api时,都会往路径容器里做登记\n\t\t调用beginPath时,清空整个路径容器\n\t2.样式容器\n\t\t每次调用样式api时,都会往样式容器里做登记\n\t\t调用save时候,将样式容器里的状态压入样式栈\n\t\t调用restor时候,将样式栈的栈顶状态弹出到样式样式容器里,进行覆盖\n\t3.样式栈\n\t\t调用save时候,将样式容器里的状态压入样式栈\n\t\t调用restore时候,将样式栈的栈顶状态弹出到样式样式容器里,进行覆盖\n\t*/\n\t\n\tvar canvas = document.querySelector(\"#test\");\n\tif(canvas.getContext){\n\t\tvar ctx = canvas.getContext(\"2d\");\n\t\t\n\t\tctx.save();\n\t\t//关于样式的设置\n\t\t//save  restore成对出现\n\t\tctx.beginPath();\n\t\t//关于路径\n\t\tctx.restore();\n\t\t\n\t\t\n\t\tctx.save();\n\t\t//关于样式的设置\n\t\tctx.beginPath();\n\t\t//关于路径\n\t\t\n\t\tctx.fill();\n\t\tctx.restore();\n\t}\n```\n\n##### 案例 签名\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\t\u003chead\u003e\n\t\t\u003cmeta charset=\"UTF-8\"\u003e\n\t\t\u003ctitle\u003e\u003c/title\u003e\n\t\t\u003cstyle type=\"text/css\"\u003e\n\t\t\t*{\n\t\t\t\tmargin: 0;\n\t\t\t\tpadding: 0;\n\t\t\t}\n\t\t\tbody{\n\t\t\t\tbackground: gray;\n\t\t\t}\n\t\t\t#test{\n\t\t\t\tposition: absolute;\n\t\t\t\tleft: 0;\n\t\t\t\tright: 0;\n\t\t\t\ttop: 0;\n\t\t\t\tbottom: 0;\n\t\t\t\tmargin: auto;\n\t\t\t\tbackground:white;\n\t\t\t}\n\t\t\u003c/style\u003e\n\t\u003c/head\u003e\n\t\u003cbody\u003e\n\t\t\u003ccanvas id=\"test\" width=\"500\" height=\"500\"\u003e\u003c/canvas\u003e\n\t\u003c/body\u003e\n\t\u003cscript type=\"text/javascript\"\u003e\n\t\t\n\t\twindow.onload=function(){\n\t\t\t\n\t\t\tvar canvas =document.getElementById(\"test\");\n\t\t\tif(canvas.getContext){\n\t\t\t\tvar ctx = canvas.getContext(\"2d\");\n\t\t\t}\n\t\t\t\n\t\t\tcanvas.onmousedown=function(ev){\n\t\t\t\tev = ev || window.event;\n\t\t\t\tif(canvas.setCapture){\n\t\t\t\t\tcanvas.setCapture();\n\t\t\t\t}\n\t\t\t\tctx.beginPath();\n\t\t\t\tctx.moveTo(ev.clientX -canvas.offsetLeft,ev.clientY -canvas.offsetTop);\n\t\t\t\tdocument.onmousemove=function(ev){\n\t\t\t\t\tctx.save();\n\t\t\t\t\tctx.strokeStyle=\"pink\";\n\t\t\t\t\tev = ev || event;\n\t\t\t\t\tctx.lineTo(ev.clientX -canvas.offsetLeft,ev.clientY -canvas.offsetTop);\n\t\t\t\t\tctx.stroke();\n\t\t\t\t\tctx.restore();\n\t\t\t\t}\n\t\t\t\tdocument.onmouseup=function(){\n\t\t\t\t\tdocument.onmousemove=document.onmouseup=null;\n\t\t\t\t\tif(document.releaseCapture){\n\t\t\t\t\t\tdocument.releaseCapture();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\t\n\t\t}\n\t\t\n\t\u003c/script\u003e\n\u003c/html\u003e\n\n```\n\n#### 圆形\n\n弧度的js表达式:`radians=(Math.PI/180)*degrees。`\n\n##### `arc(x,y,radius,startAngle,endAngle,anticlockwise)`\n\n画一个以（x,y）为圆心的以radius为半径的圆弧（圆），从startAngle开始到endAngle结束，  \n按照anticlockwise给定的方向（默认为顺时针）来生成。  \n\tture：逆时针  \n\tfalse:顺时针\n\n- x,y为绘制圆弧所在圆上的圆心坐标\n- radius为半径\n- startAngle以及endAngle参数用弧度定义了开始以及结束的弧度。这些都是以x轴为基准\n- 参数anticlockwise 为一个布尔值。为true时，是逆时针方向，否则顺时针方向。\n\n![image-20200310014732283](20210410190355.png)\n\n![image-20200310014748642](20210410190400.png)\n\n![image-20200310014801211](20210410190405.png)\n\n![image-20200310014815597](20210410190409.png)\n\n##### `arcTo(x1, y1, x2, y2, radius)`\n\n根据给定的控制点和半径画一段圆弧  \n肯定会从(x1 y1) 但不一定经过(x2 y2);(x2 y2)只是控制一个方向\n\n![image-20200310015243769](20210410190413.png)\n\n![image-20200310015302562](20210410190420.png)\n\n#### 贝塞尔\n\n`quadraticCurveTo(cp1x, cp1y, x, y)`  \n绘制二次贝塞尔曲线，cp1x,cp1y为一个控制点，x,y为结束点。  \n起始点为moveto时指定的点.  \n![image-20200310015430730](20210410190429.png)\n\n`bezierCurveTo(cp1x, cp1y, cp2x, cp2y, x, y)`  \n绘制三次贝塞尔曲线，cp1x,cp1y为控制点一，cp2x,cp2y为控制点二，x,y为结束点。  \n起始点为moveto时指定的点.\n\n![image-20200310015557211](20210410190439.png)\n\n![image-20200310015548203](20210410190445.png)\n\n#### canvas中的变换\n\n##### `translate(x, y)`\n\n我们先介绍 translate 方法，它用来移动 canvas的原点到一个不同的位置。  \ntranslate 方法接受两个参数。x 是左右偏移量，y 是上下偏移量，**在canvas中translate是累加的.**\n\n![image-20200310223725182](20210410190452.png)\n\n![image-20200310223629529](20210410190459.png)\n\n##### `rotate(angle)`\n\n这个方法只接受一个参数：旋转的角度(angle)，它是顺时针方向的，以弧度为单位的值。  \n旋转的中心点始终是 canvas 的原点，如果要改变它，我们需要用到 translate 方法,**在canvas中rotate是累加的**\n\n![image-20200310223928586](20210410190506.png)\n\n##### `scale(x, y)`\n\nscale 方法接受两个参数。x,y 分别是横轴和纵轴的缩放因子，它们都必须是正值。值比 1.0 小表示缩小，比 1.0 大则表示放大，值为 1.0 时什么效果都没有。  \n缩放一般我们用它来增减图形在 canvas 中的像素数目，对形状，位图进行缩小或者放大。**在canvas中scale是累称的。**\n\n\u003e *css像素是一个抽象单位*\n\u003e\n\u003e 放大:使画布内css像素的个数变少，单个css像素所占据的实际物理尺寸变大\n\u003e\n\u003e 缩小:使画布内css像素的个数变多，单个css像素所占据的实际物理尺寸变小\n\n##### 变换实例\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\n\u003chead\u003e\n  \u003cmeta charset=\"UTF-8\"\u003e\n  \u003ctitle\u003e\u003c/title\u003e\n  \u003cstyle type=\"text/css\"\u003e\n    * {\n      margin: 0;\n      padding: 0;\n    }\n\n    html,\n    body {\n      height: 100%;\n      overflow: hidden;\n    }\n\n    body {\n      background: pink;\n    }\n\n    #test {\n      background: gray;\n      position: absolute;\n      left: 0;\n      top: 0;\n      right: 0;\n      bottom: 0;\n      margin: auto;\n    }\n  \u003c/style\u003e\n\u003c/head\u003e\n\n\u003cbody\u003e\n  \u003ccanvas id=\"test\" width=\"300\" height=\"300\"\u003e\n    \u003cspan\u003e您的浏览器不支持画布元素 请您换成萌萌的谷歌\u003c/span\u003e\n  \u003c/canvas\u003e\n\u003c/body\u003e\n\u003cscript type=\"text/javascript\"\u003e\n  window.onload = function () {\n    var flag = 0;\n    var scale = 0;\n    var flagScale = 0;\n    var canvas = document.querySelector(\"#test\");\n    if (canvas.getContext) {\n      var ctx = canvas.getContext(\"2d\");\n      ctx.save();\n      ctx.translate(150, 150);\n      ctx.beginPath()\n      ctx.fillRect(-50, -50, 100, 100);\n      ctx.restore();\n      setInterval(function () {\n        flag++\n\n        ctx.clearRect(0, 0, canvas.width, canvas.height)\n        ctx.save()\n        ctx.translate(150, 150);\n        ctx.rotate(flag * Math.PI / 180)\n\n        if (scale == 100) {\n          flagScale = -1;\n        } else if(scale==0){\n          flagScale = 1\n        }\n        scale+=flagScale\n        ctx.scale(scale/50,scale/50)\n        ctx.beginPath()\n        ctx.fillRect(-50, -50, 100, 100);\n        ctx.restore()\n      }, 1000 / 60)\n    }\n  }\n\n\n\u003c/script\u003e\n\n\u003c/html\u003e\n```\n\n# 实践1 钟表\n\n1.初始化  \n\t\t将圆心调整到画布的中间  \n\t\t由于canvas中画圆与旋转所参照的坐标系于正常坐标系有出入  \n\t\t\t将整个画布逆时针旋转90度  \n\t\t初始化一些样式数据  \n\t\t\tctx.lineWidth = 8;  \n\t\t  \tctx.strokeStyle = \"black\";  \n\t\t  \tctx.lineCap = \"round\";  \n2.外层空心圆盘  \n\t圆盘颜色:#325FA2  \n\t圆盘宽度:14  \n\t圆盘半径:140\n\n3.时针刻度  \n\t长度为20  \n\t宽度为8  \n\t外层空心圆盘与时针刻度之间的距离也为20\n\n4.分针刻度  \n\t宽度为4  \n\t长度为3\n\n5.时针  \n\t宽度为14  \n\t圆心外溢出80 收20\n\n6.分针  \n\t宽度为10  \n\t圆心外溢出112 收28\n\n7.秒针  \n\t颜色:D40000  \n\t宽度为6  \n\t圆心外溢出83 收30  \n\t----\u003e中心实心圆盘  \n\t半径为10\n\n​\t----\u003e秒针头  \n​\t96码开外半径为10的空心圆  \n​\t宽度为6\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\t\u003chead\u003e\n\t\t\u003cmeta charset=\"UTF-8\"\u003e\n\t\t\u003ctitle\u003e\u003c/title\u003e\n\t\t\u003cstyle type=\"text/css\"\u003e\n\t\t\t*{\n\t\t\t\tmargin: 0;\n\t\t\t\tpadding: 0;\n\t\t\t}\n\t\t\thtml,body{\n\t\t\t\theight: 100%;\n\t\t\t\toverflow: hidden;\n\t\t\t\tbackground: pink;\n\t\t\t}\n\t\t\t#clock{\n\t\t\t\tbackground: gray;\n\t\t\t\tposition: absolute;\n\t\t\t\tleft: 50%;\n\t\t\t\ttop: 50%;\n\t\t\t\ttransform: translate3d(-50%,-50%,0);\n\t\t\t}\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\u003c/style\u003e\n\t\u003c/head\u003e\n\t\u003cbody\u003e\n\t\t\u003ccanvas id=\"clock\" width=\"400\" height=\"400\"\u003e\u003c/canvas\u003e\n\t\u003c/body\u003e\n\t\u003cscript type=\"text/javascript\"\u003e\n\t\t\n\t\twindow.onload=function(){\n\t\t\tvar clock = document.querySelector(\"#clock\");\n\t\t\tif(clock.getContext){\n\t\t\t\tvar ctx = clock.getContext(\"2d\");\n\t\t\t\tsetInterval(function(){\n\t\t\t\t\tctx.clearRect(0,0,clock.width,clock.height);\n\t\t\t\t\tmove();\n\t\t\t\t},1000);\n\t\t\t\t\n\t\t\t\tmove();\n\t\t\t\tfunction move(){\n\t\t\t\t\tctx.save();\n\t\t\t\t\tctx.lineWidth = 8;\n\t\t\t\t  \tctx.strokeStyle = \"black\";\n\t\t\t\t  \tctx.lineCap = \"round\";\n\t\t\t\t\tctx.translate(200,200);\n\t\t\t\t\tctx.scale(.5,.5)\n\t\t\t\t\tctx.rotate(-90*Math.PI/180);\n\t\t\t\t\tctx.beginPath();\n\t\t\t\t\t\n\t\t\t\t\t//外层空心圆盘\n\t\t\t\t\tctx.save();\n\t\t\t\t\tctx.strokeStyle=\"#325FA2\";\n\t\t\t\t\tctx.lineWidth = 14;\n\t\t\t\t\tctx.beginPath();\n\t\t\t\t\tctx.arc(0,0,140,0,360*Math.PI/180);\n\t\t\t\t\tctx.stroke();\n\t\t\t\t\tctx.restore();\n\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t//时针刻度\n\t\t\t\t\tctx.save();\n\t\t\t\t\tfor(var i=0;i\u003c12;i++){\n\t\t\t\t\t\tctx.rotate(30*Math.PI/180);\n\t\t\t\t\t\tctx.beginPath();\n\t\t\t\t\t\tctx.moveTo(100,0)\n\t\t\t\t\t\tctx.lineTo(120,0);\n\t\t\t\t\t\tctx.stroke();\n\t\t\t\t\t}\n\t\t\t\t\tctx.restore();\n\t\t\t\t\t\n\t\t\t\t\t//分针刻度\n\t\t\t\t\tctx.save();\n\t\t\t\t\tctx.lineWidth=4;\n\t\t\t\t\tfor(var i=0;i\u003c60;i++){\n\t\t\t\t\t\tctx.rotate(6*Math.PI/180);\n\t\t\t\t\t\tif((i+1)%5!=0){\n\t\t\t\t\t\t\tctx.beginPath();\n\t\t\t\t\t\t\tctx.moveTo(117,0)\n\t\t\t\t\t\t\tctx.lineTo(120,0);\n\t\t\t\t\t\t\tctx.stroke();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tctx.restore();\n\t\t\t\t\t\n\t\t\t\t\t//时针 分针 秒针 表座\n\t\t\t\t\tvar date = new Date();\n\t\t\t\t\tvar s = date.getSeconds();\n\t\t\t\t\tvar m = date.getMinutes()+s/60;\n\t\t\t\t\tvar h = date.getHours()+m/60;\n\t\t\t\t\th = h\u003e12?h-12:h;\n\t\t\t\t\t\n\t\t\t\t\t//时针\n\t\t\t\t\tctx.save()\n\t\t\t\t\tctx.lineWidth=14;\n\t\t\t\t\tctx.rotate(h*30*Math.PI/180)\n\t\t\t\t\tctx.beginPath()\n\t\t\t\t\tctx.moveTo(-20,0);\n\t\t\t\t\tctx.lineTo(80,0);\n\t\t\t\t\tctx.stroke();\n\t\t\t\t\tctx.restore()\n\t\t\t\t\t\n\t\t\t\t\t//分针\n\t\t\t\t\tctx.save()\n\t\t\t\t\tctx.lineWidth=10;\n\t\t\t\t\tctx.rotate(m*6*Math.PI/180)\n\t\t\t\t\tctx.beginPath()\n\t\t\t\t\tctx.moveTo(-28,0);\n\t\t\t\t\tctx.lineTo(112,0);\n\t\t\t\t\tctx.stroke();\n\t\t\t\t\tctx.restore()\n\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t//秒针\n\t\t\t\t\tctx.save()\n\t\t\t\t\tctx.lineWidth=6;\n\t\t\t\t\tctx.strokeStyle=\"#D40000\";\n\t\t\t\t\tctx.fillStyle=\"#D40000\";\n\t\t\t\t\tctx.rotate(s*6*Math.PI/180)\n\t\t\t\t\tctx.beginPath();\n\t\t\t\t\tctx.moveTo(-30,0);\n\t\t\t\t\tctx.lineTo(83,0);\n\t\t\t\t\tctx.stroke();\n\t\t\t\t\t\t//表座\n\t\t\t\t\t\tctx.beginPath();\n\t\t\t\t\t\tctx.arc(0,0,10,0,360*Math.PI/180);\n\t\t\t\t\t\tctx.fill();\n\t\t\t\t\t\t//秒头\n\t\t\t\t\t\tctx.beginPath();\n\t\t\t\t\t\tctx.arc(96,0,10,0,360*Math.PI/180);\n\t\t\t\t\t\tctx.stroke();\n\t\t\t\t\tctx.restore()\n\t\t\t\t\tctx.restore();\n\t\t\t\t}\n\t\t\t}\t\t\t\n        }\n\t\u003c/script\u003e\n\u003c/html\u003e\n```\n\n# 在canvas中插入图片\n\n(需要image对象)\n\n1. canvas操作图片时，必须要等图片加载完才能操作\n2. drawImage(image, x, y, width, height)  \n   其中 image 是 image 或者 canvas 对象，x 和 y 是其在目标 canvas 里的起始坐标。  \n   这个方法多了2个参数：width 和 height，这两个参数用来控制 当像canvas画入时应该缩放的大小。\n\n可以参考MDN文档了解img对象地各种属性\n\n![image-20200311011503912](20210410190610.png)\n\n# 在canvas中设置背景\n\ncreatePattern(image, repetition)\n\nimage:图像源  \nepetition:  \n\t\t\"repeat\"  \n\t\t\"repeat-x\"  \n\t\t\"repeat-y\"  \n\t\t\"no-repeat\"\n\n一般情况下，我们都会将createPattern返回的对象作为fillstyle的值。\n\n![image-20200311011708241](20210410190617.png)\n\n## 渐变\n\n### 线性渐变\n\n`createLinearGradient(x1, y1, x2, y2)`\n\n- 表示渐变的起点 (x1,y1) 与终点 (x2,y2)。\n\n`gradient.addColorStop(position, color)`\n\n- gradient :createLinearGradient的返回值\n- addColorStop 方法接受 2 个参数:\n  - position 参数必须是一个 0.0 与 1.0 之间的数值，表示渐变中颜色所在的相对位置。例如，0.5 表示颜色会出现在正中间。\n  - color 参数必须是一个有效的 CSS 颜色值（如\\#FFF ， rgba(0,0,0,1)，等等）\n\n![image-20200311012227883](20210410190625.png)\n\n### 径向渐变\n\n`createRadialGradient(x1, y1, r1, x2, y2, r2)`\n\n- 前三个参数则定义另一个以(x1,y1) 为原点，半径为 r1 的圆。\n- 后三个参数则定义另一个以 (x2,y2) 为原点，半径为 r2 的圆。\n\n![image-20200311012406701](20210410190631.png)\n\n![image-20200311012533748](20210410190634.png)\n\n# 文本相关\n\n## 绘制\n\ncanvas 提供了两种方法来渲染文本:\n\n- `fillText(text, x, y)`在指定的(x,y)位置填充指定文本\n- `strokeText(text, x, y)`在指定的(x,y)位置绘制文本边框\n\n文本样式  \nfont = value  \n当前我们用来绘制文本的样式. 这个字符串使用和 CSS font 属性相同的语法。  \n默认的字体是 10px sans-serif。font属性在指定时，必须要有大小和字体 缺一不可。\n\n![image-20200311013532482](20210410190744.png)\n\n`textAlign = value`  \n文本对齐选项,可选的值包括： left, right center.\n\n- left文本左对齐。\n- right文本右对齐。\n- center文本居中对齐。\n\n这里的textAlign=\"center\"比较特殊。textAlign的值为center时候文本的居中是基于你在fillText的时候所给的x的值，也就是说文本一半在x的左边，一半在x的右边。\n\n`textBaseline = value`  \n描述绘制文本时，当前文本基线的属性。\n\n- top文本基线在文本块的顶部。\n- middle文本基线在文本块的中间。\n- bottom文本基线在文本块的底部。\n\n### measureText\n\nmeasureText() 方法返回一个 TextMetrics 对象，包含关于文本尺寸的信息（例如文本的宽度）\n\n![image-20200311014220835](20210410190758.png)\n\n### 中文本水平垂直居中\n\n![image-20200311014526376](20210410190806.png)\n\n### 文本阴影\u0026盒阴影\n\n![image-20200313001036182](20210410190810.png)\n\n![image-20200313001128434](20210410190815.png)\n\n# 像素相关\n\n到目前为止，我们尚未深入了解Canvas画布真实像素的原理，事实上，你可以直接通过ImageData对象操纵像素数据，直接读取或将数据数组写入该对象中。\n\n## 得到场景像素数据\n\n`getImageData()`:获得一个包含画布场景像素数据的`ImageData`对像,它代表了画布区域的对象数据。\n\n`ctx.getImageData(sx, sy, sw, sh)`\n\n- sx:将要被提取的图像数据矩形区域的左上角 x 坐标。\n- sy:将要被提取的图像数据矩形区域的左上角 y 坐标。\n- sw:将要被提取的图像数据矩形区域的宽度。\n- sh:将要被提取的图像数据矩形区域的高度。\n\n![image-20200313001713085](20210410190818.png)\n\n`ImageData`对象中存储着`canvas`对象真实的像素数据，它包含以下几个只读属性：\n\n- width:图片宽度，单位是像素\n- height:图片高度，单位是像素\n- data:Uint8ClampedArray类型的一维数组，  \n  包含着RGBA格式的整型数据，范围在0至255之间（包括255）\n  - R:0 --\u003e 255(黑色到白色)\n  - G:0 --\u003e 255(黑色到白色)\n  - B:0 --\u003e 255(黑色到白色)\n  - A:0 --\u003e 255(透明到不透明)\n\n## 创建一个`ImageData`对象\n\n`ctx.createImageData(width, height);`\n\n- width : ImageData 新对象的宽度。\n- height: ImageData 新对象的高度。\n\n![image-20200313001646971](20210410190827.png)\n\n## 在场景中写入像素数据\n\n`putImageData()`方法去对场景进行像素数据的写入。  \n`putImageData(myImageData, dx, dy)`\n\n- dx和dy 参数表示你希望在场景内左上角绘制的像素数据所得到的设备坐标\n\n## 操作单个像素（行与列）\n\n![image-20200313002620658](20210410190837.png)\n\n两个操作函数,注意这里有老生常谈的xy计算问题。\n\n```javascript\nfunction getPxInfo(imgdata, x, y) {\n\tvar color = [];\n\tvar data = imgdata.data;\n\tvar w = imgdata.width;\n\tvar h = imgdata.height;\n\t//(x,y)  x*w+y\n\t//r\n\tcolor[0] = data[(y * w + x) * 4];\n\t//g\n\tcolor[1] = data[(y * w + x) * 4 + 1];\n\t//b\n\tcolor[2] = data[(y * w + x) * 4 + 2];\n\t//a\n\tcolor[3] = data[(y * w + x) * 4 + 3];\n\treturn color;\n}\nfunction setPxInfo(imgdata, x, y, color) {\n\tvar data = imgdata.data;\n\tvar w = imgdata.width;\n\tvar h = imgdata.height;\n\t//(x,y)  x*w+y   x:多少列  y：多少行\n\t//r\n\tdata[(y * w + x) * 4] = color[0];\n\t//g\n\tdata[(y * w + x) * 4 + 1] = color[1];\n\t//b\n\tdata[(y * w + x) * 4 + 2] = color[2];\n\t//a\n\tdata[(y * w + x) * 4 + 3] = color[3];\n}\n```\n\n# 实践2 给图片打马赛克\n\n![image-20200313005543527](20210410190847.png)\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\t\u003chead\u003e\n\t\t\u003cmeta charset=\"UTF-8\"\u003e\n\t\t\u003ctitle\u003e\u003c/title\u003e\n\t\t\u003cstyle type=\"text/css\"\u003e\n\t\t\t*{\n\t\t\t\tmargin: 0;\n\t\t\t\tpadding: 0;\n\t\t\t}\n\t\t\thtml,body{\n\t\t\t\theight: 100%;\n\t\t\t\toverflow: hidden;\n\t\t\t}\n\t\t\t#msk{\n\t\t\t\tposition: absolute;\n\t\t\t\tleft: 50%;\n\t\t\t\ttop: 50%;\n\t\t\t\ttransform: translate3d(-50%,-50%,0);\n\t\t\t\t/*background: gray;*/\n\t\t\t}\n\t\t\t\n\t\t\u003c/style\u003e\n\t\u003c/head\u003e\n\t\u003cbody\u003e\n\t\t\u003ccanvas id=\"msk\" \u003e\u003c/canvas\u003e\n\t\u003c/body\u003e\n\t\u003cscript type=\"text/javascript\"\u003e\n\t\tvar oc = document.querySelector(\"#msk\");\n\t\tif(oc.getContext){\n\t\t\tvar ctx = oc.getContext(\"2d\");\n\t\t\tvar img = new Image();\n\t\t\timg.src=\"2.png\";\n\t\t\timg.onload=function(){\n\t\t\t\toc.width=img.width*2;\n\t\t\t\toc.height=img.height;\n\t\t\t\tdraw();\n\t\t\t}\n\t\t\tfunction draw(){\n\t\t\t\tctx.drawImage(img,0,0);\n\t\t\t\tvar oldImgdata = ctx.getImageData(0,0,img.width,img.height);\n\t\t\t\tvar newImgdata = ctx.createImageData(img.width,img.height);\n\t\t\t\t//马赛克\n\t\t\t\t/*\n\t\t\t\t\t1.选取一个马赛克矩形\n\t\t\t\t\t2.从马赛克矩形中随机抽出一个像素点的信息(rgba)\n\t\t\t\t\t3.将整个马赛克矩形中的像素点信息统一调成随机抽出的那个\n\t\t\t\t*/\n\t\t\t\t//选取一个马赛克矩形\n\t\t\t\tvar size = 5;\n\t\t\t\tfor(var i=0;i\u003coldImgdata.width/size;i++){\n\t\t\t\t\tfor(var j=0;j\u003coldImgdata.height/size;j++){\n\t\t\t\t\t\t//(i,j)  每一个马赛克矩形的坐标\n\t\t\t\t\t\t//(0,0):  (0,0)  (4,4);[0,4]   //(1,0): (5,0) (9,4)\n\t\t\t\t\t\t//(0,1):  (0,5)  (4,9)\t \t   //(1,1): (5,5) (9,9)\n\t\t\t\t\t\t//Math.random()  [0,1)\n\t\t\t\t\t\t//Math.random()*size  [0,5)\n\t\t\t\t\t\t//Math.floor(Math.random()*size) [0,4]\n\t\t\t\t\t\t//从马赛克矩形中随机抽出一个像素点的信息(rgba)\n\t\t\t\t\t\tvar color = getPxInfo(oldImgdata,i*size+Math.floor(Math.random()*size),j*size+Math.floor(Math.random()*size));\n\t\t\t\t\t\t\n\t\t\t\t\t\t//将整个马赛克矩形中的像素点信息统一调成随机抽出的那个\n\t\t\t\t\t\tfor(var a=0;a\u003csize;a++){\n\t\t\t\t\t\t\tfor(var b=0;b\u003csize;b++){\n\t\t\t\t\t\t\t\tsetPxInfo(newImgdata,i*size+a,j*size+b,color)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// ctx.clearRect(0,0,oc.width,oc.height);\n\t\t\t\tctx.putImageData(newImgdata,0,0);\n\t\t\t}\n\t\t\tfunction getPxInfo(imgdata,x,y){\n\t\t\t\tvar color = [];\n\t\t\t\tvar data = imgdata.data;\n\t\t\t\tvar w = imgdata.width;\n\t\t\t\tvar h = imgdata.height;\n\t\t\t\tcolor[0]=data[(y*w+x)*4];\n\t\t\t\tcolor[1]=data[(y*w+x)*4+1];\n\t\t\t\tcolor[2]=data[(y*w+x)*4+2];\n\t\t\t\tcolor[3]=data[(y*w+x)*4+3];\n\t\t\t\treturn color;\n\t\t\t}\n\t\t\tfunction setPxInfo(imgdata,x,y,color){\n\t\t\t\tvar data = imgdata.data;\n\t\t\t\tvar w = imgdata.width;\n\t\t\t\tvar h = imgdata.height;\n\t\t\t\tdata[(y*w+x)*4]=color[0];\n\t\t\t\tdata[(y*w+x)*4+1]=color[1];\n\t\t\t\tdata[(y*w+x)*4+2]=color[2];\n\t\t\t\tdata[(y*w+x)*4+3]=color[3];\n\t\t\t}\n\t\t}\n\t\u003c/script\u003e\n\u003c/html\u003e\n\n```\n\n# 全局透明度\n\n`globalAlpha = value`  \n这个属性影响到 canvas 里所有图形的透明度，有效的值范围是 0.0 （完全透明）到 1.0（完全不透明）,默认是 1.0。\n\n![image-20200313010343599](20210410190854.png)\n\n# 合成\n\n`source`:新的图像(源)  \n`destination`:已经绘制过的图形(目标)\n\nglobalCompositeOperation\n\n- source-over(默认值):源在上面,新的图像层级比较高\n\n  ![image-20200313010436866](20210410190900.png)\n\n- source-in :只留下源与目标的重叠部分(源的那一部分)\n\n  ![image-20200313010755439](20210410190905.png)\n\n- source-out :只留下源超过目标的部分\n\n  ![image-20200313010742501](20210410190910.png)\n\n- source-atop:砍掉源溢出的部分\n\n  ![image-20200313010725818](20210410190917.png)\n\n- destination-over:目标在上面,旧的图像层级比较高\n\n  ![image-20200313010659761](20210410190922.png)\n\n- destination-in:只留下源与目标的重叠部分(目标的那一部分)\n\n  ![image-20200313010640123](20210410190927.png)\n\n- destination-out:只留下目标超过源的部分\n\n  ![image-20200313010617596](20210410190934.png)\n\n- destination-atop:砍掉目标溢出的部分\n\n  ![image-20200313010551400](20210410190938.png)\n\n# 实践3 刮刮卡\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\n\u003chead\u003e\n\t\u003cmeta charset=\"UTF-8\"\u003e\n\t\u003cmeta name=\"viewport\" content=\"width=device-width,initial-scale=1.0,user-scalable=no\"\u003e\n\t\u003c/meta\u003e\n\t\u003ctitle\u003e\u003c/title\u003e\n\t\u003cstyle type=\"text/css\"\u003e\n\t\t* {\n\t\t\tmargin: 0;\n\t\t\tpadding: 0;\n\t\t}\n\n\t\thtml,\n\t\tbody {\n\t\t\theight: 100%;\n\t\t\toverflow: hidden;\n\t\t}\n\n\t\t#wrap,\n\t\tul,\n\t\tul\u003eli {\n\t\t\theight: 100%;\n\t\t}\n\n\t\tul\u003eli {\n\t\t\tbackground: url(img/b.png);\n\t\t\tbackground-size: 100% 100%;\n\t\t}\n\n\t\tcanvas {\n\t\t\tposition: absolute;\n\t\t\tleft: 0;\n\t\t\ttop: 0;\n\t\t\ttransition: 1s;\n\t\t}\n\t\u003c/style\u003e\n\u003c/head\u003e\n\n\u003cbody\u003e\n\t\u003cdiv id=\"wrap\"\u003e\n\t\t\u003ccanvas\u003e\u003c/canvas\u003e\n\t\t\u003cul\u003e\n\t\t\t\u003cli\u003e\u003c/li\u003e\n\t\t\u003c/ul\u003e\n\t\u003c/div\u003e\n\u003c/body\u003e\n\u003cscript type=\"text/javascript\"\u003e\n\twindow.onload = function () {\n\t\tvar canvas = document.querySelector(\"canvas\");\n\t\tcanvas.width = document.documentElement.clientWidth;\n\t\tcanvas.height = document.documentElement.clientHeight;\n\t\tif (canvas.getContext) {\n\t\t\tvar ctx = canvas.getContext(\"2d\");\n\t\t\tvar img = new Image();\n\t\t\timg.src = \"img/a.png\";\n\t\t\timg.onload = function () {\n\t\t\t\tdraw();\n\t\t\t}\n\t\t\tfunction draw() {\n\t\t\t\tvar flag = 0;\n\t\t\t\tctx.drawImage(img, 0, 0, canvas.width, canvas.height);\n\t\t\t\tcanvas.addEventListener(\"touchstart\", function (ev) {\n\t\t\t\t\tev = ev || event;\n\t\t\t\t\tvar touchC = ev.changedTouches[0];\n\t\t\t\t\tvar x = touchC.clientX - canvas.offsetLeft;\n\t\t\t\t\tvar y = touchC.clientY - canvas.offsetTop;\n\t\t\t\t\tctx.globalCompositeOperation = \"destination-out\";\n\t\t\t\t\t//这里用destination-out，则每点击一下，只留下原来的图像除了新点的位置的部分\n\t\t\t\t\t//新点的也将消失，所以等价于每点一下则图片对应圆点消失，则只剩下背景。\n\t\t\t\t\tctx.lineWidth = 40;\n\t\t\t\t\tctx.lineCap = \"round\";\n\t\t\t\t\tctx.lineJoin = \"round\";\n\t\t\t\t\tctx.save();\n\t\t\t\t\tctx.beginPath();\n\t\t\t\t\t//\t\t\t\t\t\tctx.arc(x,y,20,0,360*Math.PI/180);\n\t\t\t\t\tctx.moveTo(x, y);\n\t\t\t\t\tctx.lineTo(x + 1, y + 1)\n\t\t\t\t\tctx.stroke();\n\t\t\t\t\tctx.restore();\n\t\t\t\t})\n\t\t\t\tcanvas.addEventListener(\"touchmove\", function (ev) {\n\t\t\t\t\tev = ev || event;\n\t\t\t\t\tvar touchC = ev.changedTouches[0];\n\t\t\t\t\tvar x = touchC.clientX - canvas.offsetLeft;\n\t\t\t\t\tvar y = touchC.clientY - canvas.offsetTop;\n\t\t\t\t\tctx.save();\n\t\t\t\t\t//\t\t\t\t\t\tctx.arc(x,y,20,0,360*Math.PI/180);\n\t\t\t\t\tctx.lineTo(x, y)\n\t\t\t\t\tctx.stroke();\n\t\t\t\t\tctx.restore();\n\t\t\t\t})\n\t\t\t\tcanvas.addEventListener(\"touchend\", function () {\n\t\t\t\t\tvar imgData = ctx.getImageData(0, 0, canvas.width, canvas.height)\n\t\t\t\t\tvar allPx = imgData.width * imgData.height;\n\t\t\t\t\tfor (var i = 0; i \u003c allPx; i++) {\n\t\t\t\t\t\tif (imgData.data[4 * i + 3] === 0) {\n\t\t\t\t\t\t\tflag++;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif (flag \u003e= allPx / 2) {\n\t\t\t\t\t\tcanvas.style.opacity = 0;\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t\tcanvas.addEventListener(\"transitionend\", function () {\n\t\t\t\t\tthis.remove();\n\t\t\t\t})\n\t\t\t}\n\t\t}\n\t}\n\u003c/script\u003e\n\u003c/html\u003e\n```\n\n# 导出为图像\n\ntoDataURL(注意是canvas元素接口上的方法)\n\n```javascript\nwindow.onload=function(){\n\t//querySelector身上有坑\n\t//拿到画布\n\tvar canvas = document.querySelector(\"#test\");\n\tif(canvas.getContext){\n\t\tvar ctx = canvas.getContext(\"2d\");\n\t\tctx.fillRect(0,0,199,199);\n\t\tvar result =  canvas.toDataURL();\n\t\tconsole.log(result);\n\t}\n}\n```\n\n![image-20200313013610527](20210410190947.png)\n\n# 事件操作\n\n`ctx.isPointInPath(x, y)`判断在当前路径中是否包含检测点\n\n- x:检测点的X坐标\n- y:检测点的Y坐标\n\n\u003e 注意！此方法只作用于最新一次性画出的canvas图像。\n\n![image-20200313014009126](20210410190953.png)\n","lastmodified":"2023-05-09T16:33:58.275366009Z","tags":[]},"/Dataview":{"title":"Dataview","content":"\n我的理解是将每一篇文章理解成一个数据库，借助编程语言可以实现高级自动化\n\n```\n```dataviewjs\nvar list=dv.pages(`\"Notes/报告\"`)\nfor(let e of list){\nif(e.title!==\"💽博文分享\"){\ndv.paragraph(`[[${e.title}]]`)\n}\n}\ndv.paragraph(`\n\n❕ 此目录由dataview自动生成`)\n\n```\n\n```dataviewjs\nvar list=dv.pages(`\"Notes/报告\"`)\nfor(let e of list){\nif(e.title!==\"💽博文分享\"){\ndv.paragraph(`[[${e.title}]]`)\n}\n}\ndv.paragraph(`\n\n❕ 此目录由dataview自动生成`)\n```\n\n数据自动化，js 无敌！可以有很多奇淫巧\n\n1. 获取所有的 tag！\n\n```\n```dataviewjs\n// 生成所有的标签且以 | 分割，修改时只需要修改 join(\" | \") 里面的内容。\ndv.paragraph(\n  dv.pages(\"\").file.tags.distinct().map(t =\u003e {return `[${t}](${t})`}).array().join(\" | \")\n)\n```\n\n```\n```\n","lastmodified":"2023-05-09T16:33:58.275366009Z","tags":["花园"]},"/Diff":{"title":"虚拟DOM和diff算法","content":"\n# 虚拟DOM和diff算法\n\n![流程](流程图.png)\n\n## snabbdom安装配置\n\nsnabbdom是瑞典语单词，单词原意“速度”,在IT方面是著名的虚拟DOM库，是diff算法的鼻祖，Vue源码就借鉴了snabbdom。官方git：https://github.com/snabbdom/snabbdom。\n\n在git上的snabbdom源码是用TypeScript写的，git上并不提供编译好的JavaScript版本，因此如果要直接使用build出来的JavaScript版的snabbdom库，可以从npm上下载：\n\n`npm i -D snabbdom`\n\nsnabbdom库是DOM库，当然不能在nodejs环境运行，所以我们需要搭建webpack和webpack-dev-server开发环境，好消息是不需要安装任何loader。\n\n\u003e 这里需要注意，必须安装最新版webpack@5，不能安装webpack@4，这是因为webpack4没有读取身份证中exports的能力。\n\n`npm i -D webpack@5 webpack-cli@3webpack-dev-server@3`\n\nwebpack.config.js配置为：\n\n```javascript\n// 从https://www.webpackjs.com/官网照着配置\nconst path = require('path');\n\nmodule.exports = {\n    // 入口\n    entry: './src/index.js',\n    // 出口\n    output: {\n        // 虚拟打包路径，就是说文件夹不会真正生成，而是在8080端口虚拟生成\n        publicPath: 'xuni',\n        // 打包出来的文件名，不会真正的物理生成\n        filename: 'bundle.js'\n    },\n    devServer: {\n        // 端口号\n        port: 8080,\n        // 静态资源文件夹\n        contentBase: 'www'\n    }\n};\n```\n\n相应的index.html为：\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n    \u003cmeta charset=\"UTF-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\n    \u003ctitle\u003eDocument\u003c/title\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n    \u003cbutton id=\"btn\"\u003e按我改变DOM\u003c/button\u003e\n    \u003cdiv id=\"container\"\u003e\u003c/div\u003e\n    \n    \u003cscript src=\"/xuni/bundle.js\"\u003e\u003c/script\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n```\n\n## 虚拟DOM简介\n\n真实DOM和虚拟DOM的区别：\n\n![image-20210627135248024](image-20210627135248024.png)\n\ndiff自然是发生在虚拟DOM上的：\n\n![image-20210627135317534](image-20210627135317534.png)\n\n\u003e 关于DOM如何变为虚拟DOM，属于模板编译原理范畴，不在本笔记学习范围之内\n\n## 生成虚拟DOM（h函数）\n\nh函数用来产生**虚拟节点（vnode）**，比如这样调用h函数：\n\n`h('a',{ props:{ href:'http://www.baidu.com'}},'百度')`\n\n将得到这样的虚拟节点：\n\n`{\"sel\":\"a\",\"data\":{props:{href:'http://www.baidu.com'}},\"text\":\"百度\"}`\n\n渲染回页面后真实的DOM为：\n\n`\u003cahref=\"http://www.baidu.com\"\u003e百度\u003c/a\u003e`\n\n\u003chr/\u003e\n\n一个虚拟节点往往有这些属性：\n\n```json\n{\n    children:undefined,\n    data:{},\n\telm:undefined,\n\tkey:undefined,\n\tsel:\"div\",\n\ttext:\"我是一个盒子\"\n}\n```\n\n```javascript\n// 函数的功能非常简单，就是把传入的5个参数组合成对象返回\nexport default function(sel, data, children, text, elm) {\n    const key = data.key;\n    return {\n        sel, data, children, text, elm, key\n    };\n}\n```\n\n**h函数可以嵌套使用，从而得到虚拟DOM树:**\n\n![image-20210627140256839](image-20210627140256839.png)\n\n![image-20210627140309003](image-20210627140309003.png)\n\n**手写h函数**\n\n```javascript\n// 编写一个低配版本的h函数，这个函数必须接受3个参数，缺一不可\n// 相当于它的重载功能较弱。\n// 也就是说，调用的时候形态必须是下面的三种之一：\n// 形态① h('div', {}, '文字')\n// 形态② h('div', {}, [])\n// 形态③ h('div', {}, h())\nexport default function (sel, data, c) {\n    // 检查参数的个数\n    if (arguments.length != 3)\n        throw new Error('对不起，h函数必须传入3个参数，我们是低配版h函数');\n    // 检查参数c的类型\n    if (typeof c == 'string' || typeof c == 'number') {\n        // 说明现在调用h函数是形态①\n        return vnode(sel, data, undefined, c, undefined);\n    } else if (Array.isArray(c)) {\n        // 说明现在调用h函数是形态②\n        let children = [];\n        // 遍历c，收集children\n        for (let i = 0; i \u003c c.length; i++) {\n            // 检查c[i]必须是一个对象，如果不满足\n            if (!(typeof c[i] == 'object' \u0026\u0026 c[i].hasOwnProperty('sel')))\n                throw new Error('传入的数组参数中有项不是h函数');\n            // 这里不用执行c[i]，因为你的测试语句中已经有了执行\n            // 此时只需要收集好就可以了\n            children.push(c[i]);\n        }\n        // 循环结束了，就说明children收集完毕了，此时可以返回虚拟节点了，它有children属性的\n        return vnode(sel, data, children, undefined, undefined);\n    } else if (typeof c == 'object' \u0026\u0026 c.hasOwnProperty('sel')) {\n        // 说明现在调用h函数是形态③\n        // 即，传入的c是唯一的children。不用执行c，因为测试语句中已经执行了c。\n        let children = [c];\n        return vnode(sel, data, children, undefined, undefined);\n    } else {\n        throw new Error('传入的第三个参数类型不对');\n    }\n};\n```\n","lastmodified":"2023-05-09T16:33:58.275366009Z","tags":[]},"/Docker%E5%9F%BA%E7%A1%80":{"title":"Docker基础","content":"\n## 1 Docker简介\n\n### 1.1 为什么会有docker？\n\n一款产品从开发到上线，从操作系统，到运行环境，再到应用配置。作为开发+运维之间的协作我们需要关心很多东西，这也是很多互联网公司都不得不面对的问题，特别是各种版本的迭代之后，不同版本环境的兼容，对运维人员都是考验  \n**Docker**之所以发展如此迅速，也是因为它对此给出了一个标准化的解决方案。  \n环境配置如此麻烦，换一台机器，就要重来一次，费力费时。很多人想到，能不能从根本上解决问题，软件可以带环境安装?也就是说，安装的时候，把原始环境一模一样地复制过来。开发人员利用Docker可以消除协作编码时“在我的机器上可正常工作”的问题。  \n![image-20211129163643863](image-20211129163643863.png)  \n之前在服务器配置一个应用的运行环境，要安装各种软件，就拿尚硅谷电商项目的环境来说，**Java/TomcatMySQL/JDBC**驱动包等。安装和配置这些东西有多麻烦就不说了，它还不能跨平台。假如我们是在**Windows**上安装的这些环境，到了Linux 又得重新装。况且就算不跨操作系统，换另一台同样操作系统的服务器，要移植应用也是非常麻烦的。\n\n传统上认为，软件编码开发/测试结束后，所产出的成果即是程序或是能够编译执行的二进制字节码等java为例)。而为了让这程序可以顺利执行，开发团队也得准备完整的部署文件，让维运团队得以部署应用程式，**开发需要清楚的告诉运维部署团队，用的全部配置文件+所有软件环境。不过，即便如此，仍然常常发生部署失败的状况。**Docker镜 像的设计**，使得Docker得以打过去「程序即应用」的观念。透过镜像(images)将作业系统核心除外，运作应用程式所需要的系统环境，由下而上打包，达到应用程式跨平台间的无缝接轨运.作。**\n\n### 1.2 docker理念\n\nDocker是基于Go语言实现的云开源项目。  \nDocker的主要目标是“**Build, Ship[ and Run Any App,Anywhere**\"，也就是通过对应用组件的封装、分发、部署、运行等生命期的管理，使用户的APP (可以是一个WEB应用或数据库应用等等)及其运行环境能够做到“**一次封装，到处运行**”。\n\n![image-20211129163840868](image-20211129163840868.png)\n\nLinux容器技术的出现就解决了这样一 一个问题，而Docker就是在它的基础上发展过来的。将应用运行在Docker容器上面，而Docker容器在任何操作系统上都是一-致的，这就实现了跨平台、跨服务器。**只需要一次配置好环境，换到别的机子上就可以一键部署好，大大简化了操作**\n\n**总结：Docker解决了运行环境和配置问题的软件容器，方便做持续集成并有助于整体发布的容器虚拟化技术**\n\n### 1.3 Docker可以做什么\n\n#### 1.3.1 虚拟机技术\n\n虚拟机**(virtual machine)**就是带环境安装的一种解决方案。\n\n它可以在一种操作系统里面运行另一种作系统，比如在**Windows系统里面运行Linux系统**。应用程序对此毫无感知，因为虚拟机看上去跟真实系统一模一样，而对于底层系统来说，虚拟机就是一个普通文件，不需要了就删掉，对其他部分毫无影响。这类虚拟机完美的运行了另一套系统，能够使应用程序，操作系统和硬件三者之间的逻辑不变。\n\n![image-20211129163953230](image-20211129163953230.png)\n\n但是有一定的缺点：\n\n- 资源占用多\n- 冗余步骤多\n- 启动慢\n\n#### 1.3.2 容器虚拟化技术\n\n由于前面虛拟机存在这些缺点，**Linux** 发展出了另一种虚拟化技术: **Linux 容器**(Linux Containers,缩为LXC)。\n\n**Linux容器不是模拟一个完整的操作系统**，而是对进程进行隔离。有了容器，就可以将软件运行所的所有资源打包到一个隔离的容器中。容器与虚拟机不同，不需要捆绑一整套操作系统，只需要软件工作所需的库资源和设置。系统因此而变得高效轻量并保证部署在任何环境中的软件都能始终如一地运行。\n\n![image-20211129164118311](image-20211129164118311.png)\n\n**Docker**和传统虚拟化方式的不同之处:\n\n1. 传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程;\n2. 而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，**而且也没有进行硬件虚拟**。因此容器要比传统虚拟机为轻便。\n3. 每个容器之间互相隔离，每个容器有自己的文件系统，容器之间进程不会相互影响，能区分计算资源。\n\n#### 1.3.3 开发/运维(DevOps)\n\n一次构建、随处运行。\n\n**更快速的应用交付和部署**\n\n传统的应用开发完成后，需要提供一堆安装程序和配置说明文档，安装部署后需根据配置文档进行繁杂的配置才能正常运行。Docker化  \n之后只需要交付少量容器镜像文件，在正式生产环境加载镜像并运行即可，应用安装配置在镜像里已经内置好，大大节省部署配置和测  \n试验证时间。\n\n**更便捷的升级和扩缩容**\n\n随着微服务架构和Docker的发展，大量的应用会通过微服务方式架构，应用的开发构建将变成搭乐高积木一样，每个Docker容器将变成-块“积木”，应用的升级将变得非常容易。当现有的容器不足以支撑业务处理时，可通过镜像运行新的容器进行快速扩容，使应用系统的扩容从原先的天级变成分钟级甚至秒级。\n\n**更简单的系统运维**\n\n应用容器化运行后，生产环境运行的应用可与开发、测试环境的应用高度--致，容器会将应用程序相关的环境和状态完全封装起来，不会因为底层基础架构和操作系统的不一致性给应用带来影响，产生新的BUG。当出现程序异常时，也可以通过测试环境的相同容器进行快速定位和修复。\n\n**更高效的计算资源利用**\n\n**Docker是内核级虚拟化**，其不像传统的虚拟化技术一样 需要额外的Hypervisor支持，所以在一台物理机上可以运行很多个容器实例，可大大提升物理服务器的CPU和内存的利用率。\n\n### 1.4 Docker基本结构\n\n![img](architecture.svg)\n\n#### 1.4.1 镜像（image）\n\n镜像(lmage)就是一个只读的模板。镜像可以用来创建Docker容器，一个镜像可以创建很多容器。\n\n![image-20211129165021803](image-20211129165021803.png)\n\n#### 1.4.2 容器( container)\n\nDocker利用容器(Container) 独立运行的一个或一组应用。**容器是用镜像创建的运行实例。**  \n它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。  \n**可以把容器看做是一个简易版的Linux环境**(包括root用户权限、进程空间、用户空间和网络空间等)和运行在其中的应用程序。  \n容器的定义和镜像几乎一模一样，也是一堆层的统一视角， 唯一区别在于容器的最上面那一层是可读可写的。\n\n#### 1.4.3 仓库( repository)\n\n仓库(**Repository**) 是**集中存放镜像**文件的场所。  \n仓库(**Repository**)和仓库注册服务器(**Registry**) 是有区别的。仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多镜像，  \n每个镜像有不同的标签(tag) 。\n\n仓库分为公开仓库(**Public**) 和私有仓库(**Private**) 两种形式。  \n**最大的公开仓库是Docker Hub(https://hub.docker.com/)**  \n存放了数量庞大的镜像供用户下载。国内的公开仓库包括阿里云、网易云等。\n\n#### 1.4.4 总结\n\n需要正确的理解仓储/镜像/容器这几个概念:  \nDocker本身是一个容器运行载体或称之为管理引擎。我们把应用程序和配置依赖打包好形成一-个可交付的运行环境，这个打好的运行环境就似乎image镜像文件。只有通过这个镜像文件才能生成Docker容器。image文件可以看作是容器的模板。Docker根据image文件生成容器的实例。同一个image文件，可以生成多个同时运行的容器实例。  \nimage文件生成的容器实例，本身也是一一个文件，称为镜像文件。  \n一个容器运行一种服务，当我们需要的时候，就可以通过docker客户端创建一-个对应的运行实例，也就是我们的容器至于仓储，就是放了一堆镜像的地方，我们可以把镜像发布到仓储中，需要的时候从仓储中拉下来就可以了。\n\n#### 1.5 底层原理\n\nDocker是一个Client-Server结构的系统，Docker守 护进程运行在主机上，然后通过Socket连 接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。**容器，是一个运行时环境，就是我们前面说到的集装箱。**  \n![image-20211129171424470](image-20211129171424470.png)\n\n**为什么Docker比较比vm快**\n\n1. docker**有着比虚拟机更少的抽象层。由亍docker不需要**Hypervisor**实现硬件资源虚拟化,运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。因此在CPU、内存利用率上docker将会在效率上有明显优势。**\n2. docker**利用的是宿主机的内核,而不需要**Guest OS。因此,当新建一个 容器时,docker不需要和虚拟机一样 重新加载- - 个操作系统内核仍而避免引寻、加载操作系统内核返个比较费时费资源的过程,当新建--个虚拟机时,虚拟机软件需要加载GuestOS,返个新建过程是分钟级别的。而docker由于直接利用宿主机的操作系统,则省略了返个过程,因此新建一-个docker容器只需要几秒钟。\n\n![image-20211129171508504](image-20211129171508504.png)\n\n## 2 安装与Helloworld\n\n\u003e **CentOS Docker安装**  \n\u003e Docker支持以下的CentOS版本:  \n\u003e CentOS 7 (64-bit)  \n\u003e CentOS 6.5 (64-bit)或更高的版本\n\u003e\n\u003e **前提条件**  \n\u003e 目前，CentOS 仅发行版本中的内核支持Docker。  \n\u003e Docker运行在CentOS 7.上，要求系统为64位、系统内核版本为3.10以上。  \n\u003e Docker运行在CentOS-6.5或更高的版本的CentOS上，要求系统为64位、系**统内核版本为2.6.32-431或者更高版本。**\n\u003e\n\u003e ![image-20211129164440659](image-20211129164440659.png)\n\n具体安装操作日新月异，参考官网文档、网上博客即可。https://docs.docker.com/engine/install/\n\n### 2.1 镜像加速\n\n#### 阿里云镜像加速\n\n是什么？https://promotion.aliyun.com/ntms/act/kubernetes.html\n\n1. 注册一个属于自己的阿里云账户( 可复用淘宝账号)\n2. 获得加速器地址连接\n\n   登录阿里云开发者平台获取加速器地址\n\n3. 配置本机Docker运行镜像加速器\n\n   鉴于国内网络问题，后续拉取Docker镜像十分缓慢，我们可以需要配置加速器来解决，  \n   我使用的是阿里云的本人自己账号的镜像地址(需要自己注册有一个属于你自己的): ht:po/. mirror aliyuncns .com\n\n- vim /etc/sysconfig/docker  \n  将获得的自己账户下的阿里云加速地址配置进  \n  other_ args-=\"--registry-mirror=https://你自己的账号加速信息.mirror .aliyuncs.com\n\n![](Snipaste_2020-10-02_20-19-10.png)\n\n重新启动 Docker 后台服务：service docker restart\n\nLinux系统下配置完加速器需要检查是否生效\n\n此外还有网易云加速等。。\n\n### 2.2 helloworld\n\n启动Docker后台容器(测试运行 hello-world )\n\n`docker run hello-world`\n\n## 3 Docker常用命令\n\n### 3.1 帮助类命令\n\n- 查看配置信息 `docker info`\n- 帮助 `docker --help`\n\n### 3.2 镜像命令\n\n#### 1.列出所有images\n\n`docker images`\n\n```dockerfile\n-a 列出本地所有的镜像(含中间映射层)\n-q 只显示镜像ID\n--digests 显示镜像的摘要信息\n--no-trunc 显示完整的镜像信息\n```\n\n\u003cimg src=\"../../pics/image-20211129192629426.png\" alt=\"image-20211129192629426\" style=\"zoom:75%;\" /\u003e\n\n#### 2.搜索image\n\n`docker search [OPTIONS] 镜像名字`\n\n```dockerfile\n-f, --filter filter   Filter output based on conditions provided\n    --format string   Pretty-print search using a Go template\n    --limit int       列出收藏数不小于指定值的镜像 (default 25)\n    --no-trunc        Don't truncate output\n```\n\n#### 3.下载image\n\n`docker pull [OPTIONS] NAME[:TAG|@DIGEST]`\n\n```dockerfile\n-a, --all-tags                Download all tagged images in the repository\n    --disable-content-trust   Skip image verification (default true)\n    --platform string         Set platform if server is multi-platform capable\n-q, --quiet                   Suppress verbose output\n```\n\n\u003e 默认为 NAME:latest\n\n#### 4.删除image\n\n`docker rmi [OPTIONS] IMAGE [IMAGE…]`\n\n```dockerfile\n-f, --force      Force removal of the image\n    --no-prune   Do not delete untagged parents\n```\n\n删除单个 docker rm -f 镜像ID\n\n删除多个 docker rm -f 镜像名1:TAG 镜像名2:TAG\n\n删除多个 docker rmi -f ${docker images -qa}\n\n#### 5.提交image\n\n`docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]`\n\ne.g.`docker commit -m=\"提交的描述信息\" -a=\"作者\" 容器ID 要创建的目标镜像名:[标签名]`\n\n```dockerfile\n-a, --author string    Author (e.g., \"John Hannibal Smith \u003channibal@a-team.com\u003e\")\n-c, --change list      Apply Dockerfile instruction to the created image\n-m, --message string   Commit message\n-p, --pause            Pause container during commit (default true)\n```\n\n\u003e 1. 故意删除上一步镜像生产tomcat容器的文档\n\u003e\n\u003e    ![image-20211130140543011](image-20211130140543011.png)\n\u003e\n\u003e 2. 也即当前的tomcat运行实例是一个没有文档内容的容器，以他为模板commit一个没有doc的tomcat新镜像 atguigu/tomcat02\n\u003e\n\u003e    ![image-20211130140601407](image-20211130140601407.png)\n\n### 3.3 容器命令\n\n\u003e  有镜像才能创建容器，这是根本前提(下载一个Centos镜像演示)\n\u003e\n\u003e `docker pull centos`\n\n#### 1.新建并启动容器\n\n`docker run [OPTIONS] IMAGE [COMMAND] [ARG…]`\n\ne.g.`docker run -it -p 8888:8080 tomcat`\n\n\u003e 8888为外主机端口\n\n```dockerfile\n# !\n-d, --detach                         Run container in background and print container ID\n\t\t--name string                    Assign a name to the container\n-i, --interactive                    Keep STDIN open even if not attached\n-t, --tty                            Allocate a pseudo-TTY\n-p, --publish list                   Publish a container's port(s) to the host\n\t#指定端口映射，有以下四种格式\n\t#ip:hostPort:containerPort\n\t#ip::containerPort\n\t#hostPort:containerPort\n\t#containerPort\n-P, --publish-all                    Publish all exposed ports to random ports 随机端口映射\n#\n      \n    --add-host list                  Add a custom host-to-IP mapping (host:ip)\n-a, --attach list                    Attach to STDIN, STDOUT or STDERR\n    --blkio-weight uint16            Block IO (relative weight), between 10 and 1000, or 0 to\n                                     disable (default 0)\n    --blkio-weight-device list       Block IO weight (relative device weight) (default [])\n    --cap-add list                   Add Linux capabilities\n    --cap-drop list                  Drop Linux capabilities\n    --cgroup-parent string           Optional parent cgroup for the container\n    --cgroupns string                Cgroup namespace to use (host|private)\n                                     'host':    Run the container in the Docker host's cgroup\n                                     namespace\n                                     'private': Run the container in its own private cgroup\n                                     namespace\n                                     '':        Use the cgroup namespace as configured by the\n                                                default-cgroupns-mode option on the daemon (default)\n    --cidfile string                 Write the container ID to the file\n    --cpu-period int                 Limit CPU CFS (Completely Fair Scheduler) period\n    --cpu-quota int                  Limit CPU CFS (Completely Fair Scheduler) quota\n    --cpu-rt-period int              Limit CPU real-time period in microseconds\n    --cpu-rt-runtime int             Limit CPU real-time runtime in microseconds\n-c, --cpu-shares int                 CPU shares (relative weight)\n    --cpus decimal                   Number of CPUs\n    --cpuset-cpus string             CPUs in which to allow execution (0-3, 0,1)\n    --cpuset-mems string             MEMs in which to allow execution (0-3, 0,1)\n    --detach-keys string             Override the key sequence for detaching a container\n    --device list                    Add a host device to the container\n    --device-cgroup-rule list        Add a rule to the cgroup allowed devices list\n    --device-read-bps list           Limit read rate (bytes per second) from a device (default [])\n    --device-read-iops list          Limit read rate (IO per second) from a device (default [])\n    --device-write-bps list          Limit write rate (bytes per second) to a device (default [])\n    --device-write-iops list         Limit write rate (IO per second) to a device (default [])\n    --disable-content-trust          Skip image verification (default true)\n    --dns list                       Set custom DNS servers\n    --dns-option list                Set DNS options\n    --dns-search list                Set custom DNS search domains\n    --domainname string              Container NIS domain name\n    --entrypoint string              Overwrite the default ENTRYPOINT of the image\n-e, --env list                       Set environment variables\n    --env-file list                  Read in a file of environment variables\n    --expose list                    Expose a port or a range of ports\n    --gpus gpu-request               GPU devices to add to the container ('all' to pass all GPUs)\n    --group-add list                 Add additional groups to join\n    --health-cmd string              Command to run to check health\n    --health-interval duration       Time between running the check (ms|s|m|h) (default 0s)\n    --health-retries int             Consecutive failures needed to report unhealthy\n    --health-start-period duration   Start period for the container to initialize before starting\n                                     health-retries countdown (ms|s|m|h) (default 0s)\n    --health-timeout duration        Maximum time to allow one check to run (ms|s|m|h) (default 0s)\n    --help                           Print usage\n-h, --hostname string                Container host name\n    --init                           Run an init inside the container that forwards signals and reaps processes\n    --ip string                      IPv4 address (e.g., 172.30.100.104)\n    --ip6 string                     IPv6 address (e.g., 2001:db8::33)\n    --ipc string                     IPC mode to use\n    --isolation string               Container isolation technology\n    --kernel-memory bytes            Kernel memory limit\n-l, --label list                     Set meta data on a container\n    --label-file list                Read in a line delimited file of labels\n    --link list                      Add link to another container\n    --link-local-ip list             Container IPv4/IPv6 link-local addresses\n    --log-driver string              Logging driver for the container\n    --log-opt list                   Log driver options\n    --mac-address string             Container MAC address (e.g., 92:d0:c6:0a:29:33)\n-m, --memory bytes                   Memory limit\n    --memory-reservation bytes       Memory soft limit\n    --memory-swap bytes              Swap limit equal to memory plus swap: '-1' to enable\n                                     unlimited swap\n    --memory-swappiness int          Tune container memory swappiness (0 to 100) (default -1)\n    --mount mount                    Attach a filesystem mount to the container\n    --network network                Connect a container to a network\n    --network-alias list             Add network-scoped alias for the container\n    --no-healthcheck                 Disable any container-specified HEALTHCHECK\n    --oom-kill-disable               Disable OOM Killer\n    --oom-score-adj int              Tune host's OOM preferences (-1000 to 1000)\n    --pid string                     PID namespace to use\n    --pids-limit int                 Tune container pids limit (set -1 for unlimited)\n    --platform string                Set platform if server is multi-platform capable\n    --privileged                     Give extended privileges to this container\n    --pull string                    Pull image before running (\"always\"|\"missing\"|\"never\")\n                                     (default \"missing\")\n    --read-only                      Mount the container's root filesystem as read only\n    --restart string                 Restart policy to apply when a container exits (default \"no\")\n    --rm                             Automatically remove the container when it exits\n    --runtime string                 Runtime to use for this container\n    --security-opt list              Security Options\n    --shm-size bytes                 Size of /dev/shm\n    --sig-proxy                      Proxy received signals to the process (default true)\n    --stop-signal string             Signal to stop a container (default \"SIGTERM\")\n    --stop-timeout int               Timeout (in seconds) to stop a container\n    --storage-opt list               Storage driver options for the container\n    --sysctl map                     Sysctl options (default map[])\n    --tmpfs list                     Mount a tmpfs directory\n    --ulimit ulimit                  Ulimit options (default [])\n-u, --user string                    Username or UID (format: \u003cname|uid\u003e[:\u003cgroup|gid\u003e])\n    --userns string                  User namespace to use\n    --uts string                     UTS namespace to use\n-v, --volume list                    Bind mount a volume\n    --volume-driver string           Optional volume driver for the container\n    --volumes-from list              Mount volumes from the specified container(s)\n-w, --workdir string                 Working directory inside the container\n```\n\n#### 2.列出当前的容器\n\n`docker ps [OPTIONS]`\n\n```bash\n-a, --all             列出当前所有正在运行的容器+历史上运行过的\n-f, --filter filter   Filter output based on conditions provided\n    --format string   Pretty-print containers using a Go template\n-n, --last int        显示最近n个创建的容器 (includes all states) (default -1)\n-l, --latest          显示最近创建的容器 (includes all states)\n    --no-trunc        不截断输出\n-q, --quiet           静默模式，只显示容器编号IDs\n-s, --size            Display total file sizes\n```\n\n\u003e 使用镜像centos:latest以后台模式启动一个容器  \n\u003e docker run -d centos\n\u003e\n\u003e 问题:然后docker ps -a进行查看,**会发现容器已经退出**  \n\u003e 很重要的要说明的一点: **Docker容器后台运行,就必须有一个前台进程.**  \n\u003e 容器运行的命令如果不是那些**一直挂起的命令** (比如运行top，tail) ，就是会自动退出的。  \n\u003e 这个是**docker**的机制问题,比如你的web容器，我们以**nginx**为例，正常情况下,我们配置启动服务只需要启动响应的**service**即可。例如  \n\u003e service nginx start  \n\u003e 但是,这样做,**nginx**为后台进程模式运行,就导致**docker**前台没有运行的应用,这样的容器后台启动后，会立即自杀因为他觉得他没事可做了.所以，最佳的解决方案是将你要运行的程序以前台进程的形式运行\n\n#### 3.退出容器\n\n`exit` 容器停止退出\n\n`ctrl+P+Q` 容器不停止退出\n\n#### 4.启动容器\n\n`docker start 容器ID或容器签名`\n\n#### 5.重启容器\n\n`docker restart 容器ID或容器签名`\n\n#### 6.停止与强制停止容器\n\n`docker stop 容器ID或容器签名`\n\n`docker kill 容器ID或容器签名`\n\n#### 7.删除已停止的容器\n\n`docker rm 容器ID  -f`\n\n\u003e 一次性删除多个容器\n\u003e\n\u003e docker rm -f $(docker ps -a -q)\n\u003e\n\u003e docker ps -a -q | xargs docker rm\n\n#### 8.查看容器日志\n\n`docker logs -f -t --tail 容器ID`\n\n```dockerfile\n    --details        Show extra details provided to logs\n-f, --follow         Follow log output 跟随最新的日志打印\n    --since string   Show logs since timestamp (e.g. 2013-01-02T13:23:37Z) or relative (e.g.\n                     42m for 42 minutes)\n-n, --tail string    数字显示最后多少条 (default \"all\")\n-t, --timestamps     是加入时间戳\n    --until string   Show logs before a timestamp (e.g. 2013-01-02T13:23:37Z) or relative\n                       (e.g. 42m for 42 minutes)\n```\n\n#### 9.查看容器内进程\n\n`docker top CONTAINER [ps OPTIONS]`\n\n#### 10.查看容器内部细节\n\n`docker inspect 容器ID`\n\n```dockerfile\n-f, --format string   Format the output using the given Go template\n-s, --size            Display total file sizes if the type is container\n    --type string     Return JSON for specified type\n```\n\n#### 11.进入正在运行的容器并以命令行交互\n\n`docker exec [OPTIONS] CONTAINER COMMAND [ARG…]`\n\ne.g.`docker exec -it 容器ID bashShell`\n\n```dockerfile\n-d, --detach               Detached mode: run command in the background\n    --detach-keys string   Override the key sequence for detaching a container\n-e, --env list             Set environment variables\n    --env-file list        Read in a file of environment variables\n-i, --interactive          Keep STDIN open even if not attached\n    --privileged           Give extended privileges to the command\n-t, --tty                  Allocate a pseudo-TTY\n-u, --user string          Username or UID (format: \u003cname|uid\u003e[:\u003cgroup|gid\u003e])\n-w, --workdir string       Working directory inside the container\n```\n\n`docker attach [OPTIONS] CONTAINER`\n\n```dockerfile\n--detach-keys string   Override the key sequence for detaching a container\n--no-stdin             Do not attach STDIN\n--sig-proxy            Proxy all received signals to the process (default true)\n```\n\nattach 直接进入容器启动命令的终端，不会启动新的进程\n\nexec 是在容器中打开新的终端，并且可以启动新的进程\n\n#### 12.从容器内拷贝文件\n\n`docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|-`\n\n`docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH`\n\ne.g.`docker cp 容器ID:容器内路径 目的主机路径`\n\n\u003e Use '-' as the source to read a tar archive from stdin and extract it to a directory destination in a container.\n\u003e\n\u003e Use '-' as the destination to stream a tar archive of a container source to stdout.\n\n```dockerfile\n-a, --archive       Archive mode (copy all uid/gid information)\n-L, --follow-link   Always follow symbol link in SRC_PATH\n```\n\n![image-20211130134642659](image-20211130134642659.png)\n\n#### 小总结\n\n![image-20211130134709798](image-20211130134709798.png)\n\n![image-20211130134714298](image-20211130134714298-8251237.png)\n\n## 4 Docker镜像\n\n镜像是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的有内容，包括代码、运行时、库、环境变量和配置文件。\n\n### 4.1 UnionFS(联合文件系统)\n\nUnionFS (联合文件系统) : Union文件系统(UnionFS)是一一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修作为一 次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a singlevirtualfilesystem)。Union文件系统是Docker镜像的基础。镜像可以通过分层来进行继承，基于基础镜像(没有父镜像)可以制作各种具.体的应用镜像。\n\n特性:一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。\n\n### 4.2 镜像加载原理\n\n**docker**的镜像实际上由一层一层的文件系统组成，这种层级的文件系统**UnionFS。**\n\n**botfs(boot file system)**主要包含**bootloader**和**kernel**, **bootloader**主 要是引导加载**kernel**, **Linux**刚启动时会加载bootfs文件系统，在**Docker**镜像的最底层是**bootfs**。这一-层与我们典型的**Linux/Unix**系统是一样的，包含boot加载器和内核。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权己由bootfs转交给内核，此时系统也会卸载bootfs。\n\n**rootfs (root file system)，**在**bootfs**之 上。 包含的就是典型Linux系统中的**/dev, /proc, /bin, /etc**等标准目录和文件。**rootfs**就是各种不同的操作系统发行版，比如**Ubuntu**，**Centos**等等。\n\n![image-20211130135141474](image-20211130135141474.png)\n\n\u003e 平时我们安装的虚拟机的Centos都是好几个G ，为什么docker这里才要200m?\n\u003e\n\u003e 对于一个精简的**OS, rootfs**可 以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用**Host**的**kernel**,自只需要提供rootfs就行了。由此可见对于不同的**linux**发行版, **bootfs**基本是一致的, **rootfs**会有差别，因此不同的发行版可以公用**bootfs**。\n\n**分层的镜像**\n\n\u003cimg src=\"../../pics/image-20211130135409596.png\" alt=\"image-20211130135409596\" style=\"zoom:50%;\" /\u003e\n\nDocker镜像都是只读的，当容器启动时，一个新的可写层被加载到**镜像的顶部**，这一层通常被称为**容器层**，容器层之下都叫**镜像层**。\n\n## 5 Docker容器数据卷\n\ndocker将运用与运行的环境打包形成容器运行，运行可以伴随着容器，但是我们对数据的要求希望是持久化的.但是docker容器产生的数据，如果不通过`docker commit`生成新的镜像，使得数据做为镜像的一部分保存下来，那么当容器删除后，数据自然也就没有了。\n\n为了能保存数据在docker中我们使用卷:卷就是目录或文件，存在于一个或多个容器中，由**docker**挂载到容器，但不属于联合文件系统，因此能够绕过Union FileSystem提供一些用于持续存储或共享数据的特性:卷的设计目的就是数据的持久化，完全独立于容器的生存周期，因此Docker不会在容器删除时删除其挂载的数据卷。\n\n特点:\n\n1. 数据卷可在容器之间共享或重用数据\n2. 卷中的更改可以直接生效\n3. 数据卷中的更改不会包含在镜像的更新中\n4. 数据卷的生命周期一直持续到没有容器使用它为止\n\n### 5.1 添加数据卷\n\n#### 5.1.1 容器内添加\n\n##### 直接命令添加\n\n`docker run -it -v /宿主机绝对路径目录:/容器内目录 镜像名`\n\ne.g. `docker run -it -v /docttt:/doct centos`\n\n\u003e 执行后centos中会新建doct目录，宿主机会有docttt目录，但本人使用mac，发现/根目录下并没有相关目录，换到个人文件夹后则出现\n\n若带权限：\n\n`docker run -it -v /宿主机绝对路径目录:/容器内目录:ro 镜像名`\n\n*查看数据卷是否挂载成功*\n\n![image-20211201144342675](image-20211201144342675.png)\n\n*容器和宿主机之间数据共享*\n\n![image-20211201144404872](image-20211201144404872.png)\n\n*容器停止退出后，主机修改后的数据是否同步*\n\n![image-20211201144418165](image-20211201144418165.png)\n\n##### DockerFile添加\n\n在dockerfile中使用VOLUME指令来给镜像添加一个或多个数据卷：\n\n`VOLUME[\"/dataVolumeContainer\",\"dataVolumeContainer2\",\"/dataVolumeContainer3\"]`\n\n\u003e 因为出于可移植和分享的考虑，用-v 主机目录:容器目录这种方法不能够直接在Dockerfile中实现。  \n\u003e 由于宿主机目录是依赖于特定宿主机的，并不能够保证在所有的宿主机上都存在这样的特定目录。\n\n在dockerfile中编写：\n\n![](Snipaste_2020-10-03_15-35-37.png)\n\nbuild后生成镜像：\n\n![](Snipaste_2020-10-03_15-36-01.png)\n\n获得一个新镜像zzyy/centos,run容器\n\n![](Snipaste_2020-10-03_15-36-31.png)\n\n通过上述步骤，容器内的卷目录地址已经知道，对应的主机目录在哪(因为上面没有指定宿主机的目录，所以这里是随机生成的)\n\n![](Snipaste_2020-10-03_15-37-05.png)\n\n**主机对应默认地址**\n\n![](Snipaste_2020-10-03_15-37-22.png)\n\n\u003e Docker挂载主机目录Docker访问出现cannot open directory . Permission denied  \n\u003e 解决办法:在挂载目录后多加一个--privileged=true参数即可\n\n### 5.2 数据卷容器\n\n命名(--name)的容器挂载数据卷，其它容器通过挂载这个(父容器)实现数据共享，挂载数据卷的容器，称之为数据卷容器.\n\n\u003e 个人理解为用一个镜像生成多个容器，这多个容器他们拥有各自的文件系统，但是在文件系统中都挂上了同一块卷，相当于一个U盘同时插在多个容器上。\n\n**例子：**\n\n1. 以上一步新建的zzyy/centos为模板并运行3个容器 doc1/doc2/doc3。\n\n   \u003e 根据dockerfile的设定，他们已经具有容器卷/dataVolumeContainer1 /dataVolumeContainer2。\n\n   先启动一个父容器doc1，并在dataVolumeContainer2新建测试文件doc1_add.txt\n\n   ![image-20211202211735820](image-20211202211735820.png)\n\n2. doc2/doc3 继承doc1（`--volumes -from`),分别在dataVolumeContainer2各自新增内容\n\n   ![image-20211202211906613](image-20211202211906613.png)\n\n3. 回到doc1可以看到02/03各自添加的都能共享了\n4. ![image-20211202211956741](image-20211202211956741.png)\n5. 检查删除doc1、 doc2修改后doc3是否可以访问\n\n   ![image-20211202212022779](image-20211202212022779-8451223.png)\n\n6. 检查删除doc02后doc3是否访问\n\n   ![image-20211202212042241](image-20211202212042241.png)\n\n7. 进一步生成doc4，然后删除doc03\n\n   ![image-20211202212101899](image-20211202212101899.png)\n\n   ![image-20211202212116830](image-20211202212116830.png)\n\n   **结论：容器之间配置信息的传递，数据卷的生命周期一直持续到没有容器使用它为止**\n\n## 6 DockerFile解析\n\n### 6.1 DockerFile简介\n\nDockerfile是用来构建Docker镜像的构建文件，由一系列命令和参数构成的脚本。\n\n构建步骤主要为：1.编写Dockerfile文件 2.docker build 3.docker run\n\n以Centos为例：\n\n```dockerfile\nFROM scratch\nADD centos-7-x86_64-docker.tar.xz /\n\nLABEL \\\n    org.label-schema.schema-version=\"1.0\" \\\n    org.label-schema.name=\"CentOS Base Image\" \\\n    org.label-schema.vendor=\"CentOS\" \\\n    org.label-schema.license=\"GPLv2\" \\\n    org.label-schema.build-date=\"20201113\" \\\n    org.opencontainers.image.title=\"CentOS Base Image\" \\\n    org.opencontainers.image.vendor=\"CentOS\" \\\n    org.opencontainers.image.licenses=\"GPL-2.0-only\" \\\n    org.opencontainers.image.created=\"2020-11-13 00:00:00+00:00\"\n\nCMD [\"/bin/bash\"]\n```\n\n### Docker执行Dockerfile的大致流程\n\n1. docker 从基础镜像运行一个容器\n2. 执行一-条指令并对容器作出修改\n3. 执行类似docker commit的操作提交一个新的镜像层\n4. docker再基于刚提交的镜像运行一一个新容器\n5. 执行dockerfile中的 下一条指令直到所有指令都执行完成\n\n从应用软件的角度来看，Dockerfile、Docker镜像与Docker容器分别代表软件的三个不同阶段，Dockerfile是软件的原材料,Docker镜像是软件的交付品,Docker容器则可以认为是包括软件的整个运行环境。Dockerfile面向开发，Docker镜像成为交付标准，Docker容器则涉及部署与运维，三者缺一不可，合力充当Docker体系的基石。\n\n\u003cimg src=\"../../pics/image-20211202213317693.png\" alt=\"image-20211202213317693\" style=\"zoom:75%;\" /\u003e\n\n1. Dockerfile，需要定义一个Dockerfile，Dockerfile定 义了进程需要的一切东西。Dockerfile涉 及的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程(当应用进程需要和系统服务和内核进程打交道，这时需要考虑如何设计namespace的权限控制)等等;\n2. Docker镜像，在用Dockerfile定义一文件之后，docker build时会产生- -个Docker镜像，当运行Docker镜像时，会真正开始提供服务;\n3. Docker容器，容器是直接提供服务的。\n\n### 6.2 DockerFile 语法\n\n1. 每条保留字指令都必须为大写字母且后面要跟随至少一个参数.\n2. 指令按照从上到下顺序执行.\n3. `#`表示注释.\n4. 每条指令都会创建一个新的镜像层，并对镜像进行提交.\n\n**保留字：**\n\n```dockerfile\nFROM 基础镜像，当前新镜像是基于哪个镜像的\nMAINTAINER 镜像维护者的姓名和邮箱地址\nRUN 容器构建时需要运行的命令\nEXPOSE 当前容器对外暴露出的端口\nWORKDIR 指定在创建容器后，终端默认登陆的进来工作目录，一个落脚点\nENV 用来在构建镜像过程中设置环境变量\nADD 将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar压缩包\nCOPY 类似ADD，拷贝文件和目录到镜像中。将从构建上下文目录中 \u003c源路径\u003e 的文件/目录复制到新的一层的镜像内的 \u003c目标路径\u003e 位置 COPY src dest / COPY [\"src\", \"dest\"]\nVOLUME 容器数据卷，用于数据保存和持久化工作\nCMD 指定一个容器启动时要运行的命令,Dockerfile 中可以有多个 CMD 指令，但只有最后一个生效，CMD 会被 docker run 之后的参数替换\nENTRYPOINT 指定一个容器启动时要运行的命令,ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数,但与CMD不同的是在docker run后追加\nONBUILD 当构建一个被继承的Dockerfile时运行命令，父镜像在被子继承后父镜像的onbuild被触发\n```\n\n![image-20211202214734383](image-20211202214734383.png)\n\n### 6.3 通过Dockerfile构建镜像\n\n#### 6.3.1 Base 镜像(scratch)\n\nDocker Hub中 99%的镜像都是通过在base镜像中安装和配置需要的软件构建出来的。\n\n![image-20211202215030615](image-20211202215030615.png)\n\n#### 6.3.2 自定义镜像mycentos\n\n已知官方镜像的三个特点：\n\n\u003cimg src=\"../../pics/image-20211202215125533.png\" alt=\"image-20211202215125533\" style=\"zoom:50%;\" /\u003e\n\n自定义mycentos目的使我们自己的镜像具备如下:\n\n1. 登陆后的默认路径\n2. vim编辑器\n3. 查看网络配置ifconfig支持\n\n**操作步骤**\n\n1. 准备Dockerfile文件\n\n   ```dockerfile\n   FROM centos\n   MAINTAINER rax\u003crax@126.com\u003e\n   # 设置环境变量\n   ENV MYPATH /usr/local\n   WORKDIR $MYPATH \n   RUN yum -y install vim\n   RUN yum -y install net-tools\n   EXPOSE 80\n   CMD echo $MYPATH\n   ```\n\n2. 构建\n\n   `docker build -t 新镜像名字:TAG .`（注意后面有个点，表示当前路径）\n\n   ![image-20211204132129842](image-20211204132129842.png)\n\n   \n2. 运行\n\n   `docker run -it 新镜像名字:TAG`\n\n   ![image-20211204132230129](image-20211204132230129.png)\n\n   \n2. 列出镜像的变更历史\n\n   `docker history 镜像名`\n\n### 6.4 CMD/ENTRYPOINT的区别\n\n两者都是指定一个容器启动时要运行的命令.\n\n#### 6.4.1 CMD\n\nDockerfile中可以有多个CMD指令，但只有最后一个生效，CMD会被`docker run`之后的参数替换,如`docker run -it -p 8080:8080 tomcat ls -l`.\n\n![image-20211204132947048](image-20211204132947048.png)\n\n可以发现tomcat并没有启动，但是在dockerfile中设置了：\n\n\u003cimg src=\"../../pics/image-20211204133106376.png\" alt=\"image-20211204133106376\" style=\"zoom:50%;\" /\u003e\n\n#### 6.4.2 ENTRYPOINT\n\n`docker run`之后的参数会被当做参数传递给 ENTRYPOINT 之后形成新的命令组合.\n\n**案例**\n\n制作可以查询IP信息的容器,首先做一个CMD版本用于比较:\n\n![image-20211204133715951](image-20211204133715951.png)\n\n\u003e curl的命令解释\n\u003e\n\u003e **curl**命令可以用来执行下载、发送各种**HTTP**请求，指定**HTTP**头部等操作。\n\u003e\n\u003e 如果系统没有**curl**可以使用**yum install curl**安装，也可以下载安装。  \n\u003e **curl是将下载文件输出到stdout**  \n\u003e 使用命令: curl http://www .baidu.com  \n\u003e 执行后，www.baidu.com的html就会显示在屏幕上了\n\u003e\n\u003e 这是最简单的使用方法。用这个命令获得了htp://curl.haxx.se指向的页面，同样，如果这里的URL指向的是一个文件或者一幅图都可以直接下载到本地。如果下载的是HTML文档，那么缺省的将只显示文件头部，即HTML文档的header。要全部显示，请加参数-i\n\n若加上参数`-i`,如下：\n\n\u003cimg src=\"../../pics/image-20211204134648910.png\" alt=\"image-20211204134648910\" style=\"zoom:50%;\" /\u003e\n\n可以看到可执行文件找不到的报错，**executable file not found。**之前说过，**跟在镜像名后面的是command,运行时会替换CMD的默认值。**因此这里的`-i`替换了原来的CMD，而不是添加在原来的`curl -s htp://ip.cn`后面。而`-i`根本不是命令，所以自然找不到。  \n那么如果希望加入`-i`这参数，我们就必须重新完整的输入这个命令:\n\n`docker run myip curl -s http://ip.cn -i`\n\n而使用**ENTRYPOINT**时，则可以实现：\n\n![image-20211204134558115](image-20211204134558115.png)\n\n### 6.5 综合案例\n\n#### 6.5.1 自定义Tomcat\n\n```bash\nmkdir -p /zzyy/mydockerfile/tomcat9\ntouch c.txt\n# 将jdk和tomcat安装的压缩包拷贝进上一步目录\n```\n\n在当前目录下新建Dockerfile文件：\n\n```dockerfile\nFROM centos\nMAINTAINER zzyy\u003cxxx@126.com\u003e\n#把宿主机当前上下文的c .txt拷贝到容器/usr/local/路径下\nCOPY c.txt /usr/local/cincontainer.txt\n#把java与tomcat添加到容器中\nADD jdk-8u171-linux x64.tar.gz /usr/local/ #解压缩拷贝\nADD apache-tomcat-9.0.8.tar.gz /usr/local/\n#安装vim编辑器\nRUN yum -y install vim\n#设置工 作访问时候的WORKDIR路径， 登录落脚点\nENV MYPATH /usr/local\nWORKDIR $MYPATH\n#配:置java与tomcat环境变量\nENV JAVA_ HOME /usr/local/jdk1.8.0_171\nENV CLASSPATH $JAVA_ HOME/lib/dt.jar:$JAVA_ HOME/lib/tools.jar\nENV CATALINA_ HOME /usr/local/apache-tomcat-9.0.8\nENV CATALINA_ BASE /usr/ocal/apache-tomcat-9.0.8\nENV PATH $PATH:$JAVA_ HOME/bin:$CATALINA_ HOME/ib:$CATALINA_ HOME/bin\n#容器运行时监听的端口\nEXPOSE 8080\n#启动时运行tomcat\n# ENTRYPOINT [\"/usrl/local/apache-tomcat-9.0.8/bin/startup.sh\" ]\n# CMD [\"/usr/local/apache-tomcat-9.0.8/bin/catalina.sh\",\"run\"]\nCMD /usr/local/apache-tomcat-9.0.8/bin/startup.sh \u0026\u0026 tail -F /usr/local/apache-tomcat-9.0.8/in/logs/catalina.out\n```\n\n构建：\n\n![image-20211204135937106](image-20211204135937106.png)\n\n![image-20211204140106215](image-20211204140106215.png)\n\n运行：\n\n```bash\ndocker run -d -p 9080:8080 -name myt9 #容器的名字\n-v /zyuse/mydockerfiletomcat9/test:/usrlocal/apache-tomcat9.0.8/webapps/test #添加容器卷\n-v /zzyyuse/mydockerfile/tomcat9/tomcat9logs/:/usrlocal/apache-tomcat-9.0.8/logs -privileged=true zzyytomcat9\n```\n\n![image-20211204140414539](image-20211204140414539.png)\n\n\u003e Docker挂载主机目录Docker访问出现cannot open directory : Permission denied解决办法:在挂载目录后多加一个-privileged=true参数即可\n\n验证:\n\n![image-20211204140905503](image-20211204140905503.png)\n\n部署外部网页：\n\n在宿主机的映射对应目录中放入文件：\n\n![image-20211204141003758](image-20211204141003758.png)\n\n在浏览器中，访问对应域名下`/test/a.jsp`即可。\n\n### 6.6 总结\n\n![image-20211204141111986](image-20211204141111986.png)\n\n## 7 Docker常用安装\n\n### 7.1 总体步骤\n\n搜索镜像-\u003e拉取镜像-\u003e查看镜像-\u003e启动镜像-\u003e停止容器-\u003e移除容器\n\n### 7.2 安装Mysql\n\n1. docker hub 上查找mysql镜像\n\n   ![image-20211204141309048](image-20211204141309048.png)\n\n2. 从docker hub(阿里云加速器)拉取mysql镜像到本地标签为5.6\n\n   ![image-20211204141338583](image-20211204141338583.png)\n\n3. 使用mysql5.6镜像创建容器(也叫运行镜像)\n\n   ```bash\n   docker run -p 12345:3306 --name mysql\n   -v /ggcc/mysql/conf:/etc/mysql/conf.d\n   -v /ggcc/mysql/logs:/logs\n   -v /ggcc/mysql/data:/var/lib/mysql\n   -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6\n   ----------------------------------------------\n   命令说明:\n   -p 12345:3306:将主机的12345端口映射到docker容器的3306端口。\n   -name mysq:运行服务名字\n   -V /ggcc/mysql/conf:/etc/mysql/conf.d :将主机/zzyyuse/mysq|录下的conf/my.cnf挂载到容器的/etc/mysql/conf.d\n   -v /ggcc/mysqlogs/logs: 将主机/zzyyuse/mysq|目 录下的logs 目录挂载到容器的/logs。\n   -V /ggcc/mysqldata:/var/lib/mysql :将主机lzzyyuse/mysql目录下的data目录挂载到容器的/var/lib/mysql .\n   -e MYSQL_ ROOT_ PASSWORD=123456: 初始化root用户的密码。.\n   -d mysql:5.6:后台程序运行mysql5.6\n   ----------------------------------------------\n   docker exec -it Mysql运行成功后的容器ID /bin/bash\n   ----------------------------------------------\n   数据备份小测试\n   docker exec mysql服务容器ID sh -c 'exec mysqldump --all-databases -uroot -p\"123456\"' \u003e/ggcc/all-database.sql\n   ```\n\n   ![image-20211204141851277](image-20211204141851277.png)\n\n   \n4. 数据备份测试：\n\n   ![image-20211204141940982](image-20211204141940982.png)\n\n### 7.3 安装Redis\n\n1. 从docker hub上(阿里云加速器)拉取redis镜像到本地标签为：3.2\n\n   ![image-20211204142111662](image-20211204142111662.png)\n\n2. 使用redis3.2镜像创建容器(也叫运行镜像)\n\n   ```bash\n   docker run -p 6379:6379 -v /ggcc/myredis/data:/data -v /ggcc/myredis/conf/redis.conf:/usr/local/etc/redis/redis.conf -d redis redis-server /usr/local/etc/redis/redis.conf --appendonly yes\n   ```\n\n3. 在主机/ggcc/myredis/conf/redis.conf目录上新建redis.conf文件\n\n   ```bash\n   vim /ggcc/myredis/conf/redis.conf/redis.conf\n   \n   ...\n   ```\n\n4. 测试 redis-cli连接\n\n   `docker exec -it 运行着redis服务容器的ID redis-cli`\n\n   ![image-20211204142358105](image-20211204142358105.png)\n\n5. 测试持久化文件生成\n\n   ![image-20211204142411901](image-20211204142411901.png)\n\n## 8 发布镜像到云\n\n### 8.1 阿里云\n\n   ![image-20211204142453916](image-20211204142453916.png)\n\n### 8.1 生成镜像\n\n   1. 前面的Dockerfile\n   2. 从容器中创建一个新的镜像\n\n      `docker commit [OPTIONS] 容器ID [REPOSITORY[:TAG]]`\n\n      ![image-20211204142611069](image-20211204142611069.png)\n\n### 8.2 推送到阿里云\n\n1. ![image-20211204142725727](image-20211204142725727.png)\n2. 阿里云开发者平台\n\n   https://promotion.aliyun.com/ntms/act/kubernetes.html\n\n   创建镜像仓库\n\n   ![image-20211204142747218](image-20211204142747218.png)\n\n3. ```bash\n   sudo docker login --username=white3e registry.cn-shenzhen.aliyuncs.com\n   sudo docker tag [ImageId] registry.cn-shenzhen.aliyuncs.com/ggccqq/mycentos:[镜像版本号]\n   sudo docker push registry.cn-shenzhen.aliyuncs.com/ggccqq/mycentos:[镜像版本号]\n   其中[ImageId][镜像版本]自己填写\n   ```\n\n   ![image-20211204142828359](image-20211204142828359.png)\n\n4. 公有云可以查询得到\n\n   ![image-20211204142858198](image-20211204142858198.png)\n\n   \n\n   ![image-20211204142901490](image-20211204142901490.png)\n\n   \n5. 将阿里云上的镜像下载到本地\n   \n\n   ![image-20211204142923943](image-20211204142923943.png)\n\n   \n   \n","lastmodified":"2023-05-09T16:33:58.275366009Z","tags":[]},"/ESLint":{"title":"ESLint","content":"\n## 关于 ESLint\n\n关于 **ESLint**，它的 Slogan 是 Find and fix problems in your JavaScript code。如上文所说，它可以发现并修复你 JavaScript 代码中的问题。来看一下官网上描述 ESLint 具备的三个特性：\n\n- **Find Problems**。ESLint 通过静态代码分析可以快速发现代码中的问题。ESLint 可以运行在大多数文本编辑器中，并且也可以在工作流中接入 ESLint\n- **Fix Automatically**。ESLint 发现的很多问题都可以自动修复\n- **Customize**。可以**定制** ESLint 检查规则\n\n基于以上描述，我们在前端工程化中可以这样使用 ESLint：\n\n1. 基于业界现有的 ESLint 规范和团队代码习惯定制一套统一的 ESLint 代码规则\n2. 将统一代码规则封装成 ESLint 规则包接入\n3. 将 ESLint 接入脚手架、编辑器以及研发工作流中\n\n## 快速上手\n\n**先简单介绍一下如何使用 ESLint，如果已经有所了解的同学，可以直接跳过这一节。**\n\n新建一个包含 `package.json` 的目录（可以在空目录下执行 `npm init -y`），新建一个 `index.js`：\n\n```\n// index.js\nconst name = 'axuebin'\n```\n\n安装 `eslint` ：\n\n```\nnpm install eslint --save-dev\n```\n\n然后执行 `./node_modules/.bin/eslint --init` 或者 `npx eslint --init` 生成一个 ESLint 配置文件 `.eslintc.js`：\n\n```javascript\nmodule.exports = {\n  env: {\n    es2021: true,\n  },\n  extends: 'eslint:recommended',\n  parserOptions: {\n    ecmaVersion: 12,\n  },\n  rules: {},\n};\n```\n\n生成好配置文件之后，就可以执行 `./node_modules/.bin/eslint index.js` 或者 `npx eslint index.js` 命令对文件进行检查。结果如下：![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)`index.js` 中的代码命中了 `no-unused-vars` 这个规则，默认情况下，这个规则是会报 `error` 的，也就是 ESLint **不允许代码中出现未被使用的变量**。这是一个好习惯，有利于代码的维护。\n\n### 简单配置\n\n我们来尝试配置 ESLint 的检查规则。以分号和引号举例，现在你作为团队代码规范的指定人，希望团队成员开发的代码，都是**单引号**和**带分号**的。\n\n打开 `.eslintrc.js` 配置文件，在 `rules` 中添加相关配置项：\n\n```javascript\nmodule.exports = {\n  env: {\n    es2021: true,\n  },\n  extends: 'eslint:recommended',\n  parserOptions: {\n    ecmaVersion: 12,\n  },\n  rules: {\n    semi: ['error', 'always'],\n    quotes: ['error', 'single'],\n  },\n};\n```\n\n然后我们将 `index.js` 中的代码改成：\n\n```javascript\n// index.js\nconst name = \"axuebin\"\n```\n\n执行 `eslint` 命令之后：![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)可以看到检查结果如下：\n\n- **[no-unused-vars]** 'name' is assigned a value but never used。定义了 name 变量却未使用。\n- **[quotes]** Strings must use singlequote。字符串必须使用单引号。\n- **[semi]**Missing semicolon。缺失分号。\n\n老老实实地按照规范修改代码，使用单引号并将加上分号。当然，如果你们希望是双引号和不带分号，修改相应的配置即可。\n\n具体各个规则如何配置可以查看：**https://eslint.org/docs/rules**[2]\n\n### 自动修复\n\n执行 `eslint xxx --fix` 可以自动修复一些代码中的问题，将无法自动修复的问题暴露出来。比如上文中提到的引号和分号的问题，就可以通过 `--fix` 自动修复，而 `no-unused-vars` 变量未使用的问题，ESLint 就无法自动修复。![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)\n\n### 使用配置包\n\n在 `init` 生成的配置文件中，我们看到包含这一行代码：\n\n```\nmodule.exports = {\n  extends: \"eslint:recommended\"\n}\n```\n\n这一行代码的意思是，使用 ESLint 的推荐配置。`extends: 'xxx'` 就是 **继承**，当前的配置继承于 `xxx` 的配置，在此基础上进行扩展。\n\n因此，我们也可以使用任意封装好的配置，可以在 **NPM**上或者 **GItHub**上搜索 `eslint-config` 关键词获取，本文我们将这类封装好的配置称作 “配置集”。比较常见的配置包有以下几个：\n\n- **eslint-config-airbnb**: Airbnb 公司提供的配置集\n- **eslint-config-prettier**: 使用这个配置集，会关闭一些可能与 Prettier 冲突的规则\n- **eslint-config-react**: create react app 使用的配置集\n- **eslint-config-vue**: vuejs 使用的配置集\n- …\n\n## 多人合作最佳实践\n\n简单了解完 ESLint 之后，对于 ESLint 的更多使用细节以及原理，在本篇文章就不展开了，感兴趣的朋友可以在官网详细了解。本文重点还是在于**如何在团队工程化体系中落地 ESLint**，这里提几个最佳实践。\n\n### 抽象配置集\n\n对于独立开发者以及业务场景比较简单的小型团队而言，使用现成、完备的第三方配置集是非常高效的，可以较低成本低接入 ESLint 代码检查。\n\n但是，对于中大型团队而言，在实际代码规范落地的过程中我们会发现，不可能存在一个能够完全符合团队风格的三方配置包，我们还是会在 `extends` 三方配置集的基础上，再手动在 `rules` 配置里加一些自定义的规则。时间长了，有可能 A 应用和 B 应用里的 `rules` 就不一样了，就很难达到统一的目的。\n\n这时候，就需要一个中心化的方式来管理配置包：**根据团队代码风格整理（或者基于现有的三方配置集）发布一个配置集，团队统一使用这个包，就可以做到中心化管理和更新**。\n\n除此之外，从技术层面考虑，目前一个前端团队的面对的场景可能比较复杂。比如：\n\n- **技术选型不一致**：框架上 PC 使用 React，H5 使用 Vue；是否使用 TypeScript\n- **跨端场景多**：Web 端和小程序端，还有 Node\n- …\n\n以上问题在真实开发中都是存在的，所以在代码规范的工程化方案落地时，一个单一功能的配置集是不够用的，这时候还需要考虑这个配置集如何抽象。\n\n为了解决以上问题，这里提供一种解决方案的思路：![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)具体拆解来看，就是有一个类似 eslint-config-standard 的基础规则集（包括代码风格、变量相关、ES6 语法等），在此基础之上集成社区的一些插件（Vue/React）等，封装成统一的一个 NPM Package 发布，消费时根据当前应用类型通过不同路径来 extends 对应的配置集。\n\n这里有一个 Demo，感兴趣的朋友可以看一下：**eslint-config-axuebin**[5]\n\n### 开发插件\n\nESLint 提供了丰富的配置供开发者选择，但是在复杂的业务场景和特定的技术栈下，这些通用规则是不够用的。ESLint 通过插件的形式赋予了扩展性，开发者可以自定义任意的检查规则，比如 eslint-plugin-vue / eslint-plugin-react 就是 Vue / React 框架中使用的扩展插件，官网也提供了**相关文档**[6]引导开发者开发一个插件。\n\n一般来说，我们也不需要开发插件，但我们至少需要了解有这么个东西。在做一些团队代码质量检查的时候，我们可能会有一些特殊的业务逻辑，这时候 ESLint 插件是可以帮助我们做一些事情。\n\n这里就不展开了，主要就是一些 AST 的用法，照着官方文档就可以上手，或者可以参考现有的一些插件写法。\n\n### 脚手架 / CLI 工具\n\n当有了团队的统一 ESLint 配置集和插件之后，我们会将它们集成到脚手架中，方便新项目集成和开箱即用。但是对于一些老项目，如果需要手动改造还是会有一些麻烦的，这时候就可以借助于 CLI 来完成一键升级。\n\n本文结合上文的 Demo **eslint-config-axuebin**[7]，设计一个简单的 CLI Demo。由于当前配置也比较简单，所以 CLI 只需要做几件简单的事情即可：\n\n- 询问用户当前项目的类型（是 JavaScript 还是 TypeScript、是 React 还是 Vue）\n- 根据项目类型写 `.eslintrc.js` 文件\n- 根据项目类型安装所需依赖（比如 vue 需要 eslint-plugin-vue）\n- 在 `package.json` 的 `scripts` 中写入 `\"lint\": \"eslint src test --fix\"`\n\n核心代码如下：\n\n```javascript\nconst path = require('path');\nconst fs = require('fs');\nconst chalk = require('chalk');\nconst spawn = require('cross-spawn');\n\nconst { askForLanguage, askForFrame } = require('./ask');\nconst { eslintrcConfig, needDeps } = require('./config');\n\nmodule.exports = async () =\u003e {\n  const language = await askForLanguage();\n  const frame = await askForFrame();\n\n  let type = language;\n  if (frame) {\n    type += `/${frame}`;\n  }\n\n  fs.writeFileSync(\n    path.join(process.cwd(), '.eslintrc.js'),\n    `// Documentation\\n// https://github.com/axuebin/eslint-config-axuebin\\nmodule.exports = ${JSON.stringify(\n      eslintrcConfig(type),\n      null,\n      2\n    )}`\n  );\n\n  const deps = needDeps.javascript;\n  if (language === 'typescript') {\n    deps.concat(needDeps.typescript);\n  }\n  if (frame) {\n    deps.concat(needDeps[frame]);\n  }\n\n  spawn.sync('npm', ['install', ...deps, '--save'], { stdio: 'inherit' });\n};\n```\n\n可运行的 CLI Demo 代码见：**axb-lint**，在项目目录下执行：`axblint eslint` 即可，如图：![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)\n\n### 自动化\n\n配置了 ESLint 之后，我们需要让开发者感知到 ESLint 的约束。开发者可以自己运行 eslint 命令来跑代码检查，这不够高效，所以我们需要一些自动化手段来做这个事情。当然 在开发时，编辑器也有提供相应的功能可以根据当前工作区下的 ESLint 配置文件来检查当前正在编辑的文件，这个不是我们关心的重点。\n\n一般我们会在有以下几种方式做 ESLint 检查：\n\n- **开发时**：依赖编辑器的能力\n- **手动运行**：在终端中手动执行 eslint 命令\n- **pre-commit**：在提交 git 前自动执行 eslint 命令\n- **ci**：依赖 git 的持续集成，可以将检查结果输出文件上传到服务器\n\n这里提一下 pre-commit 的方案，在每一次本地开发完成提交代码前就做 ESLint 检查，保证云端的代码是统一规范的。\n\n这种方式非常简单，只需要在项目中依赖 **husky**[9] 和 **lint-staged**[10] 即可完成。安装好依赖之后，在 package.json 文件加入以下配置即可：\n\n```\n{\n  \"lint-staged\": {\n    \"*.{js,jsx,ts,tsx}\": \"eslint --cache --fix\"\n  },\n  \"husky\": {\n    \"hooks\": {\n      \"pre-commit\": \"lint-staged\"\n    }\n  }\n}\n```\n\n效果如图所示：![图片](pics/640)如果代码跑 ESLint 检查抛了 Error 错误，则会中断 commit 流程：![图片](pics/640)这样就可以确保提交到 GitHub 仓库上的代码是统一规范的。（当然，如果认为将这些配置文件都删了，那也是没办法的）\n\n## 个人使用保存时自动统一代码风格\n\n先通过一些简单的配置，然后：\n\n- `Ctrl`+`s` / `command`+`s` 时自动修复代码的格式错误\n- 自动修复的规则是读取项目根目录的Eslint规则\n- 这样就能保证项目成员都是一套验证规则的代码风格\n\n### 配置\n\n1. **安装VsCode的`EsLint`和`vetur`插件**\n2. **为项目安装`EsLint`包：**\n\n   ![image-20210501020908564](image-20210501020908564.png)\n\n注意要安装在开发环境上，还有就是如果你使用的是脚手架的话，选了Eslint选项，会自带这些包。\n\n3. **在项目的根目录下添加`.eslintrc.js`**\n\n   用于校验代码格式，根据项目情况，可自行编写校验规则：\n\n```js\nmodule.exports = {\n    // Eslint规则\n}\n```\n\n4. **选项设置**\n\n   将下面这部分放入首选项设置中：\n\n```json\n\"eslint.autoFixOnSave\": true,  //  启用保存时自动修复,默认只支持.js文件\n\"eslint.validate\": [\n    \"javascript\",  //  用eslint的规则检测js文件\n    {\n        \"language\": \"vue\",   // 检测vue文件\n        \"autoFix\": true   //  为vue文件开启保存自动修复的功能\n    },\n    {\n        \"language\": \"html\",\n        \"autoFix\": true\n    },\n],\n```\n\n想了解更多的话，推荐看一下VsCode的[EsLint](https://marketplace.visualstudio.com/items?itemName=dbaeumer.vscode-eslint)插件\n\n**大功告成：**\n\n点开文件，你可能会看到如下报错，无需一个一个去改，只要保存一下文件，就可以自动修复这些代码格式上的问题了。\n\n**注意：**\n\n如果整个文件都飘红的话，不会一次性修改如果的格式问题，会一下改一部分，你可能需要多按几次保存。\n\n### 一键修复项目格式问题：\n\n遇到下面这两种情况：\n\n- 你刚刚引入这个自动修复，但你项目的文件比较多，且你又比较懒。\n- 隔一段时间，修复一下代码格式上的问题\n\n你可以像下面这样，在`package.json`里面的`scripts`里面新增一条如下命令：\n\n```js\n\"lint\": \"eslint --ext .js,.vue src --fix\"\n```\n\n![image-20210501021229766](image-20210501021229766.png)\n\n`--ext`后面跟上的`.js`、`.vue`是你要检测文件的后缀，`.vue`后面的`src`是要检测的哪个目录下面的文件。\n\n`--fix`的作用是自动修复根据你配置的规则检测出来的格式问题\n\n**一键修复**\n\n输入如下命令行，就可以自动**修复你`src`文件夹下面的所有根据你配置的规则检测出来的格式问题**。\n\n```js\nnpm run lint\n```\n\n## .eslintignore 不检测一些文件\n\n在项目的根目录创建一个`.eslintignore`文件，用于让`EsLint`不检测一些文件。\n\n比如引的一些别人的文件，插件等,比如文件中：\n\n```text\nsrc/test/* \nsrc/test2/* \n```\n\n文件中的内容像上面这样写，这里第一行是不检测src目录下的test文件夹下面的所有文件。\n\n## 自定义规则\n\n```js\n// .eslintrc.js文件\nmodule.exports = {\n    \"rules\": { // 自定义规则\n        \"no-console\": 0,\n        \"no-const-assign\": 1, \n        \"no-extra-bind\": 2,\n    }\n}\n```\n\n**0、1、2的意思：**\n\n- `\"off\"` 或 0 - 关闭这项规则\n- `\"warn\"` 或 1 - 将规则视为一个警告\n- `\"error\"` 或 2 - 将规则视为一个错误\n","lastmodified":"2023-05-09T16:33:58.275366009Z","tags":[]},"/GYM":{"title":"GYM","content":"\n# Env\n\n首先我们可以通过如下代码调用并展示（可视化）一个环境：\n\n```javascript\nimport gym\nenv = gym.make('CartPole-v0')\nenv.reset()\nfor _ in range(1000):\n    env.render() # 可视化环境\n    env.step(env.action_space.sample()) # 选择随机动作\nenv.close()\n```\n\n该代码创建了一个著名的 CartPole 环境，用于控制小车使上面的杆保持竖直不倒，如下图所示。在每一次迭代中，我们从动作空间中采样了一个随机动作（本环境中只有**「向左」**和**「向右」**两个动作）并执行。\n\n\u003cimg src=\"../pics/v49y0ayc8u.gif\" alt=\"img\" style=\"zoom:50%;\" /\u003e\n\n## Pendulum\n\nhttps://blog.csdn.net/u013745804/article/details/78397106\n\ngym学习及二次开发 https://zhuanlan.zhihu.com/p/26985029\n\n# Observation\n\n为了做出更加合适的动作，我们需要先了解环境的反馈。环境的 `step` 函数可以返回我们想要的值，其总共返回如下四个值：\n\n- `observation`（**「object」**）：一个环境特定的对象以表示当前环境的观测状态，如相机的像素数据，机器人的关节角度和速度，桌游中的即时战况等\n- `reward`（**「float」**）：前一个动作所获得的奖励值，其范围往往随着环境的变化而各不相同，但目标一般都是提升总奖励值\n- `done`（**「boolean」**）：是否需要重置（`reset`）环境，不同的环境会有不同的终止条件，包括执行动作的次数限制、状态的变化阈值等\n- `info`（**「dict」**）：输出学习过程中的相关信息，一般用于调试\n\n通过上述函数，我们可以实现经典的**「代理-环境循环」**，在每个时间步，代理选择一个动作，环境返回一个观察（状态）和一个奖励：\n\n\u003cimg src=\"../pics/image-20220526174712742.png\" alt=\"image-20220526174712742\" style=\"zoom:50%;\" /\u003e\n\n# Space\n\n在 Gym 中，状态和动作都是通过 `Space` 类型来表示的，其可以定义连续或离散的子空间。最常用的两种 `Space` 是 `Box` 和 `Discrete`，在 CartPole 环境中状态空间和动作空间就分别对应这两种 `Space`：\n\n```text\nimport gym\nenv = gym.make('CartPole-v0')\nprint(env.action_space)\n#\u003e Discrete(2)\nprint(env.observation_space)\n#\u003e Box(4,) # 注意其第二维未指定\n```\n\n\u003e `Discrete` 定义了一个从 0 开始取值的离散空间，而 `Box` 则可以表示一个 `m*n` 维的连续空间，需要为每个维度设置上下界。我们可以通过如下方式新建空间：\n\u003e\n\u003e ```python\n\u003e from gym import spaces\n\u003e space = spaces.Discrete(8) # 包含 {0, 1, 2, ..., 7} 八个元素的集合\n\u003e \n\u003e # 每一维相同的上下界\n\u003e space_box_1 = Box(low=-1.0, high=2.0, shape=(3, 4), dtype=np.float32)\n\u003e # Box(3, 4)\n\u003e \n\u003e # 每一维不同的上下界\n\u003e space_box_2 = Box(low=np.array([-1.0, -2.0]), high=np.array([2.0, 4.0]), dtype=np.float32)\n\u003e # Box(2,)\n\u003e ```\n","lastmodified":"2023-05-09T16:33:58.275366009Z","tags":[]},"/Go":{"title":"Go","content":"\n## 输入输出\n\nhttps://blog.csdn.net/weixin_44211968/article/details/124632136\n\n字节 byte 类型也属于整型\n\n证明了基于数组的切片，使用的底层数组还是原来的数组，一旦修改切片的元素值，那么底层数组对应的值也会被修改。\n\n## 1.GMP goroutine machine processor\n\n![](Pasted%20image%2020230403134733.png)\n\n### 通知协程退出的方式\n\n- 通过全局变量：如果全局变量为真就退出  \n    如果worker中再启动goroutine，就不太好控制了\n- 通过通道：协程在通道里面取到true就退出  \n    使用全局变量在跨包调用时不容易实现规范和统一，需要维护一个共用的channel\n- 通过context：通过调用ctx.Done()方法通知所有的协程退出  \n    当子goroutine又开启另外一个goroutine时，只需要将ctx传入即可\n- context.WithTimeout超时退出  \n    取消此上下文将释放与其相关的资源，因此代码应该在此上下文中运行的操作完成后立即调用cancel，通常用于数据库或者网络连接的超时控制\n\n### init() 函数是什么时候执行的\n\n`init()` 函数是 Go 程序初始化的一部分。Go 程序初始化先于 main 函数，由 runtime 初始化每个导入的包，初始化顺序不是按照从上到下的导入顺序，而是按照解析的依赖关系，没有依赖的包最先初始化。  \n拍  \n每个包首先初始化包作用域的常量和变量（常量优先于变量），然后执行包的 `init()` 函数。同一个包，甚至是同一个源文件可以有多个 `init()` 函数。`init()` 函数没有入参和返回值，不能被其他函数调用，同一个包内多个 `init()` 函数的执行顺序不作保证。\n\n一句话总结： import –\u003e const –\u003e var –\u003e `init()` –\u003e `main()`\n\n```go\npackage main  \n  \nimport \"fmt\"  \n  \nfunc init()  {  \n\tfmt.Println(\"init1:\", a)  \n}  \n  \nfunc init()  {  \n\tfmt.Println(\"init2:\", a)  \n}  \n  \nvar a = 10  \nconst b = 100  \n  \nfunc main() {  \n\tfmt.Println(\"main:\", a)  \n}  \n// 执行结果  \n// init1: 10  \n// init2: 10  \n// main: 10\n```\n\n### 局部变量分配在栈上还是堆上\n\n由编译器决定。Go 语言编译器会自动决定把一个变量放在栈还是放在堆，编译器会做逃逸分析(escape analysis)，当发现变量的作用域没有超出函数范围，就可以在栈上，反之则必须分配在堆上。\n\n```go\nfunc foo() *int {  \n\tv := 11  \n\treturn \u0026v  \n}  \n  \nfunc main() {  \n\tm := foo()  \n\tprintln(*m) // 11  \n}  \n```\n\n`foo()` 函数中，如果 v 分配在栈上，foo 函数返回时，`\u0026v` 就不存在了，但是这段函数是能够正常运行的。Go 编译器发现 v 的引用脱离了 foo 的作用域，会将其分配在堆上。因此，main 函数中仍能够正常访问该值。\n\n### 调度方式\n\n#### 协作式调度\n\n![](Pasted%20image%2020230405141640.png)  \n主动调用**Gosched**/**Goexit**方法去执行调度逻辑，**遇到阻塞**或者**GC**时都会主动去调用这两个方法，把CPU交给其他协程  \n**runtime.Gosched()** ： 会将之前的G放回待运行队列，之前的G在后面会被调度到。  \n**runtime.gopark()** ：不会将之前的G放回待运行队列，之前的G需要等待其他G恢复才能执行。  \n**runtime.Goexit()** ：不会将之前的G放回待运行队列，之前的G会被回收。\n\n#### 基于协作的抢占式调度\n\n某些协程执行时间过长，导致其他协程得不到调度，任务执行时延高。  \n垃圾回收的时候需要**STW**，需要让所有执行的协程暂停工作，但是协作式调度需要等待G主动让出CPU的时候才能执行到调度器，而且还需要等待所有的G都停止工作，其时间可想而知，极端情况下是十分漫长的。  \n**基于协作的抢占是通过给G设置**标志位（stackguard0）**实现的。当**G**在函数调用的时候会检查这个标志位，当其为**StackPreempt 时，那就说明当前G被别的G抢占，就**主动**去执行调度代码。  \n下面是一张描述多个协程调度过程中，G3协程被监控线程（**sysmon**）检测到超时运行后基于协作的抢占调度的图：  \n![](Pasted%20image%2020230405141928.png)\n\n#### 基于信号的抢占式调度\n\n虽然基于协作的抢占式调度解决了一部分问题，但是它还是不够完备。  \n在一些极端情况下，还是会出现比较严重的问题，比如协程长时间执行并且不会执行到抢占标志检查就不会触发调度  \n基于信号的抢占式调度是非协作式抢占调度  \n下面是一张描述多个协程调度过程中，G3协程被监控线程（**sysmon**）检测到超时运行后发生基于信号抢占调度的图：  \n![](Pasted%20image%2020230405142237.png)\n\n1. 基于信号的抢占调度第一步肯定是要注册信号的处理事件，这个过程在上图没有展示，因为注册信号这个动作是M0线程做的，对应信号的处理事件是全局共享的。  \n    信号：SIGURG（即上文提到的sigPreempt）  \n    回调函数：func doSigPreempt(gp g, ctxt sigctxt)\n\n2. 触发抢占，发送信号给要被抢占G的M  \n    这个动作可以看上图的(C -\u003e F):  \n    C: 检查超时运行的协程  \n    D: 发现G3运行时间大于10ms  \n    E,F: 给G3所在线程发送抢占信号\n\n3. G3所在CPU执行注册的软中断事件  \n这个动作可以看上图的(9-\u003e11):  \n-9: 开始处理中断  \n-10: 修改寄存器植入指令  \n-11: 中断结束，返回G3线程用户态\n\n4. G3所在CPU执行注册的软中断事件  \n这个动作可以看上图的(9-\u003e11):  \n-9: 开始处理中断  \n-10: 修改寄存器植入指令  \n-11: 中断结束，返回 G3线程用户态\n\n## 2.锁\n\n## 3.interface\n\n#### 2个interface可以比较吗\n\nGo 语言中，interface 的内部实现包含了 2 个字段，类型 `T` 和 值 `V`，interface 可以使用 == != 比较\n\n1. 两个 interface 均等于 nil（此时 V 和 T 都处于 unset 状态）\n2. 类型 T 相同，且对应的值 V 相等。\n\n    ```go\n\ntype Stu struct {  \n\tName string  \n}  \n\n  \n\ntype StuInt interface{}  \n\n  \n\nfunc main() {  \n\tvar stu1, stu2 StuInt = \u0026Stu{\"Tom\"}, \u0026Stu{\"Tom\"}  \n\tvar stu3, stu4 StuInt = Stu{\"Tom\"}, Stu{\"Tom\"}  \n\tfmt.Println(stu1 == stu2) // false  \n\tfmt.Println(stu3 == stu4) // true  \n}\n\n```\n`stu1` 和 `stu2` 对应的类型是 `*Stu`，**值是 Stu 结构体的地址，两个地址不同**，因此结果为 false。  \n`stu3` 和 `stu4` 对应的类型是 `Stu`，**值是 Stu 结构体，且各字段相等**，因此结果为 true。\n\n## 4.GC\n程序中定义一个变量，会在内存中开辟相应内存空间进行存储，当不需要此变量后，需要手动销毁此对象，并释放内存。而这种对不再使用的内存资源进行自动回收的功能即为**垃圾回收（Garbage Collection，缩写为GC），是一种自动内存管理机制**\n1. 如何识别垃圾\n    **引用计数算法(reference counting)**\n    引用计数通过在对象上增加自己被引用的次数，被其他对象引用时加1，引用自己的对象被回收时减1，引用数为0的对象即为可以被回收的对象，这种算法在内存比较紧张和实时性比较高的系统中使用比较广泛，如php，Python等。\n    优点：\n        方式简单，回收速度快\n    缺点：\n        需要额外的空间存放计数\n        无法处理循环引用(如a.b=b; b.a=a)\n        频繁更新引用计数降低了性能\n    **追踪式回收算法(Tracing)**\n    追踪式算法(可达性分析)的核心思想是判断一个对象是否可达，如果这个对象一旦不可达就可以立刻被GC回收了，那么我们怎么判断一个对象是否可达呢？第一步从根节点开始找出所有的全局变量和当前函数栈里的变量，标记为可达。第二部，从已经标记的数据开始，进一步标记它们可访问的变量，以此类推，专业术语叫传递闭包。当追踪结束时，没有被打上标记的对象就被判定是不可触达。\n    优点：\n        解决了循环引用的问题\n        占用的空间少了\n    缺点：\n        无法立刻识别出垃圾对象，需要依赖GC线程\n        算法在标记时必须暂停整个程序，即STW(stop the world)，否则其他线程有可能会修改对象的状态从而回收不该回收的对象\n2. 如何清理垃圾\n    **标记清除算法(Mark Sweep)**\n    标记清除算法是最常见的垃圾收集算法，标记清除收集器是跟踪式垃圾收集器，其执行过程可以分成标记(Mark)和清除(Sweep)两个阶段：\n    1 标记阶段：暂停应用程序的执行，从根对象触发查找并标记堆中所有存活的对象；\n    2 清除阶段：遍历堆中的全部对象，回收未被标记的垃圾对象并将回收的内存**加入空闲链表**，恢复应用程序的执行；\n    ![](../../pics/Pasted%20image%2020230216124106.png)\n    优点：\n        实现简单。\n    缺点：\n        执行期间需要把整个程序完全暂停，不能异步的进行垃圾回收。\n        容易产生大量不连续的内存随便，碎片太多可能会导致后续没有足够的连续内存分配给较大的对象，从而提前触发新的一次垃圾收集动作。\n    **标记复制算法**\n    它把内存空间划分为两个相等的区域，每次只使用其中一个区域。在垃圾收集时，遍历当前使用的区域，把存活对象复制到另一个区域中，最后将当前使用的区域的可回收对象进行回收。\n    **标记压缩算法**\n    在标记可回收的对象后将所有存活的对象压缩到内存的一端，使他们紧凑地排列在一起，然后对边界以外的内存进行回收，回收后，已用和未用的内存都各自一边。\n3. 设计原理\n    **三色标记算法**\n    为了解决原始标记清除算法带来的长时间STW, Go从v1.5版本实现了基于三色标记清除的并发垃圾收集器，**在不暂停程序的情况下即可完成对象的可达性分析**，三色标记算法将程序中的对象分成白色、黑色和灰色三类：\n    - 白色对象 - 潜在的垃圾，表示还未搜索到的对象，其内存可能会被垃圾收集器回收；\n    - 黑色对象 - 活跃的对象，表示搜索完成的对象，包括不存在任何引用外部指针的对象以及从根对象可达的对象\n    - 灰色对象 - 活跃的对象，表示正在搜索还未搜索完的对象，因为存在指向白色对象的外部指针，垃圾收集器会扫描这些对象的子对象；\n    三色标记法属于增量式GC算法，回收器首先将所有对象标记成白色，然后从gc root出发，逐步把所有可达的对象变成灰色再到黑色，最终所有的白色对象都是不可达对象。\n    具体实现：\n    - 初始时所有对象都是白色的\n    - 从`gc root`对象出发，扫描所有可达对象标记为灰色，放入待处理队列\n    - 从队列取出一个灰色对象并标记为黑色，将其引用对象标记为灰色，放入队列\n    - 重复上一步骤，直到灰色对象队列为空\n    - 此时剩下的所有白色对象都是垃圾对象\n\n![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/45d06eb5f99d49f385ac21e4027b4973~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)\n    优点：\n    不需要STW\n    缺点：\n    如果产生垃圾速度大于回收速度时，可能会导致程序中垃圾对象越来越多而无法及时收集\n    线程切换和上下文转换的消耗会使得垃圾回收的总体成本上升，从而降低系统吞吐量\n    三色标记法存在并发性问题，\n        可能会出现野指针(指向没有合法地址的指针)，从而造成严重的程序错误\n        漏标，错误的回收非垃圾对象\n三色不变性\n想要在并发或者增量的标记算法中保证正确性，我们需要达成一下两种三色不变性中的任意一种。\n-   强三色不变性——黑色对象不会指向白色对象，只会指向灰色对象或者黑色对象。![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8bf86801841744ab97923af572b763f2~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)\n-   弱三色不变性——黑色对象指向的白色对象必须包含一条从灰色对象经由多个白色对象的可达路径。\n![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4fc07e2bf7a743d0902fac72ed174f80~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)\n**屏障技术**\n垃圾收集中的屏障技术更像是一个钩子方法，它是在用户程序读取对象、创建新对象以及更新对象指针时执行的一段代码，根据操作类型的不同，我们可以将它们分成读屏障和写屏障两种，因为读屏障需要在读操作中加入代码片段，对用户程序的性能影响很大，所以变成语言往往都会采用写屏障保证三色不变性。\n插入写屏障\n当一个对象引用另外一个对象时，将另外一个对象标记为灰色，以此满足强三色不变性，不会存在黑色对象引用白色对象。\n删除写屏障\n在灰色对象删除对白色对象的引用时，将白色对象置为灰色，其实就是快照保存旧的引用关系，这叫STAB(snapshot-at-the-beginning),以此满足弱三色不变性。\n\n混合写屏障\nv1.8版本之前，运行时会使用插入写屏障保证强三色不变性；\n在v1.8中，组合插入写屏障和删除写屏障构成了混合写屏障，保证弱三色不变性；该写屏障会将覆盖的对象标记成灰色(删除写屏障)并在当前栈没有扫描时将新对象也标记成灰色(插入写屏障)：\n写屏障会将被覆盖的指针和新指针都标记成灰色，而所有新建的对象都会被直接标记成黑色。\n**执行周期**\nGo语言的垃圾收集可以分成清除终止、标记、标记终止和清除四个不同阶段：\n![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0a5a6c1f177d41f393de2a6a8f860c13~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)\n**清理终止阶段**\n1.  暂停程序，所有的处理器在这时会进入安全点(safe point)；\n    如果当前垃圾收集循环是强制触发的，我们还需要处理还未清理的内存管理单元；\n**标记阶段**\n1. 将状态切换至`_GCmark`、开启写屏障、用户程序协助(`Mutator Assists`)并将根对象入队；\n2. 恢复执行程序，标记进程和用于协助的用户程序会开始并发标记内存中的对象，写屏障会将被覆盖的指针和新指针都标记成灰色，而所有新创建的对象都会被直接标记成黑色；\n3. 开始扫描根对象，包括所有`Goroutine`的栈、全局对象以及不在堆中的运行时数据结构，扫描`Goroutine`栈期间会暂停当前处理器；\n4. 依次处理灰色队列中的对象，将对象标记成黑色并将它们指向的对象标记成灰色；\n5. 使用分布式的终止算法检查剩余的工作，发现标记阶段完成后进入标记终止阶段；\n**标记终止阶段**\n1. 暂停程序、将状态切换至`_GCmarktermination` 并关闭辅助标记的用户程序；\n2. 清理处理器上的线程缓存；\n**清理阶段**\n 1. 将状态切换至`_GCoff` 开始清理阶段、初始化清理状态并关闭写屏障；\n 2. 恢复用户程序，所有新创建的对象会标记成白色；\n 3. 后台并发清理所有的内存管理单元，当`Goroutine`申请新的内存管理单元时就会触发清理；\n**GC触发时机**\n当足触发垃圾收集的基本条件：允许垃圾收集、程序没有崩溃并且没有处于垃圾循环；\n注：运行时会通过如下所示的`runtime.gcTrigger.test`方法决定是否需要触发垃圾收集，该方法会根据三种不同方式触发进行不同的检查。\n```go\nfunc (t gcTrigger) test() bool {\n\tif !memstats.enablegc || panicking != 0 || gcphase != _GCoff {\n\t\treturn false\n\t}\n\tswitch t.kind {\n\tcase gcTriggerHeap:\n\t\treturn memstats.heap_live \u003e= memstats.gc_trigger\n\tcase gcTriggerTime:\n\t\tif gcpercent \u003c 0 {\n\t\t\treturn false\n\t\t}\n\t\tlastgc := int64(atomic.Load64(\u0026memstats.last_gc_nanotime))\n\t\treturn lastgc != 0 \u0026\u0026 t.now-lastgc \u003e forcegcperiod\n\tcase gcTriggerCycle:\n\t\treturn int32(t.n-work.cycles) \u003e 0\n\t}\n\treturn true\n}\n```\n\n- 超过内存大小阙值，分配内存时，当前已分配内存与上一次`GC`结束时存活对象的内存达到某个比例时就触发`GC`。(默认配置会在堆内存达到上一次垃圾收集的2倍时，触发新一轮的垃圾收集，可以通过环境变量`GOGC`调整，在默认情况下他的值为100，即增长100%的堆内存才会触发`GC`)；比如一次回收完毕后，内存的使用量为5M，那么下次回收的机制则是内存分配达到10M的时候，也就是说，并不是内存分配越多，垃圾回收频率越高。\n- 如果一直达不到内存大小的阙值，`sysmon`检测出一段时间内（由`runtime.forcegcperiod`变量控制，默认为2分钟）没有触发过`GC`，就会触发新的GC。\n- 调用`runtime.GC()`强制触发`GC`\n\n**GC调优**  \n减少堆内存的分配是最好的优化方式。比如合理重复利用对象；避免`string`和`byte[]`之间的转化等，两者发生转换的时候，底层数据结构会进行复制，因此导致gc效率会变低，少量使用`+`连接`string`，Go里面`string`是最基础的类型，是一个只读类型，针对他的每一个操作都会创建一个新的`string`，如果是少量小文本拼接，用`“+”`就好，如果是大量小文本拼接，用`strings.Join`;如果是大量大文本拼接，用`bytes.Buffer`。  \n优化努力的方向：\n\n- 尽可能保持最小的堆内存\n- 最佳的GC频率\n- 保持每次垃圾收集的内存大小\n- 最小化每次垃圾收集的STW和Mark Assist的持续时间\n\n## 4.channel\n\n### 底层实现\n\n![](Pasted%20image%2020230403125702.png)\n\n- `buf`是有缓冲的channel所特有的结构，用来存储缓存数据。是个循环链表\n- `sendx`和`recvx`用于记录`buf`这个循环链表中的~~发送或者接收的~~index\n- `lock`是个互斥锁。\n- `recvq`和`sendq`分别是接收(\u003c-channel)或者发送(channel\u003c-xxx)的goroutine抽象出来的结构体(sudog)的队列，是个双向链表。  \n**在堆中创建，实际上就是在堆中实例化了一个如上的结构体，channel本身就是一个指针，所以函数中直接传递，生产或者消费数据时，先加锁，然后sendx\\recvx变动。**\n\n## 5.grpc\n\ngRPC 和标准库的 RPC 框架有一个区别，gRPC 生成的接口并不支持异步调用。不过我们可以在多个 Goroutine 之间安全地共享 gRPC 底层的 HTTP/2 连接，因此可以通过在另一个 Goroutine 阻塞调用的方式模拟异步调用。\n\n## 6.性能调优\n\n- CPU profile：报告程序的 CPU 使用情况，按照一定频率去采集应用程序在 CPU 和寄存器上面的数据\n- Memory Profile（Heap Profile）：报告程序的内存使用情况\n- Block Profiling：报告 goroutines 不在运行状态的情况，可以用来分析和查找死锁等性能瓶颈\n- Goroutine Profiling：报告 goroutines 的使用情况，有哪些 goroutine，它们的调用关系是怎样的\n\n### 采集性能数据\n\nGo语言内置了获取程序的运行数据的工具，包括以下两个标准库：\n\n- `runtime/pprof`：采集工具型应用运行数据进行分析\n- `net/http/pprof`：采集服务型应用运行时数据进行分析  \npprof开启后，每隔一段时间（10ms）就会收集下当前的堆栈信息，获取各个函数占用的CPU以及内存资源；最后通过对这些采样数据进行分析，形成一个性能分析报告。  \n注意，我们只应该在性能测试的时候才在代码中引入pprof。\n\n### 可视化\n\n![](Pasted%20image%2020230405142852.png)\n","lastmodified":"2023-05-09T16:33:58.275366009Z","tags":[]},"/HOMEPAGE":{"title":"HOMEPAGE","content":"\n## 最近编辑的笔记\n\n```dataview\ntable WITHOUT ID file.link AS \"最近编辑过的笔记\",file.mtime as \"时间\"\nfrom \"\"\nsort file.mtime desc\nlimit 5\n```\n\n^427b8c\n\n## 七天内创建的笔记\n\n```dataview\ntable file.ctime as 创建时间\nfrom \"\"\nwhere date(today) - file.ctime \u003c=dur(7 days)\nsort file.ctime desc\nlimit 10\n```\n\n![[∑ 本库 ACCESS 的文件夹入口汇总]]\n\n```dataviewjs\nlet allFiles = dv.pages()\n\nlet ftMd = allFiles.file.sort(t =\u003e t.cday)[0]\nlet total = parseInt([new Date() - ftMd.ctime] / (60*60*24*1000))\ndv.paragraph(`==已使用obsidian== **${total}** 天`)\n\ndv.paragraph(`==文件总数== **${allFiles.length}** 个`)\n\ndv.span(`==标签== **${allFiles.file.tags.distinct().length}** 个`)\ndv.span(\"; \")\ndv.span(`==文件夹数== **${allFiles.file.folder.distinct().length}** 个`)\ndv.span(\"; \")\ndv.span(`==文件别名== **${allFiles.file.aliases.distinct().length}** 个`)\n\n// 统计未创建链接文件的正向链接总数，即broken links counts\nconst unresolvedLinks  = Object.values(app.metadataCache.unresolvedLinks)\n.filter(unresolved =\u003e Object.keys(unresolved).length \u003e 0 )\n.flatMap(unresolved =\u003e Object.keys(unresolved))\n.map(f =\u003e \"[[\" + f + \"]]\");\n//console.log(unresolvedLinks)\n\n// 统计入链数为0的文件数，即未被其他页面引用的文件数，即orphaned files counts\nconst orphanedFiles = allFiles.filter(f =\u003e f.file.inlinks.length==0)\n//console.log(orphanedFiles)\n\ndv.paragraph(\"\\n\")\ndv.span(`==正向链接== **${allFiles.file.outlinks.length}** 个`)\ndv.span(\"; \")\ndv.span(`==未创建== **${unresolvedLinks.length}** 个`)\n\ndv.paragraph(\"\\n\")\ndv.span(`==反向链接== **${allFiles.file.inlinks.length}** 个`)\ndv.span(\"; \")\ndv.span(`==孤立文件== **${orphanedFiles.length}** 个`)\n\nlet mocFiles = dv.pages(\"#索引笔记\")\nlet tocFiles = dv.pages(\"#目录笔记\")\n\ndv.paragraph(`==MOC文件== **${mocFiles.length}** 个，==TOC文件== **${tocFiles.length}** 个`)\n\nlet ankiFiles = dv.pages(\"#复习回顾\")\nlet todoFiles = dv.pages(\"#待办\")\n\ndv.paragraph(`==anki卡片== **${ankiFiles.length}** 个，==待办文件== **${todoFiles.length}** 个`)\n\n```\n","lastmodified":"2023-05-09T16:33:58.275366009Z","tags":["花园"]},"/HTTP":{"title":"HTTP","content":"\n## HTTP 基本概念\n\nHTTP 是超文本传输协议，也就是**H**yperText **T**ransfer **P**rotocol。\n\n![提纲](640.webp)\n\nHTTP 的名字「超文本协议传输」，它可以拆成三个部分：\n\n- 超文本\n\n  我们先来理解「文本」，在互联网早期的时候只是简单的字符文字，但现在「文本」。的涵义已经可以扩展为图片、视频、压缩包等，在 HTTP 眼里这些都算做「文本」。\n\n  再来理解「超文本」，它就是**超越了普通文本的文本**，它是文字、图片、视频等的混合体最关键有超链接，能从一个超文本跳转到另外一个超文本。\n\n  HTML 就是最常见的超文本了，它本身只是纯文字文件，但内部用很多标签定义了图片、视频等的链接，在经过浏览器的解释，呈现给我们的就是一个文字、有画面的网页了。\n\n  OK，经过了对 HTTP 里这三个名词的详细解释，就可以给出比「超文本传输协议」这七个字更准确更有技术含量的答案：\n\n  **HTTP 是一个在计算机世界里专门在「两点」之间「传输」文字、图片、音频、视频等「超文本」数据的「约定和规范」。**\n\n  \u003e 那「HTTP 是用于从互联网服务器传输超文本到本地浏览器的协议 HTTP」 ，这种说法正确吗？\n\n  这种说法是**不正确**的。因为也可以是「服务器\u003c -- \u003e服务器」，所以采用**两点之间**的描述会更准确。\n\n- 传输\n\n  HTTP 协议是一个**双向协议**。\n\n  我们在上网冲浪时，浏览器是请求方 A ，百度网站就是应答方 B。双方约定用 HTTP 协议来通信，于是浏览器把请求数据发送给网站，网站再把一些数据返回给浏览器，最后由浏览器渲染在屏幕，就可以看到图片、视频了。\n\n- 协议\n\n![三个部分](640-1585478385616.webp)\n\n## HTTP 常见的状态码\n\n![五大类 HTTP 状态码](640-1585478459403.webp)\n\n### *1xx*\n\n`1xx` 类状态码属于**提示信息**，是协议处理中的一种中间状态，实际用到的比较少。\n\n### *2xx*\n\n`2xx` 类状态码表示服务器**成功**处理了客户端的请求，也是我们最愿意看到的状态。\n\n「**200 OK**」是最常见的成功状态码，表示一切正常。如果是非 `HEAD` 请求，服务器返回的响应头都会有 body 数据。\n\n「**204 No Content**」也是常见的成功状态码，与 200 OK 基本相同，但响应头没有 body 数据。\n\n「**206 Partial Content**」是应用于 HTTP 分块下载或断电续传，表示响应返回的 body 数据并不是资源的全部，而是其中的一部分，也是服务器处理成功的状态。\n\n### *3xx*\n\n`3xx` 类状态码表示客户端请求的资源发送了变动，需要客户端用新的 URL 重新发送请求获取资源，也就是**重定向**。\n\n「**301 Moved Permanently**」表示永久重定向，说明请求的资源已经不存在了，需改用新的 URL 再次访问。\n\n「**302 Moved Permanently**」表示临时重定向，说明请求的资源还在，但暂时需要用另一个 URL 来访问。\n\n301 和 302 都会在响应头里使用字段 `Location`，指明后续要跳转的 URL，浏览器会自动重定向新的 URL。\n\n「**304 Not Modified**」不具有跳转的含义，表示资源未修改，重定向已存在的缓冲文件，也称缓存重定向，用于缓存控制。\n\n### *4xx*\n\n`4xx` 类状态码表示客户端发送的**报文有误**，服务器无法处理，也就是错误码的含义。\n\n「**400 Bad Request**」表示客户端请求的报文有错误，但只是个笼统的错误。\n\n「**403 Forbidden**」表示服务器禁止访问资源，并不是客户端的请求出错。\n\n「**404 Not Found**」表示请求的资源在服务器上不存在或未找到，所以无法提供给客户端。\n\n### *5xx*\n\n`5xx` 类状态码表示客户端请求报文正确，但是**服务器处理时内部发生了错误**，属于服务器端的错误码。\n\n「**500 Internal Server Error**」与 400 类型，是个笼统通用的错误码，服务器发生了什么错误，我们并不知道。\n\n「**501 Not Implemented**」表示客户端请求的功能还不支持，类似“即将开业，敬请期待”的意思。\n\n「**502 Bad Gateway**」通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，访问后端服务器发生了错误。\n\n「**503 Service Unavailable**」表示服务器当前很忙，暂时无法响应服务器，类似“网络服务正忙，请稍后重试”的意思。\n\n## 常见字段\n\n*Host*\n\n客户端发送请求时，用来指定服务器的域名。\n\n![img](640-1585478540707.webp)\n\n```\nHost: www.A.com\n```\n\n有了 `Host` 字段，就可以将请求发往「同一台」服务器上的不同网站。\n\n*Content-Length 字段*\n\n服务器在返回数据时，会有 `Content-Length` 字段，表明本次回应的数据长度。\n\n![img](640-1585478540725.webp)\n\n```\nContent-Length: 1000\n```\n\n如上面则是告诉浏览器，本次服务器回应的数据长度是 1000 个字节，后面的字节就属于下一个回应了。\n\n*Connection 字段*\n\n`Connection` 字段最常用于客户端要求服务器使用 TCP 持久连接，以便其他请求复用。\n\n![image](640-1585478540732.webp)\n\nHTTP/1.1 版本的默认连接都是持久连接，但为了兼容老版本的 HTTP，需要指定 `Connection` 首部字段的值为 `Keep-Alive`。\n\n```\nConnection: keep-alive\n```\n\n一个可以复用的 TCP 连接就建立了，直到客户端或服务器主动关闭连接。但是，这不是标准字段。\n\n*Content-Type 字段*\n\n`Content-Type` 字段用于服务器回应时，告诉客户端，本次数据是什么格式。\n\n![img](640-1585478540744.webp)\n\n```\nContent-Type: text/html; charset=utf-8\n```\n\n上面的类型表明，发送的是网页，而且编码是 UTF-8。\n\n客户端请求的时候，可以使用 `Accept` 字段声明自己可以接受哪些数据格式。\n\n```\nAccept: */*\n```\n\n上面代码中，客户端声明自己可以接受任何格式的数据。\n\n*Content-Encoding 字段*\n\n`Content-Encoding` 字段说明数据的压缩方法。表示服务器返回的数据使用了什么压缩格式\n\n![img](640-1585478540753.webp)\n\n```\nContent-Encoding: gzip\n```\n\n上面表示服务器返回的数据采用了 gzip 方式压缩，告知客户端需要用此方式解压。\n\n客户端在请求时，用 `Accept-Encoding` 字段说明自己可以接受哪些压缩方法。\n\n```\nAccept-Encoding: gzip, deflate\n```\n\n## HTTPS\n\n### 实现原理\n\n大家可能都听说过 HTTPS 协议之所以是安全的是因为 HTTPS 协议会对传输的数据进行加密，而加密过程是使用了非对称加密实现。但其实，HTTPS 在内容传输的加密上使用的是对称加密，非对称加密只作用在证书验证阶段。\n\n相关文章：\n\n[你连 HTTPS 原理都不懂，还讲“中间人攻击”？](http://mp.weixin.qq.com/s?__biz=MzI5ODI5NDkxMw==\u0026mid=2247492349\u0026idx=1\u0026sn=a338476276aead0654bc22bdbbc27682\u0026chksm=ecaaa913dbdd20050c54422158ce78e0a412ce4237268be0c9d7bf72f03e8addd2e02d1a3f40\u0026scene=21#wechat_redirect)\n\n[HTTPS 原理分析：带着疑问层层深入](http://mp.weixin.qq.com/s?__biz=MzI5ODI5NDkxMw==\u0026mid=2247491506\u0026idx=2\u0026sn=45d3c0a831abed3d80fe56ac293a8724\u0026chksm=eca9545cdbdedd4a279dc5ca11dfeee981b316d152867365614a53a10cbfd1810823c4e075ff\u0026scene=21#wechat_redirect)\n\n[HTTPS 加密原理](http://mp.weixin.qq.com/s?__biz=MzI5ODI5NDkxMw==\u0026mid=2247488437\u0026idx=1\u0026sn=1bd2e7dec37999efae9a3d35ceb15605\u0026chksm=eca9585bdbded14d0f198ce352ff3678d025b84eeabb4a354e08ee3df4212f8c615f2a34bb80\u0026scene=21#wechat_redirect)\n\nHTTPS 的整体过程分为证书验证和数据传输阶段，具体的交互过程如下：\n\n![WX20191127-133805@2x.png](pics/640.png)\n\n**① 证书验证阶段**\n\n1. 浏览器发起 HTTPS 请求\n2. 服务端返回 HTTPS 证书\n3. 客户端验证证书是否合法，如果不合法则提示告警\n\n**② 数据传输阶段**\n\n1. 当证书验证合法后，在本地生成随机数\n2. 通过公钥加密随机数，并把加密后的随机数传输到服务端\n3. 服务端通过私钥对随机数进行解密\n4. 服务端通过客户端传入的随机数构造对称加密算法，对返回结果内容进行加密后传输\n\n### **为什么数据传输是用对称加密？**\n\n首先，非对称加密的加解密效率是非常低的，而 http 的应用场景中通常端与端之间存在大量的交互，非对称加密的效率是无法接受的；\n\n另外，在 HTTPS 的场景中只有服务端保存了私钥，一对公私钥只能实现单向的加解密，所以 HTTPS 中内容传输加密采取的是对称加密，而不是非对称加密。\n\n### **为什么需要 CA 认证机构颁发证书？**\n\nHTTP 协议被认为不安全是因为传输过程容易被监听者勾线监听、伪造服务器，而 HTTPS 协议主要解决的便是网络传输的安全性问题。\n\n首先我们假设不存在认证机构，任何人都可以制作证书，这带来的安全风险便是经典的 **“中间人攻击”** 问题。  \n“中间人攻击”的具体过程如下：\n\n![WX20191126-212406@2x.png](640-20200708200146435.png)\n\n过程原理：\n\n1. 本地请求被劫持（如 DNS 劫持等），所有请求均发送到中间人的服务器\n2. 中间人服务器返回中间人自己的证书\n3. 客户端创建随机数，通过中间人证书的公钥对随机数加密后传送给中间人，然后凭随机数构造对称加密对传输内容进行加密传输\n4. 中间人因为拥有客户端的随机数，可以通过对称加密算法进行内容解密\n5. 中间人以客户端的请求内容再向正规网站发起请求\n6. 因为中间人与服务器的通信过程是合法的，正规网站通过建立的安全通道返回加密后的数据\n7. 中间人凭借与正规网站建立的对称加密算法对内容进行解密\n8. 中间人通过与客户端建立的对称加密算法对正规内容返回的数据进行加密传输\n9. 客户端通过与中间人建立的对称加密算法对返回结果数据进行解密\n\n由于缺少对证书的验证，所以客户端虽然发起的是 HTTPS 请求，但客户端完全不知道自己的网络已被拦截，传输内容被中间人全部窃取。\n\n### **浏览器是如何确保 CA 证书的合法性？**\n\n##### 1. 证书包含什么信息？\n\n- 颁发机构信息\n- 公钥\n- 公司信息\n- 域名\n- 有效期\n- 指纹\n- ……\n\n##### 2. 证书的合法性依据是什么？\n\n首先，权威机构是要有认证的，不是随便一个机构都有资格颁发证书，不然也不叫做权威机构。另外，证书的可信性基于信任制，权威机构需要对其颁发的证书进行信用背书，只要是权威机构生成的证书，我们就认为是合法的。所以权威机构会对申请者的信息进行审核，不同等级的权威机构对审核的要求也不一样，于是证书也分为免费的、便宜的和贵的。\n\n##### 3. 浏览器如何验证证书的合法性？\n\n浏览器发起 HTTPS 请求时，服务器会返回网站的 SSL 证书，浏览器需要对证书做以下验证：\n\n1. 验证域名、有效期等信息是否正确。证书上都有包含这些信息，比较容易完成验证；\n2. 判断证书来源是否合法。每份签发证书都可以根据验证链查找到对应的根证书，操作系统、浏览器会在本地存储权威机构的根证书，利用本地根证书可以对对应机构签发证书完成来源验证；\n\n   ![WX20191127-084216@2x.png](640-20200708200404189.png)\n\n3. 判断证书是否被篡改。需要与 CA 服务器进行校验；\n4. 判断证书是否已吊销。通过 CRL（Certificate Revocation List 证书注销列表）和 OCSP（Online Certificate Status Protocol 在线证书状态协议）实现，其中 OCSP 可用于第 3 步中以减少与 CA 服务器的交互，提高验证效率。\n\n以上任意一步都满足的情况下浏览器才认为证书是合法的。\n\n\u003e 这里插一个我想了很久的但其实答案很简单的问题：  \n\u003e 既然证书是公开的，如果要发起中间人攻击，我在官网上下载一份证书作为我的服务器证书，那客户端肯定会认同这个证书是合法的，如何避免这种证书冒用的情况？\n\u003e\n\u003e 其实这就是非加密对称中公私钥的用处，虽然中间人可以得到证书，但私钥是无法获取的，一份公钥是不可能推算出其对应的私钥，中间人即使拿到证书也无法伪装成合法服务端，因为无法对客户端传入的加密数据进行解密。\n\n##### 4. 只有认证机构可以生成证书吗？\n\n如果需要浏览器不提示安全风险，那只能使用认证机构签发的证书。但浏览器通常只是提示安全风险，并不限制网站不能访问，所以从技术上谁都可以生成证书，只要有证书就可以完成网站的  \nHTTPS 传输。例如早期的 12306 采用的便是手动安装私有证书的形式实现 HTTPS 访问。\n\n### **本地随机数被窃取怎么办？**\n\n证书验证是采用非对称加密实现，但是传输过程是采用对称加密，而其中对称加密算法中重要的随机数是由本地生成并且存储于本地的，HTTPS 如何保证随机数不会被窃取？\n\n其实 HTTPS 并不包含对随机数的安全保证，HTTPS 保证的只是传输过程安全，而随机数存储于本地，本地的安全属于另一安全范畴，应对的措施有安装杀毒软件、反木马、浏览器升级修复漏洞等。\n\n### **用了 HTTPS 会被抓包吗？**\n\nHTTPS 的数据是加密的，常规下抓包工具代理请求后抓到的包内容是加密状态，无法直接查看。\n\n但是，正如前文所说，浏览器只会提示安全风险，如果用户授权仍然可以继续访问网站，完成请求。因此，只要客户端是我们自己的终端，我们授权的情况下，便可以组建中间人网络，而抓包工具便是作为中间人的代理。通常 HTTPS 抓包工具的使用方法是会生成一个证书，用户需要手动把证书安装到客户端中，然后终端发起的所有请求通过该证书完成与抓包工具的交互，然后抓包工具再转发请求到服务器，最后把服务器返回的结果在控制台输出后再返回给终端，从而完成整个请求的闭环。\n\n既然 HTTPS 不能防抓包，那 HTTPS 有什么意义？  \nHTTPS 可以防止用户在不知情的情况下通信链路被监听，对于主动授信的抓包操作是不提供防护的，因为这个场景用户是已经对风险知情。要防止被抓包，需要采用应用级的安全防护，例如采用私有的对称加密，同时做好移动端的防反编译加固，防止本地算法被破解。\n\n### **总结 Q\u0026A**\n\n以下用简短的 Q\u0026A 形式进行全文总结：\n\nQ: HTTPS 为什么安全？  \nA: 因为 HTTPS 保证了传输安全，防止传输过程被监听、防止数据被窃取，可以确认网站的真实性。\n\nQ: HTTPS 的传输过程是怎样的？  \nA: 客户端发起 HTTPS  \n请求，服务端返回证书，客户端对证书进行验证，验证通过后本地生成用于改造对称加密算法的随机数，通过证书中的公钥对随机数进行加密传输到服务端，服务端接收后通过私钥解密得到随机数，之后的数据交互通过对称加密算法进行加解密。\n\nQ: 为什么需要证书？  \nA: 防止”中间人“攻击，同时可以为网站提供身份证明。\n\nQ: 使用 HTTPS 会被抓包吗？  \nA: 会被抓包，HTTPS 只防止用户在不知情的情况下通信被监听，如果用户主动授信，是可以构建“中间人”网络，代理软件可以对传输内容进行解密。\n\n顺手 po 一张学习的过程图，高清大图点这里 ☞ HTTPS 学习草稿图.jpg\n\n![img](640-20200708200146801.png)\n\n## 抓包\n\n![提纲](640-20200714184859285.png)\n\n### 显形“不可见”的网络包\n\n网络世界中的数据包交互我们肉眼是看不见的，它们就好像隐形了一样，我们对着课本学习计算机网络的时候就会觉得非常的抽象，加大了学习的难度。\n\n还别说，我自己在大学的时候，也是如此。\n\n直到工作后，认识了两大分析网络的利器：**tcpdump 和 Wireshark**，这两大利器把我们“看不见”的数据包，呈现在我们眼前，一目了然。\n\n唉，当初大学学习计网的时候，要是能知道这两个工具，就不会学的一脸懵逼。\n\n\u003e tcpdump 和 Wireshark 有什么区别？\n\ntcpdump 和 Wireshark 就是最常用的网络抓包和分析工具，更是分析网络性能必不可少的利器。\n\n- tcpdump 仅支持命令行格式使用，常用在 Linux 服务器中抓取和分析网络包。\n- Wireshark 除了可以抓包外，还提供了可视化分析网络包的图形页面。\n\n所以，这两者实际上是搭配使用的，先用 tcpdump 命令在 Linux 服务器上抓包，接着把抓包的文件拖出到 Windows 电脑后，用 Wireshark 可视化分析。\n\n当然，如果你是在 Windows 上抓包，只需要用 Wireshark 工具就可以。\n\n\u003e tcpdump 在 Linux 下如何抓包？\n\ntcpdump 提供了大量的选项以及各式各样的过滤表达式，来帮助你抓取指定的数据包，不过不要担心，只需要掌握一些常用选项和过滤表达式，就可以满足大部分场景的需要了。\n\n假设我们要抓取下面的 ping 的数据包：\n\n![img](640-20200714184900088.png)\n\n要抓取上面的 ping 命令数据包，首先我们要知道 ping 的数据包是 `icmp` 协议，接着在使用 tcpdump 抓包的时候，就可以指定只抓 icmp 协议的数据包：\n\n![img](640-20200714184859970.png)\n\n那么当 tcpdump 抓取到 icmp 数据包后， 输出格式如下：\n\n![img](640-20200714184859952.png)\n\n![img](640-20200714184900398.png)\n\n从 tcpdump 抓取的 icmp 数据包，我们很清楚的看到 `icmp echo` 的交互过程了，首先发送方发起了 `ICMP echo request` 请求报文，接收方收到后回了一个 `ICMP echo reply` 响应报文，之后 `seq` 是递增的。\n\n我在这里也帮你整理了一些最常见的用法，并且绘制成了表格，你可以参考使用。\n\n首先，先来看看常用的选项类，在上面的 ping 例子中，我们用过 `-i` 选项指定网口，用过 `-nn` 选项不对 IP 地址和端口名称解析。其他常用的选项，如下表格：\n\n![tcpdump 常用选项类](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)tcpdump 常用选项类\n\n接下来，我们再来看看常用的过滤表用法，在上面的 ping 例子中，我们用过的是 `icmp and host 183.232.231.174`，表示抓取 icmp 协议的数据包，以及源地址或目标地址为 183.232.231.174 的包。其他常用的过滤选项，我也整理成了下面这个表格。\n\n![tcpdump 常用过滤表达式类](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)tcpdump 常用过滤表达式类\n\n说了这么多，你应该也发现了，tcpdump 虽然功能强大，但是输出的格式并不直观。\n\n所以，在工作中 tcpdump 只是用来抓取数据包，不用来分析数据包，而是把 tcpdump 抓取的数据包保存成 pcap 后缀的文件，接着用 Wireshark 工具进行数据包分析。\n\n\u003e Wireshark 工具如何分析数据包？\n\nWireshark 除了可以抓包外，还提供了可视化分析网络包的图形页面，同时，还内置了一系列的汇总分析工具。\n\n比如，拿上面的 ping 例子来说，我们可以使用下面的命令，把抓取的数据包保存到 ping.pcap 文件\n\n![img](640-20200714184900102.png)\n\n接着把 ping.pcap 文件拖到电脑，再用 Wireshark 打开它。打开后，你就可以看到下面这个界面：\n\n![img](640-20200714184859490.png)\n\n是吧？在 Wireshark 的页面里，可以更加直观的分析数据包，不仅展示各个网络包的头部信息，还会用不同的颜色来区分不同的协议，由于这次抓包只有 ICMP 协议，所以只有紫色的条目。\n\n接着，在网络包列表中选择某一个网络包后，在其下面的网络包详情中，**可以更清楚的看到，这个网络包在协议栈各层的详细信息**。比如，以编号 1 的网络包为例子：\n\n![ping 网络包](640-20200714185326608.png)ping 网络包\n\n- 可以在数据链路层，看到 MAC 包头信息，如源 MAC 地址和目标 MAC 地址等字段；\n- 可以在 IP 层，看到 IP 包头信息，如源 IP 地址和目标 IP 地址、TTL、IP 包长度、协议等 IP 协议各个字段的数值和含义；\n- 可以在 ICMP 层，看到 ICMP 包头信息，比如 Type、Code 等 ICMP 协议各个字段的数值和含义；\n\nWireshark 用了分层的方式，展示了各个层的包头信息，把“不可见”的数据包，清清楚楚的展示了给我们，还有理由学不好计算机网络吗？是不是**相见恨晚**？\n\n从 ping 的例子中，我们可以看到网络分层就像有序的分工，每一层都有自己的责任范围和信息，上层协议完成工作后就交给下一层，最终形成一个完整的网络包。\n\n![img](640-20200714185326902.png)\n\n---\n\n### 解密 TCP 三次握手和四次挥手\n\n既然学会了 tcpdump 和 Wireshark 两大网络分析利器，那我们快马加鞭，接下用它俩抓取和分析 HTTP 协议网络包，并理解 TCP 三次握手和四次挥手的工作原理。\n\n本次例子，我们将要访问的 http://192.168.3.200 服务端。在终端一用 tcpdump 命令抓取数据包：\n\n![img](640-20200714185326964.png)\n\n接着，在终端二执行下面的 curl 命令：\n\n![img](640-20200714185326884.png)\n\n最后，回到终端一，按下 Ctrl+C 停止 tcpdump，并把得到的 http.pcap 取出到电脑。\n\n使用 Wireshark 打开 http.pcap 后，你就可以在 Wireshark 中，看到如下的界面：\n\n![HTTP 网络包](640-20200714185326999.png)HTTP 网络包\n\n我们都知道 HTTP 是基于 TCP 协议进行传输的，那么：\n\n- 最开始的 3 个包就是 TCP 三次握手建立连接的包\n- 中间是 HTTP 请求和响应的包\n- 而最后的 3 个包则是 TCP 断开连接的挥手包\n\nWireshark 可以用时序图的方式显示数据包交互的过程，从菜单栏中，点击 统计 (Statistics) -\u003e 流量图 (Flow Graph)，然后，在弹出的界面中的「流量类型」选择 「TCP Flows」，你可以更清晰的看到，整个过程中 TCP 流的执行过程：\n\n![TCP 流量图](640-20200714185326968.png)TCP 流量图\n\n\u003e 你可能会好奇，为什么三次握手连接过程的 Seq 是 0 ？\n\n实际上是因为 Wireshark 工具帮我们做了优化，它默认显示的是序列号 seq 是相对值，而不是真实值。\n\n如果你想看到实际的序列号的值，可以右键菜单， 然后找到「协议首选项」，接着找到「Relative Seq」后，把它给取消，操作如下：\n\n![取消序列号相对值显示](640-20200714185326860.png)取消序列号相对值显示\n\n取消后，Seq 显示的就是真实值了：\n\n![TCP 流量图](640-20200714185326984.jpeg)TCP 流量图\n\n可见，客户端和服务端的序列号实际上是不同的，序列号是一个随机值。\n\n这其实跟我们书上看到的 TCP 三次握手和四次挥手很类似，作为对比，你通常看到的 TCP 三次握手和四次挥手的流程，基本是这样的：\n\n![TCP 三次握手和四次挥手的流程](640-20200714185327231.png)TCP 三次握手和四次挥手的流程\n\n\u003e 为什么抓到的 TCP 挥手是三次，而不是书上说的四次？\n\n因为服务器端收到客户端的 `FIN` 后，服务器端同时也要关闭连接，这样就可以把 `ACK` 和 `FIN` 合并到一起发送，节省了一个包，变成了“三次挥手”。\n\n而通常情况下，服务器端收到客户端的 `FIN` 后，很可能还没发送完数据，所以就会先回复客户端一个 `ACK` 包，稍等一会儿，完成所有数据包的发送后，才会发送 `FIN` 包，这也就是四次挥手了。\n\n如下图，就是四次挥手的过程：\n\n![四次挥手](640-20200714185327090.jpeg)四次挥手\n\n---\n\n### TCP 三次握手异常情况实战分析\n\nTCP 三次握手的过程相信大家都背的滚瓜烂熟，那么你有没有想过这三个异常情况：\n\n- **TCP 第一次握手的 SYN 丢包了，会发生了什么？**\n- **TCP 第二次握手的 SYN、ACK 丢包了，会发生什么？**\n- **TCP 第三次握手的 ACK 包丢了，会发生什么？**\n\n有的小伙伴可能说：“很简单呀，包丢了就会重传嘛。”\n\n那我在继续问你：\n\n- 那会重传几次？\n- 超时重传的时间 RTO 会如何变化？\n- 在 Linux 下如何设置重传次数？\n- ….\n\n是不是哑口无言，无法回答？\n\n不知道没关系，接下里我用三个实验案例，带大家一起探究探究这三种异常。\n\n#### 实验场景\n\n本次实验用了两台虚拟机，一台作为服务端，一台作为客户端，它们的关系如下：\n\n![实验环境](640-20200714185327144.png)实验环境\n\n- 客户端和服务端都是 CentOs 6.5 Linux，Linux 内核版本 2.6.32\n- 服务端 192.168.12.36，apache web 服务\n- 客户端 192.168.12.37\n\n#### 实验一：TCP 第一次握手 SYN 丢包\n\n为了模拟 TCP 第一次握手 SYN 丢包的情况，我是在拔掉服务器的网线后，立刻在客户端执行 curl 命令：\n\n![img](640-20200714185327226-4724007.png)\n\n其间 tcpdump 抓包的命令如下：\n\n![img](640-20200714185327226.png)\n\n过了一会， curl 返回了超时连接的错误：\n\n![img](640-20200714185327252.png)\n\n从 `date` 返回的时间，可以发现在超时接近 1 分钟的时间后，curl 返回了错误。\n\n接着，把 tcp_sys_timeout.pcap 文件用 Wireshark 打开分析，显示如下图：\n\n![SYN 超时重传五次](640-20200714185327256.png)SYN 超时重传五次\n\n从上图可以发现， 客户端发起了 SYN 包后，一直没有收到服务端的 ACK ，所以一直超时重传了 5 次，并且每次 RTO 超时时间是不同的：\n\n- 第一次是在 1 秒超时重传\n- 第二次是在 3 秒超时重传\n- 第三次是在 7 秒超时重传\n- 第四次是在 15 秒超时重传\n- 第五次是在 31 秒超时重传\n\n可以发现，每次超时时间 RTO 是**指数（翻倍）上涨的**，当超过最大重传次数后，客户端不再发送 SYN 包。\n\n在 Linux 中，第一次握手的 `SYN` 超时重传次数，是如下内核参数指定的：\n\n```\n$ cat /proc/sys/net/ipv4/tcp_syn_retries\n5\n```\n\n`tcp_syn_retries` 默认值为 5，也就是 SYN 最大重传次数是 5 次。\n\n接下来，我们继续做实验，把 `tcp_syn_retries` 设置为 2 次：\n\n```\n$ echo 2 \u003e /proc/sys/net/ipv4/tcp_syn_retries\n```\n\n重传抓包后，用 Wireshark 打开分析，显示如下图：\n\n![SYN 超时重传两次](640-20200714185327304.png)SYN 超时重传两次\n\n\u003e 实验一的实验小结\n\n通过实验一的实验结果，我们可以得知，当客户端发起的 TCP 第一次握手 SYN 包，在超时时间内没收到服务端的 ACK，就会在超时重传 SYN 数据包，每次超时重传的 RTO 是翻倍上涨的，直到 SYN 包的重传次数到达 `tcp_syn_retries` 值后，客户端不再发送 SYN 包。\n\n![SYN 超时重传](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)SYN 超时重传\n\n#### 实验二：TCP 第二次握手 SYN、ACK 丢包\n\n为了模拟客户端收不到服务端第二次握手 SYN、ACK 包，我的做法是在客户端加上防火墙限制，直接粗暴的把来自服务端的数据都丢弃，防火墙的配置如下：\n\n![img](640-20200714185327454.png)\n\n接着，在客户端执行 curl 命令：\n\n![img](640-20200714185327478.png)\n\n从 `date` 返回的时间前后，可以算出大概 1 分钟后，curl 报错退出了。\n\n客户端在这其间抓取的数据包，用 Wireshark 打开分析，显示的时序图如下：\n\n![img](640-20200714185327484.png)\n\n从图中可以发现：\n\n- 客户端发起 SYN 后，由于防火墙屏蔽了服务端的所有数据包，所以 curl 是无法收到服务端的 SYN、ACK 包，当发生超时后，就会重传 SYN 包\n- 服务端收到客户的 SYN 包后，就会回 SYN、ACK 包，但是客户端一直没有回 ACK，服务端在超时后，重传了 SYN、ACK 包，**接着一会，客户端超时重传的 SYN 包又抵达了服务端，服务端收到后，超时定时器就重新计时，然后回了 SYN、ACK 包，所以相当于服务端的超时定时器只触发了一次，又被重置了。**\n- 最后，客户端 SYN 超时重传次数达到了 5 次（tcp_syn_retries 默认值 5 次），就不再继续发送 SYN 包了。\n\n所以，我们可以发现，**当第二次握手的 SYN、ACK 丢包时，客户端会超时重发 SYN 包，服务端也会超时重传 SYN、ACK 包。**\n\n\u003e 咦？客户端设置了防火墙，屏蔽了服务端的网络包，为什么 tcpdump 还能抓到服务端的网络包？\n\n添加 iptables 限制后， tcpdump 是否能抓到包 ，这要看添加的 iptables 限制条件：\n\n- 如果添加的是 `INPUT` 规则，则可以抓得到包\n- 如果添加的是 `OUTPUT` 规则，则抓不到包\n\n网络包进入主机后的顺序如下：\n\n- 进来的顺序 Wire -\u003e NIC -\u003e **tcpdump -\u003e netfilter/iptables**\n- 出去的顺序 **iptables -\u003e tcpdump** -\u003e NIC -\u003e Wire\n\n\u003e tcp_syn_retries 是限制 SYN 重传次数，那第二次握手 SYN、ACK 限制最大重传次数是多少？\n\nTCP 第二次握手 SYN、ACK 包的最大重传次数是通过 `tcp_synack_retries` 内核参数限制的，其默认值如下：\n\n```\n$ cat /proc/sys/net/ipv4/tcp_synack_retries\n5\n```\n\n是的，TCP 第二次握手 SYN、ACK 包的最大重传次数默认值是 `5` 次。\n\n为了验证 SYN、ACK 包最大重传次数是 5 次，我们继续做下实验，我们先把客户端的 `tcp_syn_retries` 设置为 1，表示客户端 SYN 最大超时次数是 1 次，目的是为了防止多次重传 SYN，把服务端 SYN、ACK 超时定时器重置。\n\n接着，还是如上面的步骤：\n\n1. 客户端配置防火墙屏蔽服务端的数据包\n2. 客户端 tcpdump 抓取 curl 执行时的数据包\n\n把抓取的数据包，用 Wireshark 打开分析，显示的时序图如下：\n\n![img](640-20200714185327500.png)\n\n从上图，我们可以分析出：\n\n- 客户端的 SYN 只超时重传了 1 次，因为 `tcp_syn_retries` 值为 1\n- 服务端应答了客户端超时重传的 SYN 包后，由于一直收不到客户端的 ACK 包，所以服务端一直在超时重传 SYN、ACK 包，每次的 RTO 也是指数上涨的，一共超时重传了 5 次，因为 `tcp_synack_retries` 值为 5\n\n接着，我把 **tcp_synack_retries 设置为 2**，`tcp_syn_retries` 依然设置为 1:\n\n```\n$ echo 2 \u003e /proc/sys/net/ipv4/tcp_synack_retries\n$ echo 1 \u003e /proc/sys/net/ipv4/tcp_syn_retries\n```\n\n依然保持一样的实验步骤进行操作，接着把抓取的数据包，用 Wireshark 打开分析，显示的时序图如下：\n\n![img](640-20200714185327464.png)\n\n可见：\n\n- 客户端的 SYN 包只超时重传了 1 次，符合 tcp_syn_retries 设置的值；\n- 服务端的 SYN、ACK 超时重传了 2 次，符合 tcp_synack_retries 设置的值\n\n\u003e 实验二的实验小结\n\n通过实验二的实验结果，我们可以得知，当 TCP 第二次握手 SYN、ACK 包丢了后，客户端 SYN 包会发生超时重传，服务端 SYN、ACK 也会发生超时重传。\n\n客户端 SYN 包超时重传的最大次数，是由 tcp_syn_retries 决定的，默认值是 5 次；服务端 SYN、ACK 包时重传的最大次数，是由 tcp_synack_retries 决定的，默认值是 5 次。\n\n#### 实验三：TCP 第三次握手 ACK 丢包\n\n为了模拟 TCP 第三次握手 ACK 包丢，我的实验方法是在服务端配置防火墙，屏蔽客户端 TCP 报文中标志位是 ACK 的包，也就是当服务端收到客户端的 TCP ACK 的报文时就会丢弃，iptables 配置命令如下：\n\n![img](640-20200714185327540.png)\n\n接着，在客户端执行如下 tcpdump 命令：\n\n![img](640-20200714185327686.png)\n\n然后，客户端向服务端发起 telnet，因为 telnet 命令是会发起 TCP 连接，所以用此命令做测试：\n\n![img](640-20200714185327715.png)\n\n此时，由于服务端收不到第三次握手的 ACK 包，所以一直处于 `SYN_RECV` 状态：\n\n![img](640-20200714185327595.png)\n\n而客户端是已完成 TCP 连接建立，处于 `ESTABLISHED` 状态：\n\n![img](640-20200714185327726.png)\n\n过了 1 分钟后，观察发现服务端的 TCP 连接不见了：\n\n![img](640-20200714185327735.png)\n\n过了 30 分别，客户端依然还是处于 `ESTABLISHED` 状态：\n\n![img](640-20200714185802114.png)\n\n接着，在刚才客户端建立的 telnet 会话，输入 123456 字符，进行发送：\n\n![img](640-20200714185327804.png)\n\n持续「好长」一段时间，客户端的 telnet 才断开连接：\n\n![img](640-20200714185327882.png)\n\n以上就是本次的实现三的现象，这里存在两个疑点：\n\n- 为什么服务端原本处于 `SYN_RECV` 状态的连接，过 1 分钟后就消失了？\n- 为什么客户端 telnet 输入 123456 字符后，过了好长一段时间，telnet 才断开连接？\n\n不着急，我们把刚抓的数据包，用 Wireshark 打开分析，显示的时序图如下：\n\n![img](640-20200714185328031.png)\n\n上图的流程：\n\n- 客户端发送 SYN 包给服务端，服务端收到后，回了个 SYN、ACK 包给客户端，此时服务端的 TCP 连接处于 `SYN_RECV` 状态；\n- 客户端收到服务端的 SYN、ACK 包后，给服务端回了个 ACK 包，此时客户端的 TCP 连接处于 `ESTABLISHED` 状态；\n- 由于服务端配置了防火墙，屏蔽了客户端的 ACK 包，所以服务端一直处于 `SYN_RECV` 状态，没有进入 `ESTABLISHED` 状态，tcpdump 之所以能抓到客户端的 ACK 包，是因为数据包进入系统的顺序是先进入 tcpudmp，后经过 iptables；\n- 接着，服务端超时重传了 SYN、ACK 包，重传了 5 次后，也就是**超过 tcp_synack_retries 的值（默认值是 5），然后就没有继续重传了，此时服务端的 TCP 连接主动中止了，所以刚才处于 SYN_RECV 状态的 TCP 连接断开了**，而客户端依然处于`ESTABLISHED` 状态；\n- 虽然服务端 TCP 断开了，但过了一段时间，发现客户端依然处于`ESTABLISHED` 状态，于是就在客户端的 telnet 会话输入了 123456 字符；\n- 此时由于服务端已经断开连接，**客户端发送的数据报文，一直在超时重传，每一次重传，RTO 的值是指数增长的，所以持续了好长一段时间，客户端的 telnet 才报错退出了，此时共重传了 15 次。**\n\n通过这一波分析，刚才的两个疑点已经解除了：\n\n- 服务端在重传 SYN、ACK 包时，超过了最大重传次数 `tcp_synack_retries`，于是服务端的 TCP 连接主动断开了。\n- 客户端向服务端发送数据包时，由于服务端的 TCP 连接已经退出了，所以数据包一直在超时重传，共重传了 15 次， telnet 就 断开了连接。\n\n\u003e TCP 第一次握手的 SYN 包超时重传最大次数是由 tcp_syn_retries 指定，TCP 第二次握手的 SYN、ACK 包超时重传最大次数是由 tcp_synack_retries 指定，那 TCP 建立连接后的数据包最大超时重传次数是由什么参数指定呢？\n\nTCP 建立连接后的数据包传输，最大超时重传次数是由 `tcp_retries2` 指定，默认值是 15 次，如下：\n\n```\n$ cat /proc/sys/net/ipv4/tcp_retries2\n15\n```\n\n如果 15 次重传都做完了，TCP 就会告诉应用层说：“搞不定了，包怎么都传不过去！”\n\n\u003e 那如果客户端不发送数据，什么时候才会断开处于 ESTABLISHED 状态的连接？\n\n这里就需要提到 TCP 的 **保活机制**。这个机制的原理是这样的：\n\n定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一个时间间隔，发送一个「探测报文」，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序。\n\n在 Linux 内核可以有对应的参数可以设置保活时间、保活探测的次数、保活探测的时间间隔，以下都为默认值：\n\n```\nnet.ipv4.tcp_keepalive_time=7200\nnet.ipv4.tcp_keepalive_intvl=75\nnet.ipv4.tcp_keepalive_probes=9\n```\n\n- tcp_keepalive_time=7200：表示保活时间是 7200 秒（2 小时），也就 2 小时内如果没有任何连接相关的活动，则会启动保活机制\n- tcp_keepalive_intvl=75：表示每次检测间隔 75 秒；\n- tcp_keepalive_probes=9：表示检测 9 次无响应，认为对方是不可达的，从而中断本次的连接。\n\n也就是说在 Linux 系统中，最少需要经过 2 小时 11 分 15 秒才可以发现一个「死亡」连接。\n\n![img](640-20200714185327924.png)\n\n这个时间是有点长的，所以如果我抓包足够久，或许能抓到探测报文。\n\n\u003e 实验三的实验小结\n\n在建立 TCP 连接时，如果第三次握手的 ACK，服务端无法收到，则服务端就会短暂处于 `SYN_RECV` 状态，而客户端会处于 `ESTABLISHED` 状态。\n\n由于服务端一直收不到 TCP 第三次握手的 ACK，则会一直重传 SYN、ACK 包，直到重传次数超过 `tcp_synack_retries` 值（默认值 5 次）后，服务端就会断开 TCP 连接。\n\n而客户端则会有两种情况：\n\n- 如果客户端没发送数据包，一直处于 `ESTABLISHED` 状态，然后经过 2 小时 11 分 15 秒才可以发现一个「死亡」连接，于是客户端连接就会断开连接。\n- 如果客户端发送了数据包，一直没有收到服务端对该数据包的确认报文，则会一直重传该数据包，直到重传次数超过 `tcp_retries2` 值（默认值 15 次）后，客户端就会断开 TCP 连接。\n\n---\n\n### TCP 快速建立连接\n\n客户端在向服务端发起 HTTP GET 请求时，一个完整的交互过程，需要 2.5 个 RTT 的时延。\n\n由于第三次握手是可以携带数据的，这时如果在第三次握手发起 HTTP GET 请求，需要 2 个 RTT 的时延。\n\n但是在下一次（不是同个 TCP 连接的下一次）发起 HTTP GET 请求时，经历的 RTT 也是一样，如下图：\n\n![常规 HTTP 请求](640-20200714185328037.png)常规 HTTP 请求\n\n在 Linux 3.7 内核版本中，提供了 TCP Fast Open 功能，这个功能可以减少 TCP 连接建立的时延。\n\n![常规 HTTP 请求 与 Fast Open HTTP 请求](640-20200714185328040.png)常规 HTTP 请求 与 Fast Open HTTP 请求\n\n- 在第一次建立连接的时候，服务端在第二次握手产生一个 `Cookie` （已加密）并通过 SYN、ACK 包一起发给客户端，于是客户端就会缓存这个 `Cookie`，所以第一次发起 HTTP Get 请求的时候，还是需要 2 个 RTT 的时延；\n- 在下次请求的时候，客户端在 SYN 包带上 `Cookie` 发给服务端，就提前可以跳过三次握手的过程，因为 `Cookie` 中维护了一些信息，服务端可以从 `Cookie` 获取 TCP 相关的信息，这时发起的 HTTP GET 请求就只需要 1 个 RTT 的时延；\n\n注：客户端在请求并存储了 Fast Open Cookie 之后，可以不断重复 TCP Fast Open 直至服务器认为 Cookie 无效（通常为过期）\n\n\u003e 在 Linux 上如何打开 Fast Open 功能？\n\n可以通过设置 `net.ipv4.tcp_fastopn` 内核参数，来打开 Fast Open 功能。\n\nnet.ipv4.tcp_fastopn 各个值的意义:\n\n- 0 关闭\n- 1 作为客户端使用 Fast Open 功能\n- 2 作为服务端使用 Fast Open 功能\n- 3 无论作为客户端还是服务器，都可以使用 Fast Open 功能\n\n\u003e TCP Fast Open 抓包分析\n\n在下图，数据包 7 号，客户端发起了第二次 TCP 连接时，SYN 包会携带 Cooike，并且有长度为 5 的数据。\n\n服务端收到后，校验 Cooike 合法，于是就回了 SYN、ACK 包，并且确认应答收到了客户端的数据包，ACK = 5 + 1 = 6\n\n![TCP Fast Open 抓包分析](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)TCP Fast Open 抓包分析\n\n---\n\n### TCP 重复确认和快速重传\n\n当接收方收到乱序数据包时，会发送重复的 ACK，以使告知发送方要重发该数据包，**当发送方收到 3 个重复 ACK 时，就会触发快速重传，立该重发丢失数据包。**\n\n![快速重传机制](640-20200714185328044.png)快速重传机制\n\nTCP 重复确认和快速重传的一个案例，用 Wireshark 分析，显示如下：\n\n![img](640-20200714185328155.png)\n\n- 数据包 1 期望的下一个数据包 Seq 是 1，但是数据包 2 发送的 Seq 却是 10945，说明收到的是乱序数据包，于是回了数据包 3 ，还是同样的 Seq = 1，Ack = 1，这表明是重复的 ACK；\n- 数据包 4 和 6 依然是乱序的数据包，于是依然回了重复的 ACK；\n- 当对方收到三次重复的 ACK 后，于是就快速重传了 Seq = 1 、Len = 1368 的数据包 8；\n- 当收到重传的数据包后，发现 Seq = 1 是期望的数据包，于是就发送了确认报文 ACK；\n\n注意：快速重传和重复 ACK 标记信息是 Wireshark 的功能，非数据包本身的信息。\n\n以上案例在 TCP 三次握手时协商开启了**选择性确认 SACK**，因此一旦数据包丢失并收到重复 ACK ，即使在丢失数据包之后还成功接收了其他数据包，也只需要重传丢失的数据包。如果不启用 SACK，就必须重传丢失包之后的每个数据包。\n\n如果要支持 `SACK`，必须双方都要支持。在 Linux 下，可以通过 `net.ipv4.tcp_sack` 参数打开这个功能（Linux 2.4 后默认打开）。\n\n---\n\n### TCP 流量控制\n\nTCP 为了防止发送方无脑的发送数据，导致接收方缓冲区被填满，所以就有了滑动窗口的机制，它可利用接收方的接收窗口来控制发送方要发送的数据量，也就是流量控制。\n\n接收窗口是由接收方指定的值，存储在 TCP 头部中，它可以告诉发送方自己的 TCP 缓冲空间区大小，这个缓冲区是给应用程序读取数据的空间：\n\n- 如果应用程序读取了缓冲区的数据，那么缓冲空间区的就会把被读取的数据移除\n- 如果应用程序没有读取数据，则数据会一直滞留在缓冲区。\n\n接收窗口的大小，是在 TCP 三次握手中协商好的，后续数据传输时，接收方发送确认应答 ACK 报文时，会携带当前的接收窗口的大小，以此来告知发送方。\n\n假设接收方接收到数据后，应用层能很快的从缓冲区里读取数据，那么窗口大小会一直保持不变，过程如下：\n\n![理想状态下的窗口变化](640-20200714185328193.png)理想状态下的窗口变化\n\n但是现实中服务器会出现繁忙的情况，当应用程序读取速度慢，那么缓存空间会慢慢被占满，于是为了保证发送方发送的数据不会超过缓冲区大小，则服务器会调整窗口大小的值，接着通过 ACK 报文通知给对方，告知现在的接收窗口大小，从而控制发送方发送的数据大小。\n\n![服务端繁忙状态下的窗口变化](640-20200714185328351.png)服务端繁忙状态下的窗口变化\n\n#### 零窗口通知与窗口探测\n\n假设接收方处理数据的速度跟不上接收数据的速度，缓存就会被占满，从而导致接收窗口为 0，当发送方接收到零窗口通知时，就会停止发送数据。\n\n如下图，可以接收方的窗口大小在不断的收缩至 0：\n\n![窗口大小在收缩](640-20200714185328210.png)窗口大小在收缩\n\n接着，发送方会**定时发送窗口大小探测报文**，以便及时知道接收方窗口大小的变化。\n\n以下图 Wireshark 分析图作为例子说明：\n\n![零窗口 与 窗口探测](640-20200714185328257.png)零窗口 与 窗口探测\n\n- 发送方发送了数据包 1 给接收方，接收方收到后，由于缓冲区被占满，回了个零窗口通知；\n- 发送方收到零窗口通知后，就不再发送数据了，直到过了 `3.4` 秒后，发送了一个 TCP Keep-Alive 报文，也就是窗口大小探测报文；\n- 当接收方收到窗口探测报文后，就立马回一个窗口通知，但是窗口大小还是 0；\n- 发送方发现窗口还是 0，于是继续等待了 `6.8`（翻倍） 秒后，又发送了窗口探测报文，接收方依然还是回了窗口为 0 的通知；\n- 发送方发现窗口还是 0，于是继续等待了 `13.5`（翻倍） 秒后，又发送了窗口探测报文，接收方依然还是回了窗口为 0 的通知；\n\n可以发现，这些窗口探测报文以 3.4s、6.5s、13.5s 的间隔出现，说明超时时间会**翻倍**递增。\n\n这连接暂停了 25s，想象一下你在打王者的时候，25s 的延迟你还能上王者吗？\n\n#### 发送窗口的分析\n\n\u003e 在 Wireshark 看到的 Windows size 也就是 \" win = \"，这个值表示发送窗口吗？\n\n这不是发送窗口，而是在向对方声明自己的接收窗口。\n\n你可能会好奇，抓包文件里有「Window size scaling factor」，它其实是算出实际窗口大小的乘法因子，「Windos size value」实际上并不是真实的窗口大小，真实窗口大小的计算公式如下：\n\n「Windos size value」 \\* 「Window size scaling factor」 = 「Caculated window size 」\n\n对应的下图案例，也就是 32 \\* 2048 = 65536。\n\n![img](640-20200714185328304.png)\n\n实际上是 Caculated window size 的值是 Wireshark 工具帮我们算好的，Window size scaling factor 和 Windos size value 的值是在 TCP 头部中，其中 Window size scaling factor 是在三次握手过程中确定的，如果你抓包的数据没有 TCP 三次握手，那可能就无法算出真实的窗口大小的值，如下图：\n\n![img](640-20200714185328277.png)\n\n\u003e 如何在包里看出发送窗口的大小？\n\n很遗憾，没有简单的办法，发送窗口虽然是由接收窗口决定，但是它又可以被网络因素影响，也就是拥塞窗口，实际上发送窗口是值是 min(拥塞窗口，接收窗口)。\n\n\u003e 发送窗口和 MSS 有什么关系？\n\n发送窗口决定了一口气能发多少字节，而 MSS 决定了这些字节要分多少包才能发完。\n\n举个例子，如果发送窗口为 16000 字节的情况下，如果 MSS 是 1000 字节，那就需要发送 1600/1000 = 16 个包。\n\n\u003e 发送方在一个窗口发出 n 个包，是不是需要 n 个 ACK 确认报文？\n\n不一定，因为 TCP 有累计确认机制，所以当收到多个数据包时，只需要应答最后一个数据包的 ACK 报文就可以了。\n\n---\n\n### TCP 延迟确认与 Nagle 算法\n\n当我们 TCP 报文的承载的数据非常小的时候，例如几个字节，那么整个网络的效率是很低的，因为每个 TCP 报文中都有会 20 个字节的 TCP 头部，也会有 20 个字节的 IP 头部，而数据只有几个字节，所以在整个报文中有效数据占有的比重就会非常低。\n\n这就好像快递员开着大货车送一个小包裹一样浪费。\n\n那么就出现了常见的两种策略，来减少小报文的传输，分别是：\n\n- Nagle 算法\n- 延迟确认\n\n\u003e Nagle 算法是如何避免大量 TCP 小数据报文的传输？\n\nNagle 算法做了一些策略来避免过多的小数据报文发送，这可提高传输效率。\n\nNagle 算法的策略：\n\n- 没有已发送未确认报文时，立刻发送数据。\n- 存在未确认报文时，直到「没有已发送未确认报文」或「数据长度达到 MSS 大小」时，再发送数据。\n\n只要没满足上面条件中的一条，发送方一直在囤积数据，直到满足上面的发送条件。\n\n![禁用 Nagle 算法 与 启用 Nagle 算法](640-20200714185328602.png)禁用 Nagle 算法 与 启用 Nagle 算法\n\n上图右侧启用了 Nagle 算法，它的发送数据的过程：\n\n- 一开始由于没有已发送未确认的报文，所以就立刻发了 H 字符；\n- 接着，在还没收到对 H 字符的确认报文时，发送方就一直在囤积数据，直到收到了确认报文后，此时就没有已发送未确认的报文，于是就把囤积后的 ELL 字符一起发给了接收方；\n- 待收到对 ELL 字符的确认报文后，于是把最后一个 O 字符发送出去\n\n可以看出，**Nagle 算法一定会有一个小报文，也就是在最开始的时候。**\n\n另外，Nagle 算法默认是打开的，如果对于一些需要小数据包交互的场景的程序，比如，telnet 或 ssh 这样的交互性比较强的程序，则需要关闭 Nagle 算法。\n\n可以在 Socket 设置 `TCP_NODELAY` 选项来关闭这个算法（关闭 Nagle 算法没有全局参数，需要根据每个应用自己的特点来关闭）。\n\n![关闭 Nagle 算法](640-20200714185328689.png)关闭 Nagle 算法\n\n\u003e 那延迟确认又是什么？\n\n事实上当没有携带数据的 ACK，他的网络效率也是很低的，因为它也有 40 个字节的 IP 头 和 TCP 头，但没有携带数据。\n\n为了解决 ACK 传输效率低问题，所以就衍生出了 **TCP 延迟确认**。\n\nTCP 延迟确认的策略：\n\n- 当有响应数据要发送时，ACK 会随着响应数据一起立刻发送给对方\n- 当没有响应数据要发送时，ACK 将会延迟一段时间，以等待是否有响应数据可以一起发送\n- 如果在延迟等待发送 ACK 期间，对方的第二个数据报文又到达了，这时就会立刻发送 ACK\n\n![TCP 延迟确认](640-20200714185328709.png)TCP 延迟确认\n\n延迟等待的时间是在 Linux 内核中的定义的，如下图：\n\n![img](640-20200714185328559.png)\n\n关键就需要 `HZ` 这个数值大小，HZ 是跟系统的时钟频率有关，每个操作系统都不一样，在我的 Linux 系统中 HZ 大小是 `1000`，如下图：\n\n![img](640-20200714185328645.png)\n\n知道了 HZ 的大小，那么就可以算出：\n\n- 最大延迟确认时间是 `200` ms （1000/5）\n- 最短延迟确认时间是 `40` ms （1000/25）\n\nTCP 延迟确认可以在 Socket 设置 `TCP_QUICKACK` 选项来关闭这个算法。\n\n![关闭 TCP 延迟确认](640-20200714185328710.png)关闭 TCP 延迟确认\n\n\u003e 延迟确认 和 Nagle 算法混合使用时，会产生新的问题\n\n当 TCP 延迟确认 和 Nagle 算法混合使用时，会导致时耗增长，如下图：\n\n![TCP 延迟确认 和 Nagle 算法混合使用](640-20200714185328823.png)TCP 延迟确认 和 Nagle 算法混合使用\n\n发送方使用了 Nagle 算法，接收方使用了 TCP 延迟确认会发生如下的过程：\n\n- 发送方先发出一个小报文，接收方收到后，由于延迟确认机制，自己又没有要发送的数据，只能干等着发送方的下一个报文到达；\n- 而发送方由于 Nagle 算法机制，在未收到第一个报文的确认前，是不会发送后续的数据；\n- 所以接收方只能等待最大时间 200 ms 后，才回 ACK 报文，发送方收到第一个报文的确认报文后，也才可以发送后续的数据。\n\n很明显，这两个同时使用会造成额外的时延，这就会使得网络\"很慢\"的感觉。\n\n要解决这个问题，只有两个办法：\n\n- 要么发送方关闭 Nagle 算法\n- 要么接收方关闭 TCP 延迟确认\n","lastmodified":"2023-05-09T16:33:58.275366009Z","tags":[]},"/Html%E5%9F%BA%E7%A1%80":{"title":"html","content":"\nh5的文档声明，声明当前的网页是按照HTML5标准编写的\n\n编写网页时一定要将h5的文档声明写在网页的最上边\n\n如果不写文档声明，则会导致有些浏览器会进入一个怪异模式，\n\n进入怪异模式以后，浏览器解析页面会导致页l面无法正常显示，所以为了避免进入该模式，一定要写文档声明\n\n```html\n\u003c!doctype *html*\u003e\n```\n\n## 实体\n\n```html\n\u003c!-- \n\t\t\t在HTML中，一些如\u003c \u003e这种特殊字符是不能直接使用，\n\t\t\t\t需要使用一些特殊的符号来表示这些特殊字符，这些特殊符号我们称为实体（转义字符串）\n\t\t\t\t浏览器解析到实体时，会自动将实体转换为其对应的字符\n\t\t\t实体的语法：\n\t\t\t\t\u0026实体的名字;\n\t\t\t\t\t\u003c  \u0026lt;\n\t\t\t\t\t\u003e  \u0026gt;\n\t\t\t\t\t空格  \u0026nbsp;\n\t\t\t\t\t版权符号 \u0026copy;\n\t\t--\u003e\n\t\ta\u0026lt;b\u0026gt;c\n\t\t\u003cp\u003e\u0026copy;\u0026divide;今天天气\u0026nbsp;\u0026nbsp;\u0026nbsp;好晴朗，处处好风光\u003c/p\u003e\n```\n\n[w3](https://www.w3school.com.cn/html/html_entities)\n\n## 块元素和内联元素和行内块元素\n\n### 块元素\n\n常见的块元素有`/\u003ch1\u003e~\u003ch6\u003e、\u003cp\u003e、\u003cdiv\u003e、\u003cul\u003e、\u003col\u003e、\u003cli\u003e`等，其中div标签是最典型的块元素。\n\n块级元素的特点：\n\n- 比较霸道，自己独占一行,无论他的内容有多少。\n- 高度，宽度、外边距以及内边距都可以控制。\n- 宽度默认是容器（父级宽度）的100%。\n- 是一个容器及盒子，里面可以放行内或者块级元素。\n\n  注意：\n\n  - 文字类的元素内不能使用块级元素\n  - \\\u003cp\u003e标签主要用于存放文字，因此里面不能放块级元素，特别是不能放\\\u003cdiv\u003e。\n  - 同理，\\\u003ch1\u003e-\\\u003ch6\u003e等都是文字类块级标签，里面也不能放其他块级元素\n\n### 内联元素\n\na strong b em i del s ins u spac iframe\n\n行内元素的特点:\n\n- 相邻行内元素在一行上，一行可以显示多个。\n- 高、宽直接设置是无效的。\n- 默认宽度就是它本身内容的宽度。\n- 行内元素只能容纳文本或其他行内元素。\n\n  注意：\n\n  - 链接里面不能再放链接\n  - 特殊情况\\\u003ca\u003e链接里面可以放块级元素，但是给\\\u003ca\u003e转换一下块级模式最安全\n\n### 行内块元素\n\n在行内元素中有几个特殊的标签img、input、td,它们同时具有块元素和行内元素的特点。 有些资料称它们为行内块元素\n\n行内块元素的特点：\n\n- 和相邻行内元素（行内块）在一行上，但是他们之间会有空白缝隙。一行可以显示多个（行内元素特点）。\n- 默认宽度就是它本身内容的宽度（行内元素特点）。\n- 高度，行高、外边距以及内边距都可以控制（块级元素特点）。\n\n## 各标签\n\n### em\u0026strong\n\n这两个标签都表示一个强调的内容，  \nem主要表示语气上的强调,em在浏览器中默认使用斜体显示  \nstrong表示强调的内容，比em更强烈，默认使用粗体显示\n\n### i\u0026b\n\ni标签中的内容会以斜体显示  \nb标签中的内容会以加粗显示\n\n\t\t\t\n\nh5规范中规定，对于不需要着重的内容而是单纯的加粗或者是斜体，就可以使用b和i标签\n\n### small\n\nsmall标签中的内容会比他的父元素中的文字要小一些  \n在h5中使用small标签来表示一些细则一类的内容  \n比如：合同中小字，网站的版权声明都可以放到small\n\n### cite\n\n网页中所有的加书名号的内容都可以使用cite标签，表示参考的内容，比如：书名 歌名 话剧名 电影名 。。。\n\n### q\n\nq标签表示一个短的引用（行内引用）  \nq标签引用的内容，浏览器会默认加上引号  \nblockquote标签表示一个长引用（块级引用）\n\n```html\n\u003cp\u003e\n\t子曰:\u003cq\u003e学而时习之不亦说乎！\u003c/q\u003e\n\u003c/p\u003e\n\t\t\n\u003cdiv\u003e\n\t子曰:\n\t\u003cblockquote\u003e\n\t\t有朋自远方来，乐呵乐呵！\n\t\u003c/blockquote\u003e\n\u003c/div\u003e\n```\n\n### sup\u0026sub\n\n使用sup/sub标签来设置一个上/下标\n\n### del\n\n使用del标签来表示一个删除的内容  \ndel标签中的内容，会自动添加删除线\n\n### ins\n\nins表示一个插入的内容  \nins中的的内容，会自动添加下划线\n\n### pre\u0026code\n\n需要页面中直接编写一些代码  \npre是一个预格式标签，会将代码中的格式保存，不会忽略多个空格  \ncode专门用来表示代码  \n我们一般结合使用pre和code来表示一段代码\n\n```php+HTML\n\u003cpre\u003e\n\t\u003ccode\u003e\n\t\twindow.onload = function(){\n\t\t\talert(\"Hello World\");\n\t\t};\n\t\u003c/code\u003e\n\u003c/pre\u003e\n```\n\n### ul\u0026li\u0026ol\n\n```html\n\u003c!-- \n列表就相当于去超市购物时的那个购物清单，\n在HTML也可以创建列表，在网页中一共有三种列表：\n\t1.无序列表\n\t2.有序列表\n\t3.定义列表\n\t\n无序列表\n\t- 使用ul标签来创建一个无序列表\n\t- 使用li在ul中创建一个一个的列表项，一个li就是一个列表项\n\t\t\t\n通过type属性可以修改无序列表的项目符号\n\t可选值：\n\t\tdisc，默认值，实心的圆点\n\t\tsquare，实心的方块\n\t\tcircle，空心的圆\n\t注意：默认的项目符号我们一般都不使用！！\n\t如果需要设置项目符号，则可以采用为li设置背景图片的方式来设置\t\n\t!!!ul和li都是块元素\t\n--\u003e\n\u003cul\u003e\n\t\u003cli\u003e西门大官人\u003c/li\u003e\n\t\u003cli\u003e柴大官人\u003c/li\u003e\n\t\u003cli\u003e许大官人\u003c/li\u003e\n\t\u003cli\u003e唐僧大官人\u003c/li\u003e\n\u003c/ul\u003e\n\t\t\n\u003c!-- \n\t有序列表和无序列表类似，只不过它使用ol来代替ul\n\t有序列表使用有序的序号作为项目符号\n\ttype属性，可以指定序号的类型\n\t\t可选值：1，默认值，使用阿拉伯数字\n\t\t\t\ta/A 采用小写或大写字母作为序号\n\t\t\t\ti/I 采用小写或大写的罗马数字作为序号\n\t\t\t\t\n\tol也是块元素\t\t\t\n--\u003e\n\u003col type=\"I\"\u003e\n\t\u003cli\u003e结构\u003c/li\u003e\n\t\u003cli\u003e表现\u003c/li\u003e\n\t\u003cli\u003e行为\u003c/li\u003e\n\u003c/ol\u003e\n\t\t\n\u003c!-- \n\t列表之间都是可以互相嵌套，可以在无序列表中放个有序列表\n\t\t也可以在有序列表中放一个无序列表\n--\u003e\t\n\u003cp\u003e菜谱\u003c/p\u003e\n\u003cul\u003e\n\t\u003cli\u003e\n\t\t鱼香肉丝\n\t\t\u003col\u003e\n\t\t\t\u003cli\u003e鱼\u003c/li\u003e\n\t\t\t\u003cli\u003e香\u003c/li\u003e\n\t\t\t\u003cli\u003e肉丝\u003c/li\u003e\n\t\t\u003c/ol\u003e\n\t\u003c/li\u003e\n\t\u003cli\u003e\n\t\t宫保鸡丁\n\t\t\u003cul\u003e\n\t\t\t\u003cli\u003e宫保\u003c/li\u003e\n\t\t\t\u003cli\u003e鸡丁\u003c/li\u003e\n\t\t\u003c/ul\u003e\n\t\u003c/li\u003e\n\t\u003cli\u003e青椒肉丝\u003c/li\u003e\n\u003c/ul\u003e\t\t\n```\n\n### dl\u0026dt\u0026dd\n\n```html\n\u003c!--\n\t定义列表用来对一些词汇或内容进行定义\n\t使用dl来创建一个定义列表\n\t\tdl中有两个子标签\n\t\t\tdt ： 被定义的内容\n\t\t\tdd ： 对定义内容的描述\n\t同样dl和ul和ol之间都可以互相嵌套\t\t\n--\u003e\n\u003cdl\u003e\n\t\u003cdt\u003e武松\u003c/dt\u003e\n\t\u003cdd\u003e景阳冈打虎英雄，战斗力99\u003c/dd\u003e\n\t\u003cdd\u003e后打死西门庆，投奔梁山\u003c/dd\u003e\n\t\u003cdt\u003e武大\u003c/dt\u003e\n\t\u003cdd\u003e著名餐饮企业家，战斗力0\u003c/dd\u003e\n\u003c/dl\u003e\n```\n\n### center\n\n```html\n\u003c!-- center标签中的内容，会默认在页面中居中显示 \n我们可以将要居中的元素，全都放到center中--\u003e\t\t\n\u003ccenter\u003e\n\t\u003cp\u003e我是一个p标签\u003c/p\u003e\n\u003c/center\u003e\n```\n\n### a\n\n```html\n\u003c!-- \n使用超链接可以让我们从一个页面跳转到另一个页面\n使用a标签来创建一个超链接\t\n属性：\n\thref:指向链接跳转的目标地址,可以写一个相对路径也可以写一个完整的地址\n\t\t--\u003e\n\u003ca href=\"http://www.baidu.com\"\u003e我是一个超链接\u003c/a\u003e \u003cbr /\u003e\u003cbr /\u003e\n\u003ca href=\"http://www.baidu1234567.com\"\u003e我是一个超链接\u003c/a\u003e \u003cbr /\u003e\u003cbr /\u003e\n\t\t\n\u003c!-- \na标签中的target属性可以用来指定打开链接的位置\n可选值：\n\t_self，表示在当前窗口中打开（默认值）\n\t_blank，在新的窗口中打开链接\n\t可以设置一个内联框架的name属性值，链接将会在指定的内联框架中打开\n\t\t--\u003e\n\u003ca href=\"demo03.html\" target=\"tom\"\u003e我是一个超链接\u003c/a\u003e\n\u003ciframe src=\"demo02.html\" name=\"tom\"\u003e\u003c/iframe\u003e\n```\n\n### iframe\n\n```html\n\u003c!-- \n使用内联框架可以引入一个外部的页面\n使用iframe来创建一个内联框架\n属性：\nsrc ：指向一个外部页面的路径，可以使用相对路径\nwidth：\nheight：\nname ：可以为内联框架指定一个name属性\n\n在现实开发中不推荐使用内联框架，因为内联框架中的内容不会被搜索引擎所检索\t\t\n--\u003e\n\u003ciframe src=\"demo02.html\" name=\"tom\"\u003e\u003c/iframe\u003e\n```\n\n### meta\n\n```html\n\u003c!-- \n\t使用meta标签还可以用来设置网页的关键字\n--\u003e\n\u003cmeta name=\"keywords\" content=\"HTML5,JavaScript,前端,Java\" /\u003e\n\t\t\n\u003c!-- \n\t还可以用来指定网页的描述\n\t搜索引擎在检索页面时，会同时检索页面中的关键词和描述，但是这两个值不会影响页面在搜索引擎中\n--\u003e\n\u003cmeta name=\"description\" content=\"发布h5、js等前端相关的信息\" /\u003e\n\t\t\n\t\t\n\u003c!-- \n\t使用meta可以用来做请求的重定向\n\t\u003cmeta http-equiv=\"refresh\" content=\"秒数;url=目标路径\" /\u003e\n--\u003e\n\u003cmeta http-equiv=\"refresh\" content=\"5;url=http://www.baidu.com\" /\u003e\n```\n\n### p\n\n```html\n\u003c!-- \n\t\t\t在HTML中，字符之间写再多的空格，浏览器也会当成一个空格解析，\n\t\t\t\t换行也会当成一个空格解析。\n\t\t\t在页面中可以使用br标签来表示一个换行，br标签是一个自结束标签\t\n\t\t--\u003e\n\t\t\u003cp\u003e\n\t\t\t锄禾日当午，\u003cbr /\u003e\n\t\t\t汗滴禾下土，\u003cbr /\u003e\n\t\t\t谁知盘中餐，\u003cbr /\u003e\n\t\t\t粒粒皆辛苦。\u003cbr /\u003e\n\t\t\u003c/p\u003e\n```\n\n## Html5\n\n### DOCTYPE\n\nDOCTYPE，或者称为 Document Type Declaration（文档类型声明，缩写 DTD）  \n通常情况下，DOCTYPE 位于一个 HTML 文档的最前面的位置，位于根元素 HTML 的起始标签之前。  \n因为浏览器必须在解析 HTML 文档正文之前就确定当前文档的类型，以决定其需要采用的渲染模式，  \n不同的渲染模式会影响到浏览器对于 CSS 代码甚至 JavaScript 脚本的解析。\n\nHTML5提供的\u003cDOCTYPE html\u003e是标准模式，向后兼容的,等同于开启了标准模式，那么浏览器就得老老实实的按照W3C的 标准解析渲染页面。一个不含任何 DOCTYPE 的网页将会以 怪异(quirks) 模式渲染。\n\n### 根元素\n\nH4中的根元素:  \n\\\u003chtml xmlns=\"http://www.w3.org/1999/xhtml\"\u003e\n\n首先这个标记没有任何问题，你喜欢的话,那就背下来继续用。它是有效的。但这个标记中的很多字节在Html5中我们都可以省略了。\n\nxmlns:这是XHTML1.0的东西，它的意思是在这个页面上的元素都位于http://www.w3.org/1999/xhtml这个命名空间内。但是HTML5中的每个元素都具有这个命名空间，不需要在页面上再显示指出。\n\nH5中的根元素\\\u003chtml\u003e\\\u003c/html\u003e\n\n### 语义化标签\n\n以前布局，我们基本用 div 来做。div 对于**搜索引擎**来说，是没有语义的。\n\n```html\n\u003cheader\u003e：头部标签，代表 网页 或 section 的页眉。通常包含h1-h6元素或hgroup。\n\u003cnav\u003e：导航标签,素代表页面的导航链接区域。用于定义页面的主要导航部分。\n\u003carticle\u003e：内容标签,最容易跟section和div容易混淆，其实article代表一个在文档，页面或者网站中自成一体的内容.\n\u003csection\u003e：定义文档某个区域,代表文档中的 节 或 段，段可以是指一篇文章里按照主题的分段；节可以是指一个页面里的分组。\n\u003caside\u003e：侧边栏标签,被包含在article元素中作为主要内容的附属信息部分，其中的内容可以是与当前文章有关的相关资料、标签、名次解释等。\n\u003cfooter\u003e：尾部标签，代表 网页 或 section 的页脚，通常含有该节的一些基本信息，譬如：作者，相关文档链接，版权资料。\n\u003chgroup\u003e元素代表 网页 或 section 的标题，当元素有多个层级时，该元素可以将h1到h6元素放在其内，譬如文章的主标题和副标题的组合。\n\t\u003chgroup\u003e\n\t   \u003ch1\u003eHTML 5\u003c/h1\u003e\n\t   \u003ch2\u003e这是一篇介绍HTML 5语义化标签和更简洁的结构\u003c/h2\u003e\n\t\u003c/hgroup\u003e\nhgroup使用注意：\n\t\t如果只需要一个h1-h6标签就不用hgroup\n\t\t如果有连续多个h1-h6标签就用hgroup\n\t\t如果有连续多个标题和其他文章数据，h1-h6标签就用hgroup包住，和其他文章元数据一起放入header标签\n```\n\n![image-20200213183856903](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/html/20210410190240.png)\n\n- 这种语义化标准主要是针对搜索引擎的\n- 这些新标签页面中可以使用多次\n- 在 IE9 中，需要把这些元素转换为块级元素\n- 其实，我们移动端更喜欢使用这些标签\n- 他们这些标签功能就是代替\u003cdiv\u003e功能中的一部分，他们没有任何的默认样式，除了会让文本另起一行外；  \n  https://gsnedders.html5.org/outliner/\n\n### video\n\n![image-20200213183956705](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/html/20210410190248.png)\n\n![image-20200213184014153](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/html/20210410190252.png)\n\n### audio\n\n![image-20200213184028075](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/html/20210410190258.png)\n\n![image-20200213184040129](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/html/20210410190302.png)\n\n### input\n\n![image-20200213184102254](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/html/20210410190306.png)\n\n![image-20200213184114083](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/html/20210410190309.png)\n\n这种语义化标准主要是针对搜索引擎的  这些新标签页面中可以使用多次  在 IE9 中，需要把这些元素转换为块级元素  其实，我们移动端更喜欢使用这些标签  HTML5 还增加了很多其他标签，我们后面再慢慢学\n","lastmodified":"2023-05-09T16:33:58.275366009Z","tags":[]},"/Image":{"title":"Image","content":"\n## 一、基础知识\n\n### 1.1 位图\n\n**「位图图像（bitmap），亦称为点阵图像或栅格图像，是由称作像素（图片元素）的单个点组成的。」** 这些点可以进行不同的排列和染色以构成图样。当放大位图时，可以看见赖以构成整个图像的无数单个方块。扩大位图尺寸的效果是增大单个像素，从而使线条和形状显得参差不齐。\n\n**「用数码相机拍摄的照片、扫描仪扫描的图片以及计算机截屏图等都属于位图。」** 位图的特点是可以表现色彩的变化和颜色的细微过渡，产生逼真的效果，缺点是在保存时需要记录每一个像素的位置和颜色值，占用较大的存储空间。常用的位图处理软件有 Photoshop、Painter 和 Windows 系统自带的画图工具等。\n\n分辨率是位图不可逾越的壁垒，在对位图进行缩放、旋转等操作时，无法生产新的像素，因此会放大原有的像素填补空白，这样会让图片显得不清晰。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/common/img/20210410164033.png)\n\n图中的小方块被称为像素，这些小方块都有一个明确的位置和被分配的色彩数值，小方格颜色和位置就决定该图像所呈现出来的样子。\n\n可以将像素视为整个图像中不可分割的单位或者是元素。**「不可分割的意思是它不能够再切割成更小单位抑或是元素，它是以一个单一颜色的小格存在。」** 每一个点阵图像包含了一定量的像素，这些像素决定图像在屏幕上所呈现的大小。\n\n### 1.2 矢量图\n\n所谓矢量图，就是使用直线和曲线来描述的图形，构成这些图形的元素是一些点、线、矩形、多边形、圆和弧线等，**「****它们都是通过数学公式计算获得的，具有编辑后不失真的特点。\\**」\\****例如一幅画的矢量图形实际上是由线段形成外框轮廓，由外框的颜色以及外框所封闭的颜色决定画显示出的颜色。\n\n**「矢量图以几何图形居多，图形可以无限放大，不变色、不模糊。」** 常用于图案、标志、VI、文字等设计。常用软件有：CorelDraw、Illustrator、Freehand、XARA、CAD 等。\n\n这里我们以 Web 开发者比较熟悉的 SVG（**「Scalable Vector Graphics —— 可缩放矢量图形」**）为例，来了解一下 SVG 的结构：\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/common/img/20210410164049.jpeg)\n\n可缩放矢量图形（英语：Scalable Vector Graphics，SVG）是一种基于可扩展标记语言（XML），用于描述二维矢量图形的图形格式。SVG 由 W3C 制定，是一个开放标准。\n\nSVG 主要支持以下几种显示对象：\n\n- 矢量显示对象，基本矢量显示对象包括矩形、圆、椭圆、多边形、直线、任意曲线等；\n- 嵌入式外部图像，包括 PNG、JPEG、SVG 等；\n- 文字对象。\n\n了解完位图与矢量图的区别，下面我们来介绍一下位图的数学表示。\n\n### 1.3 位图的数学表示\n\n位图的像素都分配有特定的位置和颜色值。每个像素的颜色信息由 RGB 组合或者灰度值表示。\n\n根据位深度，可将位图分为1、4、8、16、24 及 32 位图像等。每个像素使用的信息位数越多，可用的颜色就越多，颜色表现就越逼真，相应的数据量越大。\n\n**「1.3.1 二值图像」**\n\n位深度为 1 的像素位图只有两个可能的值（黑色和白色），所以又称为二值图像。二值图像的像素点只有黑白两种情况，因此每个像素点可以由 0 和 1 来表示。\n\n比如一张 4 * 4 二值图像：\n\n```\n1 1 0 1\n1 1 0 1\n1 0 0 0\n1 0 1 0\n```\n\n**「1.3.2 RGB 图像」**\n\nRGB 图像由三个颜色通道组成，其中 RGB 代表红、绿、蓝三个通道的颜色。8 位/通道的 RGB 图像中的每个通道有 256 个可能的值，这意味着该图像有 1600 万个以上可能的颜色值。\n\n有时将带有 8 位/通道（bpc）的 RGB 图像称作 24 位图像（8 位 x 3 通道 = 24 位数据/像素）。通常将使用 24 位 RGB 组合数据位表示的的位图称为真彩色位图。\n\nRGB 彩色图像可由三种矩阵表示：一种代表像素中红色的强度，一种代表绿色，另一种代表蓝色。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/common/img/20210410164056.png)\n\n**「图像处理的本质实际上就是对这些像素矩阵进行计算。」** 其实位图中的图像类型，除了二值图像和 RGB 图像之外，还有灰度图像、索引图像和 YUV 图像。这里我们不做过多介绍，感兴趣的小伙伴，请自行查阅相关资料。\n\n## 二、图片处理库\n\n### 2.1 AlloyImage 专业图像处理库\n\n\u003e ❝\n\u003e\n\u003e 基于 HTML 5 的专业级图像处理开源引擎。\n\u003e\n\u003e https://github.com/AlloyTeam/AlloyImage\n\u003e\n\u003e ❞\n\nAlloyImage 基于 HTML5 技术的专业图像处理库，来自腾讯 AlloyTeam 团队。它拥有以下功能特性：\n\n- 基于多图层操作 —— 一个图层的处理不影响其他图层；\n- 与 PS 对应的 17 种图层混合模式 —— 便于 PS 处理教程的无缝迁移；\n- 多种基本滤镜处理效果 —— 基本滤镜不断丰富、可扩展；\n- 基本的图像调节功能 —— 色相、饱和度、对比度、亮度、曲线等；\n- 简单快捷的 API —— 链式处理、API 简洁易用、传参灵活；\n- 多种组合效果封装 —— 一句代码轻松实现一种风格；\n- 接口一致的单、多线程支持 —— 单、多线程切换无需更改一行代码，多线程保持快捷 API 特性。\n\n对于该库 AlloyTeam 团队建议的使用场景如下：\n\n- 桌面软件客户端内嵌网页运行方式 \u003e\u003e\u003e 打包 Webkit 内核：用户较大头像上传风格处理、用户相册风格处理（处理时间平均 \u003c 1s）；\n- Win8 Metro 应用 \u003e\u003e\u003e 用户上传头像，比较小的图片风格处理后上传（Win8 下 IE 10 支持多线程）；\n- Mobile APP \u003e\u003e\u003e Andriod 平台、iOS 平台小图风格 Web 处理的需求，如 PhoneGap 应用，在线头像上传时的风格处理、Mobile Web 端分享图片时风格处理等。\n\n**「使用示例」**\n\n```\n// $AI或AlloyImage初始化一个AlloyImage对象\nvar ps = $AI(img, 600).save('jpg', 0.6);\n\n// save将合成图片保存成base64格式字符串\nvar string = AlloyImage(img).save('jpg', 0.8);\n\n// saveFile将合成图片下载到本地\nimg.onclick = function(){\n  AlloyImage(this).saveFile('处理后图像.jpg', 0.8);\n}\n```\n\n**「在线示例」**\n\n\u003e ❝\n\u003e\n\u003e http://alloyteam.github.io/AlloyImage/\n\u003e\n\u003e ❞\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/common/img/20210410164112.jpeg)\n\n### 2.2 blurify 图片模糊\n\n\u003e ❝\n\u003e\n\u003e blurify.js is a tiny(~2kb) library to blurred pictures, support graceful downgrade from `css` mode to `canvas` mode.\n\u003e\n\u003e https://github.com/JustClear/blurify\n\u003e\n\u003e ❞\n\nblurify.js 是一个用于图片模糊，很小的 JavaScript 库（约 2 kb），并支持从 CSS 模式到 Canvas 模式的优雅降级。该插件支持三种模式：\n\n- css 模式：使用 `filter` 属性，默认模式；\n- canvas 模式：使用 `canvas` 导出 base64；\n- auto 模式：优先使用 css 模式，否则自动切换到 canvas 模式。\n\n**「使用示例」**\n\n```javascript\nimport blurify from 'blurify';\n\nnew blurify({\n    images: document.querySelectorAll('.blurify'),\n    blur: 6,\n    mode: 'css',\n});\n\n// or in shorthand\n\nblurify(6, document.querySelectorAll('.blurify'));\n```\n\n**「在线示例」**\n\n\u003e ❝\n\u003e\n\u003e https://justclear.github.io/blurify/\n\u003e\n\u003e ❞\n\n看到这里是不是有些小伙伴觉得只是模糊处理而已，觉得不过瘾，能不能来点更酷的。嘿嘿，有求必应！阿宝哥立马来个 **「“酷炫叼”」** 的库 —— midori，该库用于为背景图创建动画，使用 three.js 编写并使用 WebGL。本来是想给个演示动图，无奈单个 Gif 文件太大，只能放个体验地址，感兴趣的小伙伴自行体验一下。\n\n\u003e ❝\n\u003e\n\u003e midori 示例地址：https://aeroheim.github.io/midori/\n\u003e\n\u003e ❞\n\n### 2.3 cropperjs 图片裁剪工具\n\n\u003e ❝\n\u003e\n\u003e JavaScript image cropper.\n\u003e\n\u003e https://github.com/fengyuanchen/cropperjs\n\u003e\n\u003e ❞\n\nCropper.js 是一款非常强大却又简单的图片裁剪工具，它可以进行非常灵活的配置，支持手机端使用，支持包括 IE9 以上的现代浏览器。它可以用于满足诸如裁剪头像上传、商品图片编辑之类的需求。\n\nCropper.js 支持以下特性：\n\n- 支持 39 个配置选项；\n- 支持 27 个方法；\n- 支持 6 种事件；\n- 支持 touch（移动端）；\n- 支持缩放、旋转和翻转；\n- 支持在画布上裁剪；\n- 支持在浏览器端通过画布裁剪图像；\n- 支持处理 Exif 方向信息；\n- 跨浏览器支持。\n\n\u003e ❝\n\u003e\n\u003e 可交换图像文件格式（英语：Exchangeable image file format，官方简称 Exif），是专门为数码相机的照片设定的文件格式，可以记录数码照片的属性信息和拍摄数据。Exif 可以附加于 JPEG、TIFF、RIFF 等文件之中，为其增加有关数码相机拍摄信息的内容和索引图或图像处理软件的版本信息。\n\u003e\n\u003e Exif 信息以 0xFFE1 作为开头标记，后两个字节表示 Exif 信息的长度。所以 Exif 信息最大为 64 kB，而内部采用 TIFF 格式。\n\u003e\n\u003e ❞\n\n**「使用示例」**\n\n```javascript\n// import 'cropperjs/dist/cropper.css';\nimport Cropper from 'cropperjs';\n\nconst image = document.getElementById('image');\nconst cropper = new Cropper(image, {\n  aspectRatio: 16 / 9,\n  crop(event) {\n    console.log(event.detail.x);\n    console.log(event.detail.y);\n    console.log(event.detail.width);\n    console.log(event.detail.height);\n    console.log(event.detail.rotate);\n    console.log(event.detail.scaleX);\n    console.log(event.detail.scaleY);\n  },\n});\n```\n\n**「在线示例」**\n\n\u003e ❝\n\u003e\n\u003e https://fengyuanchen.github.io/cropperjs/\n\u003e\n\u003e ❞\n\n### 2.4 compressorjs 图像压缩\n\n\u003e ❝\n\u003e\n\u003e JavaScript image compressor.\n\u003e\n\u003e https://github.com/fengyuanchen/compressorjs\n\u003e\n\u003e ❞\n\ncompressorjs 是 JavaScript 图像压缩器。使用浏览器原生的 `canvas.toBlob` API 进行压缩工作，这意味着它是有损压缩。通常的使用场景是，在浏览器端图片上传之前对其进行预压缩。\n\n在浏览器端要实现图片压缩，除了使用 `canvas.toBlob` API 之外，还可以使用 Canvas 提供的另一个 API，即 `toDataURL` API，它接收 `type` 和 `encoderOptions` 两个可选参数。\n\n其中 `type` 表示图片格式，默认为 `image/png`。而 `encoderOptions` 用于表示图片的质量，在指定图片格式为 `image/jpeg` 或 `image/webp` 的情况下，可以从 0 到 1 的区间内选择图片的质量。如果超出取值范围，将会使用默认值 `0.92`，其他参数会被忽略。\n\n相比 `canvas.toDataURL` API 来说，`canvas.toBlob` API 是异步的，因此多了个 `callback` 参数，这个 `callback` 回调方法默认的第一个参数就是转换好的 `blob` 文件信息。`canvas.toBlob` 的签名如下：\n\n```javascript\ncanvas.toBlob(callback, mimeType, qualityArgument)\n```\n\n**「使用示例」**\n\n```javascript\nimport axios from 'axios';\nimport Compressor from 'compressorjs';\n\n// \u003cinput type=\"file\" id=\"file\" accept=\"image/*\"\u003e\ndocument.getElementById('file').addEventListener('change', (e) =\u003e {\n  const file = e.target.files[0];\n\n  if (!file) {\n    return;\n  }\n  new Compressor(file, {\n    quality: 0.6,\n    success(result) {\n      const formData = new FormData();\n      // The third parameter is required for server\n      formData.append('file', result, result.name);\n\n      // Send the compressed image file to server with XMLHttpRequest.\n      axios.post('/path/to/upload', formData).then(() =\u003e {\n        console.log('Upload success');\n      });\n    },\n    error(err) {\n      console.log(err.message);\n    },\n  });\n});\n```\n\n**「在线示例」**\n\n\u003e ❝\n\u003e\n\u003e https://fengyuanchen.github.io/compressorjs/\n\u003e\n\u003e ❞\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/common/img/20210410164142.jpeg)\n\n### 2.5 fabric.js 轻松使用 HTML5 Canvas 元素\n\n\u003e ❝\n\u003e\n\u003e Javascript Canvas Library, SVG-to-Canvas (\u0026 canvas-to-SVG) Parser.\n\u003e\n\u003e https://github.com/fabricjs/fabric.js\n\u003e\n\u003e ❞\n\nFabric.js 是一个框架，可让你轻松使用 HTML5 Canvas 元素。它是一个位于 Canvas 元素之上的交互式对象模型，同时也是一个 **「SVG-to-canvas」** 的解析器。\n\n使用 Fabric.js，你可以在画布上创建和填充对象。所谓的对象，可以是简单的几何形状，比如矩形，圆形，椭圆形，多边形，或更复杂的形状，包含数百或数千个简单路径。然后，你可以使用鼠标缩放，移动和旋转这些对象。并修改它们的属性 —— 颜色，透明度，z-index 等。此外你还可以一起操纵这些对象，即通过简单的鼠标选择将它们分组。\n\nFabric.js 支持所有主流的浏览器，具体的兼容情况如下：\n\n- Firefox 2+\n- Safari 3+\n- Opera 9.64+\n- Chrome（所有版本）\n- IE10，IE11，Edge\n\n**「使用示例」**\n\n```javascript\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\u003c/head\u003e\n\u003cbody\u003e\n    \u003ccanvas id=\"canvas\" width=\"300\" height=\"300\"\u003e\u003c/canvas\u003e\n    \u003cscript src=\"lib/fabric.js\"\u003e\u003c/script\u003e\n    \u003cscript\u003e\n        var canvas = new fabric.Canvas('canvas');\n        var rect = new fabric.Rect({\n            top : 100,\n            left : 100,\n            width : 60,\n            height : 70,\n            fill : 'red'\n        });\n\n        canvas.add(rect);\n    \u003c/script\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n```\n\n**「在线示例」**\n\n\u003e ❝\n\u003e\n\u003e http://fabricjs.com/kitchensink\n\u003e\n\u003e ❞\n\n### 2.6 Resemble.js 图片的分析和比较\n\n\u003e ❝\n\u003e\n\u003e Image analysis and comparison\n\u003e\n\u003e https://github.com/rsmbl/Resemble.js\n\u003e\n\u003e ❞\n\nResemble.js 使用 HTML Canvas 和 JavaScript 来实现图片的分析和比较。兼容大于 8.0 的 Node.js 版本。\n\n**「使用示例」**\n\n```javascript\n// 比较两张图片\nvar diff = resemble(file)\n    .compareTo(file2)\n    .ignoreColors()\n    .onComplete(function(data) {\n        console.log(data);\n     /*\n     {\n        misMatchPercentage : 100, // %\n        isSameDimensions: true, // or false\n        dimensionDifference: { width: 0, height: -1 }, \n        getImageDataUrl: function(){}\n     }\n    */\n});\n```\n\n**「在线示例」**\n\n\u003e ❝\n\u003e\n\u003e http://rsmbl.github.io/Resemble.js/\n\u003e\n\u003e ❞\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/common/img/20210410164154.jpeg)\n\n### 2.7 Pica 调整图像大小\n\n\u003e ❝\n\u003e\n\u003e Resize image in browser with high quality and high speed\n\u003e\n\u003e https://github.com/nodeca/pica\n\u003e\n\u003e ❞\n\nPica 可用于在浏览器中调整图像大小，没有像素化并且相当快。它会自动选择最佳的可用技术：webworkers，webassembly，createImageBitmap，纯 JS。\n\n借助 Pica，你可以实现以下功能：\n\n- 减小大图像的上传大小，节省上传时间；\n- 在图像处理上节省服务器资源；\n- 在浏览器中生成缩略图。\n\n**「使用示例」**\n\n```javascript\nconst pica = require('pica')();\n\n// 调整画布/图片的大小\npica.resize(from, to, {\n  unsharpAmount: 80,\n  unsharpRadius: 0.6,\n  unsharpThreshold: 2\n})\n.then(result =\u003e console.log('resize done!'));\n\n// 调整大小并转换为Blob\npica.resize(from, to)\n  .then(result =\u003e pica.toBlob(result, 'image/jpeg', 0.90))\n  .then(blob =\u003e console.log('resized to canvas \u0026 created blob!'));\n```\n\n**「在线示例」**\n\n\u003e ❝\n\u003e\n\u003e http://nodeca.github.io/pica/demo/\n\u003e\n\u003e ❞![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)\n\n### 2.8 tui.image-editor 全功能图像编辑器\n\n\u003e ❝\n\u003e\n\u003e 🍞🎨 Full-featured photo image editor using canvas. It is really easy, and it comes with great filters.\n\u003e\n\u003e https://github.com/nhn/tui.image-editor\n\u003e\n\u003e ❞\n\ntui.image-editor 是使用 HTML5 Canvas 的全功能图像编辑器。它易于使用，并提供强大的过滤器。同时它支持对图像进行裁剪、翻转、旋转、绘图、形状、文本、遮罩和图片过滤等操作。\n\ntui.image-editor 的浏览器兼容情况如下：\n\n- Chrome\n- Edge\n- Safari\n- Firefox\n- IE 10+\n\n**「使用示例」**\n\n```javascript\n// Image editor\nvar imageEditor = new tui.ImageEditor(\"#tui-image-editor-container\", {\n     includeUI: {\n       loadImage: {\n         path: \"img/sampleImage2.png\",\n         name: \"SampleImage\",\n       },\n       theme: blackTheme, // or whiteTheme\n         initMenu: \"filter\",\n         menuBarPosition: \"bottom\",\n       },\n       cssMaxWidth: 700,\n       cssMaxHeight: 500,\n       usageStatistics: false,\n});\n\nwindow.onresize = function () {\n  imageEditor.ui.resizeEditor();\n};\n```\n\n在线示例\n\n\u003e ❝\n\u003e\n\u003e https://ui.toast.com/tui-image-editor/\n\u003e\n\u003e ❞![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)\n\n### 2.9 gif.js GIF 编码器\n\n\u003e ❝\n\u003e\n\u003e JavaScript GIF encoding library\n\u003e\n\u003e https://github.com/jnordberg/gif.js\n\u003e\n\u003e ❞\n\ngif.js 是运行在浏览器端的 JavaScript GIF 编码器。它使用类型化数组和 Web Worker 在后台渲染每一帧，速度真的很快。该库可工作在支持：Web Workers，File API 和 Typed Arrays 的浏览器中。\n\ngif.js 的浏览器兼容情况如下：\n\n- Google Chrome\n- Firefox 17\n- Safari 6\n- Internet Explorer 10\n- Mobile Safari iOS 6\n\n**「使用示例」**\n\n```javascript\nvar gif = new GIF({\n  workers: 2,\n  quality: 10\n});\n\n// add an image element\ngif.addFrame(imageElement);\n\n// or a canvas element\ngif.addFrame(canvasElement, {delay: 200});\n\n// or copy the pixels from a canvas context\ngif.addFrame(ctx, {copy: true});\n\ngif.on('finished', function(blob) {\n  window.open(URL.createObjectURL(blob));\n});\n\ngif.render();\n```\n\n**「在线示例」**\n\n\u003e ❝\n\u003e\n\u003e http://jnordberg.github.io/gif.js/\n\u003e\n\u003e ❞\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/common/img/20210410164208.jpeg)\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/common/img/20210410164210.gif)\n\n### 2.10 Sharp 图像转换为尺寸较小\n\n\u003e ❝\n\u003e\n\u003e High performance Node.js image processing, the fastest module to resize JPEG, PNG, WebP and TIFF images. Uses the libvips library.\n\u003e\n\u003e https://github.com/lovell/sharp\n\u003e\n\u003e ❞\n\nSharp 的典型应用场景是将常见格式的大图像转换为尺寸较小，对网络友好的 JPEG，PNG 和 WebP 格式的图像。由于其内部使用 libvips ，使得调整图像大小通常比使用 ImageMagick 和 GraphicsMagick 设置快 4-5 倍 。除了支持调整图像大小之外，Sharp 还支持旋转、提取、合成和伽马校正等功能。\n\nSharp 支持读取 JPEG，PNG，WebP，TIFF，GIF 和 SVG 图像。输出图像可以是 JPEG，PNG，WebP 和 TIFF 格式，也可以是未压缩的原始像素数据。\n\n**「使用示例」**\n\n```javascript\n// 改变图像尺寸\nsharp(inputBuffer)\n  .resize(320, 240)\n  .toFile('output.webp', (err, info) =\u003e { ... });\n       \n// 旋转输入图像并改变图片尺寸                                         \nsharp('input.jpg')\n  .rotate()\n  .resize(200)\n  .toBuffer()\n  .then( data =\u003e { ... })\n  .catch( err =\u003e { ... });                                         \n```\n\n**「在线示例」**\n\n\u003e ❝\n\u003e\n\u003e https://segmentfault.com/a/1190000012903787\n\u003e\n\u003e ❞\n\n该示例是来自阿宝哥 18 年写的 “Sharp 牛刀小试之生成专属分享图片” 这篇文章，主要是利用 Sharp 提供的图片合成功能为每个用户生成专属的分享海报，感兴趣的小伙伴可以阅读一下原文哟。\n\n```javascript\nconst sharp = require(\"sharp\");\nconst TextToSVG = require(\"text-to-svg\");\nconst path = require(\"path\");\n\n// 加载字体文件\nconst textToSVG = TextToSVG.loadSync(path.join(__dirname, \"./simhei.ttf\"));\n\n// 创建圆形SVG，用于实现头像裁剪\nconst roundedCorners = new Buffer(\n  '\u003csvg\u003e\u003ccircle r=\"90\" cx=\"90\" cy=\"90\"/\u003e\u003c/svg\u003e'\n);\n\n// 设置SVG文本元素相关参数\nconst attributes = { fill: \"white\" };\nconst svgOptions = {\n  x: 0,\n  y: 0,\n  fontSize: 32,\n  anchor: \"top\",\n  attributes: attributes\n};\n\n/**\n * 使用文本生成SVG\n * @param {*} text \n * @param {*} options \n */\nfunction textToSVGFn(text, options = svgOptions) {\n  return textToSVG.getSVG(text, options);\n}\n\n/**\n * 图层叠加生成分享图片\n * @param {*} options \n * \n */\nasync function genShareImage(options) {\n  const { backgroudPath, avatarPath, qrcodePath, \n    userName, words, likes, outFilePath\n  } = options;\n\n  // 背景图片\n  const backgroudBuffer = sharp(path.join(__dirname, backgroudPath)).toBuffer({\n    resolveWithObject: true\n  });\n\n  const backgroundImageInfo = await backgroudBuffer;\n  // 头像图片\n  const avatarBuffer = await genCircleAvatar(path.join(__dirname, avatarPath));\n\n  // 二维码图片\n  const qrCodeBuffer = await sharp(path.join(__dirname, qrcodePath))\n    .resize(180)\n    .toBuffer({\n      resolveWithObject: true\n    });\n\n  // 用户名\n  const userNameSVG = textToSVGFn(userName);\n  // 用户数据\n  const userDataSVG = textToSVGFn(`写了${words}个字   收获${likes}个赞`);\n  const userNameBuffer = await sharp(new Buffer(userNameSVG)).toBuffer({\n    resolveWithObject: true\n  });\n  const userDataBuffer = await sharp(new Buffer(userDataSVG)).toBuffer({\n    resolveWithObject: true\n  });\n\n  const buffers = [avatarBuffer, qrCodeBuffer, userNameBuffer, userDataBuffer];\n  // 图层叠加参数列表\n  const overlayOptions = [\n    { top: 150, left: 230 },\n    { top: 861, left: 227 },\n    {\n      top: 365,\n      left: (backgroundImageInfo.info.width - userNameBuffer.info.width) / 2\n    },\n    {\n      top: 435,\n      left: (backgroundImageInfo.info.width - userDataBuffer.info.width) / 2\n    }\n  ];\n\n  // 组合多个图层：图片+文字图层\n  return buffers\n    .reduce((input, overlay, index) =\u003e {\n      return input.then(result =\u003e {\n        console.dir(overlay.info);\n        return sharp(result.data)\n          .overlayWith(overlay.data, overlayOptions[index])\n          .toBuffer({ resolveWithObject: true });\n      });\n    }, backgroudBuffer)\n    .then((data) =\u003e {\n      return sharp(data.data).toFile(outFilePath);\n    }).catch(error =\u003e {\n      throw new Error('Generate Share Image Failed.');\n    });\n}\n\n/**\n * 生成圆形的头像\n * @param {*} avatarPath 头像路径\n */\nfunction genCircleAvatar(avatarPath) {\n  return sharp(avatarPath)\n    .resize(180, 180)\n    .overlayWith(roundedCorners, { cutout: true })\n    .png()\n    .toBuffer({\n      resolveWithObject: true\n    });\n}\n\nmodule.exports = {\n  genShareImage\n};\n```\n\n## 三、其他问题\n\n### 3.1 如何区分图片的类型\n\n**「计算机并不是通过图片的后缀名来区分不同的图片类型，而是通过 “魔数”（Magic Number）来区分。」** 对于某一些类型的文件，起始的几个字节内容都是固定的，跟据这几个字节的内容就可以判断文件的类型。\n\n常见图片类型对应的魔数如下表所示：\n\n| 文件类型 | 文件后缀 | 魔数               |\n| :------- | :------- | :----------------- |\n| JPEG     | jpg/jpeg | 0xFFD8FF           |\n| PNG      | png      | 0x89504E47         |\n| GIF      | gif      | 0x47494638（GIF8） |\n| BMP      | bmp      | 0x424D             |\n\n这里我们以阿宝哥的头像（abao.png）为例，验证一下该图片的类型是否正确：\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/common/img/20210410164217.jpeg)\n\n在日常开发过程中，如果遇到检测图片类型的场景，我们可以直接利用一些现成的第三方库。比如，你想要判断一张图片是否为 PNG 类型，这时你可以使用 is-png 这个库，它同时支持浏览器和 Node.js，使用示例如下：\n\n**「Node.js」**\n\n```javascript\n// npm install read-chunk\nconst readChunk = require('read-chunk'); \nconst isPng = require('is-png');\nconst buffer = readChunk.sync('unicorn.png', 0, 8);\n\nisPng(buffer);\n//=\u003e true\n```\n\n**「Browser」**\n\n```javascript\n(async () =\u003e {\n const response = await fetch('unicorn.png');\n const buffer = await response.arrayBuffer();\n\n isPng(new Uint8Array(buffer));\n //=\u003e true\n})();\n```\n\n### 3.2 如何获取图片的尺寸\n\n图片的尺寸、位深度、色彩类型和压缩算法都会存储在文件的二进制数据中，我们继续以阿宝哥的头像（abao.png）为例，来了解一下实际的情况：\n\n![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)\n\n\u003e ❝\n\u003e\n\u003e 528（十进制） =\u003e 0x0210\n\u003e\n\u003e 560（十进制）=\u003e 0x0230\n\u003e\n\u003e ❞\n\n因此如果想要获取图片的尺寸，我们就需要依据不同的图片格式对图片二进制数据进行解析。幸运的是，我们不需要自己做这件事，image-size 这个 Node.js 库已经帮我们实现了获取主流图片类型文件尺寸的功能：\n\n**「同步方式」**\n\n```javascript\nvar sizeOf = require('image-size');\n\nvar dimensions = sizeOf('images/abao.png');\nconsole.log(dimensions.width, dimensions.height);\n```\n\n**「异步方式」**\n\n```javascript\nvar sizeOf = require('image-size');\n\nsizeOf('images/abao.png', function (err, dimensions) {\n  console.log(dimensions.width, dimensions.height);\n});\n```\n\nimage-size 这个库功能还是蛮强大的，除了支持 PNG 格式之外，还支持 BMP、GIF、ICO、JPEG、SVG 和 WebP 等格式。\n\n### 3.3 如何预览本地图片\n\n利用 HTML FileReader API，我们也可以方便的实现图片本地预览功能，具体代码如下：\n\n```javascript\n\u003cinput type=\"file\" accept=\"image/*\" onchange=\"loadFile(event)\"\u003e\n\u003cimg id=\"output\"/\u003e\n\u003cscript\u003e\n  const loadFile = function(event) {\n    const reader = new FileReader();\n    reader.onload = function(){\n      const output = document.querySelector('output');\n      output.src = reader.result;\n    };\n    reader.readAsDataURL(event.target.files[0]);\n  };\n\u003c/script\u003e\n```\n\n在完成本地图片预览之后，可以直接把图片对应的 Data URLs 数据提交到服务器。针对这种情形，服务端需要做一些相关处理，才能正常保存上传的图片，这里以 Express 为例，具体处理代码如下：\n\n```javascript\nconst app = require('express')();\n\napp.post('/upload', function(req, res){\n    let imgData = req.body.imgData; // 获取POST请求中的base64图片数据\n    let base64Data = imgData.replace(/^data:image\\/\\w+;base64,/, \"\");\n    let dataBuffer = Buffer.from(base64Data, 'base64');\n    fs.writeFile(\"image.png\", dataBuffer, function(err) {\n        if(err){\n          res.send(err);\n        }else{\n          res.send(\"图片上传成功！\");\n        }\n    });\n});\n```\n\n### 3.4 如何实现图片压缩\n\n在一些场合中，我们希望在上传本地图片时，先对图片进行一定的压缩，然后再提交到服务器，从而减少传输的数据量。在前端要实现图片压缩，我们可以利用 Canvas 对象提供的 `toDataURL()` 方法，该方法接收 `type` 和 `encoderOptions` 两个可选参数。\n\n其中 `type` 表示图片格式，默认为 `image/png`。而 `encoderOptions` 用于表示图片的质量，在指定图片格式为 `image/jpeg` 或 `image/webp` 的情况下，可以从 0 到 1 的区间内选择图片的质量。如果超出取值范围，将会使用默认值 `0.92`，其他参数会被忽略。\n\n下面我们来看一下具体如何实现图片压缩：\n\n```javascript\nfunction compress(base64, quality, mimeType) {\n  let canvas = document.createElement(\"canvas\");\n  let img = document.createElement(\"img\");\n  img.crossOrigin = \"anonymous\";\n  return new Promise((resolve, reject) =\u003e {\n    img.src = base64;\n    img.onload = () =\u003e {\n      let targetWidth, targetHeight;\n      if (img.width \u003e MAX_WIDTH) {\n        targetWidth = MAX_WIDTH;\n        targetHeight = (img.height * MAX_WIDTH) / img.width;\n      } else {\n        targetWidth = img.width;\n        targetHeight = img.height;\n      }\n      canvas.width = targetWidth;\n      canvas.height = targetHeight;\n      let ctx = canvas.getContext(\"2d\");\n      ctx.clearRect(0, 0, targetWidth, targetHeight); // 清除画布\n      ctx.drawImage(img, 0, 0, canvas.width, canvas.height);\n      let imageData = canvas.toDataURL(mimeType, quality / 100);\n      resolve(imageData);\n    };\n  });\n}\n```\n\n对于返回的 Data URL 格式的图片数据，为了进一步减少传输的数据量，我们可以把它转换为 Blob 对象：\n\n```javascript\nfunction dataUrlToBlob(base64, mimeType) {\n  let bytes = window.atob(base64.split(\",\")[1]);\n  let ab = new ArrayBuffer(bytes.length);\n  let ia = new Uint8Array(ab);\n  for (let i = 0; i \u003c bytes.length; i++) {\n    ia[i] = bytes.charCodeAt(i);\n  }\n  return new Blob([ab], { type: mimeType });\n}\n```\n\n在转换完成后，我们就可以压缩后的图片对应的 Blob 对象封装在 FormData 对象中，然后再通过 AJAX 提交到服务器上：\n\n```javascript\nfunction uploadFile(url, blob) {\n  let formData = new FormData();\n  let request = new XMLHttpRequest();\n  formData.append(\"image\", blob);\n  request.open(\"POST\", url, true);\n  request.send(formData);\n}\n```\n\n### 3.5 如何操作位图像素数据\n\n如果想要操作图片像素数据，我们可以利用 CanvasRenderingContext2D 提供的 `getImageData` 来获取图片像素数据，其中 getImageData() 返回一个 ImageData 对象，用来描述 canvas 区域隐含的像素数据，这个区域通过矩形表示，起始点为（sx, sy）、宽为 sw、高为 sh。其中 `getImageData` 方法的语法如下：\n\n```javascript\nctx.getImageData(sx, sy, sw, sh);\n```\n\n相应的参数说明如下：\n\n- sx：将要被提取的图像数据矩形区域的左上角 x 坐标。\n- sy：将要被提取的图像数据矩形区域的左上角 y 坐标。\n- sw：将要被提取的图像数据矩形区域的宽度。\n- sh：将要被提取的图像数据矩形区域的高度。\n\n在获取到图片的像素数据之后，我们就可以对获取的像素数据进行处理，比如进行灰度化或反色处理。当完成处理后，若要在页面上显示处理效果，则我们需要利用 CanvasRenderingContext2D 提供的另一个 API —— `putImageData`。\n\n该 API 是 Canvas 2D API 将数据从已有的 ImageData 对象绘制到位图的方法。如果提供了一个绘制过的矩形，则只绘制该矩形的像素。此方法不受画布转换矩阵的影响。putImageData 方法的语法如下：\n\n```javascript\nvoid ctx.putImageData(imagedata, dx, dy);\nvoid ctx.putImageData(imagedata, dx, dy, dirtyX, dirtyY, dirtyWidth, dirtyHeight);\n```\n\n相应的参数说明如下：\n\n- imageData： `ImageData` ，包含像素值的数组对象。\n- dx：源图像数据在目标画布中的位置偏移量（x 轴方向的偏移量）。\n- dy：源图像数据在目标画布中的位置偏移量（y 轴方向的偏移量）。\n- dirtyX（可选）：在源图像数据中，矩形区域左上角的位置。默认是整个图像数据的左上角（x 坐标）。\n- dirtyY（可选）：在源图像数据中，矩形区域左上角的位置。默认是整个图像数据的左上角（y 坐标）。\n- dirtyWidth（可选）：在源图像数据中，矩形区域的宽度。默认是图像数据的宽度。\n- dirtyHeight（可选）：在源图像数据中，矩形区域的高度。默认是图像数据的高度。\n\n介绍完相关的 API，下面我们来举一个实际例子：\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"zh-CN\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"UTF-8\" /\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /\u003e\n    \u003ctitle\u003e图片反色和灰度化处理\u003c/title\u003e\n  \u003c/head\u003e\n  \u003cbody onload=\"loadImage()\"\u003e\n    \u003cdiv\u003e\n      \u003cbutton id=\"invertbtn\"\u003e反色\u003c/button\u003e\n      \u003cbutton id=\"grayscalebtn\"\u003e灰度化\u003c/button\u003e\n    \u003c/div\u003e\n    \u003ccanvas id=\"canvas\" width=\"800\" height=\"600\"\u003e\u003c/canvas\u003e\n    \u003cscript\u003e\n      function loadImage() {\n        var img = new Image();\n        img.crossOrigin = \"\";\n        img.onload = function () {\n          draw(this);\n        };\n        // 这是阿宝哥的头像哟\n        img.src = \"https://avatars3.githubusercontent.com/u/4220799\";\n      }\n\n      function draw(img) {\n        var canvas = document.getElementById(\"canvas\");\n        var ctx = canvas.getContext(\"2d\");\n        ctx.drawImage(img, 0, 0);\n        img.style.display = \"none\";\n        var imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);\n        var data = imageData.data;\n\n        var invert = function () {\n          for (var i = 0; i \u003c data.length; i += 4) {\n            data[i] = 255 - data[i]; // red\n            data[i + 1] = 255 - data[i + 1]; // green\n            data[i + 2] = 255 - data[i + 2]; // blue\n          }\n          ctx.putImageData(imageData, 0, 0);\n        };\n\n        var grayscale = function () {\n          for (var i = 0; i \u003c data.length; i += 4) {\n            var avg = (data[i] + data[i + 1] + data[i + 2]) / 3;\n            data[i] = avg; // red\n            data[i + 1] = avg; // green\n            data[i + 2] = avg; // blue\n          }\n          ctx.putImageData(imageData, 0, 0);\n        };\n\n        var invertbtn = document.getElementById(\"invertbtn\");\n        invertbtn.addEventListener(\"click\", invert);\n        var grayscalebtn = document.getElementById(\"grayscalebtn\");\n        grayscalebtn.addEventListener(\"click\", grayscale);\n      }\n    \u003c/script\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n```\n\n需要注意的在调用 `getImageData` 方法获取图片像素数据时，你可能会遇到跨域问题，比如：\n\n```\nUncaught DOMException: Failed to execute 'getImageData' on 'CanvasRenderingContext2D': The canvas has been tainted by cross-origin data.\n```\n\n对于这个问题，你可以阅读 **「张鑫旭」** 大神 “解决canvas图片getImageData,toDataURL跨域问题” 这一篇文章。\n\n### 3.6 如何实现图片隐写\n\n**「隐写术是一门关于信息隐藏的技巧与科学，所谓信息隐藏指的是不让除预期的接收者之外的任何人知晓信息的传递事件或者信息的内容。」** 隐写术的英文叫做 Steganography，来源于特里特米乌斯的一本讲述密码学与隐写术的著作 Steganographia，该书书名源于希腊语，意为 “隐秘书写”。\n\n下图是阿宝哥采用在线的图片隐写工具，将 **「“全栈修仙之路”」** 这 6 个字隐藏到原始的图片中，然后使用对应的解密工具，解密出隐藏信息的结果：\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/common/img/20210410164236.jpeg)\n\n（在线图片隐写体验地址：https://c.p2hp.com/yinxietu/）\n\n目前有多种方案可以实现图片隐写，以下是几种常见的方案：\n\n- 附加式的图片隐写；\n- 基于文件结构的图片隐写；\n- 基于 LSB 原理的图片隐写；\n- 基于 DCT 域的 JPG 图片隐写；\n- 数字水印的隐写；\n- 图片容差的隐写。\n","lastmodified":"2023-05-09T16:33:58.275366009Z","tags":[]},"/JavaScript-%E4%B9%8B%E6%B7%B7%E6%B7%86%E7%9A%84%E7%B1%BB":{"title":"JavaScript 之混淆的“类”","content":"\n## JavaScript之混淆的类\n\n#### 序言\n\n学习JavaScript时，如果你已经对C++、Java等传统面向对象编程语言有一定了解，或者老师教导过你使用类(类是面向对象编程的实现核心)把过程化风格的代码转换成结构清晰的代码，那么你很自然地会想在js中使用类。如在万物皆为类的Java中，严格区分类和对象，对象由类实例化而成，继承实际上是类的扩展。但JavaScript中只有对象，没有类，这是本质上的区别，可在JavaScript中有许多模拟类的语法糖在试图掩盖这个事实，很多教程中并未对此作详细解释，而是直接教初学者使用这些语法糖，导致初学者会在无形中出现困惑。\n\n虽然JavaScript中没有类，但俗话说“没有的才是最好的”，开发者们通过不断探索总结，成功地模拟出了“类”。由于大家定义类的方法五花八门，风格不一。对于模拟面向对象的封装、继承、多态，更有许多研究，实现办法更加晦涩，不利于JavaScript新手使用。\n\n这就引出了本文的话题：JavaScript中类的机制。\n\n#### 类的封装、继承与多态\n\n说到面向对象编程，不得不先了解类。\n\n根据维基百科的定义，**类**(class)在面向对象编程中是一种面向对象计算机编程语言的构造，是创建对象的蓝图，描述了所创建的对象共同的属性和方法。有封装性、继承性、多态性三个最重要的特性。\n\n类与**继承**抽象了一种代码的组织形式，一种编程领域对真实世界中问题的建模。比如，轮船可以被看做交通工具的一个特例，后者是更广泛的类，可以用`Vehicle`和`Steamer`两个类进行建模。Vehicle可以定义引擎、载人能力等几乎所有交通工具都具有的属性，而在具体的交通工具类中，定义同样的属性是没有意义的，所以在定义`Steamer`类时，只需声明它继承了`Vehicle`这个基类，那么它就能拥有基类的属性及方法。\n\n有了`Steamer`类，就有了轮船的所有属性和行为，这便是类的**封装**。我们迫不及待地想上船航行，可类好比蓝图，正如轮船的图纸，并非真正可以交互的轮船，只有根据图纸建造出物理实物，才能上船。真的轮船便是蓝图的物理实例，本质上是对蓝图的复制，即**实例化**。\n\n类的另一个核心概念是**多态**，指父类(基类)的通用行为可以被子类用特殊行为重写。如Vehicle类中为所有交通工具定义了一个`decelerate()`减速方法，默认操作是踩刹车，但在轮船的减速中，可能还需要抛锚，所以在`Steamer`类中，可以重写`decelerate()`方法，在引用`Vehicle`类中`decelerate()`方法的基础上再加一步抛锚操作。即任何方法都可引用继承层次中高层的方法(方法名可以不同)。\n\n有以下伪代码：\n\n```javascript\nclass Vehicle{\n    engines=1\n    startEngine(){\n        console.log('start engine 1')\n    }\n    drive(){\n        startEngine()\n        console.log('driving')\n    }\n}\nclass Steamer extends Vehicle{\n    engines=2//轮船有两个引擎\n    startEngine(){\n    \tsuper.startEngine()//实现相对多态，调用父类的同名方法\n        console.log('start engine 2')//增加自身需要的代码\n    }\n    drive(){//重写了父类的方法\n        startEngine()\n        console.log('sailing')\n    }\n}\n```\n\n上面的代码中`Steamer`下的`drive`方法会调用自身相对多态的`startEngine`方法，即多态性取决于引用的实例所来自的类。\n\n可以看出，类是一种设计模式，只是Java等许多语言提供了面向类的原生语法，所以一般不为我们所感知，JavaScript也有类似的语法，但和其他语言中的类完全不同，这便是本文想解释清楚的地方。\n\n类代表着复制，在Java等语言中，类被实例化时，它的行为会被复制到实例中，被继承时行为也会被复制到子类中，多态也是如此。但JavaScript的对象系统基于原型，而不是类，不会自动生成对象的副本。\n\n#### 原型继承\n\n要想在JavaScript中优雅地使用“类”，首先需要了解JavaScript的“类”本质上是基于原型的继承。\n\n**原型(Prototype)**是JS对象的一个特殊内置属性，是对于其他对象的引用。创建一个方法时，会根据一组特定规则为该方法添加一个``prototype`属性，这个属性指向方法的原型对象。在默认情况下，原型对象会自动生成一个`constructor`属性，这个属性包含一个指向`prototype`属性所在函数对象的指针，如图所示。\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rad-figure-bed/PicGo/blogs/js混淆的类/20200514215003.jpg)\n\n需访问一个对象的属性时，先在对象的本身上查找，如果找不到引擎就会继续在``prototype`所关联的对象上继续查找，直到找到为止，`prototype`最终都指向`Object.prototype`。这一系列查找对象的链接即是**原型链**。如下图。\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rad-figure-bed/PicGo/blogs/js混淆的类/20200514215009.jpg)\n\n##### 继承属性\n\n```javascript\nlet f = function () {\n   this.a = 1\n   this.b = 2\n}\nlet g = new f(); // {a: 1, b: 2}\n// 在f函数的原型上定义属性\nf.prototype.b = 3;\nf.prototype.c = 4;\n\n// 可得原型链如下: \n// {a:1, b:2} ---\u003e {b:3, c:4} ---\u003e Object.prototype---\u003e null\n\nconsole.log(g.a) // 1\n// a是g自身的属性，值为 1\n\nconsole.log(g.b) // 2\n// b是g自身的属性，值为 2\n// 原型上也有一个'b'属性，但是它不会被访问到。\n\nconsole.log(g.c) // 4\n// c不是g的自身属性，那看看它的原型上有没有\n// c是g.[[Prototype]]属性该属性的值为 4\n\nconsole.log(g.d) // undefined\n// d 不是 g 的自身，那看看它的原型上有没有\n// d 不是 g.[[Prototype]] 的属性，那看看它的原型上有没有\n// g.[[Prototype]].[[Prototype]] 为 null，停止搜索\n// 无 d 属性，故为 undefined\n```\n\n上例中`console.log(g.b)`时输出2而不是3，即'b'属性既出现在g中也出现在了g的原型链上层，那么会触发**屏蔽**，即g中的'b'属性会屏蔽原型链上层的所有'b'属性。这种情况相当于其他语言的方法重写。\n\n##### 继承方法\n\n```js\nlet o = {\n  a: 2,\n  m: function(){\n    return this.a + 1;\n  }\n}\nconsole.log(o.m()) // 3\n// 当调用 o.m 时，'this' 指向了 o.\n\nlet p = Object.create(o)\n// p是一个继承自 o 的对象\np.a = 4 // 创建 p 的自身属性 'a'\nconsole.log(p.m()) // 5\n// 调用 p.m 时，'this' 指向了 p\n// 又因为 p 继承了 o 的 m 函数\n// 所以，此时的 'this.a' 即 p.a，就是 p 的自身属性 'a' \n```\n\nJavaScript没有那些基于类的语言定义的“方法”。JavaScript中，任何函数(方法)都可以添加到对象上作为对象的属性。函数的继承与其他的属性继承没有差别，包括上面的“**屏蔽**”。继承的函数被调用时，this指向当前继承的对象，而不是继承的函数所在的原型对象。\n\nObject.create(obj)返回一个与obj的`prototype`关联的对象，实现了p对于o的“继承”，虽然p对象并无a属性，但访问时若原对象无此属性，便会顺着其prototype链一直查找，直到prototype的尽头——Object.prototype，因此顺着prototype链访问到了o的a属性，是不是有点继承的感觉了？\n\n#### 模拟“类”\n\n所以，在JavaScript中使用类，ES6之前大多为用函数模拟。回到上面Vehicle与Steamer的伪代码，用js代码实现：\n\n```javascript\nfunction Vehicle(props) {\n    this.id = props.id||'not bind'\n    this.engines=props.engines||'not bind'\n}\nVehicle.prototype.startEngine = function () {\n    console.log('start engine ')\n}\n\nfunction Steamer(props) {\n    // 调用Vehicle构造函数，绑定this\n    Vehicle.call(this, props)\n    this.cabin = props.cabin || 1\n}\nlet titanic = new Steamer({ cabin: 10, id: '12412321', engines: 10 })\nconsole.log(titanic);\nconsole.log(Object.getPrototypeOf(titanic));\nconsole.log(Object.getPrototypeOf(Object.getPrototypeOf(titanic)));\nconsole.log(Object.getPrototypeOf(Object.getPrototypeOf(Object.getPrototypeOf(titanic))));\n//Steamer { id: '12412321', engines: 10, cabin: 10 }\n//Steamer {}\n//{}\n//null\n```\n\n但调用了Vehicle()作为“构造函数”不代表继承了Vehicle，Steamer创建的对象原型是``new Steamer()--\u003e Steamer.prototype--\u003e Object.prototype--\u003e null`，继承关系的原型链应为`new Steamer() --\u003e Steamer.prototype--\u003e Vehicle.prototype--\u003e Object.prototype--\u003e null`，这样新的Steamer对象不仅能调用`Steamer.prototype`绑定的方法，也可以使用`Vehicle.prototype`绑定的方法。但要想达到这个状态，直接`Steamer.prototype = Vehicle.prototype`是不行的，这样两者指向同一个对象，继承关系就不存在了。此时需要借助一个中间对象，如下：\n\n```javascript\nfunction Vehicle(props) {\n    this.id = props.id||'not bind'\n    this.engines=props.engines||'not bind'\n}\nVehicle.prototype.startEngine = function () {\n    console.log('start engine ')\n}\n\nfunction Steamer(props) {\n    // 调用Vehicle构造函数，绑定this\n    Vehicle.call(this, props)\n    this.cabin = props.cabin || 1\n}\n\nfunction Tmp() {\n}\nTmp.prototype = Vehicle.prototype\n//把Steamer的原型指向一个新Tmp对象，Tmp对象的原型正好指向Vehicle.prototype\nSteamer.prototype = new Tmp()\n// 把Steamer原型的“构造函数”变回Steamer\nSteamer.prototype.constructor = Steamer\n\nSteamer.prototype.getCabin = function () {\n    return this.cabin\n}\n\nlet titanic=new Steamer({\n    cabin:9999,\n    engines:4,\n    id:'21435452378454'\n})\nconsole.log(titanic.cabin)//9999\nconsole.log(titanic.engines)//4\n//继承关系验证\ntitanic instanceof Steamer//true\ntitanic instanceof Vehicle//true\n//查看原型链\nconsole.log(Object.getPrototypeOf(titanic));\n//Steamer { constructor: [Function: Steamer], getCabin: [Function] }\nconsole.log(Object.getPrototypeOf(Object.getPrototypeOf(titanic)));\n//Vehicle { startEngine: [Function] }\n```\n\n由于Tmp()仅用于两者继承的连接，所以可以用一个函数把这个行为封装起来：\n\n```javascript\nfunction(f){\n    function Tmp(){}\n    Tmp.prototype=f\n    return new Tmp()\n}\n```\n\n这即是`Object.create()`的简单实现。`Object.create()`创建一个新对象，使用现有的对象来提供新创建的对象的prototype。\n\n`Steamer.prototype.constructor = Steamer`的作用是补上constructor属性(默认对象的prototype都有这个属性，可以理解为“构造函数”，一般指向函数自身，即`new Son()`时调用`Son()`来“构造”一个新对象，这个新对象的prototype与Son.prototype相关联)，如果将这里也封装进函数：\n\n```javascript\nfunction extend(son, father) {\n  var prototype = Object.create(father.prototype);// 创建对象，注意！这里和Tmp()方式相同\n  prototype.constructor = son;\n  son.prototype = prototype;\n}\n```\n\n抽离出来的`extend`方法被称作**寄生组合式继承**，是目前最成熟的方法，仅使用`Object.create()`而不补上`constructor`被称作**原型式继承**，缺点是原型链继承多个实例的引用类型属性指向相同，可能被篡改，以及无法传递参数。还有**混入**、**寄生式**等几种方式实现继承，各有各的优缺点，就不一一赘述了。\n\n下面你会读到`extends`关键字，什么？和上面的`extend`很像？没错，`extends`关键字的核心实现就是**寄生组合式继承**，不然怎么叫语法糖。\n\n#### class 关键字\n\nECMAScript 6规范中，引入了`class`的概念。使得 JS 开发者终于告别了直接使用原型对象模仿面向对象中的类和类继承的时代。但是 `class` 仅仅只是对原型对象运用语法糖，如果认为它像其它面向类语言中的`class`那样，使用时只会增加新手的困惑。\n\n```javascript\nclass P {\n  // ...\n}\ntypeof P // function\n```\n\n可以看出，一个class实际上就是function。\n\n```javascript\nclass Vehicle {\n  constructor(props) {\n    this.engines = props.engines;\n    this.id = props.id;\n  }\n  startEngine() {\n    console.log(\"start \",this.engines,' engines');\n  }\n}\nclass Steamer extends Vehicle {\n  constructor(props) {\n    super(props);\n    this.cabin = props.cabin;\n  }\n  getCabin() {\n    this.startEngine();\n    console.log(\"I have\", this.cabin, \" cabins\");\n  }\n}\n\nlet titanic = new Steamer({\n  cabin: 9999,\n  engines: 4,\n  id: \"21435452378454\"\n});\ntitanic.getCabin(); \n//start  4  engines\n//I have 9999  cabins\n```\n\n当titanic调用 `getCabin`方法时，titanic自身没有需要的 `startEngine`方法，所以会到 `titanic.prototype` 原型对象上查找，最后调用``Vehicle.prototype`原型对象上的`startEngine`方法。调用时，`this` 指向的是titanic对象。\n\n实际上，ECMAScript 6中的`class`仍然遵循你了解的JavaScript模式，继承的原理还和以前一样基于原型链，方法添加在原型上，只是用了更简单的关键字来代替，却隐藏了许多问题，要说优点可能只有一个:可以打更少的字。\n\n#### 总结\n\n传统基于类的面向对象思维在一定程度上妨碍了大家对JavaScript面向对象特性的理解，虽然这些机制和Java等传统面向类语言中的“类初始化”“继承”很像，但JavaScript有一个本质区别就是不会进行复制，对象之间通过内部的prototype链进行关联。所以说，在一定程度上JavaScript模拟类是得不偿失的，可解决当前问题，更可埋下隐患。实际上，对象之间的关系用**委托**形容更加贴切。\n\n#### 参考文献\n\n\u003e- 类 (计算机科学)——维基百科\n\u003e\n\u003e- 《你不知道的JavaScript》——KYLE SIMPSON\n\u003e- 《JavaScript继承机制研究》—— 周 岚\n\u003e- 《JavaScript需要类吗?》——紫云飞\n\u003e- 《继承与原型链》——MDN web docs\n\u003e- 《原型继承》——廖雪峰的官方网站\n\u003e- 《JavaScript常用八种继承方案》——木易杨说\n","lastmodified":"2023-05-09T16:33:58.275366009Z","tags":[]},"/Javascript%E5%9F%BA%E7%A1%80":{"title":"Javascript基础","content":"\n## 继承\n\n### 1、原型链继承\n\n#### Array.includes Array.indexOf()\n\n构造函数、原型和实例之间的关系：每个构造函数都有一个原型对象，原型对象都包含一个指向构造函数的指针，而实例都包含一个原型对象的指针。\n\n继承的本质就是**复制，即重写原型对象，代之以一个新类型的实例**。\n\n```javascript\nfunction SuperType() {\n  this.property = true;\n}\n\nSuperType.prototype.getSuperValue = function () {\n  return this.property;\n};\n\nfunction SubType() {\n  this.subproperty = false;\n}\n\n// 这里是关键，创建SuperType的实例，并将该实例赋值给SubType.prototype\nSubType.prototype = new SuperType();\n\nSubType.prototype.getSubValue = function () {\n  return this.subproperty;\n};\n\nvar instance = new SubType();\nconsole.log(instance.getSuperValue()); // true\n```\n\n原型链方案存在的缺点：多个实例对引用类型的操作会被篡改。\n\n```javascript\nfunction SuperType() {\n  this.colors = [\"red\", \"blue\", \"green\"];\n}\nfunction SubType() {}\n\nSubType.prototype = new SuperType();\n\nvar instance1 = new SubType();\ninstance1.colors.push(\"black\");\nalert(instance1.colors); //\"red,blue,green,black\"\n\nvar instance2 = new SubType();\nalert(instance2.colors); //\"red,blue,green,black\"\n```\n\n### 2、借用构造函数继承\n\n使用父类的构造函数来增强子类**实例**，等同于复制父类的实例给子类（不使用原型）\n\n```javascript\nfunction SuperType() {\n  this.color = [\"red\", \"green\", \"blue\"];\n}\nfunction SubType() {\n  //继承自SuperType\n  SuperType.call(this);\n}\nvar instance1 = new SubType();\ninstance1.color.push(\"black\");\nalert(instance1.color); //\"red,green,blue,black\"\n\nvar instance2 = new SubType();\nalert(instance2.color); //\"red,green,blue\"\n```\n\n核心代码是`SuperType.call(this)`，创建子类实例时调用`SuperType`构造函数，于是`SubType`的每个实例都会将 SuperType 中的属性复制一份。\n\n缺点：\n\n- 只能继承父类的**实例**属性和方法，不能继承原型属性/方法\n- 无法实现复用，每个子类都有父类实例函数的副本，影响性能\n\n### 3、组合继承\n\n组合上述两种方法就是组合继承。用原型链实现对**原型**属性和方法的继承，用借用构造函数技术来实现**实例**属性的继承。\n\n```javascript\nfunction SuperType(name) {\n  this.name = name;\n  this.colors = [\"red\", \"blue\", \"green\"];\n}\nSuperType.prototype.sayName = function () {\n  alert(this.name);\n};\n\nfunction SubType(name, age) {\n  // 继承属性\n  // 第二次调用SuperType()\n  SuperType.call(this, name);\n  this.age = age;\n}\n\n// 继承方法\n// 构建原型链\n// 第一次调用SuperType()\nSubType.prototype = new SuperType();\n// 重写SubType.prototype的constructor属性，指向自己的构造函数SubType\nSubType.prototype.constructor = SubType;\nSubType.prototype.sayAge = function () {\n  alert(this.age);\n};\n\nvar instance1 = new SubType(\"Nicholas\", 29);\ninstance1.colors.push(\"black\");\nalert(instance1.colors); //\"red,blue,green,black\"\ninstance1.sayName(); //\"Nicholas\";\ninstance1.sayAge(); //29\n\nvar instance2 = new SubType(\"Greg\", 27);\nalert(instance2.colors); //\"red,blue,green\"\ninstance2.sayName(); //\"Greg\";\ninstance2.sayAge(); //27\n```\n\n缺点：\n\n- 第一次调用`SuperType()`：给`SubType.prototype`写入两个属性 name，color。\n- 第二次调用`SuperType()`：给`instance1`写入两个属性 name，color。\n\n实例对象`instance1`上的两个属性就屏蔽了其原型对象 SubType.prototype 的两个同名属性。所以，组合模式的缺点就是在使用子类创建实例对象时，其原型中会存在两份相同的属性/方法。\n\n### 4、原型式继承\n\n利用一个空对象作为中介，将某个对象直接赋值给空对象构造函数的原型。\n\n```javascript\nfunction object(obj) {\n  function F() {}\n  F.prototype = obj;\n  return new F();\n}\n```\n\nobject()对传入其中的对象执行了一次`浅复制`，将构造函数 F 的原型直接指向传入的对象。\n\n```javascript\nvar person = {\n  name: \"Nicholas\",\n  friends: [\"Shelby\", \"Court\", \"Van\"],\n};\n\nvar anotherPerson = object(person);\nanotherPerson.name = \"Greg\";\nanotherPerson.friends.push(\"Rob\");\n\nvar yetAnotherPerson = object(person);\nyetAnotherPerson.name = \"Linda\";\nyetAnotherPerson.friends.push(\"Barbie\");\n\nalert(person.friends); //\"Shelby,Court,Van,Rob,Barbie\"\n```\n\n缺点：\n\n- 原型链继承多个实例的引用类型属性指向相同，存在篡改的可能。\n- 无法传递参数\n\n另外，ES5 中存在`Object.create()`的方法，能够代替上面的 object 方法。\n\n#### 5、寄生式继承\n\n核心：在原型式继承的基础上，增强对象，返回构造函数\n\n```javascript\nfunction createAnother(original) {\n  var clone = object(original); // 通过调用 object() 函数创建一个新对象\n  clone.sayHi = function () {\n    // 以某种方式来增强对象\n    alert(\"hi\");\n  };\n  return clone; // 返回这个对象\n}\n复制代码;\n```\n\n函数的主要作用是为构造函数新增属性和方法，以**增强函数**\n\n```javascript\nvar person = {\n  name: \"Nicholas\",\n  friends: [\"Shelby\", \"Court\", \"Van\"],\n};\nvar anotherPerson = createAnother(person);\nanotherPerson.sayHi(); //\"hi\"\n复制代码;\n```\n\n缺点（同原型式继承）：\n\n- 原型链继承多个实例的引用类型属性指向相同，存在篡改的可能。\n- 无法传递参数\n\n#### 6、寄生组合式继承\n\n结合借用构造函数传递参数和寄生模式实现继承\n\n```javascript\nfunction inheritPrototype(subType, superType) {\n  var prototype = Object.create(superType.prototype); // 创建对象，创建父类原型的一个副本\n  prototype.constructor = subType; // 增强对象，弥补因重写原型而失去的默认的constructor 属性\n  subType.prototype = prototype; // 指定对象，将新创建的对象赋值给子类的原型\n}\n\n// 父类初始化实例属性和原型属性\nfunction SuperType(name) {\n  this.name = name;\n  this.colors = [\"red\", \"blue\", \"green\"];\n}\nSuperType.prototype.sayName = function () {\n  alert(this.name);\n};\n\n// 借用构造函数传递增强子类实例属性（支持传参和避免篡改）\nfunction SubType(name, age) {\n  SuperType.call(this, name);\n  this.age = age;\n}\n\n// 将父类原型指向子类\ninheritPrototype(SubType, SuperType);\n\n// 新增子类原型属性\nSubType.prototype.sayAge = function () {\n  alert(this.age);\n};\n\nvar instance1 = new SubType(\"xyc\", 23);\nvar instance2 = new SubType(\"lxy\", 23);\n\ninstance1.colors.push(\"2\"); // [\"red\", \"blue\", \"green\", \"2\"]\ninstance1.colors.push(\"3\"); // [\"red\", \"blue\", \"green\", \"3\"]\n复制代码;\n```\n\n这个例子的高效率体现在它只调用了一次`SuperType` 构造函数，并且因此避免了在`SubType.prototype` 上创建不必要的、多余的属性。于此同时，原型链还能保持不变；因此，还能够正常使用`instanceof` 和`isPrototypeOf()`\n\n**这是最成熟的方法，也是现在库实现的方法**\n\n#### 7、混入方式继承多个对象\n\n```javascript\nfunction MyClass() {\n  SuperClass.call(this);\n  OtherSuperClass.call(this);\n}\n\n// 继承一个类\nMyClass.prototype = Object.create(SuperClass.prototype);\n// 混合其它\nObject.assign(MyClass.prototype, OtherSuperClass.prototype);\n// 重新指定constructor\nMyClass.prototype.constructor = MyClass;\n\nMyClass.prototype.myMethod = function () {\n  // do something\n};\n```\n\n`Object.assign`会把 `OtherSuperClass`原型上的函数拷贝到 `MyClass`原型上，使 MyClass 的所有实例都可用 OtherSuperClass 的方法。\n\n#### 8、ES6 类继承 extends\n\n`extends`关键字主要用于类声明或者类表达式中，以创建一个类，该类是另一个类的子类。其中`constructor`表示构造函数，一个类中只能有一个构造函数，有多个会报出`SyntaxError`错误,如果没有显式指定构造方法，则会添加默认的 `constructor`方法，使用例子如下。\n\n```javascript\nclass Rectangle {\n    // constructor\n    constructor(height, width) {\n        this.height = height;\n        this.width = width;\n    }\n\n    // Getter\n    get area() {\n        return this.calcArea()\n    }\n\n    // Method\n    calcArea() {\n        return this.height * this.width;\n    }\n}\n\nconst rectangle = new Rectangle(10, 20);\nconsole.log(rectangle.area);\n// 输出 200\n\n-----------------------------------------------------------------\n// 继承\nclass Square extends Rectangle {\n\n  constructor(length) {\n    super(length, length);\n\n    // 如果子类中存在构造函数，则需要在使用“this”之前首先调用 super()。\n    this.name = 'Square';\n  }\n\n  get area() {\n    return this.height * this.width;\n  }\n}\n\nconst square = new Square(10);\nconsole.log(square.area);\n// 输出 100\n```\n\n`extends`继承的核心代码如下，其实现和上述的寄生组合式继承方式一样\n\n```javascript\nfunction _inherits(subType, superType) {\n  // 创建对象，创建父类原型的一个副本\n  // 增强对象，弥补因重写原型而失去的默认的constructor 属性\n  // 指定对象，将新创建的对象赋值给子类的原型\n  subType.prototype = Object.create(superType \u0026\u0026 superType.prototype, {\n    constructor: {\n      value: subType,\n      enumerable: false,\n      writable: true,\n      configurable: true,\n    },\n  });\n\n  if (superType) {\n    Object.setPrototypeOf\n      ? Object.setPrototypeOf(subType, superType)\n      : (subType.__proto__ = superType);\n  }\n}\n```\n\n#### 总结\n\n1、函数声明和类声明的区别\n\n函数声明会提升，类声明不会。首先需要声明你的类，然后访问它，否则像下面的代码会抛出一个 ReferenceError。\n\n```javascript\nlet p = new Rectangle();\n// ReferenceError\n\nclass Rectangle {}\n```\n\n2、ES5 继承和 ES6 继承的区别\n\n- ES5 的继承实质上是先创建子类的实例对象，然后再将父类的方法添加到 this 上（Parent.call(this)）.\n- ES6 的继承有所不同，实质上是先创建父类的实例对象 this，然后再用子类的构造函数修改 this。因为子类没有自己的 this 对象，所以必须先调用父类的 super()方法，否则新建实例报错。\n\n## Proxy\n\n使用`Proxy`，你可以将一只猫伪装成一只老虎。下面大约有 6 个例子，我希望它们能让你相信，Proxy 提供了强大的 Javascript 元编程。\n\n尽管它不像其他 ES6 功能用的普遍，但`Proxy`有许多用途，包括运算符重载，对象模拟，简洁而灵活的 API 创建，对象变化事件，甚至 Vue 3 背后的内部响应系统提供动力。\n\n`Proxy`用于修改某些操作的默认行为，也可以理解为在目标对象之前架设一层**拦截**，外部所有的访问都必须先通过这层拦截，因此提供了一种机制，可以对外部的访问进行过滤和修改。这个词的原理为代理，在这里可以表示由它来“代理”某些操作，译为“代理器”。\n\n`var proxy = new Proxy(target, handler);`\n\n`Proxy`对象的所有用法，都是上面的这种形式。不同的只是`handle`参数的写法。其中`new Proxy`用来生成`Proxy`实例，`target`是表示所要拦截的对象，`handle`是用来定制拦截行为的对象。\n\n下面是 Proxy 最简单的例子是，这是一个有陷阱的代理，一个`get`陷阱，总是返回`42`。\n\n```javascript\nlet target = { x: 10, y: 20 };\nlet hanler = { get: (obj, prop) =\u003e 42 };\ntarget = new Proxy(target, hanler);\ntarget.x; //42target.y; //42target.x; // 42\n```\n\n结果是一个对象将为任何属性访问操作都返回“42”。这包括`target.x`，`target['x']`，`Reflect.get(target, 'x')`等。\n\n但是，Proxy 陷阱当然不限于属性的读取。它只是十几个不同陷阱中的一个：\n\n- handler.get\n- handler.set\n- handler.has\n- handler.apply\n- handler.construct\n- handler.ownKeys\n- handler.deleteProperty\n- handler.defineProperty\n- handler.isExtensible\n- handler.preventExtensions\n- handler.getPrototypeOf\n- handler.setPrototypeOf\n- handler.getOwnPropertyDescriptor\n\n#### 用途\n\n![image-20200309213734606](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/js/20210410182644.png)\n\n##### **默认值/“零值”**\n\n在 Go 语言中，有零值的概念，零值是特定于类型的隐式默认结构值。其思想是提供类型安全的默认基元值，或者用 gopher 的话说，给结构一个有用的零值。\n\n虽然不同的创建模式支持类似的功能，但 Javascript 无法用隐式初始值包装对象。Javascript 中未设置属性的默认值是`undefined`。但 Proxy 可以改变这种情况。\n\n```javascript\nconst withZeroValue = (target, zeroValue) =\u003e\n  new Proxy(target, {\n    get: (obj, prop) =\u003e (prop in obj ? obj[prop] : zeroValue),\n  });\n```\n\n函数`withZeroValue` 用来包装目标对象。如果设置了属性，则返回属性值。否则，它返回一个默认的**“零值”**。\n\n从技术上讲，这种方法也不是隐含的，但如果我们扩展`withZeroValue`，以 Boolean (`false`), Number (`0`), String (`\"\"`), Object (`{}`)，Array (`[]`)等对应的零值，则可能是隐含的。\n\n```javascript\nlet pos = { x: 4, y: 19 };\nconsole.log(pos.x, pos.y, pos.z); // 4, 19, undefined\npos = withZeroValue(pos, 0);\nconsole.log(pos.z, pos.y, pos.z); // 4, 19, 0\n```\n\n此功能可能有用的一个地方是坐标系。绘图库可以基于数据的形状自动支持 2D 和 3D 渲染。不是创建两个单独的模型，而是始终将`z`默认为 `0` 而不是`undefined`，这可能是有意义的。\n\n## Promise\n\n**Promise** 对象用于表示一个异步操作的最终完成 (或失败), 及其结果值.\n\n```javascript\nconst promise1 = new Promise(function (resolve, reject) {\n  setTimeout(function () {\n    resolve(\"foo\");\n  }, 300);\n});\n\npromise1.then(function (value) {\n  console.log(value);\n  // expected output: \"foo\"\n});\n\nconsole.log(promise1);\n// expected output: [object Promise]\n```\n\n### 构造函数语法\n\n`new Promise( function(resolve, reject) {…} /* executor */ );`\n\nexecutor\n\n\u003e executor 是带有 `resolve` 和 `reject` 两个参数的函数 。Promise 构造函数执行时立即调用`executor` 函数， `resolve` 和 `reject` 两个函数作为参数传递给`executor`（executor 函数在 Promise 构造函数返回所建 promise 实例对象前被调用）。`resolve` 和 `reject` 函数被调用时，分别将 promise 的状态改为*fulfilled（*完成）或 rejected（失败）。executor 内部通常会执行一些异步操作，一旦异步操作执行完毕(可能成功/失败)，要么调用 resolve 函数来将 promise 状态改成*fulfilled*，要么调用`reject` 函数将 promise 的状态改为 rejected。如果在 executor 函数中抛出一个错误，那么该 promise 状态为 rejected。executor 函数的返回值被忽略。\n\n### 介绍\n\n`Promise` 对象是一个代理对象（代理一个值），被代理的值在 Promise 对象创建时可能是未知的。它允许你为异步操作的成功和失败分别绑定相应的处理方法（handlers）。 这让异步方法可以像同步方法那样返回值，**但并不是立即返回最终执行结果，而是一个能代表未来出现的结果的 promise 对象**。\n\n一个 `Promise`有以下几种状态:\n\n- *pending*: 初始状态，既不是成功，也不是失败状态。\n- *fulfilled*: 意味着操作成功完成。\n- *rejected*: 意味着操作失败。\n\npending 状态的 Promise 对象可能会变为 fulfilled 状态并传递一个值给相应的状态处理方法，也可能变为失败状态（rejected）并传递失败信息。**当其中任一种情况出现时，Promise 对象的 `then` 方法绑定的处理方法（handlers ）就会被调用**（then 方法包含两个参数：onfulfilled 和 onrejected，它们都是 Function 类型。当 Promise 状态为*fulfilled*时，调用 then 的 onfulfilled 方法，当 Promise 状态为*rejected*时，调用 then 的 onrejected 方法， 所以在异步操作的完成和绑定处理方法之间不存在竞争）。\n\n因为 `Promise.prototype.then` 和 `Promise.prototype.catch` 方法返回 promise 对象， 所以它们可以被链式调用。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/js/20210410182652.png)\n\n**不要和惰性求值混淆：** 有一些语言中有惰性求值和延时计算的特性，它们也被称为“promises”，例如 Scheme. Javascript 中的 promise 代表一种已经发生的状态， 而且可以通过回调方法链在一起。 如果你想要的是表达式的延时计算，考虑无参数的\"[箭头方法](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Functions/Arrow_functions)\": `f = () =\u003e`*`表达式`* 创建惰性求值的表达式*，*使用 `f()` 求值。\n\n**注意：** 如果一个 promise 对象处在 fulfilled 或 rejected 状态而不是 pending 状态，那么它也可以被称为*settled*状态。你可能也会听到一个术语*resolved* ，它表示 promise 对象处于 settled 状态。关于 promise 的术语， Domenic Denicola 的 [States and fates](https://github.com/domenic/promises-unwrapping/blob/master/docs/states-and-fates.md) 有更多详情可供参考。\n\n### 属性\n\n`Promise.length`\n\nlength 属性，其值总是为 1 (构造器参数的数目).\n\n[`Promise.prototype`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Promise/prototype)\n\n表示 `Promise` 构造器的原型.\n\n### 方法\n\n- [`Promise.all(iterable)`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Promise/all)\n\n  这个方法返回一个新的 promise 对象，该 promise 对象在 iterable 参数对象里所有的 promise 对象都成功的时候才会触发成功，一旦有任何一个 iterable 里面的 promise 对象失败则立即触发该 promise 对象的失败。这个新的 promise 对象在触发成功状态以后，会把一个包含 iterable 里所有 promise 返回值的数组作为成功回调的返回值，顺序跟 iterable 的顺序保持一致；如果这个新的 promise 对象触发了失败状态，它会把 iterable 里第一个触发失败的 promise 对象的错误信息作为它的失败错误信息。Promise.all 方法常被用于处理多个 promise 对象的状态集合。（可以参考 jQuery.when 方法---译者注）\n\n- [`Promise.race(iterable)`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Promise/race)\n\n  当 iterable 参数里的任意一个子 promise 被成功或失败后，父 promise 马上也会用子 promise 的成功返回值或失败详情作为参数调用父 promise 绑定的相应句柄，并返回该 promise 对象。\n\n- [`Promise.reject(reason)`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Promise/reject)\n\n  返回一个状态为失败的 Promise 对象，并将给定的失败信息传递给对应的处理方法\n\n- [`Promise.resolve(value)`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Promise/resolve)\n\n  返回一个状态由给定 value 决定的 Promise 对象。如果该值是 thenable(即，带有 then 方法的对象)，返回的 Promise 对象的最终状态由 then 方法执行决定；否则的话(该 value 为空，基本类型或者不带 then 方法的对象),返回的 Promise 对象状态为 fulfilled，并且将该 value 传递给对应的 then 方法。通常而言，如果你不知道一个值是否是 Promise 对象，使用 Promise.resolve(value) 来返回一个 Promise 对象,这样就能将该 value 以 Promise 对象形式使用。\n\n### 原型\n\n#### 属性\n\n- `Promise.prototype.constructor`\n\n  返回被创建的实例函数. 默认为 [`Promise`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Promise) 函数.\n\n#### 方法\n\n- [`Promise.prototype.catch(onRejected)`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Promise/catch)\n\n  添加一个拒绝(rejection) 回调到当前 promise, 返回一个新的 promise。当这个回调函数被调用，新 promise 将以它的返回值来 resolve，否则如果当前 promise 进入 fulfilled 状态，则以当前 promise 的完成结果作为新 promise 的完成结果.\n\n- [`Promise.prototype.then(onFulfilled, onRejected)`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Promise/then)\n\n  添加解决(fulfillment)和拒绝(rejection)回调到当前 promise, 返回一个新的 promise, 将以回调的返回值来 resolve.\n\n- [`Promise.prototype.finally(onFinally)`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Promise/finally)\n\n  添加一个事件处理回调于当前 promise 对象，并且在原 promise 对象解析完毕后，返回一个新的 promise 对象。回调会在当前 promise 运行完毕后被调用，无论当前 promise 的状态是完成(fulfilled)还是失败(rejected)\n\n### 自己实现剖析\n\nhttps://mp.weixin.qq.com/s/3xfLpQ2h0v8yt2W7opLwGw\n\n### 20 行案例\n\nhttps://mp.weixin.qq.com/s/oHBv7r6x7tVOwm-LsnIbgA\n\n```javascript\nfunction Promise(excutor) {\n  var self = this\n  self.onResolvedCallback = []\n  function resolve(value) {\n    setTimeout(() =\u003e {\n      self.data = value\n      self.onResolvedCallback.forEach(callback =\u003e callback(value))\n    })\n  }\n  excutor(resolve.bind(self))\n}\nPromise.prototype.then = function(onResolved) {\n  var self = this\n  returnnewPromise(resolve =\u003e {\n    self.onResolvedCallback.push(function() {\n      var result = onResolved(self.data)\n      if (result instanceofPromise) {\n        result.then(resolve)\n      } else {\n        resolve(result)\n      }\n    })\n  })\n}\n```\n\n#### 实现过程\n\n1. 首先来实现 Promise 构造函数\n\n   ```javascript\n   function Promise(excutor) {\n     var self = this;\n     self.onResolvedCallback = []; // Promise resolve时的回调函数集\n\n     // 传递给Promise处理函数的resolve\n     // 这里直接往实例上挂个data\n     // 然后把onResolvedCallback数组里的函数依次执行一遍就可以\n     function resolve(value) {\n       // 注意promise的then函数需要异步执行\n       setTimeout(() =\u003e {\n         self.data = value;\n         self.onResolvedCallback.forEach((callback) =\u003e callback(value));\n       });\n     }\n\n     // 执行用户传入的函数\n     excutor(resolve.bind(self));\n   }\n   ```\n\n   好，写到这里先回过头来看案例\n\n\u003c\u003c\u003c\u003c\u003c\u003c\u003c HEAD\n\n## Event Loop\n\n`Event Loop是一个程序结构，用于等待和分派消息和事件`，我个人的理解是 JS 中的 Event Loop 是浏览器或 Node 的一种协调 JavaScript 单线程运行时不会阻塞的一种机制。\n\n### JS 的单线程\n\n很多人都知道的是，JavaScript 是一门**动态的解释型的语言**，具有**跨平台性**。在被问到 JavaScript 为什么是一门单线程的语言，有的人可能会这么回答：“语言特性决定了 JavaScript 是一个单线程语言，JavaScript 天生是一个单线程语言”，这只不过是一层糖衣罢了。\n\nJavaScript 从诞生起就是单线程，原因大概是不想让浏览器变得太复杂，因为多线程需要共享资源、且有可能修改彼此的运行结果，对于一种网页脚本语言来说，这就太复杂了。\n\n准确的来说，我认为 JavaScript 的单线程是指 **JavaScript 引擎是单线程**的，JavaScript 的引擎并不是独立运行的，跨平台意味着 JavaScript 依赖其运行的宿主环境 --- 浏览器(大部分情况下是浏览器)。\n\n浏览器需要渲染 DOM，JavaScript 可以修改 DOM 结构，JavaScript 执行时，浏览器 DOM 渲染停止。如果 JavaScript 引擎线程不是单线程的，那么可以同时执行多段 JavaScript，如果这多段 JavaScript 都操作 DOM，那么就会出现 DOM 冲突。\n\nJavaScript 的单线程，与它的用途有关。作为浏览器脚本语言，JavaScript 的主要用途是与用户互动，以及操作 DOM。这决定了它只能是单线程，否则会带来很复杂的同步问题。比如，假定 JavaScript 同时有两个线程，一个线程在某个 DOM 节点上添加内容，另一个线程删除了这个节点，这时浏览器应该以哪个线程为准？\n\n所以，为了避免复杂性，从一诞生，JavaScript 就是单线程，这已经成了这门语言的核心特征，将来也不会改变。\n\n为了利用多核 CPU 的计算能力，HTML5 提出 Web Worker 标准，允许 JavaScript 脚本创建多个线程，但是子线程完全受主线程控制，且不得操作 DOM。所以，这个新标准并没有改变 JavaScript 单线程的本质。\n\n举个例子来说，在同一时刻执行两个 script 对同一个 DOM 元素进行操作，一个修改 DOM，一个删除 DOM，那这样话浏览器就会懵逼了，它就不知道到底该听谁的，会有资源竞争，这也是 JavaScript 单线程的原因之一。\n\n### 浏览器\n\n#### 浏览器的多线程\n\n之前说过，JavaScript 运行的宿主环境浏览器是多线程的。\n\n以 Chrome 来说，我们可以通过 Chrome 的任务管理器来看看。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/js/20210410182700.png)\n\n当你打开一个 Tab 页面的时候，就创建了一个进程。如果从一个页面打开了另一个页面，打开的页面和当前的页面属于同一站点的话，那么这个页面会复用父页面的渲染进程。\n\n#### 浏览器主线程常驻线程\n\n1. GUI 渲染线程\n2. - 绘制页面，解析 HTML、CSS，构建 DOM 树，布局和绘制等\n   - 页面重绘和回流\n   - 与 JS 引擎线程互斥，也就是所谓的 JS 执行阻塞页面更新\n\n3. JS 引擎线程\n4. - 负责 JS 脚本代码的执行\n   - 负责准执行准备好待执行的事件，即定时器计数结束，或异步请求成功并正确返回的事件\n   - 与 GUI 渲染线程互斥，执行时间过长将阻塞页面的渲染\n\n5. 事件触发线程\n6. - 负责将准备好的事件交给 JS 引擎线程执行\n   - 多个事件加入任务队列的时候需要排队等待(JS 的单线程)\n\n7. 定时器触发线程\n8. - 负责执行异步的定时器类的事件，如 setTimeout、setInterval\n   - 定时器到时间之后把注册的回调加到任务队列的队尾\n\n9. HTTP 请求线程\n10. - 负责执行异步请求\n    - 主线程执行代码遇到异步请求的时候会把函数交给该线程处理，当监听到状态变更事件，如果有回调函数，该线程会把回调函数加入到任务队列的队尾等待执行\n\n这里没看懂没关系，后面我会再说。\n\n### 浏览器端的 Event Loop\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/js/20210410182709.png)\n\n上图是一张 JS 的运行机制图，Js 运行时大致会分为几个部分：\n\n1. Call Stack：调用栈(执行栈)，所有同步任务在主线程上执行，形成一个执行栈，因为 JS 单线程的原因，所以调用栈中每次只能执行一个任务，当遇到的同步任务执行完之后，由任务队列提供任务给调用栈执行。\n2. Task Queue：任务队列，存放着异步任务，当异步任务可以执行的时候，任务队列会通知主线程，然后该任务会进入主线程执行。任务队列中的都是已经完成的异步操作，而不是说注册一个异步任务就会被放在这个任务队列中。\n\n说到这里，Event Loop 也可以理解为：不断地从任务队列中取出任务执行的一个过程。\n\n#### 同步任务和异步任务\n\n上文已经说过了 JavaScript 是一门单线程的语言，一次只能执行一个任务，如果所有的任务都是同步任务，那么程序可能因为等待会出现假死状态，这对于一个用户体验很强的语言来说是非常不友好的。\n\n比如说向服务端请求资源，你不可能一直不停的循环判断有没有拿到数据，就好像你点了个外卖，点完之后就开始一直打电话问外卖有没有送到，外卖小哥都会抄着锅铲来打你(狗头)。因此，在 JavaScript 中任务有了同步任务和异步任务，异步任务通过注册回调函数，等到数据来了就通知主程序。\n\n1. 同步任务：必须等到结果来了之后才能做其他的事情，举例来说就是你烧水的时候一直等在水壶旁边等水烧开，期间不做其他的任何事情。\n2. 异步任务：不需要等到结果来了才能继续往下走，等结果期间可以做其他的事情，结果来了会收到通知。举例来说就是你烧水的时候可以去做自己想做的事情，听到水烧开的声音之后再去处理。\n\n从概念就可以看出来，异步任务从一定程度上来看比同步任务更高效一些，核心是提高了用户体验。\n\n#### Event Loop\n\nEvent Loop 很好的调度了任务的运行，宏任务和微任务也知道了，现在我们就来看看它的调度运行机制。\n\nJavaScript 的代码执行时，主线程会从上到下一步步的执行代码，同步任务会被依次加入执行栈中先执行，异步任务会在拿到结果的时候将注册的回调函数放入任务队列，当执行栈中的没有任务在执行的时候，引擎会从任务队列中读取任务压入执行栈(Call Stack)中处理执行。\n\n#### 宏任务和微任务\n\n现在就有一个问题了，任务队列是一个消息队列，先进先出，那就是说，后来的事件都是被加在队尾等到前面的事件执行完了才会被执行。如果在执行的过程中突然有重要的数据需要获取，或是说有事件突然需要处理一下，**按照队列的先进先出顺序这些是无法得到及时处理的。这个时候就催生了宏任务和微任务，微任务使得一些异步任务得到及时的处理**。\n\n曾经看到的一个例子很好，宏任务和微任务形象的来说就是：你去营业厅办一个业务会有一个排队号码，当叫到你的号码的时候你去窗口办充值业务(宏任务执行)，在你办理充值的时候你又想改个套餐(微任务)，这个时候工作人员会直接帮你办，不可能让你重新排队。\n\n所以上文说过的异步任务又分为宏任务和微任务，JS 运行时任务队列会分为宏任务队列和微任务队列，分别对应宏任务和微任务。\n\n先介绍一下(浏览器环境的)宏任务和微任务大致有哪些：\n\n- 宏任务：\n- 1. script(整体的代码)\n  2. setTimeout\n  3. setInterval\n  4. I/O 操作\n  5. UI 渲染 (对这个笔者持保留意见)\n\n- 微任务：\n- 1. Promise.then\n  2. MutationObserver\n\n#### 事件运行顺序\n\n1. 执行同步任务，同步任务不需要做特殊处理，直接执行(下面的步骤中遇到同步任务都是一样处理) --- 第一轮从 script 开始\n2. 从宏任务队列中取出队头任务执行\n3. 如果产生了宏任务，将宏任务放入宏任务队列，下次轮循的时候执行\n4. 如果产生了微任务，将微任务放入微任务队列\n5. 执行完当前宏任务之后，取出微任务队列中的所有任务依次执行\n6. 如果微任务执行过程中产生了新的微任务，则继续执行微任务，直到微任务的队列为空\n7. 轮循，循环以上 2 - 6\n\n总的来说就是：同步任务/宏任务 -\u003e 执行产生的所有微任务(包括微任务产生的微任务) -\u003e 同步任务/宏任务 -\u003e 执行产生的所有微任务(包括微任务产生的微任务) -\u003e 循环……\n\n注意：微任务队列\n\n#### 举个栗子\n\n光说不练假把式，现在就来看一个例子：\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/js/20210410182716.png)举个栗子\n\n放图的原因是为了让大家在看解析之前可以先自己按照运行顺序走一遍，写好答案之后再来看解析。  \n解析：  \n(用绿色的表示同步任务和宏任务，红色表示微任务)\n\n```\n+  console.log('script start')\n+  setTimeout(function() {\n+    console.log('setTimeout')\n+  }, 0)\n+  new Promise((resolve, reject)=\u003e{\n+    console.log(\"promise1\")\n+    resolve()\n+  })\n-  .then(()=\u003e{\n-    console.log(\"then11\")\n+    new Promise((resolve, reject)=\u003e{\n+      console.log(\"promise2\")\n+      resolve();\n+    })\n-    .then(() =\u003e {\n-      console.log(\"then2-1\")\n-    })\n-    .then(() =\u003e {\n-      console.log(\"then2-2\")\n-    })\n-  })\n-  .then(()=\u003e{\n-    console.log(\"then12\")\n-  })\n+  console.log('script end')\n```\n\n1. 首先遇到 console.log()，输出 `script start`\n2. 遇到 setTimeout 产生宏任务，注册到**宏任务队列[setTimeout]**，下一轮 Event Loop 的时候在执行\n3. 然后遇到 new Promise 构造声明(同步)，log 输出 `promise1`，然后 resolve\n4. resolve 匹配到 **promise1 的第一个 then**，把这个 then 注册到**微任务队列[then11]中**，继续当前整体脚本的执行\n5. 遇到最后的一个 log，输出 `script end`，**当前执行栈清空**\n6. **从微任务队列中取出队头任务'then11'** 进行执行，其中有一个 log，输出 `then11`\n7. 往下遇到 new Promise 构造声明(同步)，log 输出 `promise2`，然后 resolve\n8. resolve 匹配到 **promise2 的第一个 then**，把这个 then 注册到**微任务队列[then2-1]**，当前 then11 可执行部分结束，然后产生了 **promise1 的第二个 then**，把这个 then 注册到**微任务队列[then2-1, then12]**\n9. **拿出微任务队头任务'then2-1'** 执行，log 输出 `then2-1`，触发 **promise2 的第二个 then**，注册到**微任务队列[then12, then2-2]**\n10. **拿出微任务队头任务'then12'**，log 输出 `then12`\n11. **拿出微任务队头任务'then2-2'**，log 输出 `then2-2`\n12. 微任务队列执行完毕，别忘了宏任务队列中的 setTimeout，log 输出 `setTimeout`\n\n经过以上一番缜(xia)密(gao)分析，希望没有绕晕你，最后的输出结果就是：  \n`script start -\u003e promise1 -\u003e script end -\u003e then11 -\u003e promise2 -\u003e then2-1 -\u003e then12 -\u003e then2-2 -\u003e setTimeout`\n\n#### 宏任务？微任务？\n\n不知道大家看了宏任务和微任务之后会不会有一个疑惑，宏任务和微任务都是异步任务，微任务之前说过了是为了及时解决一些必要事件而产生的。\n\n- 为什么要有微任务？  \n  为什么要有微任务的原因前面已经说了，这里就不再赘述，简单说一下就是为了及时处理一些任务，不然等到最后再执行的时候拿到的数据可能已经是被污染的数据达不到预期目标了。\n\n- 是什么宏任务？什么是微任务？  \n  相信大家在学习 Event Loop 查找资料的时候，肯定各种资料里面都会讲到宏任务和微任务，但是不知道你有没有灵魂拷问过你自己：`什么是宏任务？什么是微任务？怎么区分宏任务和微任务？`不能只是默许接受这个概念，在这里，我根据我的个人理解进行一番说(hu)明(che)\n\n- 宏任务和微任务的真面目  \n  其实在 Chrome 的源码中并没有什么宏任务和微任务的代码或是说明，在 **JS 大会**[3]上提到过微任务这个名词，但是也没有说到底什么是微任务。\n\n  宏任务  \n  文章最开始的时候说过，在 chrome 里，每个页面都对应一个进程。而该进程又有多个线程，比如 JS 线程、渲染线程、IO 线程、网络线程、定时器线程等等，这些线程之间的通信是通过向对象的任务队列中添加一个任务（postTask）来实现的。**宏任务的本质可以认为是多线程事件循环或消息循环，也就是线程间通信的一个消息队列。**\n\n  就拿 setTimeout 举例来说，当遇到它的时候，浏览器就会对 Event Loop 说：嘿，我有一个任务交给你，Event Loop 就会说：好的，我会把它加到我的 todoList 中，之后我会执行它，它是需要调用 API 的。\n\n  **宏任务的真面目是浏览器派发，与 JS 引擎无关的，参与了 Event Loop 调度的任务**\n\n  微任务  \n  微任务是在运行宏任务/同步任务的时候产生的，是属于当前任务的，所以它不需要浏览器的支持，内置在 JS 当中，直接在 JS 的引擎中就被执行掉了。\n\n#### 特殊的点\n\n1. async 隐式返回 Promise 作为结果\n2. 执行完 await 之后直接跳出 async 函数，让出执行的所有权\n3. 当前任务的其他代码执行完之后再次获得执行权进行执行\n4. 立即 resolve 的 Promise 对象，是在本轮\"事件循环\"的结束时执行，而不是在下一轮\"事件循环\"的开始时\n\n#### 再举个栗子\n\n```javascript\nconsole.log('script start')\n\n  asyncfunction async1() {\n      await async2()\n      console.log('async1 end')\n  }\n  asyncfunction async2() {\n      console.log('async2 end')\n  }\n  async1()\n\n  setTimeout(function() {\n      console.log('setTimeout')\n  }, 0)\n\n  newPromise(resolve =\u003e {\n      console.log('Promise')\n      resolve()\n  })\n  .then(function() {\n      console.log('promise1')\n  })\n  .then(function() {\n      console.log('promise2')\n  })\n\n  console.log('script end')\n```\n\n按照之前的分析方法去分析之后就会得出一个结果：  \n`script start =\u003e async2 end =\u003e Promise =\u003e script end =\u003e promise1 =\u003e promise2 =\u003e async1 end =\u003e setTimeout`\n\n可以看出 async1 函数获取执行权是作为微任务的队尾，但是，在 Chrome73(金丝雀) 版本之后，async 的执行优化了，它会在 promise1 和 promise2 的输出之前执行。笔者大概了解了一下应该是用 PromiseResolve 对 await 进行了优化，减少了 Promise 的再次创建，有兴趣的小伙伴可以看看 Chrome 的源码。\n\n### Node 中的 Event Loop\n\nNode 中也有宏任务和微任务，与浏览器中的事件循环类似。Node 与浏览器事件循环不同，其中有多个宏任务队列，而浏览器是只有一个宏任务队列。\n\nNode 的架构底层是有 libuv，它是 Node 自身的动力来源之一，通过它可以去调用一些底层操作，Node 中的 Event Loop 功能就是在 libuv 中封装实现的。\n\n#### 宏任务和微任务\n\nNode 中的宏任务和微任务在浏览器端的 JS 相比增加了一些，这里只列出浏览器端没有的：\n\n- 宏任务\n- 1. setImmediate\n- 微任务\n- 1. process.nextTick\n\n#### **事件循环机制的六个阶段**\n\n![image-20200402223627057](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/js/20210410182725.png)\n\nNode 的事件循环分成了六个阶段，每个阶段对应一个宏任务队列，相当于是宏任务进行了一个分类。\n\n1. timers(计时器)  \n   执行 setTimeout 以及 setInterval 的回调\n2. I/O callbacks  \n   处理网络、流、TCP 的错误回调\n3. idel, prepare --- 闲置阶段  \n   node 内部使用\n4. poll(轮循)  \n   执行 poll 中的 I/O 队列，检查定时器是否到时间\n5. check(检查)  \n   存放 setImmediate 回调\n6. close callbacks  \n   关闭回调，例如 sockect.on('close')\n\n#### 轮循顺序\n\n执行的轮循顺序 --- 每个阶段都要等对应的宏任务队列执行完毕才会进入到下一个阶段的宏任务队列\n\n1. timers\n2. I/O callbacks\n3. poll\n4. setImmediate\n5. close events\n\n每两个阶段之间执行微任务队列\n\n#### Event Loop 过程\n\n1. 执行全局的 script 同步代码\n2. 执行微任务队列，先执行所有 Next Tick 队列中的所有任务，再执行其他的微任务队列中的所有任务\n3. 开始执行宏任务，共六个阶段，从第一个阶段开始执行自己宏任务队列中的所有任务(浏览器是从宏任务队列中取第一个执行！！)\n4. 每个阶段的宏任务执行完毕之后，开始执行微任务\n5. TimersQueue -\u003e 步骤 2 -\u003e I/O Queue -\u003e 步骤 2 -\u003e Check Queue -\u003e 步骤 2 -\u003e Close Callback Queue -\u003e 步骤 2 -\u003e TimersQueue …\n\n这里要注意的是，nextTick 事件是一个单独的队列，它的优先级会高于微任务，所以在当前宏任务/同步任务执行完成之后，会先执行 nextTick 队列中的所有任务，再去执行微任务队列中的所有任务。\n\n### setTimeout 和 setImmediate\n\n在这里要单独说一下 setTimeout 和 setImmediate，setTimeout 定时器很熟悉，那就说说 setImmediate\n\nsetImmediate() 方法用于把一些需要长时间运行的操作放在一个回调函数里，并在浏览器完成其他操作（如事件和显示更新）后立即运行回调函数。从定义来看就是为了防止一些耗时长的操作阻塞后面的操作，这也是为什么 check 阶段运行顺序排的比较后。\n\n### 举个栗子\n\n我们来看这样的一个例子：\n\n```\nsetTimeout(() =\u003e {\n  console.log('setTimeout')\n}, 0)\n\nsetImmediate(() =\u003e {\n  console.log('setImmediate')\n})\n```\n\n这里涉及 timers 阶段和 check 阶段，按照上面的运行顺序来说，timers 阶段是在第一个执行的，会早于 check 阶段。运行这段程序可以看到如下的结果：\n\n![image-20200402224016596](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/js/20210410182741.png)\n\n可是再多运行几次，你就会看到如下的结果：\n\n![image-20200402224028075](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/js/20210410182747.png)\n\nsetImmediate 的输出跑到 setTimeout 前面去了，这时候就是：小朋友你是否有很多的问号 ❓\n\n#### 分析\n\n我们来分析一下原因，timers 阶段确实是在 check 阶段之前，但是在 timers 阶段时候，这里的 setTimeout 真的到了执行的时间吗？\n\n这里就要先看看 `setTiemout(fn, 0)`，这个语句的意思不是指不延迟的执行，而是指在可以执行 setTimeout 的时候就立即执行它的回调，也就是处理完当前事件的时候立即执行回调。\n\n在 Node 中 setTimeout 第二个时间参数的最小值是 1ms，小于 1ms 会被初始化为 1(浏览器中最小值是 4ms)，所以在这里 `setTimeout(fn, 0) === setTimeout(fn, 1)`\n\nsetTimeout 的回调函数在 timers 阶段执行，setImmediate 的回调函数在 check 阶段执行，Event Loop 的开始会先检查 timers 阶段，但是在代码开始运行之前到 timers 阶段(代码的启动、运行)会消耗一定的时间，所以会出现两种情况：\n\n1. timers 前的准备时间超过 1ms，满足 loop -\u003e timers \u003e= 1，setTimeout 的时钟周期到了，则执行 timers 阶段(setTimeout)的回调函数\n2. timers 前的准备时间小于 1ms，还没到 setTimeout 预设的时间，则先执行 check 阶段(setImmediate)的回调函数，下一次 Event Loop 再进入 timers 阶段执行 timer 阶段(setTimeout)的回调函数\n\n最开始就说了，一个优秀的程序员要让自己的代码按照自己想要的顺序运行，下面我们就来控制一下 setTimeout 和 setImediate 的运行。\n\n- 让 setTimeout 先执行  \n  上面代码运行顺序不同无非就是因为 Node 准备时间的不确定性，我们可以直接手动延长准备时间 👇\n\n```\nconst start = Date.now()\n  while (Date.now() - start \u003c 10)\n  setTimeout(() =\u003e {\n  console.log('setTimeout')\n  }, 0)\n\n  setImmediate(() =\u003e {\n    console.log('setImmediate')\n  })\n```\n\n- 让 setImmediate 先执行  \n  setImmediate 是在 check 阶段执行，相对于 setTimeout 来说是在 timers 阶段之后，只需要想办法把程序的运行环境控制在 timers 阶段之后就可以了。\n\n  让程序至少从 I/O callbacks 阶段开始 --- 可以套一层文件读写把把程序控制在 I/O callbacks 阶段的运行环境中 👇\n\n```\nconst fs = require('fs')\n\nfs.readFile(__dirname, () =\u003e {\n  setTimeout(() =\u003e {\n    console.log('setTimeout')\n  }, 0)\n\n  setImmediate(() =\u003e {\n    console.log('setImmediate')\n  })\n})\n```\n\n### Node 11.x 的变化\n\ntimers 阶段的执行有所变化\n\n```\nsetTimeout(() =\u003econsole.log('timeout1'))\nsetTimeout(() =\u003e {\n console.log('timeout2')\n Promise.resolve().then(() =\u003econsole.log('promise resolve'))\n})\n```\n\n1. node 10 及之前的版本：  \n   要考虑上一个定时器执行完成时，下一个定时器是否到时间加入了任务队列中，如果未到时间，先执行其他的代码。  \n   比如：  \n   timer1 执行完之后 timer2 到了任务队列中，顺序为 `timer1 -\u003e timer2 -\u003e promise resolve`  \n   timer2 执行完之后 timer2 还没到任务队列中，顺序为 `timer1 -\u003e promise resolve -\u003e timer2`\n2. node 11 及其之后的版本：  \n   `timeout1 -\u003e timeout2 -\u003e promise resolve`  \n   一旦执行某个阶段里的一个宏任务之后就立刻执行微任务队列，这和浏览器端运行是一致的。\n\n### 小结\n\nNode 和端浏览器端有什么不同\n\n1. 浏览器端的 Event Loop 和 Node.js 中的 Event Loop 是不同的，实现机制也不一样\n2. Node.js 可以理解成有 4 个宏任务队列和 2 个微任务队列，但是执行宏任务时有 6 个阶段\n3. Node.js 中限制性全局 script 代码，执行完同步代码后，先从微任务队列 Next Tick Queue 中取出所有任务放入调用栈执行，再从其他微任务队列中取出所有任务放入调用栈中执行，然后开始宏任务的 6 个阶段，每个阶段都将其宏任务队列中的所有任务都取出来执行(浏览器是只取第一个执行)，每个宏任务阶段执行完毕之后开始执行微任务，再开始执行下一阶段宏任务，以此构成事件循环\n4. 宏任务包括 ….\n5. 微任务包括 ….\n\n看到这里，你应该对浏览器端和 Node 端的 Event Loop 有了一定的了解，那就留一个题目。\n\n![image-20200402224502960](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/js/20210410182756.png)\n\n不直接放代码是想让大家先自己思考然后在敲代码运行一遍~\n\n## void\n\nhttps://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Operators/void\n\n### 语法\n\n```\nvoid expression\n```\n\n### 描述\n\n这个运算符能向期望一个表达式的值是[`undefined`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/undefined)的地方插入会产生副作用的表达式。\n\nvoid 运算符通常只用于获取 `undefined`的原始值，一般使用`void(0)`（等同于`void 0`）。在上述情况中，也可以使用全局变量[`undefined`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/undefined) 来代替（假定其仍是默认值）。\n\n### 立即调用的函数表达式\n\n在使用[立即执行的函数表达式](https://developer.mozilla.org/zh-CN/docs/Glossary/IIFE)时，可以利用 `void` 运算符让 JavaScript 引擎把一个`function`关键字识别成函数表达式而不是函数声明（语句）。\n\n```js\nvoid (function iife() {\n  var bar = function () {};\n  var baz = function () {};\n  var foo = function () {\n    bar();\n    baz();\n  };\n  var biz = function () {};\n\n  foo();\n  biz();\n})();\n```\n\n### JavaScript URIs\n\n当用户点击一个以 `javascript:` URI 时，它会执行 URI 中的代码，然后用返回的值替换页面内容，除非返回的值是[`undefined`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/undefined)。`void`运算符可用于返回[`undefined`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/undefined)。例如：\n\n```html\n\u003ca href=\"javascript:void(0);\"\u003e\n  这个链接点击之后不会做任何事情，如果去掉 void()，\n  点击之后整个页面会被替换成一个字符 0。\n\u003c/a\u003e\n\u003cp\u003e chrome中即使\u003ca href=\"javascript:0;\"\u003e也没变化，firefox中会变成一个字符串0 \u003c/p\u003e\n\u003ca href=\"javascript:void(document.body.style.backgroundColor='green');\"\u003e\n  点击这个链接会让页面背景变成绿色。\n\u003c/a\u003e\n```\n\n注意，虽然这么做是可行的，但利用 `javascript:` 伪协议来执行 JavaScript 代码是不推荐的，推荐的做法是为链接元素绑定事件。\n\n### 在箭头函数中避免泄漏\n\n箭头函数标准中，允许在函数体不使用括号来直接返回值。 如果右侧调用了一个原本没有返回值的函数，其返回值改变后，则会导致非预期的副作用。 安全起见，当函数返回值是一个不会被使用到的时候，应该使用 `void` 运算符，来确保返回 [`undefined`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/undefined)（如下方示例），这样，当 API 改变时，并不会影响箭头函数的行为。\n\n```js\nbutton.onclick = () =\u003e void doSomething();\n```\n\n确保了当 `doSomething` 的返回值从 [`undefined`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/undefined) 变为 `true` 的时候，不会改变函数的行为\n\n## undefined 与 null 的区别\n\n大多数计算机语言，有且仅有一个表示\"无\"的值，比如，C 语言的 NULL，Java 语言的 null，Python 语言的 None，Ruby 语言的 nil。\n\n有点奇怪的是，JavaScript 语言居然有**两个**表示\"无\"的值：undefined 和 null。这是为什么？\n\n在 JavaScript 中，将一个变量赋值为 undefined 或 null，老实说，几乎没区别。\n\n\u003e ```javascript\n\u003e var a = undefined;\n\u003e\n\u003e var a = null;\n\u003e ```\n\n上面代码中，a 变量分别被赋值为 undefined 和 null，这两种写法几乎等价。\n\nundefined 和 null 在 if 语句中，都会被自动转为 false，相等运算符甚至直接报告两者相等。\n\n\u003e ```javascript\n\u003e if (!undefined) console.log(\"undefined is false\");\n\u003e // undefined is false\n\u003e\n\u003e if (!null) console.log(\"null is false\");\n\u003e // null is false\n\u003e\n\u003e undefined == null;\n\u003e // true\n\u003e ```\n\n上面代码说明，两者的行为是何等相似！\n\n既然 undefined 和 null 的含义与用法都差不多，为什么要同时设置两个这样的值，这不是无端增加 JavaScript 的复杂度，令初学者困扰吗？Google 公司开发的 JavaScript 语言的替代品 Dart 语言，就明确规定只有 null，没有 undefined！\n\n### 历史原因\n\n原来，这与 JavaScript 的历史有关。1995 年[JavaScript 诞生](http://www.ruanyifeng.com/blog/2011/06/birth_of_javascript.html)时，最初像 Java 一样，只设置了 null 作为表示\"无\"的值。\n\n根据 C 语言的传统，null 被设计成可以自动转为 0。\n\n\u003e ```javascript\n\u003e Number(null);\n\u003e // 0\n\u003e\n\u003e 5 + null;\n\u003e // 5\n\u003e ```\n\n但是，JavaScript 的设计者 Brendan Eich，觉得这样做还不够，有两个原因。\n\n首先，null 像在 Java 里一样，被当成一个对象。但是，JavaScript 的数据类型分成原始类型（primitive）和合成类型（complex）两大类，Brendan Eich 觉得表示\"无\"的值最好不是对象。\n\n其次，JavaScript 的最初版本没有包括错误处理机制，发生数据类型不匹配时，往往是自动转换类型或者默默地失败。Brendan Eich 觉得，如果 null 自动转为 0，很不容易发现错误。\n\n因此，Brendan Eich 又设计了一个 undefined。\n\n### 最初设计\n\nJavaScript 的最初版本是这样区分的：**null 是一个表示\"无\"的对象，转为数值时为 0；undefined 是一个表示\"无\"的原始值，转为数值时为 NaN。**\n\n\u003e ```javascript\n\u003e Number(undefined);\n\u003e // NaN\n\u003e\n\u003e 5 + undefined;\n\u003e // NaN\n\u003e ```\n\n### 目前的用法\n\n但是，上面这样的区分，在实践中很快就被证明不可行。目前，null 和 undefined 基本是同义的，只有一些细微的差别。\n\n**null 表示\"没有对象\"，即该处不应该有值。**典型用法是：\n\n\u003e （1） 作为函数的参数，表示该函数的参数不是对象。\n\u003e\n\u003e （2） 作为对象原型链的终点。\n\n\u003e ```javascript\n\u003e Object.getPrototypeOf(Object.prototype);\n\u003e // null\n\u003e ```\n\n**undefined 表示\"缺少值\"，就是此处应该有一个值，但是还没有定义。**典型用法是：\n\n\u003e （1）变量被声明了，但没有赋值时，就等于 undefined。\n\u003e\n\u003e （2) 调用函数时，应该提供的参数没有提供，该参数等于 undefined。\n\u003e\n\u003e （3）对象没有赋值的属性，该属性的值为 undefined。\n\u003e\n\u003e （4）函数没有返回值时，默认返回 undefined。\n\n\u003e ```javascript\n\u003e var i;\n\u003e i; // undefined\n\u003e\n\u003e function f(x) {\n\u003e   console.log(x);\n\u003e }\n\u003e f(); // undefined\n\u003e\n\u003e var o = new Object();\n\u003e o.p; // undefined\n\u003e\n\u003e var x = f();\n\u003e x; // undefined\n\u003e ```\n\n## ES6\n\n### 什么是 ES6\n\nES 的全称是 ECMAScript , 它是由 ECMA 国际标准化组织,制定的一项脚本语言的标准化规范。\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/js/20210410182846.png)\n\n### 为什么使用 ES6 ?\n\n每一次标准的诞生都意味着语言的完善，功能的加强。JavaScript 语言本身也有一些令人不满意的地方。\n\n- 变量提升特性增加了程序运行时的不可预测性\n- 语法过于松散，实现相同的功能，不同的人可能会写出不同的代码\n\n### ES6 新增语法\n\n#### let（★★★）\n\nES6 中新增了用于声明变量的关键字\n\n##### let 声明的变量只在所处于的块级有效\n\n```javascript\nif (true) {\n  let a = 10;\n}\nconsole.log(a); // a is not defined\n```\n\n**注意：**使用 let 关键字声明的变量才具有块级作用域，使用 var 声明的变量不具备块级作用域特性。\n\n##### 不存在变量提升\n\n```javascript\nconsole.log(a); // a is not defined\nlet a = 20;\n```\n\n##### 暂时性死区\n\n利用 let 声明的变量会绑定在这个块级作用域，不会受外界的影响\n\n```javascript\nvar tmp = 123;\nif (true) {\n  tmp = \"abc\";\n  let tmp;\n}\n```\n\n##### 经典面试题\n\n```javascript\nvar arr = [];\nfor (var i = 0; i \u003c 2; i++) {\n  arr[i] = function () {\n    console.log(i);\n  };\n}\narr[0]();\narr[1]();\n```\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/js/20210410182851.png)\n\n**经典面试题图解：**此题的关键点在于变量 i 是全局的，函数执行时输出的都是全局作用域下的 i 值。\n\n```javascript\nlet arr = [];\nfor (let i = 0; i \u003c 2; i++) {\n  arr[i] = function () {\n    console.log(i);\n  };\n}\narr[0]();\narr[1]();\n```\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/js/20210410182855.png)\n\n**经典面试题图解：**此题的关键点在于每次循环都会产生一个块级作用域，每个块级作用域中的变量都是不同的，函数执行时输出的是自己上一级（循环产生的块级作用域）作用域下的 i 值.\n\n##### 小结\n\n- let 关键字就是用来声明变量的\n- 使用 let 关键字声明的变量具有块级作用域\n- 在一个大括号中 使用 let 关键字声明的变量才具有块级作用域 var 关键字是不具备这个特点的\n- 防止循环变量变成全局变量\n- 使用 let 关键字声明的变量没有变量提升\n- 使用 let 关键字声明的变量具有暂时性死区特性\n\n#### const（★★★）\n\n声明常量，常量就是值（内存地址）不能变化的量\n\n##### 具有块级作用域\n\n```javascript\nif (true) {\n  const a = 10;\n}\nconsole.log(a); // a is not defined\n```\n\n##### 声明常量时必须赋值\n\n```javascript\nconst PI; // Missing initializer in const declaration\n```\n\n##### 常量赋值后，值不能修改\n\n```javascript\nconst PI = 3.14;\nPI = 100; // Assignment to constant variable.\n\nconst ary = [100, 200];\nary[0] = \"a\";\nary[1] = \"b\";\nconsole.log(ary); // ['a', 'b'];\nary = [\"a\", \"b\"]; // Assignment to constant variable.\n```\n\n##### 小结\n\n- const 声明的变量是一个常量\n- 既然是常量不能重新进行赋值，如果是基本数据类型，不能更改值，如果是复杂数据类型，不能更改地址值\n- 声明 const 时候必须要给定值\n\n#### let、const、var 的区别\n\n- 使用 var 声明的变量，其作用域为该语句所在的函数内，且存在变量提升现象\n- 使用 let 声明的变量，其作用域为该语句所在的代码块内，不存在变量提升\n- 使用 const 声明的是常量，在后面出现的代码中不能再修改该常量的值\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/js/20210410182901.png)\n\n#### 解构赋值（★★★）\n\nES6 中允许从数组中提取值，按照对应位置，对变量赋值，对象也可以实现解构\n\n##### 数组解构\n\n```javascript\nlet [a, b, c] = [1, 2, 3];\nconsole.log(a); //1\nconsole.log(b); //2\nconsole.log(c); //3\n//如果解构不成功，变量的值为undefined\n```\n\n##### 对象解构\n\n```javascript\nlet person = { name: \"zhangsan\", age: 20 };\nlet { name, age } = person;\nconsole.log(name); // 'zhangsan'\nconsole.log(age); // 20\n\nlet { name: myName, age: myAge } = person; // myName myAge 属于别名\nconsole.log(myName); // 'zhangsan'\nconsole.log(myAge); // 20\n```\n\n##### 小结\n\n- 解构赋值就是把数据结构分解，然后给变量进行赋值\n- 如果结构不成功，变量跟数值个数不匹配的时候，变量的值为 undefined\n- 数组解构用中括号包裹，多个变量用逗号隔开，对象解构用花括号包裹，多个变量用逗号隔开\n- 利用解构赋值能够让我们方便的去取对象中的属性跟方法\n\n#### 箭头函数（★★★）\n\nES6 中新增的定义函数的方式。\n\n```javascript\n() =\u003e {}; //()：代表是函数； =\u003e：必须要的符号，指向哪一个代码块；{}：函数体\nconst fn = () =\u003e {}; //代表把一个函数赋值给fn\n```\n\n函数体中只有一句代码，且代码的执行结果就是返回值，可以省略大括号\n\n```javascript\nfunction sum(num1, num2) {\n  return num1 + num2;\n}\n//es6写法\nconst sum = (num1, num2) =\u003e num1 + num2;\n```\n\n如果形参只有一个，可以省略小括号\n\n```javascript\nfunction fn(v) {\n  return v;\n}\n//es6写法\nconst fn = (v) =\u003e v;\n```\n\n箭头函数不绑定 this 关键字，箭头函数中的 this，指向的是函数定义位置的上下文 this\n\n```javascript\nconst obj = { name: \"张三\" };\nfunction fn() {\n  console.log(this); //this 指向 是obj对象\n  return () =\u003e {\n    console.log(this); //this 指向 的是箭头函数定义的位置，那么这个箭头函数定义在fn里面，而这个fn指向是的obj对象，所以这个this也指向是obj对象\n  };\n}\nconst resFn = fn.call(obj);\nresFn();\n```\n\n##### 小结\n\n- 箭头函数中不绑定 this，箭头函数中的 this 指向是它所定义的位置，可以简单理解成，定义箭头函数中的作用域的 this 指向谁，它就指向谁\n- 箭头函数的优点在于解决了 this 执行环境所造成的一些问题。比如：解决了匿名函数 this 指向的问题（匿名函数的执行环境具有全局性），包括 setTimeout 和 setInterval 中使用 this 所造成的问题\n\n##### 面试题\n\n```javascript\nvar age = 100;\n\nvar obj = {\n  age: 20,\n  say: () =\u003e {\n    alert(this.age);\n  },\n};\n\nobj.say(); //箭头函数this指向的是被声明的作用域里面，而对象没有作用域的，所以箭头函数虽然在对象中被定义，但是this指向的是全局作用域\n```\n\n#### 剩余参数（★★）\n\n剩余参数语法允许我们将一个不定数量的参数表示为一个数组，不定参数定义方式，这种方式很方便的去声明不知道参数情况下的一个函数\n\n```javascript\nfunction sum(first, ...args) {\n  console.log(first); // 10\n  console.log(args); // [20, 30]\n}\nsum(10, 20, 30);\n```\n\n##### 剩余参数和解构配合使用\n\n```javascript\nlet students = [\"wangwu\", \"zhangsan\", \"lisi\"];\nlet [s1, ...s2] = students;\nconsole.log(s1); // 'wangwu'\nconsole.log(s2); // ['zhangsan', 'lisi']\n```\n\n### ES6 的内置对象扩展\n\n#### Array 的扩展方法（★★）\n\n##### 扩展运算符（展开语法）\n\n扩展运算符可以将数组或者对象转为用逗号分隔的参数序列\n\n```javascript\n let ary = [1, 2, 3];\n ...ary  // 1, 2, 3\n console.log(...ary);    // 1 2 3,相当于下面的代码\n console.log(1,2,3);\n```\n\n**扩展运算符可以应用于合并数组**\n\n```javascript\n// 方法一\nlet ary1 = [1, 2, 3];\nlet ary2 = [3, 4, 5];\nlet ary3 = [...ary1, ...ary2];\n// 方法二\nary1.push(...ary2);\n```\n\n**将类数组或可遍历对象转换为真正的数组**\n\n```javascript\nlet oDivs = document.getElementsByTagName(\"div\");\noDivs = [...oDivs];\n```\n\n##### 构造函数方法：Array.from()\n\n将伪数组或可遍历对象转换为真正的数组\n\n```javascript\n//定义一个集合\nlet arrayLike = {\n  0: \"a\",\n  1: \"b\",\n  2: \"c\",\n  length: 3,\n};\n//转成数组\nlet arr2 = Array.from(arrayLike); // ['a', 'b', 'c']\n```\n\n方法还可以接受第二个参数，作用类似于数组的 map 方法，用来对每个元素进行处理，将处理后的值放入返回的数组\n\n```javascript\nlet arrayLike = {\n  0: 1,\n  1: 2,\n  length: 2,\n};\nlet newAry = Array.from(arrayLike, (item) =\u003e item * 2); //[2,4]\n```\n\n注意：如果是对象，那么属性需要写对应的索引\n\n##### 实例方法：find()\n\n用于找出第一个符合条件的数组成员，如果没有找到返回 undefined\n\n```javascript\nlet ary = [\n  {\n    id: 1,\n    name: \"张三\",\n  },\n  {\n    id: 2,\n    name: \"李四\",\n  },\n];\nlet target = ary.find((item, index) =\u003e item.id == 2); //找数组里面符合条件的值，当数组中元素id等于2的查找出来，注意，只会匹配第一个\n```\n\n##### 实例方法：findIndex()\n\n用于找出第一个符合条件的数组成员的位置，如果没有找到返回-1\n\n```javascript\nlet ary = [1, 5, 10, 15];\nlet index = ary.findIndex((value, index) =\u003e value \u003e 9);\nconsole.log(index); // 2\n```\n\n##### 实例方法：includes()\n\n判断某个数组是否包含给定的值，返回布尔值。\n\n```javascript\n[1, 2, 3]\n  .includes(2) // true\n  [(1, 2, 3)].includes(4); // false\n```\n\n#### String 的扩展方法\n\n##### 模板字符串（★★★）\n\nES6 新增的创建字符串的方式，使用反引号定义\n\n```javascript\nlet name = `zhangsan`;\n```\n\n**模板字符串中可以解析变量**\n\n```javascript\nlet name = \"张三\";\nlet sayHello = `hello,my name is ${name}`; // hello, my name is zhangsan\n```\n\n**模板字符串中可以换行**\n\n```javascript\nlet result = {\n  name: \"zhangsan\",\n  age: 20,\n  sex: \"男\",\n};\nlet html = ` \u003cdiv\u003e\n     \u003cspan\u003e${result.name}\u003c/span\u003e\n     \u003cspan\u003e${result.age}\u003c/span\u003e\n     \u003cspan\u003e${result.sex}\u003c/span\u003e\n \u003c/div\u003e `;\n```\n\n**在模板字符串中可以调用函数**\n\n```javascript\nconst sayHello = function () {\n  return \"哈哈哈哈 追不到我吧 我就是这么强大\";\n};\nlet greet = `${sayHello()} 哈哈哈哈`;\nconsole.log(greet); // 哈哈哈哈 追不到我吧 我就是这么强大 哈哈哈哈\n```\n\n##### 实例方法：startsWith() 和 endsWith()\n\n- startsWith()：表示参数字符串是否在原字符串的头部，返回布尔值\n- endsWith()：表示参数字符串是否在原字符串的尾部，返回布尔值\n\n```javascript\nlet str = \"Hello world!\";\nstr.startsWith(\"Hello\"); // true\nstr.endsWith(\"!\"); // true\n```\n\n##### 实例方法：repeat()\n\nrepeat 方法表示将原字符串重复 n 次，返回一个新字符串\n\n```javascript\n\"x\".repeat(3); // \"xxx\"\n\"hello\".repeat(2); // \"hellohello\"\n```\n\n#### Set 数据结构（★★）\n\nES6 提供了新的数据结构 Set。它类似于数组，但是成员的值都是唯一的，没有重复的值。\n\nSet 本身是一个构造函数，用来生成 Set 数据结构\n\n```javascript\nconst s = new Set();\n```\n\nSet 函数可以接受一个数组作为参数，用来初始化。\n\n```javascript\nconst set = new Set([1, 2, 3, 4, 4]); //{1, 2, 3, 4}\n```\n\n##### 实例方法\n\n- add(value)：添加某个值，返回 Set 结构本身\n- delete(value)：删除某个值，返回一个布尔值，表示删除是否成功\n- has(value)：返回一个布尔值，表示该值是否为 Set 的成员\n- clear()：清除所有成员，没有返回值\n\n```javascript\nconst s = new Set();\ns.add(1).add(2).add(3); // 向 set 结构中添加值\ns.delete(2); // 删除 set 结构中的2值\ns.has(1); // 表示 set 结构中是否有1这个值 返回布尔值\ns.clear(); // 清除 set 结构中的所有值\n//注意：删除的是元素的值，不是代表的索引\n```\n\n##### 遍历\n\nSet 结构的实例与数组一样，也拥有 forEach 方法，用于对每个成员执行某种操作，没有返回值。\n\n```javascript\ns.forEach((value) =\u003e console.log(value));\n```\n","lastmodified":"2023-05-09T16:33:58.275366009Z","tags":[]},"/Linux":{"title":"Linux目录","content":"\n[[Linux常用命令与环境变量]]\n[[Linux性能优化]]\n\n[[排坑]]\n","lastmodified":"2023-05-09T16:33:58.275366009Z","tags":[]},"/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F":{"title":"Linux常用命令与环境变量","content":"\n## linux的目录结构\n\n打开终端，输入```ls```查看linux根目录下的情况\n\n```bash\n$ ls /\n----------------\nbin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\n```\n\n![image-20200316193324424](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/linux-softs/20210208203649.png)\n\n- /bin (/usr/bin /usr/local/bin )\n\n  是Binary的缩写, 这个目录存放着最经常使用的命令\n\n- /sbin (/usr/sbin 、 /usr/local/sbin)  \n  s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序\n\n- /**home**  \n  存放普通用户的主目录，在Linux中每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。\n\n- **/root**  \n  该目录为系统管理员，也称作超级权限者的用户主目录。\n\n- /lib\n\n  系统开机所需要最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。\n\n- /lost+found\n\n  这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。\n\n- **/etc**\n\n  所有的系统管理所需要的配置文件和子目录。\n\n- **/usr**\n\n  这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似与windows下的program files目录。\n\n- /boot\n\n  这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件，**自己的安装别放这里**\n\n- /proc\n\n  这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。\n\n- /srv\n\n  service缩写，该目录存放一些服务启动之后需要提取的数据。\n\n- /sys\n\n  这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。\n\n- /tmp\n\n  这个目录是用来存放一些临时文件的。\n\n- /dev\n\n  类似于windows的设备管理器，把所有的硬件用文件的形式存储，**在linux中一切皆文件**。\n\n- /media\n\n  linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。\n\n- /mnt\n\n  系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将外部的存储挂载在/mnt/上，然后进入该目录就可以查看里的内容了，比如你挂载的其他硬盘或者u盘等\n\n- **/opt**\n\n  这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。**hadoop就可以安装在此**\n\n- **/usr/local**\n\n  这是另一个给主机额外安装软件所摆放的目录。一般是通过编译源码方式安装的程序。\n\n- **/var**\n\n  这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。\n\n- /selinux\n\n  SELinux是一种安全子系统,它能控制程序只能访问特定文件。\n\n## 环境变量\n\n输入echo $SHELL后，我的电脑显示bash，说明是Bourne Shell的一个变种，可以把你要添加的环境变量添加到你主目录下面的.profile或者.bash_profile，如果存在没有关系添加进去即可，如果没有生成一个。\n\n### MAC\n\n#### Mac配置环境变量的地方\n\n - 1./etc/profile （建议不修改这个文件 ）全局（公有）配置，不管是哪个用户，登录时都会读取该文件。\n - 2./etc/bashrc （一般在这个文件中添加系统级环境变量）全局（公有）配置，bash shell执行时，不管是何种方式，都会读取此文件。\n - 3.~/.bash_profile （一般在这个文件中添加用户级环境变量）每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次!\n\n#### Mac系统的环境变量，加载顺序为：\n\na. /etc/profile  \nb. /etc/paths  \nc. ~/.bash_profile  \nd. ~/.bash_login  \ne. ~/.profile  \nf. ~/.bashrc\n\n其中a和b是系统级别的，系统启动就会加载，其余是用户接别的。c,d,e按照从前往后的顺序读取，如果c文件存在，则后面的几个文件就会被忽略不读了，以此类推。~/.bashrc没有上述规则，它是bash shell打开的时候载入的。这里建议在c中添加环境变量，以下也是以在c中添加环境变量来演示的。\n\n#### MAC 修改host文件\n\nsudo vi /etc/hosts\n\n### 查看环境变量\n\n```echo $PATH``` 需要在名字前加上$，如echo $HOME，还能 ls $HOME\n\n```shell\nenv \n```\n\n```\nprintenv\n```\n\n\u003e  后面加上具体的名字可以查看单个变量，如printenv HOME\n\n### 添加环境变量\n\n```\t\necho $my\n\nmy=hello\n\necho $my #hello\n```\n\nmy、=、hello中间不能有空格,若赋予的字符串有空格，需要\n\n\u003e my=\"hello wo\"\n\n若要把一个临时变量变成全局变量\n\n``` \nexport 变量名(前面不需要$)\n```\n\n同时注意，子shell无法使用export改变父shell中全局环境变量的值\n\n### 删除环境变量\n\n```\nunset 变量名（也不用加$）\n```\n\n\u003e 当操作变量时不需要加$,用到变量时则要加，唯一例外就是printenv （不需要加）\n\n### 添加的path格式\n\nPATH=$PATH:\u003cPATH 1\u003e:\u003cPATH 2\u003e:\u003cPATH 3\u003e:------:\\\u003cPATH N\u003e中间用冒号隔开\n\n$PATH指的是整个path语句，所以上述命令加上：《path》即在后面追加\n\n比如export PATH=/opt/STM/STLinux-2.3/devkit/sh4/bin:$PATH\n\n### **立即生效**\n\n\\# source /etc/profile 不报错则成功\n\nflutter例子中，我用了用户变量，生效运行 source $HOME/.bash_profile 注意Mac 系统，如果只在终端使用 export 这个命令写入环境变量，它配置的只是临时变量，不能长期保存，电脑开关机后或重新打开终端或另开一个窗口，仍然会回到没有配置环境变量的状态。\n\n## 文件权限命令\n\n### 权限介绍\n\n当执行ls -l 或 ls -al 命令后显示的结果中，最前面的第2～10个字符是用来表示权限。第一个字符一般用来区分文件和目录：\n\n- d：表示是一个目录，事实上在ext2fs中，目录是一个特殊的文件。\n- －：表示这是一个普通的文件。\n- l: 表示这是一个符号链接文件，实际上它指向另一个文件。\n- b、c：分别表示区块设备和其他的外围设备，是特殊类型的文件。\n- s、p：这些文件关系到系统的数据结构和管道，通常很少见到。\n\n第2～10个字符当中的每3个为一组，左边三个字符表示所有者权限，中间3个字符表示与所有者同一组的用户的权限，右边3个字符是其他用户的权限。这三个一组共9个字符，代表的意义如下：\n\n- **r(Read，读取)：对文件而言，具有读取文件内容的权限；对目录来说，具有浏览目录的权**\n- **w(Write,写入)：对文件而言，具有新增、修改文件内容的权限；对目录来说，具有删除、移动目录内文件的权限。****\n- **x(eXecute，执行)：对文件而言，具有执行文件的权限；对目录了来说该用户具有进入目录的权限。**\n\n![image-20200314151453960](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/linux-softs/20210208203707.png)\n\n－：表示不具有该项权限。  \n下面举例说明：\n\n\u003e －rwx------: 文件所有者对文件具有读取、写入和执行的权限。  \n\u003e -rwxr―r--: 文件所有者具有读、写与执行的权限，其他用户则具有读取的权限。  \n\u003e -rw-rw-r-x: 文件所有者与同组用户对文件具有读写的权限，而其他用户仅具有读取和执行的权限。  \n\u003e drwx--x--x: 目录所有者具有读写与进入目录的权限,其他用户近能进入该目录，却无法读取任何数据。  \n\u003e Drwx------: 除了目录所有者具有完整的权限之外，其他用户对该目录完全没有任何权限。\n\n还有所谓的特殊权限。由于特殊权限会拥有一些“特权”，因而用户若无特殊需求，不应该启用这些权限，避免安全方面出现严重漏洞，造成黑客入侵，甚至摧毁系统!!!\n\n- s或S（SUID,Set UID）：可执行的文件搭配这个权限，便能得到特权，任意存取该文件的所有者能使用的全部系统资源。请注意具备SUID权限的文件，黑客经常利用这种权限，以SUID配上root帐号拥有者，无声无息地在系统中开扇后门，供日后进出使用。\n- s或S（SGID，Set GID）：设置在文件上面，其效果与SUID相同，只不过将文件所有者换成用户组，该文件就可以任意存取整个用户组所能使用的系统资源。\n- T或T（Sticky）：/tmp和 /var/tmp目录供所有用户暂时存取文件，亦即每位用户皆拥有完整的权限进入该目录，去浏览、删除和移动文件。\n\n因为SUID、SGID、Sticky占用x的位置来表示，所以在表示上会有大小写之分。加入同时开启执行权限和SUID、SGID、Sticky，则权限表示字符是小写的：\n\n-rwsr-sr-t 1 root root 4096 6月 23 08：17 conf\n\n如果关闭执行权限，则表示字符会变成大写：\n\n-rwSr-Sr-T 1 root root 4096 6月 23 08：17 conf\n\n文件基本属性介绍，如图所示：\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/linux-softs/20210208203714.jpg)\n\n（1）如果查看到是文件：链接数指的是硬链接个数。创建硬链接方法\n\n  ```bash\n$ ln [原文件] [目标文件]\t \n[root@hadoop101 ~]# ln xiyou/dssz/houge.txt ./hg.txt\n  ```\n\n（2）如果查看的是文件夹：链接数指的是子文件夹个数。\n\n```bash\n[root@hadoop101 ~]# ls -al xiyou/\n--------------------------\n总用量 16\ndrwxr-xr-x.  4 root root 4096 1月  12 14:00 .\ndr-xr-x---. 29 root root 4096 1月  12 14:32 ..\ndrwxr-xr-x.  2 root root 4096 1月  12 14:30 dssz\ndrwxr-xr-x.  2 root root 4096 1月  12 14:04 mingjie\n```\n\n### chmod 改变权限\n\n```bash\nchmod [ugoa][+-=][rwx] [-R] 文件名或目录\n/\nchmod nnn [-R] 文件名或目录名\n第一个n: 属主的权限值\n第二个n：属组的权限值\n第三个n：其它用户的权限值\n```\n\n- 简单的八进制格式：`chmod 760 newfile`\n- 复杂的符号模式\n\n  ```\n  [ugoa...][[+-=][rwxX]...][,...]\n  ```\n\n  ![image-20200314151650651](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/linux-softs/20210208203720.png)\n\n  其中：\n\n  - u 表示该文件的拥有者，g 表示与该文件的拥有者属于同一个群体(group)者，o 表示其他以外的人，a 表示这三者皆是。\n  - \\+ 表示增加权限、- 表示移除权限、= 表示等于左边的权限。\n  - r 表示可读取，w 表示可写入，x 表示可执行，X 表示只有当该文件是个子目录或者该文件已经被设定过为可执行。t保留文件目录，s运行时重新设置uid或gid，u权限设置和属主一样，g权限设置和属组一样，o权限设置和其他用户一样\n\n    其他参数说明：\n\n  - -c : 若该文件权限确实已经更改，才显示其更改动作\n  \n  - -f : 若该文件权限无法被更改也不要显示错误讯息\n  \n  - -v : 显示权限变更的详细资料\n  \n  - -R : 对目前目录下的所有文件与子目录进行相同的权限变更(即以**递回**的方式逐个变更)\n  \n  - --help : 显示辅助说明\n  \n  - --version : 显示版本\n\n#### 案例\n\n（1）修改文件使其所属主用户具有执行权限\n\n```bash\n[root@hadoop101 ~]# cp xiyou/dssz/houge.txt ./\n[root@hadoop101 ~]# chmod u+x houge.txt\n```\n\n（2）修改文件使其所属组用户具有执行权限\n\n```bash\n[root@hadoop101 ~]# chmod g+x houge.txt\n```\n\n（3）修改文件所属主用户执行权限,并使其他用户具有执行权限\n\n```bash\n[root@hadoop101 ~]# chmod u-x,o+x houge.txt\n```\n\n（4）采用数字的方式，设置文件所有者、所属组、其他用户都具有可读可写可执行权限。\n\n```bash\n[root@hadoop101 ~]# chmod 777 houge.txt\n```\n\n（5）修改整个文件夹里面的所有文件的所有者、所属组、其他用户都具有可读可写可执行权限。\n\n```bash\n[root@hadoop101 ~]# chmod -R 777 xiyou/\n```\n\n---修改当前目录下文件ana.cfg的权限为属主读写执行，属组读写，其它用户为读  \n`chmod u+x,g=rw,o=r ana.cfg`  \n--修改当前目录下文件ana.cfg的权限所有用户都为读写  \n`chmod a=rw ana.cfg`  \n---递归修改当前目录下aaa目录以及子目录子文件的权限都为所有用户读写执行  \n`chmod -R a=rwx aaa`\n\n读： 4  \n写： 2  \n执行： 1\n\n读写执行：4+2+1=7  \n读写： 4+2=6  \n---递归修改当前目录下aaa目录以及子目录子文件的权限都为所有用户读写执行  \nchmod -R 777 aaa  \n---修改当前目录下文件ana.cfg的权限为属主读写执行，属组读写，其它用户为读  \nchmod 764 ana.cfg\n\n### chown 改变文件/目录属主\n\n```bash\nchown [-cfhvR] [--help] [--version] user[:group] file...\n常用：\n  chown 属主 [-R] 文件或目录\n  chown :属组 [-R] 文件或目录\n  chown 属主:属组 [-R] 文件或目录\n```\n\n**参数** :\n\n- user : 新的文件拥有者的使用者 ID\n- group : 新的文件拥有者的使用者组(group)\n- -c : 显示更改的部分的信息\n- -f : 忽略错误信息\n- -h :修复符号链接\n- -v : 显示详细的处理信息\n- **-R : 处理指定目录以及其子目录下的所有文件**\n- --help : 显示辅助说明\n- --version : 显示版本\n\n**只有root能改变文件的属主，任何属主都能改变文件的属组，只要属主使原属组和目标属组的成员**\n\n#### 案例\n\n（1）修改文件所有者\n\n```bash\n[root@hadoop101 ~]# chown neuedu houge.txt \n[root@hadoop101 ~]# ls -al\n---------------------------\n-rwxrwxrwx. 1 neuedu root 551 5月  23 13:02 houge.txt\n```\n\n（2）递归改变文件所有者和所有组\n\n```bash\n[root@hadoop101 xiyou]# ll\n---------------------------------------\ndrwxrwxrwx. 2 root root 4096 9月   3 21:20 xiyou\n----------------------------------------\n[root@hadoop101 xiyou]# chown -R neuedu:neuedu xiyou/\n[root@hadoop101 xiyou]# ll\n-----------------------------------------------------\ndrwxrwxrwx. 2 neuedu neuedu 4096 9月   3 21:20 xiyou\n```\n\n---修改当前目录ana.cfg 属主为test2, 属组为neusoft  \n`chown test2:neusoft ana.cfg`  \n-修改当前目录下的aaa目录以及子目录子文件的属主为test2, 属组为neusoft  \n` chown test2:neusoft -R aaa`\n\n### chgrp 改变文件/目录属组\n\n在UNIX系统家族里，文件或目录权限的掌控以拥有者及所属群组来管理。您可以使用chgrp指令去变更文件与目录的所属群组，设置方式采用群组名称或群组识别码皆可。\n\n语法\n\n```bash\n$ chgrp [-cfhRv][--help][--version][所属群组][文件或目录...] 或 chgrp [-cfhRv][--help][--reference=\u003c参考文件或目录\u003e][--version][文件或目录...]\n$ chgrp [最终用户组] [文件或目录]\t（功能描述：改变文件或者目录的所属组）\n```\n\n参数说明\n\n- -c或--changes 效果类似\"-v\"参数，但仅回报更改的部分。　　\n- -f或--quiet或--silent 　不显示错误信息。\n- -h或--no-dereference 　只对符号连接的文件作修改，而不更动其他任何相关文件。\n- -R或--recursive 　递归处理，将指定目录下的所有文件及子目录一并处理。\n- -v或--verbose 　显示指令执行过程。\n- --help 　在线帮助。\n- --reference=\u003c参考文件或目录\u003e 　把指定文件或目录的所属群组全部设成和参考文件或目录的所属群组相同。\n- --version 　显示版本信息。\n\n实例：改变文件的群组属性：\n\n```bash\nchgrp -v bin log2012.log\n```\n\n修改文件的所属组\n\n```bash\n[root@hadoop101 ~]# chgrp root houge.txt\n[root@hadoop101 ~]# ls -al\n------------------------------------\n-rwxrwxrwx. 1 neuedu root 551 5月  23 13:02 houge.txt\n```\n\n## 搜索查找命令\n\n### find 查找文件/目录\n\n有一个相似的命令：which ---查找命令程序的位置，查找范围是$PATH环境变量中所列出的路径。\n\nfind指令将从指定目录向下递归地遍历其各个子目录，将满足条件的文件显示在终端。\n\n#### 基本语法\n\n```bash\n$ find [搜索范围] [选项]\n```\n\n#### 选项说明\n\n/: 在整个linux系统中进行查找  \n/etc: 查找配置文件时指定的路径\n\n| 选项            | 功能                                                         |\n| --------------- | ------------------------------------------------------------ |\n| -name\u003c查询方式\u003e | 按照指定的文件名查找模式查找文件                             |\n| -user\u003c用户名\u003e   | 查找属于指定用户名所有文件                                   |\n| -size\u003c文件大小\u003e | 按照指定的文件大小查找文件。                                 |\n| -type           | 按文件类型    f: 普通文件    d: 目录文件    l: 链接文件   b: 块设备文件   c:  字符设备文件 |\n\n#### 案例\n\n（1）按文件名：根据名称查找/目录下的filename.txt文件。\n\n```bash\n[root@hadoop101 ~]# find xiyou/ -name “*.txt”\n```\n\n（2）按拥有者：查找/opt目录下，用户名称为-user的文件\n\n```bash\n[root@hadoop101 ~]# find xiyou/ -user neuedu\n```\n\n（3）按文件大小：在/home目录下查找大于200m的文件（+n 大于 -n小于 n等于）\n\n```bash\n[root@hadoop101 ~]find /home -size +204800\n```\n\n### grep 过滤查找及“|”管道符\n\n管道符，“|”，表示将前一个命令的处理结果输出传递给后面的命令处理。\n\n前一个命令的输出是后一个命令的输入。\n\n`ls -R / | more` ---将查找根目录以及子目录的列表分页显示\n\n#### 基本语法\n\n```bash\n$ grep 选项 查找内容 源文件\n```\n\n#### 选项说明\n\n| 选项 | 功能               |\n| ---- | ------------------ |\n| -n   | 显示匹配行及行号。 |\n\n##### 案例\n\n（1）查找某文件在第几行\n\n```bash\n[root@hadoop101 ~]# ls | grep -n test\n```\n\n### **which** 查找命令\n\n查找命令在那个目录下\n\n#### 基本语法\n\n```bash\n$ which 命令\n```\n\n##### 案例\n\n```bash\n$ which ll\n```\n\n## 压缩和解压命令\n\n### gzip/gunzip 压缩\n\n特点注意：\n\n一个压缩包只能压缩一个文件  \n只能压缩文件不能压缩目录  \n压缩后原文件不保留  \n解压后压缩文件不保留\n\n#### 基本语法\n\n```bash\n$ gzip 文件\t\t（功能描述：压缩文件，只能将文件压缩为*.gz文件）\n\n$ gunzip 文件.gz\t（功能描述：解压缩文件命令）\n```\n\n#### 经验技巧\n\n（1）**只能压缩文件**不能压缩目录\n\n（2）**不保留原来的文件**\n\n#### 案例\n\n（1）gzip压缩\n\n```bash\n[root@hadoop101 ~]# ls\n------\ntest.java\n-----\n[root@hadoop101 ~]# gzip houge.txt\n[root@hadoop101 ~]# ls\n----------\nhouge.txt.gz\n```\n\n（2）gunzip解压缩文件\n\n```bash\n[root@hadoop101 ~]# gunzip houge.txt.gz \n[root@hadoop101 ~]# ls\nhouge.txt\n```\n\n### zip/unzip 压缩\n\n压缩和解压.zip包\n\n#### 基本语法\n\n```bash\n$ zip  [选项] XXX.zip  将要压缩的内容 \t\t（功能描述：压缩文件和目录的命令）\n$ unzip [选项] XXX.zip\t\t\t\t\t\t（功能描述：解压缩文件）\n```\n\n#### 选项说明\n\n| zip选项 | 功能     |\n| ------- | -------- |\n| -r      | 压缩目录 |\n\n| unzip选项 | 功能                     |\n| --------- | ------------------------ |\n| -d\u003c目录\u003e  | 指定解压后文件的存放目录 |\n\n#### 经验技巧\n\nzip 压缩命令在window/linux都通用，**可以压缩目录且保留源文件**。\n\n#### 案例\n\n（1）压缩 1.txt 和2.txt，压缩后的名称为mypackage.zip\n\n```bash\n[root@hadoop101 opt]# touch bailongma.txt\n[root@hadoop101 ~]# zip houma.zip houge.txt bailongma.txt \n-----------------------------\n  adding: houge.txt (stored 0%)\n  adding: bailongma.txt (stored 0%)\n\n[root@hadoop101 opt]# ls\n-------------------\nhouge.txt\tbailongma.txt\thouma.zip \n```\n\n（2）解压 mypackage.zip\n\n```bash\n[root@hadoop101 ~]# unzip houma.zip \n----------------------\nArchive:  houma.zip\nextracting: houge.txt               \nextracting: bailongma.txt       \n\n[root@hadoop101 ~]# ls\n-----------------------\nhouge.txt\tbailongma.txt\thouma.zip \n```\n\n（3）解压mypackage.zip到指定目录-d\n\n```bash\n[root@hadoop101 ~]# unzip houma.zip -d /opt\n[root@hadoop101 ~]# ls /opt/\n```\n\n### tar 打包\n\n#### 基本语法\n\n```bash\n$ tar  [选项]  XXX.tar.gz  将要打包进去的内容\t\t（功能描述：打包目录，压缩后的文件格式.tar.gz）\n```\n\n压缩.tar  \n     tar -cvf 压缩包名.tar 压缩的文件或目录列表  \n压缩.tar.gz  \n    tar -zcvf 压缩包名.tar.gz 压缩的文件或目录列表  \n解压.tar:  \n    tar -xvf 压缩包名.tar [-C 目标解压目录]  \n解压.tar.gz:  \n   tar -zxvf 压缩包名.tar.gz [-C 目标解压目录]\n\n#### 选项说明\n\n| 选项   | 功能                 |\n| ------ | -------------------- |\n| -z     | 打包同时压缩         |\n| **-c** | **产生.tar打包文件** |\n| -v     | 显示详细信息         |\n| -f     | 指定压缩后的文件名   |\n| **-x** | **解包.tar文件**     |\n\n#### 案例\n\n（1）压缩多个文件\n\n```bash\n[root@hadoop101 opt]# tar -zcvf houma.tar.gz houge.txt bailongma.txt \n------------\nhouge.txt\nbailongma.txt\n\n[root@hadoop101 opt]# ls\n----------------\nhouma.tar.gz houge.txt bailongma.txt \n```\n\n（2）压缩目录\n\n```bash\n[root@hadoop101 ~]# tar -zcvf xiyou.tar.gz xiyou/\n----------\nxiyou/\nxiyou/mingjie/\nxiyou/dssz/\nxiyou/dssz/houge.txt\n```\n\n（3）解压到当前目录\n\n```bash\n[root@hadoop101 ~]# tar -zxvf houma.tar.gz\n```\n\n（4）解压到指定目录\n\n```bash\n[root@hadoop101 ~]# tar -zxvf xiyou.tar.gz - /opt\n[root@hadoop101 ~]# ll /opt/\n```\n\n## 磁盘分区命令\n\n### df 查看磁盘空间使用情况\n\n`df: disk free 空余硬盘`\n\n#### 基本语法\n\n```bash\n$ df  选项\t（功能描述：列出文件系统的整体磁盘使用量，检查文件系统的磁盘空间占用情况）\n```\n\n#### 选项说明\n\n| 选项 | 功能                                                     |\n| ---- | -------------------------------------------------------- |\n| -h   | 以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示； |\n| -t   | 查看指定文件系统的空间占用情况                           |\n| -T   | 在结果中显示文件系统类型                                 |\n\n#### 案例\n\n（1）查看磁盘使用情况\n\n```bash\n[root@hadoop101 ~]# df -h\n------------------------\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sda2        15G  3.5G   11G  26% /\ntmpfs           939M  224K  939M   1% /dev/shm\n/dev/sda1       190M   39M  142M  22% /boot\n```\n\n### fdisk 查看分区\n\n#### 基本语法\n\n```bash\n$ fdisk -l\t\t\t（功能描述：查看磁盘分区详情）\n```\n\n#### 选项说明\n\n| 选项 | 功能                   |\n| ---- | ---------------------- |\n| -l   | 显示所有硬盘的分区列表 |\n\n#### 经验技巧\n\n该命令必须在root用户下才能使用\n\n#### 功能说明\n\n（1）Linux分区\n\n```bash\nDevice：分区序列\n\nBoot：引导\n\nStart：从X磁柱开始\n\nEnd：到Y磁柱结束\n\nBlocks：容量\n\nId：分区类型ID\n\nSystem：分区类型\n```\n\n（2）Win7分区，如图\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/linux-softs/20210208203732.jpg)\n\n##### 案例\n\n（1）查看系统分区情况\n\n```bash\n[root@hadoop101 /]# fdisk -l\n----------------------------\nDisk /dev/sda: 21.5 GB, 21474836480 bytes\n255 heads, 63 sectors/track, 2610 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0x0005e654\nDevice Boot      Start         End      Blocks   Id  System\n/dev/sda1   *           1          26      204800   83  Linux\nPartition 1 does not end on cylinder boundary.\n/dev/sda2              26        1332    10485760   83  Linux\n/dev/sda3            1332        1593     2097152   82  Linux swap / Solaris\n```\n\n### mount/umount 挂载/卸载\n\n​\t对于Linux用户来讲，不论有几个分区，分别分给哪一个目录使用，它总归就是一个根目录、一个独立且唯一的文件结构。\n\n​\tLinux中每个分区都是用来组成整个文件系统的一部分，它在用一种叫做“挂载”的处理方法，它整个文件系统中包含了一整套的文件和目录，并将一个分区和一个目录联系起来，要载入的那个分区将使它的存储空间在这个目录下获得。\n\n#### 1．挂载前准备（必须要有光盘或者已经连接镜像文件）\n\n![img](linux\u0026mac.assets/wps5.jpg)\n\n![img](linux\u0026mac.assets/wps6.jpg)\n\n#### 基本语法\n\n```bash\n$ mount [-t vfstype] [-o options] device dir\t（功能描述：挂载设备）\n\n$ umount 设备文件名或挂载点\t\t\t（功能描述：卸载设备）\n```\n\n#### 参数说明\n\n| 参数       | 功能                                                         |\n| ---------- | ------------------------------------------------------------ |\n| -t vfstype | 指定文件系统的类型，通常不必指定。mount 会自动选择正确的类型。常用类型有：光盘或光盘镜像：iso9660DOS fat16文件系统：msdos[Windows](http://blog.csdn.net/hancunai0017/article/details/6995284) 9x fat32文件系统：vfatWindows NT ntfs文件系统：ntfsMount Windows文件[网络](http://blog.csdn.net/hancunai0017/article/details/6995284)共享：smbfs[UNIX](http://blog.csdn.net/hancunai0017/article/details/6995284)(LINUX) 文件网络共享：nfs |\n| -o options | 主要用来描述设备或档案的挂接方式。常用的参数有：loop：用来把一个文件当成硬盘分区挂接上系统ro：采用只读方式挂接设备rw：采用读写方式挂接设备　  iocharset：指定访问文件系统所用字符集 |\n| device     | 要挂接(mount)的设备                                          |\n| dir        | 设备在系统上的挂接点(mount point)                            |\n\n#### 案例\n\n（1）挂载光盘镜像文件\n\n```bash\n[root@hadoop101 ~]# mkdir /mnt/cdrom/\t\t\t\t\t\t建立挂载点\n[root@hadoop101 ~]# mount -t iso9660 /dev/cdrom /mnt/cdrom/\t设备/dev/cdrom挂载到 挂载点 ：  /mnt/cdrom中\n[root@hadoop101 ~]# ll /mnt/cdrom/\n```\n\n（2）卸载光盘镜像文件\n\n```bash\n[root@hadoop101 ~]# umount /mnt/cdrom\n```\n\n5．设置开机自动挂载\n\n```bash\n[root@hadoop101 ~]# vi /etc/fstab\n```\n\n添加红框中内容，保存退出。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/linux-softs/20210208203745.jpg)\n\n## 进程线程命令\n\n进程是正在执行的一个程序或命令，每一个进程都是一个运行的实体，都有自己的地址空间，并占用一定的系统资源。\n\n### free\n\nfree -h:以易读方式显示内存的空间占用情况\n\n### ps 查看当前系统进程状态\n\n`ps:process status 进程状态`\n\n#### 基本语法\n\n```bash\n$ ps aux | grep xxx\t\t（功能描述：查看系统中所有进程）\n$ ps -ef | grep xxx\t\t（功能描述：可以查看子父进程之间的关系）\n```\n\n#### 选项说明\n\n| 选项 | 功能                   |\n| ---- | ---------------------- |\n| -a   | 选择所有进程           |\n| -u   | 显示所有用户的所有进程 |\n| -x   | 显示没有终端的进程     |\n\n#### 功能说明\n\n（1）ps aux显示信息说明\n\n​\t**USER**：该进程是由哪个用户产生的\n\n​\t**PID**：进程的ID号\n\n​\t**%CPU**：该进程占用CPU资源的百分比，占用越高，进程越耗费资源；\n\n​\t**%MEM**：该进程占用物理内存的百分比，占用越高，进程越耗费资源；\n\n​\t**VSZ**：该进程占用虚拟内存的大小，单位KB；\n\n​\t**RSS**：该进程占用实际物理内存的大小，单位KB；\n\n​\t**TTY**：该进程是在哪个终端中运行的。其中tty1-tty7代表本地控制台终端，tty1-tty6是本地的字符界面终端，\n\n​ tty7是图形终端。pts/0-255代表虚拟终端。\n\n​\t**STAT**：进程状态。常见的状态有：R：运行、S：睡眠、T：停止状态、s：包含子进程、+：位于后台\n\n​\t**START**：该进程的启动时间\n\n​\t**TIME**：该进程占用CPU的运算时间，注意不是系统时间\n\n​\t**COMMAND**：产生此进程的命令名\n\n（2）ps -ef显示信息说明\n\n​\t**UID**：用户ID\n\n​\t**PID**：进程ID\n\n​\t**PPID**：父进程ID\n\n​\t**C**：CPU用于计算执行优先级的因子。数值越大，表明进程是CPU密集型运算，执行优先级会降低；数值越 小，表明进程是I/O密集型运算，执行优先级会提高\n\n​\t**STIME**：进程启动的时间\n\n​\t**TTY**：完整的终端名称\n\n​\t**TIME**：CPU时间\n\n​\t**CMD**：启动进程所用的命令和参数\n\n#### 经验技巧\n\n​\t如果想查看进程的**CPU占用率和内存占用率**，可以使用aux; 如果想查看**进程的父进程ID**可以使用ef;\n\n#### 案例\n\n```bash\n[root@hadoop101 datas]# ps aux\n```\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/linux-softs/20210208203750.jpg)\n\n```bash\n[root@hadoop101 datas]# ps -ef\n```\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/linux-softs/20210208203806.png)\n\n### kill 终止进程\n\n#### 基本语法\n\n```bash\n$ kill  [选项] 进程号\t\t（功能描述：通过进程号杀死进程）\n$ killall 进程名称\t\t\t（功能描述：通过进程名称杀死进程，也支持通配符，这在系统因负载过大而变得很慢时很有用）\t\n```\n\n#### 选项说明\n\n| 选项 | 功能                 |\n| ---- | -------------------- |\n| -9   | 表示强迫进程立即停止 |\n\n#### 案例\n\n（1）杀死浏览器进程\n\n```bash\n[root@hadoop101 桌面]# kill -9 5102\n```\n\n（2）通过进程名称杀死进程\n\n```bash\n[root@hadoop101 桌面]# killall firefox\n```\n\n### pstree 查看进程树\n\npstree命令在centos minimal版中要单独安装\n\n```bash\n$ yum -y install psmisc \n```\n\n#### 基本语法\n\n```bash\n$ pstree [选项]\n```\n\n#### 选项说明\n\n| 选项 | 功能               |\n| ---- | ------------------ |\n| -p   | 显示进程的PID      |\n| -u   | 显示进程的所属用户 |\n\n#### 案例\n\n（1）显示进程pid\n\n```bash\n[root@hadoop101 datas]# pstree -p\n```\n\n（2）显示进程所属用户\n\n```bash\n[root@hadoop101 datas]# pstree -u\n```\n\n### top 查看系统健康状态\n\n#### 基本语法\n\n```bash\n$ top [选项]\t\n```\n\n#### 选项说明\n\n| 选项    | 功能                                                         |\n| ------- | ------------------------------------------------------------ |\n| -d 秒数 | 指定top命令每隔几秒更新。默认是3秒在top命令的交互模式当中可以执行的命令： |\n| -i      | 使top不显示任何闲置或者僵死进程。                            |\n| -p      | 通过指定监控进程ID来仅仅监控某个进程的状态。                 |\n| -b      | 批处理模式                                                   |\n| -n      | 设置迭代数量                                                 |\n| -u      | 可以用这些选项浏览特定用户的进程。用户名或者UID可以在选项中指定。-p、-u和-U选项是互斥的，同时只可以使用这其中一个选项。当你试图组合使用这些选项时，你会得到一个错误。 |\n\n#### 操作说明（交互命令）\n\n| 操作       | 功能                                                         |\n| ---------- | ------------------------------------------------------------ |\n| P          | 以CPU使用率排序，默认就是此项                                |\n| M          | 以内存的使用率排序                                           |\n| N          | 以PID排序                                                    |\n| q          | 退出top                                                      |\n| h          | 帮助                                                         |\n| 会车或空格 | 手动刷新                                                     |\n| A          | 切换交替显示模式，这个命令在全屏和交替模式间切换。在交替模式下会显示4个窗口（译注：分别关注不同的字段）。这四组字段共有一个独立的可配置的概括区域和它自己的可配置任务区域。4个窗口中只有一个窗口是当前窗口。当前窗口的名称显示在左上方。（译注：只有当前窗口才会接受你键盘交互命令）我们可以用’a’和’w’在4个 窗口间切换。’a’移到后一个窗口，’w’移到前一个窗口。用’g’命令你可以输入一个数字来选择当前窗口。 |\n| B          | 触发粗体显示                                                 |\n| d/s        | 当按下’d’或’s’时，你将被提示输入一个值（以秒为单位），它会以设置的值作为刷新间隔。 |\n| l/t/m      | 切换负载、任务、内存信息的显示，这会相应地切换顶部的平均负载、任务/CPU状态和内存信息的概况显示。 |\n| f          | 用于选择你想要显示的字段。用’*’标记的是已选择的。上下光标键在字段内导航，左光标键可以选择字段，回车或右光标键确认。按'\u003c‘移动已排序的字段到左边，’\u003e’则移动到右边。 |\n| R          | 切换反向/常规排序。                                          |\n| c          | 切换是否显示进程启动时的完整路径和程序名。                   |\n| i          | 切换显示空闲任务。                                           |\n| v          | 切换树视图。                                                 |\n| Z          | 按下’Z’向用户显示一个改变top命令的输出颜色的屏幕。可以为8个任务区域选择8种颜色。 |\n| z          | 切换彩色，即打开或关闭彩色显示。                             |\n| x/y        | 切换高亮信息：’x’将排序字段高亮显示（纵列）；’y’将运行进程高亮显示（横行）。依赖于你的显示设置，你可能需要让输出彩色来看到这些高亮。 |\n| u          | 显示特定用户的进程。你会被提示输入用户名。空白将会显示全部用户。 |\n| n/#        | 设置最大显示的任务数量                                       |\n| k          | top命令中最重要的一个命令之一。用于发送信号给任务（通常是结束任务）。 |\n| r          | 重新设置一个任务的调度优先级。                               |\n\n查询结果字段解释\n\n第一行信息为任务队列信息\n\n| 内容                             | 说明                                                         |\n| -------------------------------- | ------------------------------------------------------------ |\n| 12:26:46                         | 系统当前时间                                                 |\n| up 1 day, 13:32                  | 系统的运行时间，本机已经运行1天13小时32分钟                  |\n| 2 users                          | 当前登录了两个用户                                           |\n| load  average:  0.00, 0.00, 0.00 | 系统在之前1分钟，5分钟，15分钟的平均负载。一般认为小于1时，负载较小。如果大于1，系统已经超出负荷。 |\n\n第二行为进程信息\n\n| Tasks:  95 total | 系统中的进程总数                          |\n| ---------------- | ----------------------------------------- |\n| 1 running        | 正在运行的进程数                          |\n| 94 sleeping      | 睡眠的进程                                |\n| 0 stopped        | 正在停止的进程                            |\n| 0 zombie         | 僵尸进程。如果不是0，需要手工检查僵尸进程 |\n\n第三行为CPU信息\n\n| Cpu(s):  0.1%us | 用户模式占用的CPU百分比                                      |\n| --------------- | ------------------------------------------------------------ |\n| 0.1%sy          | 系统模式占用的CPU百分比                                      |\n| 0.0%ni          | 改变过优先级的用户进程占用的CPU百分比                        |\n| 99.7%id         | 空闲CPU的CPU百分比                                           |\n| 0.1%wa          | 等待输入/输出的进程的占用CPU百分比                           |\n| 0.0%hi          | 硬中断请求服务占用的CPU百分比                                |\n| 0.1%si          | 软中断请求服务占用的CPU百分比                                |\n| 0.0%st          | st（Steal  time）虚拟时间百分比。就是当有虚拟机时，虚拟CPU等待实际CPU的时间百分比。 |\n\n第四行为物理内存信息\n\n| Mem:    625344k total | 物理内存的总量，单位KB                                       |\n| --------------------- | ------------------------------------------------------------ |\n| 571504k used          | 已经使用的物理内存数量                                       |\n| 53840k free           | 空闲的物理内存数量，我们使用的是虚拟机，总共只分配了628MB内存，所以只有53MB的空闲内存了 |\n| 65800k buffers        | 作为缓冲的内存数量                                           |\n\n第五行为交换分区（swap）信息\n\n| Swap:   524280k total | 交换分区（虚拟内存）的总大小 |\n| --------------------- | ---------------------------- |\n| 0k used               | 已经使用的交互分区的大小     |\n| 524280k free          | 空闲交换分区的大小           |\n| 409280k cached        | 作为缓存的交互分区的大小     |\n\n![image-20200321190806968](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/linux-softs/20210208203806.png)\n\n在横向列出的系统属性和状态下面，是以列显示的进程。不同的列代表下面要解释的不同属性。\n\n默认上，top显示这些关于进程的属性：\n\n1. PID 进程ID，进程的唯一标识符\n2. USER 进程所有者的实际用户名。\n3. PR 进程的调度优先级。这个字段的一些值是’rt’。这意味这这些进程运行在实时态\n4. NI 进程的nice值（优先级）。越小的值意味着越高的优先级。\n5. VIRT 进程使用的虚拟内存。\n6. RES 驻留内存大小。驻留内存是任务使用的非交换物理内存大小。\n7. SHR SHR是进程使用的共享内存。\n8. S 这个是进程的状态。它有以下不同的值:\n\n- D – 不可中断的睡眠态。\n- R – 运行态\n- S – 睡眠态\n- T – 被跟踪或已停止\n- Z – 僵尸态\n\n9. %CPU 自从上一次更新时到现在任务所使用的CPU时间百分比。\n10. %MEM 进程使用的可用物理内存百分比。\n11. TIME+ 任务启动后到现在所使用的全部CPU时间，精确到百分之一秒。\n12. COMMAND 运行进程所使用的命令。\n\n还有许多在默认情况下不会显示的输出，它们可以显示进程的页错误、有效组和组ID和其他更多的信息。\n\n#### 案例\n\n```bash\n[root@hadoop101 neuedu]# top -d 1\n[root@hadoop101 neuedu]# top -i\n[root@hadoop101 neuedu]# top -p 2575\n```\n\n执行上述命令后，可以按P、M、N对查询出的进程结果进行排序。\n\n### lsof\n\n- lsof -i:端口号 查看端口占用情况\n- lsof -i:8080：查看8080端口占用\n- lsof abc.txt：显示开启文件abc.txt的进程\n- lsof -c abc：显示abc进程现在打开的文件\n- lsof -c -p 1234：列出进程号为1234的进程所打开的文件\n- lsof -g gid：显示归属gid的进程情况\n- lsof +d /usr/local/：显示目录下被进程开启的文件\n- lsof +D /usr/local/：同上，但是会搜索目录下的目录，时间较+ 长\n- lsof -d 4：显示使用fd为4的进程\n- lsof -i -U：显示所有打开的端口和UNIX domain文件\n\n### netstat 显示网络统计信息和端口占用情况\n\n#### 基本语法\n\n```bash\n$ netstat -anp |grep 进程号\t（功能描述：查看该进程网络信息）\n$ netstat -nlp\t| grep 端口号\t（功能描述：查看网络端口号占用情况）\n```\n\n#### 选项说明\n\n| 选项 | 功能                                     |\n| ---- | ---------------------------------------- |\n| -n   | 拒绝显示别名，能显示数字的全部转化成数字 |\n| -l   | 仅列出有在listen（监听）的服务状态       |\n| -p   | 表示显示哪个进程在调用                   |\n\n#### 案例\n\n（1）通过进程号查看该进程的网络信息\n\n```bash\n[root@hadoop101 hadoop-2.7.2]# netstat -anp | grep 火狐浏览器进程号\n--------------\nunix  2      [ ACC ]     STREAM     LISTENING     **20670**  3115/firefox        /tmp/orbit-root/linc-c2b-0-5734667cbe29\nunix  3      [ ]         STREAM     CONNECTED     20673  3115/firefox        /tmp/orbit-root/linc-c2b-0-5734667cbe29\nunix  3      [ ]         STREAM     CONNECTED     20668  3115/firefox        \nunix  3      [ ]         STREAM     CONNECTED     20666  3115/firefox     \n```\n\n（2）查看某端口号是否被占用\n\n```bash\n[root@hadoop101 桌面]# netstat -nlp | grep 20670\n----------------------------\nunix  2      [ ACC ]     STREAM     LISTENING     20670  3115/firefox        /tmp/orbit-root/linc-c2b-0-5734667cbe29\n```\n\n## crond 定时任务\n\n### crond 服务管理\n\n重新启动crond服务\n\n```bash\n[root@hadoop101 ~]# sytemctl restart crond\n```\n\n### crontab 定时任务设置\n\n#### 基本语法\n\n```bash\n$ crontab [选项]\n```\n\n#### 选项说明\n\n| 选项 | 功能                          |\n| ---- | ----------------------------- |\n| -e   | 编辑crontab定时任务           |\n| -l   | 查询crontab任务               |\n| -r   | 删除当前用户所有的crontab任务 |\n\n#### 参数说明\n\n```bash\n[root@hadoop101 ~]# crontab -e \n```\n\n（1）进入crontab编辑界面。会打开vim编辑你的工作。\n\n\\* * * * * 执行的任务\n\n| 项目      | 含义                 | 范围                    |\n| --------- | -------------------- | ----------------------- |\n| 第一个“*” | 一小时当中的第几分钟 | 0-59                    |\n| 第二个“*” | 一天当中的第几小时   | 0-23                    |\n| 第三个“*” | 一个月当中的第几天   | 1-31                    |\n| 第四个“*” | 一年当中的第几月     | 1-12                    |\n| 第五个“*” | 一周当中的星期几     | 0-7（0和7都代表星期日） |\n\n（2）特殊符号\n\n| 特殊符号 | 含义                                                         |\n| -------- | ------------------------------------------------------------ |\n| *        | 代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思。 |\n| ，       | 代表不连续的时间。比如“0 8,12,16 * * * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令 |\n| -        | 代表连续的时间范围。比如“0 5  *  *  1-6命令”，代表在周一到周六的凌晨5点0分执行命令 |\n| */n      | 代表每隔多久执行一次。比如“*/10  *  *  *  *  命令”，代表每隔10分钟就执行一遍命令 |\n\n（3）特定时间执行命令\n\n| 时间              | 含义                                                         |\n| ----------------- | ------------------------------------------------------------ |\n| 45 22 * * * 命令  | 在22点45分执行命令                                           |\n| 0 17 * * 1 命令   | 每周1 的17点0分执行命令                                      |\n| 0 5 1,15 * * 命令 | 每月1号和15号的凌晨5点0分执行命令                            |\n| 40 4 * * 1-5 命令 | 每周一到周五的凌晨4点40分执行命令                            |\n| */10 4 * * * 命令 | 每天的凌晨4点，每隔10分钟执行一次命令                        |\n| 0 0 1,15 * 1 命令 | 每月1号和15号，每周1的0点0分都会执行命令。注意：星期几和几号最好不要同时出现，因为他们定义的都是天。非常容易让管理员混乱。 |\n\n#### 案例\n\n（1）每隔1分钟，向/root/bailongma.txt文件中添加一个11的数字\n\n```bash\n$ */1 * * * * /bin/echo ”11” \u003e\u003e /root/bailongma.txt\n```\n\n## 基本文件命令\n\n### man 获得帮助信息\n\n#### 基本语法\n\n```bash\n$ man [命令或配置文件]\t\t（功能描述：获得帮助信息）\n```\n\n#### 显示说明\n\n| 信息        | 功能                     |\n| ----------- | ------------------------ |\n| NAME        | 命令的名称和单行描述     |\n| SYNOPSIS    | 怎样使用命令             |\n| DESCRIPTION | 命令功能的深入讨论       |\n| EXAMPLES    | 怎样使用命令的例子       |\n| SEE ALSO    | 相关主题（通常是手册页） |\n\n#### 案例\n\n查看ls命令的帮助信息\n\n```bash\n$ man ls\n```\n\n### help 获得shell内置命令的帮助信息\n\n注意：help命令只能获取shell脚本对应的内置命令\n\n#### 基本语法\n\n```bash\n$ help 命令\t（功能描述：获得shell内置命令的帮助信息\n```\n\n#### 案例\n\n查看cd命令的帮助信息```help cd```\n\n### pwd 显示当前工作目录绝对路径\n\n- 用于切换到新的工作目录\n\n#### 基本语法\n\n```bash\n$ pwd\t\t（功能描述：显示当前工作目录的绝对路径）\n```\n\n#### 案例\n\n显示当前工作目录的绝对路径\n\n```bash\n[root@hadoop150 ~]# pwd\n/root\n```\n\n### ls 列出目录的内容\n\n#### 基本\n\n- ls -alF 直接使用用于按列排序输出当前目录下的文件和目录\n- ls -F 区分文件和目录\n- ls -a 显示包括隐藏文件\n- ls -l 长列表输出，班汉每个文件的相关信息\n- ls -i 查看一个文件的编号\n- ls -F -R\n\n还可合并 ls -FR\n\n显示当前目录下的所有文件以及他们的子文件 可以理解为遍历整个当前目录作为根节点的文件树\n\n#### 过滤\n\nls -l 《想要的文件名称》\n\n如果不确定可用模糊查询  \n\n\u003e ？一个字符  \n\u003e\n\u003e \\* 0或多个字符\n\n[ai] 表示某个位置可以是a或i\n\n[a-i] 可以是a到i\n\n[!a] 不可以是a\n\n### touch 创建文件\n\ntouch 文件名 访问已经存在的文件时会改变修改时间\n\ntouch -a 文件名 仅改变访问时间\n\n用 ls -l —time=atime 文件名 显示文件的访问时间\n\n### cp 复制文件或目录\n\n#### 基本语法\n\n```bash\n$ cp [选项] source dest \t\t\t\t（功能描述：复制source文件到dest）\n```\n\n#### 选项说明\n\n| 选项 | 功能               |\n| ---- | ------------------ |\n| -r   | 递归复制整个文件夹 |\n\n#### 参数说明\n\n| 参数   | 功能     |\n| ------ | -------- |\n| source | 源文件   |\n| dest   | 目标文件 |\n\n#### 经验技巧\n\n​\t强制覆盖不提示的方法：\\cp\n\n#### 案例\n\n（1）复制文件\n\n```bash\n$ cp xiyou/dssz/suwukong.txt xiyou/mingjie/\n```\n\n（2）递归复制整个文件夹\n\n```bash\n$ cp -r xiyou/dssz/ ./\n```\n\n### mv 移动文件与目录或重命名\n\n#### 基本语法\n\n```bash\n$ mv oldNameFile newNameFile （功能描述：重命名 两个参数都不带 / 表示重命名）\n$ mv /temp/movefile /targetFolder\t（功能描述：移动文件）\n```\n\n#### 案例\n\n（1）重命名\n\n```bash\n$ mv xiyou/dssz/suwukong.txt xiyou/dssz/houge.txt\n```\n\n（2）移动文件\n\n```bash\n$ mv xiyou/dssz/houge.txt ./\n```\n\n（3）把aaa的所有内容连同aaa放入bbb（两个文件夹）\n\n```bash\n$ mv aaa bbb \n```\n\n### rm 删除文件\n\nrm -i aaa 带有询问是否真的删除aaa文件,文件名可以模糊查询\n\n-f, --force 忽略不存在的文件，从不给出提示。  \n-i, --interactive 进行交互式删除  \n-r, -R, --recursive 指示rm将参数中列出的全部目录和子目录均递归地删除。(不推荐，可用下面的删除目录)  \n-v, --verbose 详细显示进行的步骤  \n--help 显示此帮助信息并退出  \n--version 输出版本信息并退出\n\n#### 案例\n\n（1）删除目录中的内容\n\n```bash\n$ rm xiyou/mingjie/sunwukong.txt\n```\n\n（2）递归删除目录中所有内容\n\n```bash\n$ rm -rf dssz/\n```\n\n### mkdir创建一个新的目录\n\nmkdir:Make directory 建立目录\n\n#### 基本语法\n\n```bash\nmkdir [选项] 要创建的目录\n```\n\n#### 选项说明\n\n表1-10 选项说明\n\n| 选项      | 功能                              |\n| --------- | --------------------------------- |\n| -p        | 创建多层目录                      |\n| -m=*mode* | 为目录指定访问权限，与chmod类似。 |\n| -v        | 为每个目录显示提示信息。          |\n\n#### 案例\n\n（1）创建一个目录\n\n```bash\n$ mkdir xiyou\n$ mkdir xiyou/mingjie\n```\n\n（2）创建一个多级目录\n\n```bash\n$ mkdir -p xiyou/dssz/meihouwang\n```\n\n### rmdir 删除目录\n\nrmdir 只能删除空目录\n\n####file 查看文件类型\n\n-b 列出文件辨识结果时，不显示文件名称。\n\n-c 详细显示指令执行过程，便于排错或分析程序执行的情形\n\n-f 列出文件中文件名的文件类型\n\n-F 使用指定分隔符号替换输出文件名后的默认的\"：\"分隔符。\n\n-i 输出mime类型的字符串\n\n-L 查看对应软链接对应文件的文件类型\n\n-z 尝试去解读压缩文件的内容\n\n--help 显示命令在线帮助\n\n-version 显示命令版本信息\n\n### cat 文件名 查看整个文件\n\n一般查看比较小的文件，**一屏幕能显示全的**。\n\n#### 基本语法\n\n```bash\n$ cat  [选项] 要查看的文件\n```\n\n-n 或 –number 由 1 开始对所有输出的行数编号  \n-b 或 –number-nonblank 和 -n 相似，只不过对于空白行不编号  \n-s 或 –squeeze-blank 当遇到有连续两行以上的空白行，就代换为一行的空白行  \n-v 或 –show-nonprinting\n\n\u003e 范例：  \n\u003e cat -n linuxfile1 \u003e linuxfile2 把 linuxfile1 的档案内容加上行号后输入 linuxfile2 这个档案里  \n\u003e cat -b linuxfile1 linuxfile2 \u003e\u003e linuxfile3 把 linuxfile1 和 linuxfile2 的档案内容加上行号(空白行不加)之后将内容附加到linuxfile3 里。  \n\u003e 范例：  \n\u003e 把 linuxfile1 的档案内容加上行号后输入 linuxfile2 这个档案里  \n\u003e cat -n linuxfile1 \u003e linuxfile2  \n\u003e 把 linuxfile1 和 linuxfile2 的档案内容加上行号(空白行不加)之后将内容附加到 linuxfile3 里。  \n\u003e cat -b linuxfile1 linuxfile2 \u003e\u003e linuxfile3  \n\u003e cat /dev/null \u003e /etc/test.txt 此为清空/etc/test.txt档案内容\n\n### more\n\n`more [-dlfpcsu] [-num] [+/pattern] [+linenum] [fileNames..]`  \n**参数**：\n\n- -num 一次显示的行数\n- -d 提示使用者，在画面下方显示 [Press space to continue, 'q' to quit.] ，如果使用者按错键，则会显示 [Press 'h' for instructions.] 而不是 '哔' 声\n- -l 取消遇见特殊字元 ^L（送纸字元）时会暂停的功能\n- -f 计算行数时，以实际上的行数，而非自动换行过后的行数（有些单行字数太长的会被扩展为两行或两行以上）\n- -p 不以卷动的方式显示每一页，而是先清除萤幕后再显示内容\n- -c 跟 -p 相似，不同的是先显示内容再清除其他旧资料\n- -s 当遇到有连续两行以上的空白行，就代换为一行的空白行\n- -u 不显示下引号 （根据环境变数 TERM 指定的 terminal 而有所不同）\n- +/pattern 在每个文档显示前搜寻该字串（pattern），然后从该字串之后开始显示\n- +num 从第 num 行开始显示\n- fileNames 欲显示内容的文档，可为复数个数\n\n#### 操作说明\n\n| 操作           | 功能说明                                 |\n| -------------- | ---------------------------------------- |\n| 空白键 (space) | 代表向下翻一页；                         |\n| Enter          | 代表向下翻『一行』；                     |\n| q              | 代表立刻离开 more ，不再显示该文件内容。 |\n| Ctrl+F         | 向下滚动一屏                             |\n| Ctrl+B         | 返回上一屏                               |\n| =              | 输出当前行的行号                         |\n| :f             | 输出文件名和当前行的行号                 |\n\n### less 分屏显示文件内容\n\nless 与 more 类似，但使用 less 可以随意浏览文件，而 more 仅能向前移动，却不能向后移动，而且 less 在查看之前不会加载整个文件。less指令在显示文件内容时，并不是一次将整个文件加载之后才显示，而是根据显示需要加载内容，对于显示**大型文件具有较高的效率**。\n\n#### 基本语法\n\n`less [参数] 文件`\n\n#### **参数说明**：\n\n- -b \u003c缓冲区大小\u003e 设置缓冲区的大小\n- -e 当文件显示结束后，自动离开\n- -f 强迫打开特殊文件，例如外围设备代号、目录和二进制文件\n- -g 只标志最后搜索的关键词\n- -i 忽略搜索时的大小写\n- -m 显示类似more命令的百分比\n- -N 显示每行的行号\n- -o \u003c文件名\u003e 将less 输出的内容在指定文件中保存起来\n- -Q 不使用警告音\n- -s 显示连续空行为一行\n- -S 行过长时间将超出部分舍弃\n- -x \u003c数字\u003e 将\"tab\"键显示为规定的数字空格\n- /字符串：向下搜索\"字符串\"的功能\n- ?字符串：向上搜索\"字符串\"的功能\n- n：重复前一个搜索（与 / 或 ? 有关）\n- N：反向重复前一个搜索（与 / 或 ? 有关）\n- b 向后翻一页\n- d 向后翻半页\n- h 显示帮助界面\n- Q 退出less 命令\n- u 向前滚动半页\n- y 向前滚动一行\n- 空格键 滚动一页\n- 回车键 滚动一行\n- [pagedown]： 向下翻动一页\n- [pageup]： 向上翻动一页\n\n#### 操作说明\n\n| 操作       | 功能说明                                           |\n| ---------- | -------------------------------------------------- |\n| 空白键     | 向下翻动一页；                                     |\n| [pagedown] | 向下翻动一页                                       |\n| [pageup]   | 向上翻动一页；                                     |\n| /字串      | 向下搜寻『字串』的功能；n：向下查找；N：向上查找； |\n| ?字串      | 向上搜寻『字串』的功能；n：向上查找；N：向下查找； |\n| q          | 离开 less 这个程序；                               |\n\n### head 显示文件头部内容\n\nhead用于显示文件的开头部分内容，默认情况下head指令显示文件的前10行内容。\n\n#### 基本语法\n\n```bash\n$ head 文件\t      （功能描述：查看文件头10行内容）\n$ head -n 5 文件      （功能描述：查看文件头5行内容，5可以是任意行数）\n```\n\n#### 选项说明\n\n| 选项      | 功能                   |\n| --------- | ---------------------- |\n| -n \u003c行数\u003e | 指定显示头部内容的行数 |\n\n#### 案例\n\n（1）查看文件的头2行\n\n```bash\n$ head -n 2 smartd.conf\n```\n\n### tail 输出文件尾部内容\n\ntail用于输出文件中尾部的内容，默认情况下tail指令显示文件的后10行内容。\n\n#### 基本语法\n\n```bash\n$ tail  文件 \t\t\t（功能描述：查看文件后10行内容）\n$ tail  -n 5 文件 \t\t（功能描述：查看文件后5行内容，5可以是任意行数）\n$ tail  -f  文件\t\t（功能描述：实时追踪该文档的所有更新）\n```\n\n#### 选项说明\n\n| 选项     | 功能                                 |\n| -------- | ------------------------------------ |\n| -n\u003c行数\u003e | 输出文件尾部n行内容                  |\n| -f       | 显示文件最新追加的内容，监视文件变化 |\n\n#### 案例\n\n（1）查看文件头1行内容\n\n```bash\n$ tail -n 1 smartd.conf \n```\n\n（2）实时追踪该档的所有更新\n\n```bash\n$ tail -f houge.txt\n```\n\n### history 查看已经执行过历史命令\n\n#### 基本语法\n\n```bash\n$ history\t\t\t\t\t\t（功能描述：查看已经执行过历史命令）\n```\n\n#### 案例\n\n（1）查看已经执行过的历史命令\n\n```bash\n$ history\n```\n\n## 关机重启命令\n\n在linux领域内大多用在服务器上，很少遇到关机的操作。毕竟服务器上跑一个服务是永无止境的，除非特殊情况下，不得已才会关机。\n\n**正确的关机流程为**：sync \u003e shutdown \u003e reboot \u003e halt\n\n### 1. 基本语法\n\n（1）sync  \t\t\t（功能描述：将数据由内存同步到硬盘中）\n\n（2）halt \t\t\t（功能描述：关闭系统，等同于shutdown -h now 和 poweroff）\n\n（3）reboot \t\t\t（功能描述：就是重启，等同于 shutdown -r now）\n\n（4）shutdown [选项] 时间\n\n| 选项 | 功能          |\n| ---- | ------------- |\n| -h   | -h=halt关机   |\n| -r   | -r=reboot重启 |\n\n| 参数 | 功能                                   |\n| ---- | -------------------------------------- |\n| now  | 立刻关机                               |\n| 时间 | 等待多久后关机（时间单位是**分钟**）。 |\n\n### 2. 经验技巧\n\n​\tLinux系统中为了提高磁盘的读写效率，对磁盘采取了 “预读迟写”操作方式。当用户保存文件时，Linux核心并不一定立即将保存数据写入物理磁盘中，而是将数据保存在缓冲区中，等缓冲区满时再写入磁盘，这种方式可以极大的提高磁盘写入数据的效率。但是，也带来了安全隐患，如果数据还未写入磁盘时，系统掉电或者其他严重问题出现，则将导致数据丢失。使用sync指令可以立即将缓冲区的数据写入磁盘。\n\n### 3. 案例\n\n（1）将数据由内存同步到硬盘中\n\n```bash\n$ sync\n```\n\n（2）重启\n\n```bash\n$ reboot\n```\n\n（3）关机\n\n```bash\n$ halt\n```\n\n（4）计算机将在1分钟后关机，并且会显示在登录用户的当前屏幕中\n\n```bash\n$ shutdown -h 1 ‘This server will shutdown after 1 mins’\n```\n\n（5）立马关机（等同于 halt）\n\n```bash\n$ shutdown -h now \n```\n\n（6）系统立马重启（等同于 reboot）\n\n```bash\n$ shutdown -r now\n```\n\n## 时间日期命令\n\n### date命令\n\n#### 基本语法\n\n```bash\n$ date [OPTION]... [+FORMAT]\n```\n\n#### 选项说明\n\n| 选项           | 功能                                           |\n| -------------- | ---------------------------------------------- |\n| -d\u003c时间字符串\u003e | 显示指定的“时间字符串”表示的时间，而非当前时间 |\n| -s\u003c日期时间\u003e   | 设置系统日期时间                               |\n\n#### 参数说明\n\n| 参数            | 功能                         |\n| --------------- | ---------------------------- |\n| \u003c+日期时间格式\u003e | 指定显示时使用的日期时间格式 |\n\n#### date 显示当前时间\n\n##### 基本语法\n\n```bash\n$ date\t\t\t\t\t\t\t\t（功能描述：显示当前时间）\n$ date +%Y\t\t\t\t\t\t\t（功能描述：显示当前年份）\n$ date +%m\t\t\t\t\t\t\t（功能描述：显示当前月份）\n$ date +%d\t\t\t\t\t\t\t（功能描述：显示当前是哪一天）\n$ date \"+%Y-%m-%d %H:%M:%S\"\t\t    （功能描述：显示年月日时分秒）\n```\n\n##### 案例\n\n（1）显示当前时间信息\n\n```bash\n$ date\n```\n\n（2）显示当前时间年月日\n\n```bash\n$ date +%Y%m%d\n```\n\n（3）显示当前时间年月日时分秒\n\n```bash\n$ date \"+%Y-%m-%d %H:%M:%S\"\n```\n\n#### date 显示非当前时间\n\n##### 基本语法\n\n```bash\n$ date -d '1 days ago'\t\t\t（功能描述：显示前一天时间）\n\n$ date -d '-1 days ago'\t\t\t（功能描述：显示明天时间）\n```\n\n##### 案例\n\n（1）显示前一天\n\n```bash\n$ date -d '1 days ago'\n```\n\n（2）显示明天时间\n\n```bash\n$ date -d '-1 days ago'\n```\n\n#### date 设置系统时间\n\n##### 基本语法\n\n```bash\n$ date -s 字符串时间\n```\n\n##### 案例\n\n（1）设置系统当前时间\n\n```bash\n$ date -s \"2017-06-19 20:52:18\"\n```\n\n### cal 查看日历\n\n#### 基本语法\n\n```bash\n$ cal [选项](功能描述：不加选项，显示本月日历）\n```\n\n#### 选项说明\n\n| 选项       | 功能             |\n| ---------- | ---------------- |\n| 具体某一年 | 显示这一年的日历 |\n\n#### 案例\n\n（1）查看当前月的日历\n\n```bash\n$ cal\n```\n\n（2）查看2017年的日历\n\n```bash\n$ cal 2017\n```\n\n## 用户管理命令\n\n### useradd 添加新用户\n\n#### 基本语法\n\n```bash\n$ useradd 用户名\t\t\t（功能描述：添加新用户）\n\n$ useradd -g 组名 用户名\t（功能描述：添加新用户到某个组）\n```\n\n#### 参数\n\n- u: 指定UID\n- d:指宿主目录或家目录， 缺省的家目录为/home/用户名\n- g:指定用户的基本组名\n- G:指定用户的附加组名\n- s:指定用户的登录shell\n\n#### 案例\n\n（1）添加一个用户\n\n```bash\n$ useradd tangseng\n$ ll /home/\n```\n\n  (2) 创建用户test2, 指定用户UID为888，基本组为root组，登录shell为/bin/csh，指定家目录\n\n```bash\n useradd -u 888 -g root -s /bin/csh -d /opt/aaa test2\n```\n\n### passwd 设置用户密码\n\n#### 基本语法\n\n```bash\n$ passwd 用户名\t（功能描述：设置用户密码）\n```\n\n#### 参数\n\n- -l: 锁定密码\n- -u: 解锁密码\n- -S：查看用户的帐号的状态\n\n#### 案例\n\n1. 如果是重置自己的密码：  \n   `passwd`\n2. 如果是设置指定用户的密码（root操作）：  \n   `passwd 用户名`\n\n### id 查看用户是否存在\n\n#### 基本语法\n\n```bash\n$ id 用户名\n```\n\n#### 案例\n\n（1）查看用户是否存在\n\n### su 切换用户\n\n`su 用户名` //切换用户身份，但是不更改当前工作目录  \n`su - 用户名` //切换用户身份，并且当前工作目录会更改为新用户的家目录\n\n---当前登录用户是root， 切换到其它用户下，不需要输入密码  \n`su - 用户名`\n\n---当有登录用户是普通用户，切换到其它用户下，都需要输入密码  \n`su 用户名`  \n注意：如果是想切换到root身份下，也可以直接输入su, 回车输入密码即可。\n\n### userdel 删除用户\n\n`userdel [-r] 用户名`\n\n-r: 删除用户时将用户的家目录一并删除\n\n### who /w 查看当前登录用户信息\n\n![image-20200325141458185](linux\u0026mac.assets/image-20200325141458185.png)\n\n#### 案例\n\n（1）显示自身用户名称\n\n```bash\n[root@hadoop101 opt]# whoami\n```\n\n（2）显示登录用户的用户名\n\n```bash\n[root@hadoop101 opt]# who am i\n```\n\n### **sudo** **设置普通用户具有root权限**\n\n要想让普通用户具有root的权限，我们需要使用sudo命令，但前提是这个用户必须在sudoers名单中\n\n#### 1．添加neuedu用户，并对其设置密码。\n\n```bash\n[root@hadoop101 ~]#useradd neuedu\n\n[root@hadoop101 ~]#passwd neuedu\n```\n\n#### 2．修改配置文件\n\n```bash\n[root@hadoop101 ~]#vi /etc/sudoers\n```\n\n修改 /etc/sudoers 文件，找到下面一行(91行)，在root下面添加一行，如下所示：\n\n```bash\n## Allow root to run any commands anywhere\nroot    ALL=(ALL)     ALL\nneuedu   ALL=(ALL)     ALL\n```\n\n或者配置成采用sudo命令时，不需要输入密码\n\n```bash\n\\## Allow root to run any commands anywhere\nroot      ALL=(ALL)     ALL\nneuedu   ALL=(ALL)     NOPASSWD:ALL\n```\n\n修改完毕，现在可以用neuedu帐号登录，然后用命令 sudo ，即可获得root权限进行操作。\n\n#### 3．案例\n\n（1）用普通用户在/opt目录下创建一个文件夹\n\n```bash\n[neuedu@hadoop101 opt]$ sudo mkdir module\n[root@hadoop101 opt]# chown neuedu:neuedu module/\n```\n\n### usermod 修改用户\n\n#### 基本语法\n\n```bash\n$ usermod -g 用户组 用户名\n```\n\n#### 选项说明\n\n| 选项 | 功能                                   |\n| ---- | -------------------------------------- |\n| -g   | 修改用户的初始登录组，给定的组必须存在 |\n| -u   | 修改用户的uid                          |\n| -d   | 修改用户的家目录                       |\n| -s   | 修改用户的登录shell                    |\n\n#### 案例\n\n（1）将用户加入到用户组\n\n```bash\n[root@hadoop101 opt]#usermod -g root zhubajie\n```\n\n----修改test3用户的登录shell为/bin/csh, 用户基本组为root  \n`usermod -s /bin/csh -g root test3`\n\n## 用户组管理命令\n\n每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同Linux 系统对用户组的规定有所不同，如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。\n\n### groupadd 新增组\n\n#### 基本语法\n\n```bash\n$ groupadd 组名\n```\n\n#### 案例\n\n（1）添加一个xitianqujing组\n\n```bash\n[root@hadoop101 opt]#groupadd xitianqujing\n```\n\n---创建一个组neu, 组id为666  \n`groupadd -g 666 neu`\n\n### groupdel 删除组\n\n#### 基本语法\n\n```bash\n$ groupdel 组名\n```\n\n#### 案例\n\n（1）删除xitianqujing组\n\n```bash\n[root@hadoop101 opt]# groupdel xitianqujing\n```\n\n### groupmod 修改组\n\n#### 基本语法\n\n```bash\n$ groupmod -n 新组名 老组名\n```\n\n#### 选项说明\n\n| 选项       | 功能描述           |\n| ---------- | ------------------ |\n| -n\u003c新组名\u003e | 指定工作组的新组名 |\n\n#### 案例\n\n（1）修改neuedu组名称为neuedu1\n\n```bash\n[root@hadoop101 ~]#groupadd xitianqujing\n[root@hadoop101 ~]# groupmod -n xitian xitianqujing\n```\n\n#### cat /etc/group 查看创建了哪些组\n\n##### 基本操作\n\n```bash\n[root@hadoop101 neuedu]# cat  /etc/group\n```\n\n### cat /etc/group 查看创建了哪些组\n\n#### 基本操作\n\n```bash\n[root@hadoop101 neuedu]# cat  /etc/group\n```\n\n## 后台运行命令\n\nUnix/Linux下一般比如想让某个程序在后台运行，很多都是使用\u0026 在程序结尾来让程序自动运行。比如我们要运行mysql在后台：  \n/usr/local/mysql/bin/mysqld_safe --user=mysql \u0026  \n但是加入我们很多程序并不象mysqld一样做成守护进程，可能我们的程序只是普通程序而已，一般这种程序使用\u0026 结尾，但是如果终端关闭，那么程序也会被关闭。但是为了能够后台运行，那么我们就可以使用nohup这个命令，比如我们有个test.php需要在后台运行，并且希望在后台能够定期运行，那么就使用nohup。\n\n### nohup\n\nnohup命令+ \u0026命令，可以让你的程序在后台运行，这样如果你是用xshell来连接到服务器，即使xshell断开了，程序仍然可以运行。nohup可以不受关闭信号所影响， \u0026用于将程序后台运行。\n\n具体执行命令为: `nohup 你的命令 \u0026`  \nnohup /root/test.php \u0026\n\n\u003e 　　提示：  \n\u003e 　　[~]$ appending output to nohup.out  \n\u003e 　　嗯，证明运行成功，同时把程序运行的输出信息放到当前目录的nohup.out 文件中去。\n\n#### 部署jar包\n\n`$ nohup java -jar test.jar \u003etemp.txt \u0026`\n\n\u003e  注意没成功可能是没有在文件路径下\n\n\u003e  这种方法会把日志文件输入到你指定的文件中，没有则会自动创建。进程会在后台运行。  \n\u003e 然后输入 cat temp.txt 查看结果\n\n## vim编辑器\n\n![vi-vim-cheat-sheet-sch](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/linux-softs/20210208203840.gif)\n\n基本上 vi/vim 共分为三种模式，分别是命令模式（Command mode）**，**输入模式（Insert mode)和底线命令模式（Last line mode)。\n\n**这三种模式的作用分别是**：\n\n### **命令模式/一般模式：**\n\n用户刚刚启动 vi/vim，便进入了命令模式。\n\n此状态下敲击键盘动作会被Vim识别为命令，而非输入字符。比如我们此时按下i，并不会输入一个字符，i被当作了一个命令。\n\n在这个模式中， 你可以使用『上下左右』按键来移动光标，你可以使用『删除字符』或『删除整行』来处理档案内容， 也可以使用『复制、贴上』来处理你的文件数据。\n\n以下是常用的几个命令：\n\n- **i** 切换到输入模式，以输入字符。\n- **x** 删除当前光标所在处的字符。\n- **:** 切换到底线命令模式，以在最底一行输入命令。\n- **G** 移到最后一行\n- **gg** 移到第一行\n- **num G** 移到第num行\n- **方向键**或**hjkl**，在文本中移动光标\n- **Page Up**/**Page Down**或**ctrl+F/B**，上/下翻页\n- **dd** 删除光标所在行\n- **dw** 删除光标所在的单词\n- **d$** 删除光标所在位置至行尾\n- **J** 删除当前行尾的换行符\n- **u** 撤销上一个命令\n- **a** 当前光标后追加数据\n- **A** 当前所在行行尾追加数据\n- **r char** char替换当前光标位置的单个字符\n- **R text** text覆盖当前光标所在位置的数据直到按下esc\n\n其他命令\n\n| 语法                        | 功能描述                                           |\n| --------------------------- | -------------------------------------------------- |\n| yy                          | **复制**光标当前一行                               |\n| y数字y                      | 复制一段（从第几行到第几行）                       |\n| p                           | 箭头移动到目的行**粘贴**                           |\n| u                           | **撤销上一步**                                     |\n| dd                          | **删除**光标当前行                                 |\n| d数字d                      | 删除光标（含）后多少行                             |\n| x                           | 删除一个字母，相当于del，**向后删**                |\n| X                           | 删除一个字母，相当于Backspace，向前删              |\n| yw                          | 复制一个词                                         |\n| dw                          | 删除一个词                                         |\n| h 或 向左箭头键(←)          | 光标向左移动一个字符                               |\n| j 或 向下箭头键(↓)          | 光标向下移动一个字符                               |\n| k 或 向上箭头键(↑)          | 光标向上移动一个字符                               |\n| l 或 向右箭头键(→)          | 光标向右移动一个字符                               |\n| [Ctrl] + [f]                | 屏幕『向上』移动一页，相当于 [Page Up] 按键 (常用) |\n| [Ctrl] + [b]                | 屏幕『向下』移动半页                               |\n| shift+^                     | **移动到行头**                                     |\n| shift+$                     | **移动到行尾**                                     |\n| gg或者1+G                   | **移动到页头**                                     |\n| G                           | **移动到页尾**                                     |\n| 数字+G（先输入数字，在按G） | **移动到目标行**                                   |\n\n​\t\t\t\t\t\t\t\t块选择模式\n\n| V         | : 字符选择，会把光标经过的位置反白选择   |\n| --------- | ---------------------------------------- |\n| v         | 行选择，会把光标经过的行反白选择         |\n| Ctrl + v: | 块选择，可以使用长方形的方式反白选择内容 |\n| y         | 将反白的地方复制                         |\n| d         | 将反白的地方删除                         |\n| r         | 修改内容                                 |\n\n命令模式只有一些最基本的命令，因此仍要依靠底线命令模式输入更多命令。\n\n### 输入模式\n\n在命令模式下按下以下对应键就以不同方式进入了输入模式。\n\n注意了！通常在Linux中，按下这些按键时，在画面的左下方会出现『INSERT或 REPLACE』的字样，此时才可以进行编辑。而如果要回到一般模式时， 则必须要按下『Esc』这个按键即可退出编辑模式。\n\n| 按键 | 功能                   |\n| ---- | ---------------------- |\n| i    | **当前光标前**         |\n| a    | 当前光标后             |\n| o    | **当前光标行的下一行** |\n| I    | 光标所在行最前         |\n| A    | 光标所在行最后         |\n| O    | 当前光标行的上一行     |\n\n在输入模式中，可以使用以下按键：\n\n- **字符按键以及Shift组合**，输入字符\n- **ENTER**，回车键，换行\n- **BACK SPACE**，退格键，删除光标前一个字符\n- **DEL**，删除键，删除光标后一个字符\n- **方向键**或**hjkl**，在文本中移动光标\n- **HOME**/**END**，移动光标到行首/行尾\n- **Page Up**/**Page Down**或**ctrl+F/B**，上/下翻页\n- **Insert**，切换光标为输入/替换模式，光标将变成竖线/下划线\n- **ESC**，退出输入模式，切换到命令模式\n\n### 底线命令模式/指令模式\n\n在命令模式下按下:（英文冒号）就进入了底线命令模式。\n\n底线命令模式可以输入单个或多个字符的命令，可用的命令非常多。\n\n在底线命令模式中，基本的命令有（已经省略了冒号）：\n\n| 命令                                               | 功能                                                         |\n| -------------------------------------------------- | ------------------------------------------------------------ |\n| :w                                                 | **保存**                                                     |\n| :q                                                 | **退出**                                                     |\n| :!                                                 | **强制执行**                                                 |\n| / 要查找的词                                       | n 查找下一个，N 往上查找                                     |\n| ? 要查找的词                                       | n是查找上一个，N是往下查找                                   |\n| :nohlsearch 或:noh                                 | 取消查找高亮                                                 |\n| n                                                  | 搜索下一个匹配字符串                                         |\n| N                                                  | 搜索上一个匹配                                               |\n| :n1,n2s/word1/word2/g                              | n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2举例来说，在 100 到 200 行之间搜寻 vbird 并取代为 VBIRD 则：『:100,200s/vbird/VBIRD/g』。(常用) |\n| **:1,$s/word1/word2/g** 或 **:%s/word1/word2/g**   | 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用) |\n| **:1,$s/word1/word2/gc** 或 **:%s/word1/word2/gc** | 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用) |\n| :w [filename]                                      | 将编辑的数据储存成另一个档案（类似另存新档）                 |\n| :n1,n2 w [filename]                                | 将 n1 到 n2 的内容储存成 filename 这个档案。                 |\n| :set nu                                            | 显示行号                                                     |\n| :set nonu                                          | 关闭行号                                                     |\n| ZZ（shift+zz）                                     | **没有修改文件直接退出，如果修改了文件保存后退出**           |\n| :! command                                         | 暂时离开 vi 到指令行模式下执行 command 的显示结果！例如\u003cbr/\u003e『:! ls /home』即可在 vi 当中察看 /home 底下以 ls 输出的档案信息！ |\n\n按ESC键可随时退出底线命令模式。\n\n![vim-vi-workmodel](./linux\u0026mac.assets/vim-vi-workmodel.png)\n\n### 举个栗子\n\n- 强制保存退出 `:wq!`\n- 将数列\n\n  ```\n  10.1.1.214 \n  10.1.1.212 \n  10.1.1.210\n  ```\n\n  编辑成序列：\n\n  ```shell\n  ping -c 4 10.5.5.214 \u003e\u003e result0 \n  ping -c 4 10.5.5.212 \u003e\u003e result0 \n  ping -c 4 10.5.5.210 \u003e\u003e result0\n  ```\n\n  这是一个将 IP 数列修改成可执行的 ping 命令序列的过程。\n\n  1. 修改\n\n     将 IP 数列中第二段所有数字“1” 修改为“5”：\n\n     将游标定位第一个行 IP 地址第二段的“1”\n\n     `ctrl-v `进入纵向编辑模式\n\n     G `移动游标到最后一行，可视块覆盖所要修改的列`\n\n     r `进入修改模式`\n\n     5 `输入数字“5”`\n\n     ESC `退出纵向编辑模式，同时所有被选中的数字都被改成了“5”，并回到命令模式\n\n     结果如下:\n\n     ```shell\n     10.5.5.214 \n     10.5.5.212\n     10.5.5.210\n     ```\n\n  2. 前添加\n\n     在所有行之前添加“ping – c 4 ”：\n\n     将游标定位到第一行第一列\n\n     `ctrl-v `进入纵向编辑模式\n\n     `G `移动游标到最后一行第一列，可视块覆盖了第一列\n\n     `I `进入行首插入模式\n\n     `ping -c 4 `输入所要求字符“ping – c 4 ”\n\n     `ESC `**按两下**退出纵向编辑模式的同时所有选中的字符前都添加了“ping – c 4 ”，回到命令模式\n\n     结果如下：\n\n     ```shell\n     ping -c 4 10.5.5.214 \n     ping -c 4 10.5.5.212 \n     ping -c 4 10.5.5.210\n     ```\n\n  3. 后添加\n\n     在所有行之后添加“\u003e\u003e result0”：\n\n     将游标定位到第一行最后一列\n\n     `ctrl-v` 进入纵向编辑模式\n\n     `G` 移动游标到最后一行最后一列，VISUAL 　 BLOCK 　覆盖了最后一列\n\n     `A` 进入行尾插入模式\n\n     `\u003e\u003e result`\t输入所要求字符“\u003e\u003e result0”\n\n     `ESC` **按两下**退出纵向编辑模式的同时所有选中的字符后都添加了“ \u003e\u003e result0”，回到命令模式\n\n     结果如下：\n\n     ```shell\n     ping -c 4 10.5.5.214 \u003e\u003e result0 \n     ping -c 4 10.5.5.212 \u003e\u003e result0 \n     ping -c 4 10.5.5.210 \u003e\u003e result0\n     ```\n\n  以上三个步骤有一个共同特点，就是都纵向为编辑方向。以上由三行代码为例的方法同样也可以适用于更多的行。\n\n## shell脚本\n\n### **准备工作**\n\n创建好脚本文件后，在bash'中输入绝对路径或添加path后就能运行，\n\n如权限不够，可用`chmod 777 filename`\n\n### 基本语法\n\n#### echo\n\n| 控制字符 | 作用                |\n| -------- | ------------------- |\n| \\\\       | 输出\\本身           |\n| \\n       | 换行符              |\n| \\t       | 制表符，也就是Tab键 |\n\n```\necho hello\necho \"I'am\"\necho ‘I\"am'\n```\n\n若想其他内容和该语句在一行中输出，则要用引号把字符串框住，并最后多一个空格，以及-n命令\n\n```\necho -n \"hello: \"\n```\n\n#### 变量\n\n- 对于系统环境变量，可以用$NAME来调用，用set查看所有系统环境变量\n\n  ```\n  echo $HOME\n  ```\n\n- 若要真实显示$符号，在前面加一个\\即可\n- 用户变量\n  - 直接 name=“sdsd”创立变量\n  - 使用时用$引用\n  - 赋值value1=10 value1=$value2\n- 命令替换\n  \n  - 用val=\\` date\\`或val=$(date)将某一命令的输出赋给变量\n\n#### 重定向输入输出\n\n其实这是UNIX系统的标准输入与标准输出功能，在shell中会说到，这里先知道基本用法\n\n##### 基本语法\n\n- **输入重定向**：\u003c 了解： 命令 \u003c 文件名 ---将文件的内容输入给命令 wc \u003c hadoop.list\n- **输出重定向**：还有一个新的功能---创建新文件\n  - 标准输出重定向\n    - 标准输出：指的是命令正确运行的输出结果  \n      \\\u003e : 命令 \u003e 文件名 --- 会前面命令的输出结果写入到文件中,如果文件不存在，会创建该文件后写入命令的输出内容，如果文件存在，会将文件中原来的信息清除掉之后将命令的输出写入  \n      \\\u003e\\\u003e：命令\u003e\u003e 文件名 ----与上面的命令一样，只是当文件存在时，会在文件原来内容的后面追加写入命令的输出。标准错误输出重定向\n    - 标准错误输出： 指的是命令没有正确运行的报错输出  \n      \t\t2\u003e: 与\u003e一样，会清除原内容再写入  \n        \t\t2\u003e\u003e: 追加写入\n    - 标准输出和标准错误输出重定向\n      - \u0026\u003e:会清除原内容再写入\n      - \u0026\u003e\u003e:追加写入\n\n```bash\n$ ll \u003e文件\t\t（功能描述：列表的内容写入文件a.txt中（覆盖写））\n\n$ ll \u003e\u003e文件\t\t（功能描述：列表的内容**追加**到文件aa.txt的末尾）\n\n$ cat 文件1 \u003e 文件2\t（功能描述：将文件1的内容覆盖到文件2）\n\n$ echo “内容” \u003e\u003e 文件\n```\n\n##### 案例\n\n（1）将ls查看信息写入到文件中\n\n```bash\n$ ls -l\u003ehouge.txt\n```\n\n（2）将ls查看信息追加到文件中\n\n```bash\n$ ls -l\u003e\u003ehouge.txt\n```\n\n（3）采用echo将hello单词追加到文件中\n\n```bash\n$ echo hello\u003e\u003ehouge.txt\n```\n\n#### ln 软链接\n\n软链接也成为符号链接，类似于windows里的快捷方式，有自己的数据块，主要存放了链接其他文件的路径。\n\n\u003e linux连接种类参考：https://www.runoob.com/linux/linux-comm-ln.html\n\n##### 基本语法\n\n```bash\n$ ln -s [原文件或目录] [软链接名]\t\t（功能描述：给原文件创建一个软链接）\n```\n\n##### 经验技巧\n\n删除软链接： rm -rf 软链接名，而不是rm -rf 软链接名/\n\n查询：通过ll就可以查看，列表属性第1位是l，尾部会有位置指向。\n\n##### 案例\n\n（1）创建软连接\n\n```bash\n[root@hadoop101 ~]# mv houge.txt xiyou/dssz/\n[root@hadoop101 ~]# ln -s xiyou/dssz/houge.txt ./houzi\n[root@hadoop101 ~]# ll\n-------------------\nlrwxrwxrwx. 1 root    root      20 6月  17 12:56 houzi -\u003e xiyou/dssz/houge.txt\n```\n\n（2）删除软连接\n\n```bash\n[root@hadoop101 ~]# rm -rf houzi\n```\n\n（3）进入软连接实际物理路径\n\n```bash\n[root@hadoop101 ~]# ln -s xiyou/dssz/ ./dssz\n[root@hadoop101 ~]# cd -P dssz/\n```\n\n####\n\n\n#### 管道\n\n```\nrpm -qa | sort | more\n```\n\n#### 数学运算\n\n- expr 1+5 不推荐\n- 用$[ ]将算式框住，\n\n  ```\n  var1=$[$var2*$var3]\n  ```\n\n  但都只能进行整数运算，如100/45=2\n\n- 使用bc运算器\n\n  ```\n  var1=100\n  var2=45\n  var3=$(echo \"scale=4; $var1/$var2\" | bc)\n  ------\n  var1=10.34\n  var2=12.5\n  var3=34.52\n  var4=56.4\n  \n  var5=$(bc\u003c\u003ceof\n  scale=4\n  a1=($var1*vsr2)\n  b1=($var3*$var4)\n  a1+b1\n  eof\n  )\n  ```\n\n#### 退出脚本\n\n- 退出时都有一个退出状态吗，用echo $? 可以查看\n- 默认脚本以最后一个命令的状态吗退出\n- exit num用来指定自己退出的状态码\n- exit $num 用变量值退出，最大255，超出取模\n\n### 结构化命令\n\n- if-then\n\n  ```\n  if command\n  then\n  \tcommands\n  fi\n  ```\n\n  ```\n  if command; then\n  \tcommands\n  fi\n  ```\n\n  仅当command运行退出状态吗为0时then中的命令才会被运行\n\n- if-then-else\n\n  ```\n  if command\n  then\n  \tcommands\n  else\n  \tcommands\n  fi\n  ```\n\n- 嵌套if\n\n  ```\n  if command\n  then\n  \tcommands\n  elif command2\n  then\n  \tmore commands\n  fi\n  ```\n\n**在elif中，紧跟其后的else语句属于elif代码块，他们并不属于之前的if-then代码块**\n\n- 使用test可以模拟出和一般语言中if一样的效果\n\n  ```\n  if test command\n  then\n   ...\n  fi\n  ```\n\n  ```\n  if [command]\n  then \n   ...\n  fi\n  ```\n\n  如\n\n  ```\n  if [$value1 -gt 5]\n  then\n  \t...\n  fi\n  ```\n\n  gt表示大于，eq为等于\n\n=======  \ntitle: \"Linux 命令总结  \"\ncategories:\n\n - 笔记  \ntags:\n - Linux\n","lastmodified":"2023-05-09T16:33:58.279366106Z","tags":[]},"/Linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96":{"title":"Linux性能优化","content":"\n# Linux性能优化\n\n# 00开篇词 别再让Linux性能问题成为你的绊脚石\n\n你好，我是倪朋飞，微软 Azure 的资深工程师，同时也是 Kubernetes 项目维护者，主要负责开源容器编排系统 Kubernetes 在 Azure 的落地实践。\n\n一直以来，我都在云计算领域工作。对于服务器性能的关注，可以追溯到我刚参加工作那会儿。为什么那么早就开始探索性能问题呢？其实是源于一次我永远都忘不了的“事故”。\n\n那会儿我在盛大云工作，忙活了大半夜把产品发布上线后，刚刚躺下打算休息，却突然收到大量的告警。匆忙爬起来登录到服务器之后，我发现有一些系统进程的 CPU 使用率高达 100%。\n\n当时我完全是两眼一抹黑，可以说是只能看到症状，却完全不知道该从哪儿下手去排查和解决它。直到最后，我也没能想到好办法，这次发布也成了我心中之痛。\n\n从那之后，我开始到处查看各种相关书籍，从操作系统原理、到 Linux 内核，再到硬件驱动程序等等。可是，学了那么多知识之后，我还是不能很快解决类似的性能问题。\n\n于是，我又通过网络搜索，或者请教公司的技术大拿，学习了大量性能优化的思路和方法，这期间尝试了大量的 Linux 性能工具。在不断的实践和总结后，我终于知道，怎么**把观察到的性能问题跟系统原理关联起来，特别是把系统从应用程序、库函数、系统调用、再到内核和硬件等不同的层级贯穿起来。**\n\n这段学习可以算得上是我的“黑暗”经历了。我想，不仅是我一个人，很多人应该都有过这样的挫折。比如说：\n\n- 流量高峰期，服务器 CPU 使用率过高报警，你登录 Linux 上去 top 完之后，却不知道怎么进一步定位，到底是系统 CPU 资源太少，还是程序并发部分写的有问题？\n- 系统并没有跑什么吃内存的程序，但是敲完 free 命令之后，却发现系统已经没有什么内存了，那到底是哪里占用了内存？为什么？\n- 一大早就收到 Zabbix 告警，你发现某台存放监控数据的数据库主机的 iowait 较高，这个时候该怎么办？\n\n这些问题或者场景，你肯定或多或少都遇到过。\n\n实际上，**性能优化一直都是大多数软件工程师头上的“紧箍咒”**，甚至许多工作多年的资深工程师，也无法准确地分析出线上的很多性能问题。\n\n性能问题为什么这么难呢？我觉得主要是因为性能优化是个系统工程，总是牵一发而动全身。它涉及了从程序设计、算法分析、编程语言，再到系统、存储、网络等各种底层基础设施的方方面面。每一个组件都有可能出问题，而且很有可能多个组件同时出问题。\n\n毫无疑问，性能优化是软件系统中最有挑战的工作之一，但是换个角度看，**它也是最考验体现你综合能力的工作之一**。如果说你能把性能优化的各个关键点吃透，那我可以肯定地说，你已经是一个非常优秀的软件工程师了。\n\n那怎样才能掌握这个技能呢？你可以像我前面说的那样，花大量的时间和精力去钻研，从内功到实战一一苦练。当然，那样可行，但也会走很多弯路，而且可能你啃了很多大块头的书，终于拿下了最难的底层体系，却因为缺乏实战经验，在实际开发工作中仍然没有头绪。\n\n其实，对于我们大多数人来说，**最好的学习方式一定是带着问题学习**，而不是先去啃那几本厚厚的原理书籍，这样很容易把自己的信心压垮。\n\n我认为，**学习要会抓重点**。其实只要你了解少数几个系统组件的基本原理和协作方式，掌握基本的性能指标和工具，学会实际工作中性能优化的常用技巧，你就已经可以准确分析和优化大多数的性能问题了。在这个认知的基础上，再反过来去阅读那些经典的操作系统或者其它图书，你才能事半功倍。\n\n所以，在这个专栏里，我会以**案例驱动**的思路，给你讲解 Linux 性能的基本指标、工具，以及相应的观测、分析和调优方法。\n\n具体来看，我会分为 5 个模块。前 4 个模块我会从资源使用的视角出发，带你分析各种 Linux 资源可能会碰到的性能问题，包括 **CPU 性能**、**磁盘 I/O 性能**、**内存性能**以及**网络性能**。每个模块还由浅入深划分为四个不同的篇章。\n\n- **基础篇**，介绍 Linux 必备的基本原理以及对应的性能指标和性能工具。比如怎么理解平均负载，怎么理解上下文切换，Linux 内存的工作原理等等。\n- **案例篇**，这里我会通过模拟案例，帮你分析高手在遇到资源瓶颈时，是如何观测、定位、分析并优化这些性能问题的。\n- **套路篇**，在理解了基础，亲身体验了模拟案例之后，我会帮你梳理出排查问题的整体思路，也就是检查性能问题的一般步骤，这样，以后你遇到问题，就可以按照这样的路子来。\n- **答疑篇**，我相信在学习完每一个模块之后，你都会有很多的问题，在答疑篇里，我会拿出提问频次较高的问题给你系统解答。\n\n第 5 个综合实战模块，我将为你还原真实的工作场景，手把手带你在“**高级战场**”中演练，这样你能把前面学到的所有知识融会贯通，并且看完专栏，马上就能用在工作中。\n\n整个专栏，我会把内容尽量写得通俗易懂，并帮你划出重点、理出知识脉络，再通过案例分析和套路总结，让你学得更透、用得更熟。\n\n明天就要正式开课了，开始之前，我要把何炅说过的那句我特别认同的鸡汤送给你，“**想要得到你就要学会付出，要付出还要坚持；如果你真的觉得很难，那你就放弃，如果你放弃了就不要抱怨。人生就是这样，世界是平衡的，每个人都是通过自己的努力，去决定自己生活的样子。**”\n\n不为别的，就希望你能和我坚持下去，一直到最后一篇文章。这中间，有想不明白的地方，你要先自己多琢磨几次；还是不懂的，你可以在留言区找我问；有需要总结提炼的知识点，你也要自己多下笔。你还可以写下自己的经历，记录你的分析步骤和思路，我都会及时回复你。\n\n最后，你可以在留言区给自己立个 Flag，**哪怕只是在留言区打卡你的学习天数，我相信都是会有效果的**。3 个月后，我们一起再来验收。\n\n# 01 如何学习Linux性能优化？\n\n你好，我是倪朋飞。\n\n你是否也曾跟我一样，看了很多书、学了很多 Linux 性能工具，但在面对 Linux 性能问题时，还是束手无策？实际上，性能分析和优化始终是大多数软件工程师的一个痛点。但是，面对难题，我们真的就无解了吗？\n\n固然，性能问题的复杂性增加了学习难度，但这并不能成为我们进阶路上的“拦路虎”。在我看来，大多数人对性能问题“投降”，原因可能只有两个。\n\n一个是你没找到有效的方法学原理，一听到“系统”、“底层”这些词就发怵，觉得东西太难，自己一定学不会，自然也就无法深入学下去，从而不能建立起性能的全局观。\n\n再一个就是，你看到性能问题的根源太复杂，既不懂怎么去分析，也不能抽丝剥茧找到瓶颈。\n\n你可能会想，反正程序出了问题，上网查就是了，用别人的方法，囫囵吞枣地多试几次，有可能就解决了。于是，你懒得深究这些方法为啥有效，更不知道为什么，很多方法在别人的环境有效，到你这儿就不行了。\n\n所以，相同的错误重复在犯，相同的状况也是重复出现。\n\n其实，性能问题并没有你想像得那么难，**只要你理解了应用程序和系统的少数几个基本原理，再进行大量的实战练习，建立起整体性能的全局观**，大多数性能问题的优化就会水到渠成。\n\n我见过很多工程师，在分析应用程序所使用的第三方组件的性能时，并不熟悉这些组件所用的编程语言，却依然可以分析出线上问题的根源，并能通过一些方法进行优化，比如修改应用程序对它们的调用逻辑，或者调整组件的配置选项等。\n\n还是那句话，**你不需要了解每个组件的所有实现细节**，只要能理解它们最基本的工作原理和协作方式，你也可以做到。\n\n## 性能指标是什么？\n\n学习性能优化的第一步，一定是了解“性能指标”这个概念。\n\n当看到性能指标时，你会首先想到什么呢？我相信“**高并发**”和“**响应快**”一定是最先出现在你脑海里的两个词，而它们也正对应着性能优化的两个核心指标——“吞吐”和“延时”。这两个指标是**从应用负载的视角**来考察性能，直接影响了产品终端的用户体验。跟它们对应的，是**从系统资源的视角**出发的指标，比如资源使用率、饱和度等。\n\n![img](920601da775da08844d231bc2b4c301d.png)\n\n我们知道，随着应用负载的增加，系统资源的使用也会升高，甚至达到极限。而**性能问题的本质**，就是系统资源已经达到瓶颈，但请求的处理却还不够快，无法支撑更多的请求。\n\n性能分析，其实就是**找出应用或系统的瓶颈，并设法去避免或者缓解它们**，从而更高效地利用系统资源处理更多的请求。这包含了一系列的步骤，比如下面这六个步骤。\n\n- 选择指标评估应用程序和系统的性能；\n- 为应用程序和系统设置性能目标；\n- 进行性能基准测试；\n- 性能分析定位瓶颈；\n- 优化系统和应用程序；\n- 性能监控和告警。\n\n了解了这些性能相关的基本指标和核心步骤后，该怎么学呢？接下来，我来说说要学好 Linux 性能优化的几个重要问题。\n\n## 学这个专栏需要什么基础\n\n首先你要明白，我们这个专栏的核心是性能的分析和优化，而不是最基本的 Linux 操作系统的使用方法。\n\n因而，我希望你最好用过 Ubuntu 或其他 Linux 操作系统，然后要具备一些**编程基础**，比如：\n\n- 了解 Linux 常用命令的使用方法；\n- 知道怎么安装和管理软件包；\n- 知道怎么通过编程语言开发应用程序等。\n\n这样，在我讲性能时，你就更容易理解性能背后的原理，特别是在结合专栏里的案例实践后，对性能分析能有更直观的体会。\n\n这个专栏不会像教科书那样，详细教你操作系统、算法原理、网络协议乃至各种编程语言的全部细节，但一些重要的系统原理还是必不可少的。我还会用实际案例一步步教你，贯穿从应用程序到操作系统的各个组件。\n\n## 学习的重点是什么？\n\n想要学习好性能分析和优化，**建立整体系统性能的全局观**是最核心的话题。因而，\n\n- 理解最基本的几个系统知识原理；\n- 掌握必要的性能工具；\n- 通过实际的场景演练，贯穿不同的组件。\n\n这三点，就是我们学习的重中之重。我会在专栏的每篇文章中，针对不同场景，把这三个方面给你讲清楚，你也一定要花时间和心思来消化它们。\n\n其实说到性能工具，就不得不提性能领域的大师布伦丹·格雷格（Brendan Gregg）。他不仅是动态追踪工具 DTrace 的作者，还开发了许许多多的性能工具。我相信你一定见过他所描绘的 Linux 性能工具图谱：\n\n![img](http://learn.lianglianglee.com/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/assets/9ee6c1c5d88b0468af1a3280865a6b7a.png)\n\n（图片来自[brendangregg.com](http://www.brendangregg.com/Perf/linux_perf_tools_full.png)）\n\n这个图是 Linux 性能分析最重要的参考资料之一，它告诉你，在 Linux 不同子系统出现性能问题后，应该用什么样的工具来观测和分析。\n\n比如，当遇到 I/O 性能问题时，可以参考图片最下方的 I/O 子系统，使用 iostat、iotop、blktrace 等工具分析磁盘 I/O 的瓶颈。你可以把这个图保存下来，在需要的时候参考查询。\n\n另外，我还要特别强调一点，就是**性能工具的选用**。有句话是这么说的，一个正确的选择胜过千百次的努力。虽然夸张了些，但是选用合适的性能工具，确实可以大大简化整个性能优化过程。在什么场景选用什么样的工具、以及怎么学会选择合适工具，都是我想教给你的东西。\n\n但是切记，**千万不要把性能工具当成学习的全部**。工具只是解决问题的手段，关键在于你的用法。只有真正理解了它们背后的原理，并且结合具体场景，融会贯通系统的不同组件，你才能真正掌握它们。\n\n最后，为了让你对性能有个全面的认识，我画了一张思维导图，里面涵盖了大部分性能分析和优化都会包含的知识，专栏中也基本都会讲到。你可以保存或者打印下来，每学会一部分就标记出来，记录并把握自己的学习进度。\n\n![img](0faf56cd9521e665f739b03dd04470ba.png)\n\n## 怎么学更高效？\n\n前面我给你讲了 Linux 性能优化的学习重点，接下来我再跟你分享一下，我的几个学习技巧。掌握这些技巧，可以让你学得更轻松。\n\n**技巧一：虽然系统的原理很重要，但在刚开始一定不要试图抓住所有的实现细节。**\n\n深陷到系统实现的内部，可能会让你丢掉学习的重点，而且繁杂的实现逻辑，很可能会打退你学习的积极性。所以，我个人观点是一定要适度。\n\n你可以先学会我给你讲的这些系统工作原理，但不要去深究 Linux 内核是如何做到的，而是要把你的重点放到如何观察和运用这些原理上，比如：\n\n- 有哪些指标可以衡量性能？\n- 使用什么样的性能工具来观察指标？\n- 导致这些指标变化的因素等。\n\n**技巧二：边学边实践，通过大量的案例演习掌握 Linux 性能的分析和优化。**\n\n只有通过在机器上练习，把我讲的知识和案例自己过一遍，这些东西才能转化成你的。我精心设计这些案例，正是为了让你有更好的学习理解和操作体验。\n\n所以我强烈推荐你去实际运行、分析这些案例，或者用学到的知识去分析你自己的系统，这样你会有更直观的感受，获得更好的学习效果。\n\n**技巧三：勤思考，多反思，善总结，多问为什么。**\n\n想真正学懂一门知识，最好的方法就是问问题。当你能提出好的问题时，就说明你已经深入了解了它。\n\n你可以随时在留言区给我留言，写下自己的疑问、思考和总结，和我还有其他的学习者一起讨论切磋。你也可以写下自己经历过的性能问题，记录你的分析步骤和优化思路，我们一起互动探讨。\n\n## 学习之前，你的准备\n\n作为一个包含大量案例实践的课程，我会在每篇文章中，使用一到两台 Ubuntu 18.04 虚拟机，作为案例运行和分析的环境。如果你只是单纯听音频的讲解，却从不动手实践，学习的效果一定会大打折扣。\n\n所以，你是不是可以准备好一台 Linux 机器，用于课程案例的实践呢？任意的虚拟机或物理机都可以，并不局限于 Ubuntu 系统。\n\n## 思考\n\n今天的内容是我们后续学习的热身准备。从下篇文章开始，我们就要正式进入 Linux 性能分析和优化了。所以，我想请你来聊一聊，你之前在解决 Linux 性能问题时，有遇到过什么样的困难或者疑惑吗？或者是之前自己学习 Linux 性能优化时，有哪些问题吗？参考我今天所讲的内容，你又打算怎么来学这个专栏？\n\n# 02 基础篇：到底应该怎么理解“平均负载”？\n\n你好，我是倪朋飞。\n\n每次发现系统变慢时，我们通常做的第一件事，就是执行 top 或者 uptime 命令，来了解系统的负载情况。比如像下面这样，我在命令行里输入了 uptime 命令，系统也随即给出了结果。\n\n```shell\n$ uptime\n02:34:03 up 2 days, 20:14,  1 user,  load average: 0.63, 0.83, 0.88\n```\n\n但我想问的是，你真的知道这里每列输出的含义吗？\n\n我相信你对前面的几列比较熟悉，它们分别是当前时间、系统运行时间以及正在登录用户数。\n\n```cpp\n02:34:03              // 当前时间\nup 2 days, 20:14      // 系统运行时间\n1 user                // 正在登录用户数\n```\n\n而最后三个数字呢，依次则是过去 1 分钟、5 分钟、15 分钟的平均负载（Load Average）。\n\n**平均负载**？这个词对很多人来说，可能既熟悉又陌生，我们每天的工作中，也都会提到这个词，但你真正理解它背后的含义吗？如果你们团队来了一个实习生，他揪住你不放，你能给他讲清楚什么是平均负载吗？\n\n其实，6 年前，我就遇到过这样的一个场景。公司一个实习生一直追问我，什么是平均负载，我支支吾吾半天，最后也没能解释明白。明明总看到也总会用到，怎么就说不明白呢？后来我静下来想想，其实还是自己的功底不够。\n\n于是，这几年，我遇到问题，特别是基础问题，都会多问自己几个“为什么”，以求能够彻底理解现象背后的本质原理，用起来更灵活，也更有底气。\n\n今天，我就带你来学习下，如何观测和理解这个最常见、也是最重要的系统指标。\n\n我猜一定有人会说，平均负载不就是单位时间内的 CPU 使用率吗？上面的 0.63，就代表 CPU 使用率是 63%。其实并不是这样，如果你方便的话，可以通过执行 man uptime 命令，来了解平均负载的详细解释。\n\n简单来说，平均负载是指单位时间内，系统处于**可运行状态**和**不可中断状态**的平均进程数，也就是**平均活跃进程数**，它和 CPU 使用率并没有直接关系。这里我先解释下，可运行状态和不可中断状态这俩词儿。\n\n所谓可运行状态的进程，是指正在使用 CPU 或者正在等待 CPU 的进程，也就是我们常用 ps 命令看到的，处于 R 状态（Running 或 Runnable）的进程。\n\n不可中断状态的进程则是正处于内核态关键流程中的进程，并且这些流程是不可打断的，比如最常见的是等待硬件设备的 I/O 响应，也就是我们在 ps 命令中看到的 D 状态（Uninterruptible Sleep，也称为 Disk Sleep）的进程。\n\n比如，当一个进程向磁盘读写数据时，为了保证数据的一致性，在得到磁盘回复前，它是不能被其他进程或者中断打断的，这个时候的进程就处于不可中断状态。如果此时的进程被打断了，就容易出现磁盘数据与进程数据不一致的问题。\n\n所以，不可中断状态实际上是系统对进程和硬件设备的一种保护机制。\n\n因此，你可以简单理解为，平均负载其实就是平均活跃进程数。平均活跃进程数，直观上的理解就是单位时间内的活跃进程数，但它实际上是活跃进程数的指数衰减平均值。这个“指数衰减平均”的详细含义你不用计较，这只是系统的一种更快速的计算方式，你把它直接当成活跃进程数的平均值也没问题。\n\n既然平均的是活跃进程数，那么最理想的，就是每个 CPU 上都刚好运行着一个进程，这样每个 CPU 都得到了充分利用。比如当平均负载为 2 时，意味着什么呢？\n\n- 在只有 2 个 CPU 的系统上，意味着所有的 CPU 都刚好被完全占用。\n- 在 4 个 CPU 的系统上，意味着 CPU 有 50% 的空闲。\n- 而在只有 1 个 CPU 的系统中，则意味着有一半的进程竞争不到 CPU。\n\n## 平均负载为多少时合理\n\n讲完了什么是平均负载，现在我们再回到最开始的例子，不知道你能否判断出，在 uptime 命令的结果里，那三个时间段的平均负载数，多大的时候能说明系统负载高？或是多小的时候就能说明系统负载很低呢？\n\n我们知道，平均负载最理想的情况是等于 CPU 个数。所以在评判平均负载时，**首先你要知道系统有几个 CPU**，这可以通过 top 命令或者从文件 /proc/cpuinfo 中读取，比如：\n\n```shell\n# 关于 grep 和 wc 的用法请查询它们的手册或者网络搜索\n$ grep 'model name' /proc/cpuinfo | wc -l\n2\n```\n\n有了 CPU 个数，我们就可以判断出，当平均负载比 CPU 个数还大的时候，系统已经出现了过载。\n\n不过，且慢，新的问题又来了。我们在例子中可以看到，平均负载有三个数值，到底该参考哪一个呢？\n\n实际上，都要看。三个不同时间间隔的平均值，其实给我们提供了，分析**系统负载趋势**的数据来源，让我们能更全面、更立体地理解目前的负载状况。\n\n打个比方，就像初秋时北京的天气，如果只看中午的温度，你可能以为还在 7 月份的大夏天呢。但如果你结合了早上、中午、晚上三个时间点的温度来看，基本就可以全方位了解这一天的天气情况了。\n\n同样的，前面说到的 CPU 的三个负载时间段也是这个道理。\n\n- 如果 1 分钟、5 分钟、15 分钟的三个值基本相同，或者相差不大，那就说明系统负载很平稳。\n- 但如果 1 分钟的值远小于 15 分钟的值，就说明系统最近 1 分钟的负载在减少，而过去 15 分钟内却有很大的负载。\n- 反过来，如果 1 分钟的值远大于 15 分钟的值，就说明最近 1 分钟的负载在增加，这种增加有可能只是临时性的，也有可能还会持续增加下去，所以就需要持续观察。一旦 1 分钟的平均负载接近或超过了 CPU 的个数，就意味着系统正在发生过载的问题，这时就得分析调查是哪里导致的问题，并要想办法优化了。\n\n这里我再举个例子，假设我们在一个单 CPU 系统上看到平均负载为 1.73，0.60，7.98，那么说明在过去 1 分钟内，系统有 73% 的超载，而在 15 分钟内，有 698% 的超载，从整体趋势来看，系统的负载在降低。\n\n那么，在实际生产环境中，平均负载多高时，需要我们重点关注呢？\n\n在我看来，**当平均负载高于 CPU 数量 70% 的时候**，你就应该分析排查负载高的问题了。一旦负载过高，就可能导致进程响应变慢，进而影响服务的正常功能。\n\n但 70% 这个数字并不是绝对的，最推荐的方法，还是把系统的平均负载监控起来，然后根据更多的历史数据，判断负载的变化趋势。当发现负载有明显升高趋势时，比如说负载翻倍了，你再去做分析和调查。\n\n## 平均负载与 CPU 使用率\n\n现实工作中，我们经常容易把平均负载和 CPU 使用率混淆，所以在这里，我也做一个区分。\n\n可能你会疑惑，既然平均负载代表的是活跃进程数，那平均负载高了，不就意味着 CPU 使用率高吗？\n\n我们还是要回到平均负载的含义上来，平均负载是指单位时间内，处于可运行状态和不可中断状态的进程数。所以，它不仅包括了**正在使用 CPU** 的进程，还包括**等待 CPU** 和**等待 I/O**的进程。\n\n而 CPU 使用率，是单位时间内 CPU 繁忙情况的统计，跟平均负载并不一定完全对应。比如：\n\n- CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时这两者是一致的；\n- I/O 密集型进程，等待 I/O 也会导致平均负载升高，但 CPU 使用率不一定很高；\n- 大量等待 CPU 的进程调度也会导致平均负载升高，此时的 CPU 使用率也会比较高。\n\n## 平均负载案例分析\n\n下面，我们以三个示例分别来看这三种情况，并用 iostat、mpstat、pidstat 等工具，找出平均负载升高的根源。\n\n因为案例分析都是基于机器上的操作，所以不要只是听听、看看就够了，最好还是跟着我实际操作一下。\n\n### 你的准备\n\n下面的案例都是基于 Ubuntu 18.04，当然，同样适用于其他 Linux 系统。我使用的案例环境如下所示。\n\n- 机器配置：2 CPU，8GB 内存。\n- 预先安装 stress 和 sysstat 包，如 apt install stress sysstat。\n\n在这里，我先简单介绍一下 stress 和 sysstat。\n\nstress 是一个 Linux 系统压力测试工具，这里我们用作异常进程模拟平均负载升高的场景。\n\n而 sysstat 包含了常用的 Linux 性能工具，用来监控和分析系统的性能。我们的案例会用到这个包的两个命令 mpstat 和 pidstat。\n\n- mpstat 是一个常用的多核 CPU 性能分析工具，用来实时查看每个 CPU 的性能指标，以及所有 CPU 的平均指标。\n- pidstat 是一个常用的进程性能分析工具，用来实时查看进程的 CPU、内存、I/O 以及上下文切换等性能指标。\n\n此外，每个场景都需要你开三个终端，登录到同一台 Linux 机器中。\n\n实验之前，你先做好上面的准备。如果包的安装有问题，可以先在 Google 一下自行解决，如果还是解决不了，再来留言区找我，这事儿应该不难。\n\n另外要注意，下面的所有命令，我们都是默认以 root 用户运行。所以，如果你是用普通用户登陆的系统，一定要先运行 sudo su root 命令切换到 root 用户。\n\n如果上面的要求都已经完成了，你可以先用 uptime 命令，看一下测试前的平均负载情况：\n\n```shell\n$ uptime\n...,  load average: 0.11, 0.15, 0.09\n```\n\n### 场景一：CPU 密集型进程\n\n首先，我们在第一个终端运行 stress 命令，模拟一个 CPU 使用率 100% 的场景：\n\n```scss\n$ stress --cpu 1 --timeout 600\n```\n\n接着，在第二个终端运行 uptime 查看平均负载的变化情况：\n\n```shell\n# -d 参数表示高亮显示变化的区域\n$ watch -d uptime\n...,  load average: 1.00, 0.75, 0.39\n```\n\n最后，在第三个终端运行 mpstat 查看 CPU 使用率的变化情况：\n\n```perl\n# -P ALL 表示监控所有 CPU，后面数字 5 表示间隔 5 秒后输出一组数据\n$ mpstat -P ALL 5\nLinux 4.15.0 (ubuntu) 09/22/18 _x86_64_ (2 CPU)\n13:30:06     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle\n13:30:11     all   50.05    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.95\n13:30:11       0    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00\n13:30:11       1  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00\n```\n\n从终端二中可以看到，1 分钟的平均负载会慢慢增加到 1.00，而从终端三中还可以看到，正好有一个 CPU 的使用率为 100%，但它的 iowait 只有 0。这说明，平均负载的升高正是由于 CPU 使用率为 100% 。\n\n那么，到底是哪个进程导致了 CPU 使用率为 100% 呢？你可以使用 pidstat 来查询：\n\n```perl\n# 间隔 5 秒后输出一组数据\n$ pidstat -u 5 1\n13:37:07      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command\n13:37:12        0      2962  100.00    0.00    0.00    0.00  100.00     1  stress\n```\n\n从这里可以明显看到，stress 进程的 CPU 使用率为 100%。\n\n### 场景二：I/O 密集型进程\n\n首先还是运行 stress 命令，但这次模拟 I/O 压力，即不停地执行 sync：\n\n```shell\n$ stress -i 1 --timeout 600\n```\n\n还是在第二个终端运行 uptime 查看平均负载的变化情况：\n\n```shell\n$ watch -d uptime\n...,  load average: 1.06, 0.58, 0.37\n```\n\n然后，第三个终端运行 mpstat 查看 CPU 使用率的变化情况：\n\n```perl\n# 显示所有 CPU 的指标，并在间隔 5 秒输出一组数据\n$ mpstat -P ALL 5 1\nLinux 4.15.0 (ubuntu)     09/22/18     _x86_64_    (2 CPU)\n13:41:28     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle\n13:41:33     all    0.21    0.00   12.07   32.67    0.00    0.21    0.00    0.00    0.00   54.84\n13:41:33       0    0.43    0.00   23.87   67.53    0.00    0.43    0.00    0.00    0.00    7.74\n13:41:33       1    0.00    0.00    0.81    0.20    0.00    0.00    0.00    0.00    0.00   98.99\n```\n\n从这里可以看到，1 分钟的平均负载会慢慢增加到 1.06，其中一个 CPU 的系统 CPU 使用率升高到了 23.87，而 iowait 高达 67.53%。这说明，平均负载的升高是由于 iowait 的升高。\n\n那么到底是哪个进程，导致 iowait 这么高呢？我们还是用 pidstat 来查询：\n\n```makefile\n# 间隔 5 秒后输出一组数据，-u 表示 CPU 指标\n$ pidstat -u 5 1\nLinux 4.15.0 (ubuntu)     09/22/18     _x86_64_    (2 CPU)\n13:42:08      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command\n13:42:13        0       104    0.00    3.39    0.00    0.00    3.39     1  kworker/1:1H\n13:42:13        0       109    0.00    0.40    0.00    0.00    0.40     0  kworker/0:1H\n13:42:13        0      2997    2.00   35.53    0.00    3.99   37.52     1  stress\n13:42:13        0      3057    0.00    0.40    0.00    0.00    0.40     0  pidstat\n```\n\n可以发现，还是 stress 进程导致的。\n\n### 场景三：大量进程的场景\n\n当系统中运行进程超出 CPU 运行能力时，就会出现等待 CPU 的进程。\n\n比如，我们还是使用 stress，但这次模拟的是 8 个进程：\n\n```shell\n$ stress -c 8 --timeout 600\n```\n\n由于系统只有 2 个 CPU，明显比 8 个进程要少得多，因而，系统的 CPU 处于严重过载状态，平均负载高达 7.97：\n\n```shell\n$ uptime\n...,  load average: 7.97, 5.93, 3.02\n```\n\n接着再运行 pidstat 来看一下进程的情况：\n\n```makefile\n# 间隔 5 秒后输出一组数据\n$ pidstat -u 5 1\n14:23:25      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command\n14:23:30        0      3190   25.00    0.00    0.00   74.80   25.00     0  stress\n14:23:30        0      3191   25.00    0.00    0.00   75.20   25.00     0  stress\n14:23:30        0      3192   25.00    0.00    0.00   74.80   25.00     1  stress\n14:23:30        0      3193   25.00    0.00    0.00   75.00   25.00     1  stress\n14:23:30        0      3194   24.80    0.00    0.00   74.60   24.80     0  stress\n14:23:30        0      3195   24.80    0.00    0.00   75.00   24.80     0  stress\n14:23:30        0      3196   24.80    0.00    0.00   74.60   24.80     1  stress\n14:23:30        0      3197   24.80    0.00    0.00   74.80   24.80     1  stress\n14:23:30        0      3200    0.00    0.20    0.00    0.20    0.20     0  pidstat\n```\n\n可以看出，8 个进程在争抢 2 个 CPU，每个进程等待 CPU 的时间（也就是代码块中的 %wait 列）高达 75%。这些超出 CPU 计算能力的进程，最终导致 CPU 过载。\n\n## 小结\n\n分析完这三个案例，我再来归纳一下平均负载的理解。\n\n平均负载提供了一个快速查看系统整体性能的手段，反映了整体的负载情况。但只看平均负载本身，我们并不能直接发现，到底是哪里出现了瓶颈。所以，在理解平均负载时，也要注意：\n\n- 平均负载高有可能是 CPU 密集型进程导致的；\n- 平均负载高并不一定代表 CPU 使用率高，还有可能是 I/O 更繁忙了；\n- 当发现负载高的时候，你可以使用 mpstat、pidstat 等工具，辅助分析负载的来源。\n\n## 思考\n\n最后，我想邀请你一起来聊聊你所理解的平均负载，当你发现平均负载升高后，又是怎么分析排查的呢？你可以结合我前面的讲解，来总结自己的思考。欢迎在留言区和我讨论。\n\n# 03 基础篇：经常说的 CPU 上下文切换是什么意思？（上）\n\n你好，我是倪朋飞。\n\n上一节，我给你讲了要怎么理解平均负载（ Load Average），并用三个案例展示了不同场景下平均负载升高的分析方法。这其中，多个进程竞争 CPU 就是一个经常被我们忽视的问题。\n\n我想你一定很好奇，进程在竞争 CPU 的时候并没有真正运行，为什么还会导致系统的负载升高呢？看到今天的主题，你应该已经猜到了，CPU 上下文切换就是罪魁祸首。\n\n我们都知道，Linux 是一个多任务操作系统，它支持远大于 CPU 数量的任务同时运行。当然，这些任务实际上并不是真的在同时运行，而是因为系统在很短的时间内，将 CPU 轮流分配给它们，造成多任务同时运行的错觉。\n\n而在每个任务运行前，CPU 都需要知道任务从哪里加载、又从哪里开始运行，也就是说，需要系统事先帮它设置好 **CPU 寄存器和程序计数器**（Program Counter，PC）。\n\nCPU 寄存器，是 CPU 内置的容量小、但速度极快的内存。而程序计数器，则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。它们都是 CPU 在运行任何任务前，必须的依赖环境，因此也被叫做 **CPU 上下文**。\n\n![img](98ac9df2593a193d6a7f1767cd68eb5f.png)\n\n知道了什么是 CPU 上下文，我想你也很容易理解 **CPU 上下文切换**。CPU 上下文切换，就是先把前一个任务的 CPU 上下文（也就是 CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。\n\n而这些保存下来的上下文，会存储在系统内核中，并在任务重新调度执行时再次加载进来。这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。\n\n我猜肯定会有人说，CPU 上下文切换无非就是更新了 CPU 寄存器的值嘛，但这些寄存器，本身就是为了快速运行任务而设计的，为什么会影响系统的 CPU 性能呢？\n\n在回答这个问题前，不知道你有没有想过，操作系统管理的这些“任务”到底是什么呢？\n\n也许你会说，任务就是进程，或者说任务就是线程。是的，进程和线程正是最常见的任务。但是除此之外，还有没有其他的任务呢？\n\n不要忘了，硬件通过触发信号，会导致中断处理程序的调用，也是一种常见的任务。\n\n所以，根据任务的不同，CPU 的上下文切换就可以分为几个不同的场景，也就是**进程上下文切换**、**线程上下文切换**以及**中断上下文切换**。\n\n这节课我就带你来看看，怎么理解这几个不同的上下文切换，以及它们为什么会引发 CPU 性能相关问题。\n\n## 进程上下文切换\n\nLinux 按照特权等级，把进程的运行空间分为内核空间和用户空间，分别对应着下图中， CPU 特权等级的 Ring 0 和 Ring 3。\n\n- 内核空间（Ring 0）具有最高权限，可以直接访问所有资源；\n- 用户空间（Ring 3）只能访问受限资源，不能直接访问内存等硬件设备，必须通过系统调用陷入到内核中，才能访问这些特权资源。\n\n![img](4d3f622f272c49132ecb9760310ce1a7.png)\n\n换个角度看，也就是说，进程既可以在用户空间运行，又可以在内核空间中运行。进程在用户空间运行时，被称为进程的用户态，而陷入内核空间的时候，被称为进程的内核态。\n\n从用户态到内核态的转变，需要通过**系统调用**来完成。比如，当我们查看文件内容时，就需要多次系统调用来完成：首先调用 open() 打开文件，然后调用 read() 读取文件内容，并调用 write() 将内容写到标准输出，最后再调用 close() 关闭文件。\n\n那么，系统调用的过程有没有发生 CPU 上下文的切换呢？答案自然是肯定的。\n\nCPU 寄存器里原来用户态的指令位置，需要先保存起来。接着，为了执行内核态代码，CPU 寄存器需要更新为内核态指令的新位置。最后才是跳转到内核态运行内核任务。\n\n而系统调用结束后，CPU 寄存器需要**恢复**原来保存的用户态，然后再切换到用户空间，继续运行进程。所以，一次系统调用的过程，其实是发生了两次 CPU 上下文切换。\n\n不过，需要注意的是，系统调用过程中，并不会涉及到虚拟内存等进程用户态的资源，也不会切换进程。这跟我们通常所说的进程上下文切换是不一样的：\n\n- 进程上下文切换，是指从一个进程切换到另一个进程运行。\n- 而系统调用过程中一直是同一个进程在运行。\n\n所以，**系统调用过程通常称为特权模式切换，而不是上下文切换**。但实际上，系统调用过程中，CPU 的上下文切换还是无法避免的。\n\n那么，进程上下文切换跟系统调用又有什么区别呢？\n\n首先，你需要知道，进程是由内核来管理和调度的，进程的切换只能发生在内核态。所以，进程的上下文不仅包括了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的状态。\n\n因此，进程的上下文切换就比系统调用时多了一步：在保存当前进程的内核状态和 CPU 寄存器之前，需要先把该进程的虚拟内存、栈等保存下来；而加载了下一进程的内核态后，还需要刷新进程的虚拟内存和用户栈。\n\n如下图所示，保存上下文和恢复上下文的过程并不是“免费”的，需要内核在 CPU 上运行才能完成。\n\n![img](395666667d77e718da63261be478a96b.png)\n\n根据[Tsuna](https://blog.tsunanet.net/2010/11/how-long-does-it-take-to-make-context.html)的测试报告，每次上下文切换都需要几十纳秒到数微秒的 CPU 时间。这个时间还是相当可观的，特别是在进程上下文切换次数较多的情况下，很容易导致 CPU 将大量时间耗费在寄存器、内核栈以及虚拟内存等资源的保存和恢复上，进而大大缩短了真正运行进程的时间。这也正是上一节中我们所讲的，导致平均负载升高的一个重要因素。\n\n另外，我们知道， Linux 通过 TLB（Translation Lookaside Buffer）来管理虚拟内存到物理内存的映射关系。当虚拟内存更新后，TLB 也需要刷新，内存的访问也会随之变慢。特别是在多处理器系统上，缓存是被多个处理器共享的，刷新缓存不仅会影响当前处理器的进程，还会影响共享缓存的其他处理器的进程。\n\n知道了进程上下文切换潜在的性能问题后，我们再来看，究竟什么时候会切换进程上下文。\n\n显然，进程切换时才需要切换上下文，换句话说，只有在进程调度的时候，才需要切换上下文。Linux 为每个 CPU 都维护了一个就绪队列，将活跃进程（即正在运行和正在等待 CPU 的进程）按照优先级和等待 CPU 的时间排序，然后选择最需要 CPU 的进程，也就是优先级最高和等待 CPU 时间最长的进程来运行。\n\n那么，进程在什么时候才会被调度到 CPU 上运行呢？\n\n最容易想到的一个时机，就是进程执行完终止了，它之前使用的 CPU 会释放出来，这个时候再从就绪队列里，拿一个新的进程过来运行。其实还有很多其他场景，也会触发进程调度，在这里我给你逐个梳理下。\n\n其一，为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，就会被系统挂起，切换到其它正在等待 CPU 的进程运行。\n\n其二，进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行。\n\n其三，当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度。\n\n其四，当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行。\n\n最后一个，发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。\n\n了解这几个场景是非常有必要的，因为一旦出现上下文切换的性能问题，它们就是幕后凶手。\n\n## 线程上下文切换\n\n说完了进程的上下文切换，我们再来看看线程相关的问题。\n\n线程与进程最大的区别在于，**线程是调度的基本单位，而进程则是资源拥有的基本单位**。说白了，所谓内核中的任务调度，实际上的调度对象是线程；而进程只是给线程提供了虚拟内存、全局变量等资源。所以，对于线程和进程，我们可以这么理解：\n\n- 当进程只有一个线程时，可以认为进程就等于线程。\n- 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源。这些资源在上下文切换时是不需要修改的。\n- 另外，线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的。\n\n这么一来，线程的上下文切换其实就可以分为两种情况：\n\n第一种， 前后两个线程属于不同进程。此时，因为资源不共享，所以切换过程就跟进程上下文切换是一样。\n\n第二种，前后两个线程属于同一个进程。此时，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据。\n\n到这里你应该也发现了，虽然同为上下文切换，但同进程内的线程切换，要比多进程间的切换消耗更少的资源，而这，也正是多线程代替多进程的一个优势。\n\n## 中断上下文切换\n\n除了前面两种上下文切换，还有一个场景也会切换 CPU 上下文，那就是中断。\n\n为了快速响应硬件的事件，**中断处理会打断进程的正常调度和执行**，转而调用中断处理程序，响应设备事件。而在打断其他进程时，就需要将进程当前的状态保存下来，这样在中断结束后，进程仍然可以从原来的状态恢复运行。\n\n跟进程上下文不同，中断上下文切换并不涉及到进程的用户态。所以，即便中断过程打断了一个正处在用户态的进程，也不需要保存和恢复这个进程的虚拟内存、全局变量等用户态资源。中断上下文，其实只包括内核态中断服务程序执行所必需的状态，包括 CPU 寄存器、内核堆栈、硬件中断参数等。\n\n**对同一个 CPU 来说，中断处理比进程拥有更高的优先级**，所以中断上下文切换并不会与进程上下文切换同时发生。同样道理，由于中断会打断正常进程的调度和执行，所以大部分中断处理程序都短小精悍，以便尽可能快的执行结束。\n\n另外，跟进程上下文切换一样，中断上下文切换也需要消耗 CPU，切换次数过多也会耗费大量的 CPU，甚至严重降低系统的整体性能。所以，当你发现中断次数过多时，就需要注意去排查它是否会给你的系统带来严重的性能问题。\n\n## 小结\n\n总结一下，不管是哪种场景导致的上下文切换，你都应该知道：\n\n1. CPU 上下文切换，是保证 Linux 系统正常工作的核心功能之一，一般情况下不需要我们特别关注。\n2. 但过多的上下文切换，会把 CPU 时间消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，从而缩短进程真正运行的时间，导致系统的整体性能大幅下降。\n\n今天主要为你介绍这几种上下文切换的工作原理，下一节，我将继续案例实战，说说上下文切换问题的分析方法。\n\n## 思考\n\n最后，我想邀请你一起来聊聊，你所理解的 CPU 上下文切换。你可以结合今天的内容，总结自己的思路和看法，写下你的学习心得。\n\n# 04 基础篇：经常说的 CPU 上下文切换是什么意思？（下）\n\n你好，我是倪朋飞。\n\n上一节，我给你讲了 CPU 上下文切换的工作原理。简单回顾一下，CPU 上下文切换是保证 Linux 系统正常工作的一个核心功能，按照不同场景，可以分为进程上下文切换、线程上下文切换和中断上下文切换。具体的概念和区别，你也要在脑海中过一遍，忘了的话及时查看上一篇。\n\n今天我们就接着来看，究竟怎么分析 CPU 上下文切换的问题。\n\n## 怎么查看系统的上下文切换情况\n\n通过前面学习我们知道，过多的上下文切换，会把 CPU 时间消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，缩短进程真正运行的时间，成了系统性能大幅下降的一个元凶。\n\n既然上下文切换对系统性能影响那么大，你肯定迫不及待想知道，到底要怎么查看上下文切换呢？在这里，我们可以使用 vmstat 这个工具，来查询系统的上下文切换情况。\n\nvmstat 是一个常用的系统性能分析工具，主要用来分析系统的内存使用情况，也常用来分析 CPU 上下文切换和中断的次数。\n\n比如，下面就是一个 vmstat 的使用示例：\n\n```scss\n# 每隔 5 秒输出 1 组数据\n$ vmstat 5\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 0  0      0 7005360  91564 818900    0    0     0     0   25   33  0  0 100  0  0\n```\n\n我们一起来看这个结果，你可以先试着自己解读每列的含义。在这里，我重点强调下，需要特别关注的四列内容：\n\n- cs（context switch）是每秒上下文切换的次数。\n- in（interrupt）则是每秒中断的次数。\n- r（Running or Runnable）是就绪队列的长度，也就是正在运行和等待 CPU 的进程数。\n- b（Blocked）则是处于不可中断睡眠状态的进程数。\n\n可以看到，这个例子中的上下文切换次数 cs 是 33 次，而系统中断次数 in 则是 25 次，而就绪队列长度 r 和不可中断状态进程数 b 都是 0。\n\nvmstat 只给出了系统总体的上下文切换情况，要想查看每个进程的详细情况，就需要使用我们前面提到过的 pidstat 了。给它加上 -w 选项，你就可以查看每个进程上下文切换的情况了。\n\n比如说：\n\n```makefile\n# 每隔 5 秒输出 1 组数据\n$ pidstat -w 5\nLinux 4.15.0 (ubuntu)  09/23/18  _x86_64_  (2 CPU) \n08:18:26      UID       PID   cswch/s nvcswch/s  Command\n08:18:31        0         1      0.20      0.00  systemd\n08:18:31        0         8      5.40      0.00  rcu_sched\n...\n```\n\n这个结果中有两列内容是我们的重点关注对象。一个是 cswch ，表示每秒自愿上下文切换（voluntary context switches）的次数，另一个则是 nvcswch ，表示每秒非自愿上下文切换（non voluntary context switches）的次数。\n\n这两个概念你一定要牢牢记住，因为它们意味着不同的性能问题：\n\n- 所谓**自愿上下文切换，是指进程无法获取所需资源，导致的上下文切换**。比如说， I/O、内存等系统资源不足时，就会发生自愿上下文切换。\n- 而**非自愿上下文切换，则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换**。比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换。\n\n## 案例分析\n\n知道了怎么查看这些指标，另一个问题又来了，上下文切换频率是多少次才算正常呢？别急着要答案，同样的，我们先来看一个上下文切换的案例。通过案例实战演练，你自己就可以分析并找出这个标准了。\n\n### 你的准备\n\n今天的案例，我们将使用 sysbench 来模拟系统多线程调度切换的情况。\n\nsysbench 是一个多线程的基准测试工具，一般用来评估不同系统参数下的数据库负载情况。当然，在这次案例中，我们只把它当成一个异常进程来看，作用是模拟上下文切换过多的问题。\n\n下面的案例基于 Ubuntu 18.04，当然，其他的 Linux 系统同样适用。我使用的案例环境如下所示：\n\n- 机器配置：2 CPU，8GB 内存\n- 预先安装 sysbench 和 sysstat 包，如 apt install sysbench sysstat\n\n正式操作开始前，你需要打开三个终端，登录到同一台 Linux 机器中，并安装好上面提到的两个软件包。包的安装，可以先 Google 一下自行解决，如果仍然有问题的，在留言区写下你的情况。\n\n另外注意，下面所有命令，都**默认以 root 用户运行**。所以，如果你是用普通用户登陆的系统，记住先运行 sudo su root 命令切换到 root 用户。\n\n安装完成后，你可以先用 vmstat 看一下空闲系统的上下文切换次数：\n\n```scss\n# 间隔 1 秒后输出 1 组数据\n$ vmstat 1 1\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 0  0      0 6984064  92668 830896    0    0     2    19   19   35  1  0 99  0  0\n```\n\n这里你可以看到，现在的上下文切换次数 cs 是 35，而中断次数 in 是 19，r 和 b 都是 0。因为这会儿我并没有运行其他任务，所以它们就是空闲系统的上下文切换次数。\n\n### 操作和分析\n\n接下来，我们正式进入实战操作。\n\n首先，在第一个终端里运行 sysbench ，模拟系统多线程调度的瓶颈：\n\n```perl\n# 以 10 个线程运行 5 分钟的基准测试，模拟多线程切换的问题\n$ sysbench --threads=10 --max-time=300 threads run\n```\n\n接着，在第二个终端运行 vmstat ，观察上下文切换情况：\n\n```scss\n# 每隔 1 秒输出 1 组数据（需要 Ctrl+C 才结束）\n$ vmstat 1\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 6  0      0 6487428 118240 1292772    0    0     0     0 9019 1398830 16 84  0  0  0\n 8  0      0 6487428 118240 1292772    0    0     0     0 10191 1392312 16 84  0  0  0\n```\n\n你应该可以发现，cs 列的上下文切换次数从之前的 35 骤然上升到了 139 万。同时，注意观察其他几个指标：\n\n- r 列：就绪队列的长度已经到了 8，远远超过了系统 CPU 的个数 2，所以肯定会有大量的 CPU 竞争。\n- us（user）和 sy（system）列：这两列的 CPU 使用率加起来上升到了 100%，其中系统 CPU 使用率，也就是 sy 列高达 84%，说明 CPU 主要是被内核占用了。\n- in 列：中断次数也上升到了 1 万左右，说明中断处理也是个潜在的问题。\n\n综合这几个指标，我们可以知道，系统的就绪队列过长，也就是正在运行和等待 CPU 的进程数过多，导致了大量的上下文切换，而上下文切换又导致了系统 CPU 的占用率升高。\n\n那么到底是什么进程导致了这些问题呢？\n\n我们继续分析，在第三个终端再用 pidstat 来看一下， CPU 和进程上下文切换的情况：\n\n```makefile\n# 每隔 1 秒输出 1 组数据（需要 Ctrl+C 才结束）\n# -w 参数表示输出进程切换指标，而 -u 参数则表示输出 CPU 使用指标\n$ pidstat -w -u 1\n08:06:33      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command\n08:06:34        0     10488   30.00  100.00    0.00    0.00  100.00     0  sysbench\n08:06:34        0     26326    0.00    1.00    0.00    0.00    1.00     0  kworker/u4:2 \n08:06:33      UID       PID   cswch/s nvcswch/s  Command\n08:06:34        0         8     11.00      0.00  rcu_sched\n08:06:34        0        16      1.00      0.00  ksoftirqd/1\n08:06:34        0       471      1.00      0.00  hv_balloon\n08:06:34        0      1230      1.00      0.00  iscsid\n08:06:34        0      4089      1.00      0.00  kworker/1:5\n08:06:34        0      4333      1.00      0.00  kworker/0:3\n08:06:34        0     10499      1.00    224.00  pidstat\n08:06:34        0     26326    236.00      0.00  kworker/u4:2\n08:06:34     1000     26784    223.00      0.00  sshd\n```\n\n从 pidstat 的输出你可以发现，CPU 使用率的升高果然是 sysbench 导致的，它的 CPU 使用率已经达到了 100%。但上下文切换则是来自其他进程，包括非自愿上下文切换频率最高的 pidstat ，以及自愿上下文切换频率最高的内核线程 kworker 和 sshd。\n\n不过，细心的你肯定也发现了一个怪异的事儿：pidstat 输出的上下文切换次数，加起来也就几百，比 vmstat 的 139 万明显小了太多。这是怎么回事呢？难道是工具本身出了错吗？\n\n别着急，在怀疑工具之前，我们再来回想一下，前面讲到的几种上下文切换场景。其中有一点提到， Linux 调度的基本单位实际上是线程，而我们的场景 sysbench 模拟的也是线程的调度问题，那么，是不是 pidstat 忽略了线程的数据呢？\n\n通过运行 man pidstat ，你会发现，pidstat 默认显示进程的指标数据，加上 -t 参数后，才会输出线程的指标。\n\n所以，我们可以在第三个终端里， Ctrl+C 停止刚才的 pidstat 命令，再加上 -t 参数，重试一下看看：\n\n```makefile\n# 每隔 1 秒输出一组数据（需要 Ctrl+C 才结束）\n# -wt 参数表示输出线程的上下文切换指标\n$ pidstat -wt 1\n08:14:05      UID      TGID       TID   cswch/s nvcswch/s  Command\n...\n08:14:05        0     10551         -      6.00      0.00  sysbench\n08:14:05        0         -     10551      6.00      0.00  |__sysbench\n08:14:05        0         -     10552  18911.00 103740.00  |__sysbench\n08:14:05        0         -     10553  18915.00 100955.00  |__sysbench\n08:14:05        0         -     10554  18827.00 103954.00  |__sysbench\n...\n```\n\n现在你就能看到了，虽然 sysbench 进程（也就是主线程）的上下文切换次数看起来并不多，但它的子线程的上下文切换次数却有很多。看来，上下文切换罪魁祸首，还是过多的 sysbench 线程。\n\n我们已经找到了上下文切换次数增多的根源，那是不是到这儿就可以结束了呢？\n\n当然不是。不知道你还记不记得，前面在观察系统指标时，除了上下文切换频率骤然升高，还有一个指标也有很大的变化。是的，正是中断次数。中断次数也上升到了 1 万，但到底是什么类型的中断上升了，现在还不清楚。我们接下来继续抽丝剥茧找源头。\n\n既然是中断，我们都知道，它只发生在内核态，而 pidstat 只是一个进程的性能分析工具，并不提供任何关于中断的详细信息，怎样才能知道中断发生的类型呢？\n\n没错，那就是从 /proc/interrupts 这个只读文件中读取。/proc 实际上是 Linux 的一个虚拟文件系统，用于内核空间与用户空间之间的通信。/proc/interrupts 就是这种通信机制的一部分，提供了一个只读的中断使用情况。\n\n我们还是在第三个终端里， Ctrl+C 停止刚才的 pidstat 命令，然后运行下面的命令，观察中断的变化情况：\n\n```python-repl\n# -d 参数表示高亮显示变化的区域\n$ watch -d cat /proc/interrupts\n           CPU0       CPU1\n...\nRES:    2450431    5279697   Rescheduling interrupts\n...\n```\n\n观察一段时间，你可以发现，变化速度最快的是**重调度中断**（RES），这个中断类型表示，唤醒空闲状态的 CPU 来调度新的任务运行。这是多处理器系统（SMP）中，调度器用来分散任务到不同 CPU 的机制，通常也被称为**处理器间中断**（Inter-Processor Interrupts，IPI）。\n\n所以，这里的中断升高还是因为过多任务的调度问题，跟前面上下文切换次数的分析结果是一致的。\n\n通过这个案例，你应该也发现了多工具、多方面指标对比观测的好处。如果最开始时，我们只用了 pidstat 观测，这些很严重的上下文切换线程，压根儿就发现不了了。\n\n现在再回到最初的问题，每秒上下文切换多少次才算正常呢？\n\n**这个数值其实取决于系统本身的 CPU 性能**。在我看来，如果系统的上下文切换次数比较稳定，那么从数百到一万以内，都应该算是正常的。但当上下文切换次数超过一万次，或者切换次数出现数量级的增长时，就很可能已经出现了性能问题。\n\n这时，你还需要根据上下文切换的类型，再做具体分析。比方说：\n\n- 自愿上下文切换变多了，说明进程都在等待资源，有可能发生了 I/O 等其他问题；\n- 非自愿上下文切换变多了，说明进程都在被强制调度，也就是都在争抢 CPU，说明 CPU 的确成了瓶颈；\n- 中断次数变多了，说明 CPU 被中断处理程序占用，还需要通过查看 /proc/interrupts 文件来分析具体的中断类型。\n\n## 小结\n\n今天，我通过一个 sysbench 的案例，给你讲了上下文切换问题的分析思路。碰到上下文切换次数过多的问题时，**我们可以借助 vmstat 、 pidstat 和 /proc/interrupts 等工具**，来辅助排查性能问题的根源。\n\n## 思考\n\n最后，我想请你一起来聊聊，你之前是怎么分析和排查上下文切换问题的。你可以结合这两节的内容和你自己的实际操作，来总结自己的思路。\n\n欢迎在留言区和我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中学习。\n\n# 05 基础篇：某个应用的CPU使用率居然达到100%，我该怎么办？\n\n你好，我是倪朋飞。\n\n通过前两节对平均负载和 CPU 上下文切换的学习，我相信你对 CPU 的性能已经有了初步了解。不过我还是想问一下，在学这个专栏前，你最常用什么指标来描述系统的 CPU 性能呢？我想你的答案，可能不是平均负载，也不是 CPU 上下文切换，而是另一个更直观的指标—— CPU 使用率。\n\n我们前面说过，CPU 使用率是单位时间内 CPU 使用情况的统计，以百分比的方式展示。那么，作为最常用也是最熟悉的 CPU 指标，你能说出 CPU 使用率到底是怎么算出来的吗？再有，诸如 top、ps 之类的性能工具展示的 %user、%nice、 %system、%iowait 、%steal 等等，你又能弄清楚它们之间的不同吗？\n\n今天我就带你了解 CPU 使用率的内容，同时，我也会以我们最常用的反向代理服务器 Nginx 为例，带你在一步步操作和分析中深入理解。\n\n## CPU 使用率\n\n在上一期我曾提到，Linux 作为一个多任务操作系统，将每个 CPU 的时间划分为很短的时间片，再通过调度器轮流分配给各个任务使用，因此造成多任务同时运行的错觉。\n\n为了维护 CPU 时间，Linux 通过事先定义的节拍率（内核中表示为 HZ），触发时间中断，并使用全局变量 Jiffies 记录了开机以来的节拍数。每发生一次时间中断，Jiffies 的值就加 1。\n\n节拍率 HZ 是内核的可配选项，可以设置为 100、250、1000 等。不同的系统可能设置不同数值，你可以通过查询 /boot/config 内核选项来查看它的配置值。比如在我的系统中，节拍率设置成了 250，也就是每秒钟触发 250 次时间中断。\n\n```shell\n$ grep 'CONFIG_HZ=' /boot/config-$(uname -r)\nCONFIG_HZ=250\n```\n\n同时，正因为节拍率 HZ 是内核选项，所以用户空间程序并不能直接访问。为了方便用户空间程序，内核还提供了一个用户空间节拍率 USER_HZ，它总是固定为 100，也就是 1/100 秒。这样，用户空间程序并不需要关心内核中 HZ 被设置成了多少，因为它看到的总是固定值 USER_HZ。\n\nLinux 通过 /proc 虚拟文件系统，向用户空间提供了系统内部状态的信息，而 /proc/stat 提供的就是系统的 CPU 和任务统计信息。比方说，如果你只关注 CPU 的话，可以执行下面的命令：\n\n```shell\n# 只保留各个 CPU 的数据\n$ cat /proc/stat | grep ^cpu\ncpu  280580 7407 286084 172900810 83602 0 583 0 0 0\ncpu0 144745 4181 176701 86423902 52076 0 301 0 0 0\ncpu1 135834 3226 109383 86476907 31525 0 282 0 0 0\n```\n\n这里的输出结果是一个表格。其中，第一列表示的是 CPU 编号，如 cpu0、cpu1 ，而第一行没有编号的 cpu ，表示的是所有 CPU 的累加。其他列则表示不同场景下 CPU 的累加节拍数，它的单位是 USER_HZ，也就是 10 ms（1/100 秒），所以这其实就是不同场景下的 CPU 时间。\n\n当然，这里每一列的顺序并不需要你背下来。你只要记住，有需要的时候，查询 man proc 就可以。不过，你要清楚 man proc 文档里每一列的涵义，它们都是 CPU 使用率相关的重要指标，你还会在很多其他的性能工具中看到它们。下面，我来依次解读一下。\n\n- user（通常缩写为 us），代表用户态 CPU 时间。注意，它不包括下面的 nice 时间，但包括了 guest 时间。\n- nice（通常缩写为 ni），代表低优先级用户态 CPU 时间，也就是进程的 nice 值被调整为 1-19 之间时的 CPU 时间。这里注意，nice 可取值范围是 -20 到 19，数值越大，优先级反而越低。\n- system（通常缩写为 sys），代表内核态 CPU 时间。\n- idle（通常缩写为 id），代表空闲时间。注意，它不包括等待 I/O 的时间（iowait）。\n- iowait（通常缩写为 wa），代表等待 I/O 的 CPU 时间。\n- irq（通常缩写为 hi），代表处理硬中断的 CPU 时间。\n- softirq（通常缩写为 si），代表处理软中断的 CPU 时间。\n- steal（通常缩写为 st），代表当系统运行在虚拟机中的时候，被其他虚拟机占用的 CPU 时间。\n- guest（通常缩写为 guest），代表通过虚拟化运行其他操作系统的时间，也就是运行虚拟机的 CPU 时间。\n- guest_nice（通常缩写为 gnice），代表以低优先级运行虚拟机的时间。\n\n而我们通常所说的 **CPU 使用率，就是除了空闲时间外的其他时间占总 CPU 时间的百分比**，用公式来表示就是：\n\n![img](3edcc7f908c7c1ddba4bbcccc0277c09.png)根据这个公式，我们就可以从 /proc/stat 中的数据，很容易地计算出 CPU 使用率。当然，也可以用每一个场景的 CPU 时间，除以总的 CPU 时间，计算出每个场景的 CPU 使用率。\n\n不过先不要着急计算，你能说出，直接用 /proc/stat 的数据，算的是什么时间段的 CPU 使用率吗？\n\n看到这里，你应该想起来了，这是开机以来的节拍数累加值，所以直接算出来的，是开机以来的平均 CPU 使用率，一般没啥参考价值。\n\n事实上，为了计算 CPU 使用率，性能工具一般都会取间隔一段时间（比如 3 秒）的两次值，作差后，再计算出这段时间内的平均 CPU 使用率，即\n\n![img](8408bb45922afb2db09629a9a7eb1d5a.png)\n\n这个公式，就是我们用各种性能工具所看到的 CPU 使用率的实际计算方法。\n\n现在，我们知道了系统 CPU 使用率的计算方法，那进程的呢？跟系统的指标类似，Linux 也给每个进程提供了运行情况的统计信息，也就是 /proc/[pid]/stat。不过，这个文件包含的数据就比较丰富了，总共有 52 列的数据。\n\n当然，不用担心，因为你并不需要掌握每一列的含义。还是那句话，需要的时候，查 man proc 就行。\n\n回过头来看，是不是说要查看 CPU 使用率，就必须先读取 /proc/stat 和 /proc/[pid]/stat 这两个文件，然后再按照上面的公式计算出来呢？\n\n当然不是，各种各样的性能分析工具已经帮我们计算好了。不过要注意的是，**性能分析工具给出的都是间隔一段时间的平均 CPU 使用率，所以要注意间隔时间的设置**，特别是用多个工具对比分析时，你一定要保证它们用的是相同的间隔时间。\n\n比如，对比一下 top 和 ps 这两个工具报告的 CPU 使用率，默认的结果很可能不一样，因为 top 默认使用 3 秒时间间隔，而 ps 使用的却是进程的整个生命周期。\n\n## 怎么查看 CPU 使用率\n\n知道了 CPU 使用率的含义后，我们再来看看要怎么查看 CPU 使用率。说到查看 CPU 使用率的工具，我猜你第一反应肯定是 top 和 ps。的确，top 和 ps 是最常用的性能分析工具：\n\n- top 显示了系统总体的 CPU 和内存使用情况，以及各个进程的资源使用情况。\n- ps 则只显示了每个进程的资源使用情况。\n\n比如，top 的输出格式为：\n\n```yaml\n# 默认每 3 秒刷新一次\n$ top\ntop - 11:58:59 up 9 days, 22:47,  1 user,  load average: 0.03, 0.02, 0.00\nTasks: 123 total,   1 running,  72 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  0.3 us,  0.3 sy,  0.0 ni, 99.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nKiB Mem :  8169348 total,  5606884 free,   334640 used,  2227824 buff/cache\nKiB Swap:        0 total,        0 free,        0 used.  7497908 avail Mem \n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n    1 root      20   0   78088   9288   6696 S   0.0  0.1   0:16.83 systemd\n    2 root      20   0       0      0      0 S   0.0  0.0   0:00.05 kthreadd\n    4 root       0 -20       0      0      0 I   0.0  0.0   0:00.00 kworker/0:0H\n...\n```\n\n这个输出结果中，第三行 %Cpu 就是系统的 CPU 使用率，具体每一列的含义上一节都讲过，只是把 CPU 时间变换成了 CPU 使用率，我就不再重复讲了。不过需要注意，top 默认显示的是所有 CPU 的平均值，这个时候你只需要按下数字 1 ，就可以切换到每个 CPU 的使用率了。\n\n继续往下看，空白行之后是进程的实时信息，每个进程都有一个 %CPU 列，表示进程的 CPU 使用率。它是用户态和内核态 CPU 使用率的总和，包括进程用户空间使用的 CPU、通过系统调用执行的内核空间 CPU 、以及在就绪队列等待运行的 CPU。在虚拟化环境中，它还包括了运行虚拟机占用的 CPU。\n\n所以，到这里我们可以发现， top 并没有细分进程的用户态 CPU 和内核态 CPU。那要怎么查看每个进程的详细情况呢？你应该还记得上一节用到的 pidstat 吧，它正是一个专门分析每个进程 CPU 使用情况的工具。\n\n比如，下面的 pidstat 命令，就间隔 1 秒展示了进程的 5 组 CPU 使用率，包括：\n\n- 用户态 CPU 使用率 （%usr）；\n- 内核态 CPU 使用率（%system）；\n- 运行虚拟机 CPU 使用率（%guest）；\n- 等待 CPU 使用率（%wait）；\n- 以及总的 CPU 使用率（%CPU）。\n\n最后的 Average 部分，还计算了 5 组数据的平均值。\n\n```perl\n# 每隔 1 秒输出一组数据，共输出 5 组\n$ pidstat 1 5\n15:56:02      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command\n15:56:03        0     15006    0.00    0.99    0.00    0.00    0.99     1  dockerd \n... \nAverage:      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command\nAverage:        0     15006    0.00    0.99    0.00    0.00    0.99     -  dockerd\n```\n\n## CPU 使用率过高怎么办？\n\n通过 top、ps、pidstat 等工具，你能够轻松找到 CPU 使用率较高（比如 100% ）的进程。接下来，你可能又想知道，占用 CPU 的到底是代码里的哪个函数呢？找到它，你才能更高效、更针对性地进行优化。\n\n我猜你第一个想到的，应该是 GDB（The GNU Project Debugger）， 这个功能强大的程序调试利器。的确，GDB 在调试程序错误方面很强大。但是，我又要来“挑刺”了。请你记住，GDB 并不适合在性能分析的早期应用。\n\n为什么呢？因为 GDB 调试程序的过程会中断程序运行，这在线上环境往往是不允许的。所以，GDB 只适合用在性能分析的后期，当你找到了出问题的大致函数后，线下再借助它来进一步调试函数内部的问题。\n\n那么哪种工具适合在第一时间分析进程的 CPU 问题呢？我的推荐是 perf。perf 是 Linux 2.6.31 以后内置的性能分析工具。它以性能事件采样为基础，不仅可以分析系统的各种事件和内核性能，还可以用来分析指定应用程序的性能问题。\n\n使用 perf 分析 CPU 性能问题，我来说两种最常见、也是我最喜欢的用法。\n\n第一种常见用法是 perf top，类似于 top，它能够实时显示占用 CPU 时钟最多的函数或者指令，因此可以用来查找热点函数，使用界面如下所示：\n\n```javascript\n$ perf top\nSamples: 833  of event 'cpu-clock', Event count (approx.): 97742399\nOverhead  Shared Object       Symbol\n   7.28%  perf                [.] 0x00000000001f78a4\n   4.72%  [kernel]            [k] vsnprintf\n   4.32%  [kernel]            [k] module_get_kallsym\n   3.65%  [kernel]            [k] _raw_spin_unlock_irqrestore\n...\n```\n\n输出结果中，第一行包含三个数据，分别是采样数（Samples）、事件类型（event）和事件总数量（Event count）。比如这个例子中，perf 总共采集了 833 个 CPU 时钟事件，而总事件数则为 97742399。\n\n另外，**采样数需要我们特别注意**。如果采样数过少（比如只有十几个），那下面的排序和百分比就没什么实际参考价值了。\n\n再往下看是一个表格式样的数据，每一行包含四列，分别是：\n\n- 第一列 Overhead ，是该符号的性能事件在所有采样中的比例，用百分比来表示。\n- 第二列 Shared ，是该函数或指令所在的动态共享对象（Dynamic Shared Object），如内核、进程名、动态链接库名、内核模块名等。\n- 第三列 Object ，是动态共享对象的类型。比如 [.] 表示用户空间的可执行程序、或者动态链接库，而 [k] 则表示内核空间。\n- 最后一列 Symbol 是符号名，也就是函数名。当函数名未知时，用十六进制的地址来表示。\n\n还是以上面的输出为例，我们可以看到，占用 CPU 时钟最多的是 perf 工具自身，不过它的比例也只有 7.28%，说明系统并没有 CPU 性能问题。 perf top 的使用你应该很清楚了吧。\n\n接着再来看第二种常见用法，也就是 perf record 和 perf report。 perf top 虽然实时展示了系统的性能信息，但它的缺点是并不保存数据，也就无法用于离线或者后续的分析。而 perf record 则提供了保存数据的功能，保存后的数据，需要你用 perf report 解析展示。\n\n```perl\n$ perf record # 按 Ctrl+C 终止采样\n[ perf record: Woken up 1 times to write data ]\n[ perf record: Captured and wrote 0.452 MB perf.data (6093 samples) ] \n$ perf report # 展示类似于 perf top 的报告\n```\n\n在实际使用中，我们还经常为 perf top 和 perf record 加上 -g 参数，开启调用关系的采样，方便我们根据调用链来分析性能问题。\n\n## 案例\n\n下面我们就以 Nginx + PHP 的 Web 服务为例，来看看当你发现 CPU 使用率过高的问题后，要怎么使用 top 等工具找出异常的进程，又要怎么利用 perf 找出引发性能问题的函数。\n\n### 你的准备\n\n以下案例基于 Ubuntu 18.04，同样适用于其他的 Linux 系统。我使用的案例环境如下所示：\n\n- 机器配置：2 CPU，8GB 内存\n- 预先安装 docker、sysstat、perf、ab 等工具，如 apt install [docker.io](https://docker.io/) sysstat linux-tools-common apache2-utils\n\n我先简单介绍一下这次新使用的工具 ab。ab（apache bench）是一个常用的 HTTP 服务性能测试工具，这里用来模拟 Ngnix 的客户端。由于 Nginx 和 PHP 的配置比较麻烦，我把它们打包成了两个 [Docker 镜像](https://github.com/feiskyer/linux-perf-examples/tree/master/nginx-high-cpu)，这样只需要运行两个容器，就可以得到模拟环境。\n\n注意，这个案例要用到两台虚拟机，如下图所示：\n\n![img](90c30b4f555218f77241bfe2ac27723d.png)\n\n你可以看到，其中一台用作 Web 服务器，来模拟性能问题；另一台用作 Web 服务器的客户端，来给 Web 服务增加压力请求。使用两台虚拟机是为了相互隔离，避免“交叉感染”。\n\n接下来，我们打开两个终端，分别 SSH 登录到两台机器上，并安装上面提到的工具。\n\n还是同样的“配方”。下面的所有命令，都默认假设以 root 用户运行，如果你是普通用户身份登陆系统，一定要先运行 sudo su root 命令切换到 root 用户。到这里，准备工作就完成了。\n\n不过，操作之前，我还想再说一点。这次案例中 PHP 应用的核心逻辑比较简单，大部分人一眼就可以看出问题，但你要知道，实际生产环境中的源码就复杂多了。\n\n所以，我希望你在按照步骤操作之前，先不要查看源码（避免先入为主），而是**把它当成一个黑盒来分析。**这样，你可以更好地理解整个解决思路，怎么从系统的资源使用问题出发，分析出瓶颈所在的应用、以及瓶颈在应用中的大概位置。\n\n### 操作和分析\n\n接下来，我们正式进入操作环节。\n\n首先，在第一个终端执行下面的命令来运行 Nginx 和 PHP 应用：\n\n```shell\n$ docker run --name nginx -p 10000:80 -itd feisky/nginx\n$ docker run --name phpfpm -itd --network container:nginx feisky/php-fpm\n```\n\n然后，在第二个终端使用 curl 访问 http://[VM1 的 IP]:10000，确认 Nginx 已正常启动。你应该可以看到 It works! 的响应。\n\n```ruby\n# 192.168.0.10 是第一台虚拟机的 IP 地址\n$ curl http://192.168.0.10:10000/\nIt works!\n```\n\n接着，我们来测试一下这个 Nginx 服务的性能。在第二个终端运行下面的 ab 命令：\n\n```ruby\n# 并发 10 个请求测试 Nginx 性能，总共测试 100 个请求\n$ ab -c 10 -n 100 http://192.168.0.10:10000/\nThis is ApacheBench, Version 2.3 \u003c$Revision: 1706008 $\u003e\nCopyright 1996 Adam Twiss, Zeus Technology Ltd, \n...\nRequests per second:    11.63 [#/sec] (mean)\nTime per request:       859.942 [ms] (mean)\n...\n```\n\n从 ab 的输出结果我们可以看到，Nginx 能承受的每秒平均请求数只有 11.63。你一定在吐槽，这也太差了吧。那到底是哪里出了问题呢？我们用 top 和 pidstat 再来观察下。\n\n这次，我们在第二个终端，将测试的请求总数增加到 10000。这样当你在第一个终端使用性能分析工具时， Nginx 的压力还是继续。\n\n继续在第二个终端，运行 ab 命令：\n\n```ruby\n$ ab -c 10 -n 10000 http://10.240.0.5:10000/\n```\n\n接着，回到第一个终端运行 top 命令，并按下数字 1 ，切换到每个 CPU 的使用率：\n\n```shell\n$ top\n...\n%Cpu0  : 98.7 us,  1.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu1  : 99.3 us,  0.7 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n...\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n21514 daemon    20   0  336696  16384   8712 R  41.9  0.2   0:06.00 php-fpm\n21513 daemon    20   0  336696  13244   5572 R  40.2  0.2   0:06.08 php-fpm\n21515 daemon    20   0  336696  16384   8712 R  40.2  0.2   0:05.67 php-fpm\n21512 daemon    20   0  336696  13244   5572 R  39.9  0.2   0:05.87 php-fpm\n21516 daemon    20   0  336696  16384   8712 R  35.9  0.2   0:05.61 php-fpm\n```\n\n这里可以看到，系统中有几个 php-fpm 进程的 CPU 使用率加起来接近 200%；而每个 CPU 的用户使用率（us）也已经超过了 98%，接近饱和。这样，我们就可以确认，正是用户空间的 php-fpm 进程，导致 CPU 使用率骤升。\n\n那再往下走，怎么知道是 php-fpm 的哪个函数导致了 CPU 使用率升高呢？我们来用 perf 分析一下。在第一个终端运行下面的 perf 命令：\n\n```ruby\n# -g 开启调用关系分析，-p 指定 php-fpm 的进程号 21515\n$ perf top -g -p 21515\n```\n\n按方向键切换到 php-fpm，再按下回车键展开 php-fpm 的调用关系，你会发现，调用关系最终到了 sqrt 和 add_function。看来，我们需要从这两个函数入手了。\n\n![img](6e58d2f7b1ace94501b1833bab16f210.png)\n\n我们拷贝出 [Nginx 应用的源码](https://github.com/feiskyer/linux-perf-examples/blob/master/nginx-high-cpu/app/index.php)，看看是不是调用了这两个函数：\n\n```bash\n# 从容器 phpfpm 中将 PHP 源码拷贝出来\n$ docker cp phpfpm:/app . \n# 使用 grep 查找函数调用\n$ grep sqrt -r app/ # 找到了 sqrt 调用\napp/index.php:  $x += sqrt($x);\n$ grep add_function -r app/ # 没找到 add_function 调用，这其实是 PHP 内置函数\n```\n\nOK，原来只有 sqrt 函数在 app/index.php 文件中调用了。那最后一步，我们就该看看这个文件的源码了：\n\n```php\n$ cat app/index.php\n\u003c?php\n// test only.\n$x = 0.0001;\nfor ($i = 0; $i \u003c= 1000000; $i++) {\n  $x += sqrt($x);\n} \necho \"It works!\"\n```\n\n呀，有没有发现问题在哪里呢？我想你要笑话我了，居然犯了一个这么傻的错误，测试代码没删就直接发布应用了。为了方便你验证优化后的效果，我把修复后的应用也打包成了一个 Docker 镜像，你可以在第一个终端中执行下面的命令来运行它：\n\n```shell\n# 停止原来的应用\n$ docker rm -f nginx phpfpm\n# 运行优化后的应用\n$ docker run --name nginx -p 10000:80 -itd feisky/nginx:cpu-fix\n$ docker run --name phpfpm -itd --network container:nginx feisky/php-fpm:cpu-fix\n```\n\n接着，到第二个终端来验证一下修复后的效果。首先 Ctrl+C 停止之前的 ab 命令后，再运行下面的命令：\n\n```yaml\n$ ab -c 10 -n 10000 http://10.240.0.5:10000/\n...\nComplete requests:      10000\nFailed requests:        0\nTotal transferred:      1720000 bytes\nHTML transferred:       90000 bytes\nRequests per second:    2237.04 [#/sec] (mean)\nTime per request:       4.470 [ms] (mean)\nTime per request:       0.447 [ms] (mean, across all concurrent requests)\nTransfer rate:          375.75 [Kbytes/sec] received\n...\n```\n\n从这里你可以发现，现在每秒的平均请求数，已经从原来的 11 变成了 2237。\n\n你看，就是这么很傻的一个小问题，却会极大的影响性能，并且查找起来也并不容易吧。当然，找到问题后，解决方法就简单多了，删除测试代码就可以了。\n\n## 小结\n\nCPU 使用率是最直观和最常用的系统性能指标，更是我们在排查性能问题时，通常会关注的第一个指标。所以我们更要熟悉它的含义，尤其要弄清楚用户（%user）、Nice（%nice）、系统（%system） 、等待 I/O（%iowait） 、中断（%irq）以及软中断（%softirq）这几种不同 CPU 的使用率。比如说：\n\n- 用户 CPU 和 Nice CPU 高，说明用户态进程占用了较多的 CPU，所以应该着重排查进程的性能问题。\n- 系统 CPU 高，说明内核态占用了较多的 CPU，所以应该着重排查内核线程或者系统调用的性能问题。\n- I/O 等待 CPU 高，说明等待 I/O 的时间比较长，所以应该着重排查系统存储是不是出现了 I/O 问题。\n- 软中断和硬中断高，说明软中断或硬中断的处理程序占用了较多的 CPU，所以应该着重排查内核中的中断服务程序。\n\n碰到 CPU 使用率升高的问题，你可以借助 top、pidstat 等工具，确认引发 CPU 性能问题的来源；再使用 perf 等工具，排查出引起性能问题的具体函数。\n\n## 思考\n\n最后，我想邀请你一起来聊聊，你所理解的 CPU 使用率，以及在发现 CPU 使用率升高时，你又是怎么分析的呢？你可以结合今天的内容，和你自己的操作记录，来总结思路。\n\n欢迎在留言区和我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 06 案例篇：系统的 CPU 使用率很高，但为啥却找不到高 CPU 的应用？\n\n你好，我是倪朋飞。\n\n上一节我讲了 CPU 使用率是什么，并通过一个案例教你使用 top、vmstat、pidstat 等工具，排查高 CPU 使用率的进程，然后再使用 perf top 工具，定位应用内部函数的问题。不过就有人留言了，说似乎感觉高 CPU 使用率的问题，还是挺容易排查的。\n\n那是不是所有 CPU 使用率高的问题，都可以这么分析呢？我想，你的答案应该是否定的。\n\n回顾前面的内容，我们知道，系统的 CPU 使用率，不仅包括进程用户态和内核态的运行，还包括中断处理、等待 I/O 以及内核线程等。所以，**当你发现系统的 CPU 使用率很高的时候，不一定能找到相对应的高 CPU 使用率的进程**。\n\n今天，我就用一个 Nginx + PHP 的 Web 服务的案例，带你来分析这种情况。\n\n## 案例分析\n\n### 你的准备\n\n今天依旧探究系统 CPU 使用率高的情况，所以这次实验的准备工作，与上节课的准备工作基本相同，差别在于案例所用的 Docker 镜像不同。\n\n本次案例还是基于 Ubuntu 18.04，同样适用于其他的 Linux 系统。我使用的案例环境如下所示：\n\n- 机器配置：2 CPU，8GB 内存\n- 预先安装 docker、sysstat、perf、ab 等工具，如 apt install [docker.io](https://docker.io/) sysstat linux-tools-common apache2-utils\n\n前面我们讲到过，ab（apache bench）是一个常用的 HTTP 服务性能测试工具，这里同样用来模拟 Nginx 的客户端。由于 Nginx 和 PHP 的配置比较麻烦，我把它们打包成了两个 [Docker 镜像](https://github.com/feiskyer/linux-perf-examples/tree/master/nginx-short-process)，这样只需要运行两个容器，就可以得到模拟环境。\n\n注意，这个案例要用到两台虚拟机，如下图所示：\n\n![img](90c30b4f555218f77241bfe2ac27723d-1550566910582.png)\n\n你可以看到，其中一台用作 Web 服务器，来模拟性能问题；另一台用作 Web 服务器的客户端，来给 Web 服务增加压力请求。使用两台虚拟机是为了相互隔离，避免“交叉感染”。\n\n接下来，我们打开两个终端，分别 SSH 登录到两台机器上，并安装上述工具。\n\n同样注意，下面所有命令都默认以 root 用户运行，如果你是用普通用户身份登陆系统，请运行 sudo su root 命令切换到 root 用户。\n\n走到这一步，准备工作就完成了。接下来，我们正式进入操作环节。\n\n\u003e 温馨提示：案例中 PHP 应用的核心逻辑比较简单，你可能一眼就能看出问题，但实际生产环境中的源码就复杂多了。所以，我依旧建议，**操作之前别看源码**，避免先入为主，而要把它当成一个黑盒来分析。这样，你可以更好把握，怎么从系统的资源使用问题出发，分析出瓶颈所在的应用，以及瓶颈在应用中大概的位置。\n\n### 操作和分析\n\n首先，我们在第一个终端，执行下面的命令运行 Nginx 和 PHP 应用：\n\n```shell\n$ docker run --name nginx -p 10000:80 -itd feisky/nginx:sp\n$ docker run --name phpfpm -itd --network container:nginx feisky/php-fpm:sp\n```\n\n然后，在第二个终端，使用 curl 访问 http://[VM1 的 IP]:10000，确认 Nginx 已正常启动。你应该可以看到 It works! 的响应。\n\n```ruby\n# 192.168.0.10 是第一台虚拟机的 IP 地址\n$ curl http://192.168.0.10:10000/\nIt works!\n```\n\n接着，我们来测试一下这个 Nginx 服务的性能。在第二个终端运行下面的 ab 命令。要注意，与上次操作不同的是，这次我们需要并发 100 个请求测试 Nginx 性能，总共测试 1000 个请求。\n\n```yaml\n# 并发 100 个请求测试 Nginx 性能，总共测试 1000 个请求\n$ ab -c 100 -n 1000 http://192.168.0.10:10000/\nThis is ApacheBench, Version 2.3 \u003c$Revision: 1706008 $\u003e\nCopyright 1996 Adam Twiss, Zeus Technology Ltd, \n...\nRequests per second:    87.86 [#/sec] (mean)\nTime per request:       1138.229 [ms] (mean)\n...\n```\n\n从 ab 的输出结果我们可以看到，Nginx 能承受的每秒平均请求数，只有 87 多一点，是不是感觉它的性能有点差呀。那么，到底是哪里出了问题呢？我们再用 top 和 pidstat 来观察一下。\n\n这次，我们在第二个终端，将测试的并发请求数改成 5，同时把请求时长设置为 10 分钟（-t 600）。这样，当你在第一个终端使用性能分析工具时， Nginx 的压力还是继续的。\n\n继续在第二个终端运行 ab 命令：\n\n```ruby\n$ ab -c 5 -t 600 http://192.168.0.10:10000/\n```\n\n然后，我们在第一个终端运行 top 命令，观察系统的 CPU 使用情况：\n\n```yaml\n$ top\n...\n%Cpu(s): 80.8 us, 15.1 sy,  0.0 ni,  2.8 id,  0.0 wa,  0.0 hi,  1.3 si,  0.0 st\n... \n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n 6882 root      20   0    8456   5052   3884 S   2.7  0.1   0:04.78 docker-containe\n 6947 systemd+  20   0   33104   3716   2340 S   2.7  0.0   0:04.92 nginx\n 7494 daemon    20   0  336696  15012   7332 S   2.0  0.2   0:03.55 php-fpm\n 7495 daemon    20   0  336696  15160   7480 S   2.0  0.2   0:03.55 php-fpm\n10547 daemon    20   0  336696  16200   8520 S   2.0  0.2   0:03.13 php-fpm\n10155 daemon    20   0  336696  16200   8520 S   1.7  0.2   0:03.12 php-fpm\n10552 daemon    20   0  336696  16200   8520 S   1.7  0.2   0:03.12 php-fpm\n15006 root      20   0 1168608  66264  37536 S   1.0  0.8   9:39.51 dockerd\n 4323 root      20   0       0      0      0 I   0.3  0.0   0:00.87 kworker/u4:1\n...\n```\n\n观察 top 输出的进程列表可以发现，CPU 使用率最高的进程也只不过才 2.7%，看起来并不高。\n\n然而，再看系统 CPU 使用率（ %Cpu ）这一行，你会发现，系统的整体 CPU 使用率是比较高的：用户 CPU 使用率（us）已经到了 80%，系统 CPU 为 15.1%，而空闲 CPU （id）则只有 2.8%。\n\n为什么用户 CPU 使用率这么高呢？我们再重新分析一下进程列表，看看有没有可疑进程：\n\n- docker-containerd 进程是用来运行容器的，2.7% 的 CPU 使用率看起来正常；\n- Nginx 和 php-fpm 是运行 Web 服务的，它们会占用一些 CPU 也不意外，并且 2% 的 CPU 使用率也不算高；\n- 再往下看，后面的进程呢，只有 0.3% 的 CPU 使用率，看起来不太像会导致用户 CPU 使用率达到 80%。\n\n那就奇怪了，明明用户 CPU 使用率都 80% 了，可我们挨个分析了一遍进程列表，还是找不到高 CPU 使用率的进程。看来 top 是不管用了，那还有其他工具可以查看进程 CPU 使用情况吗？不知道你记不记得我们的老朋友 pidstat，它可以用来分析进程的 CPU 使用情况。\n\n接下来，我们还是在第一个终端，运行 pidstat 命令：\n\n```makefile\n# 间隔 1 秒输出一组数据（按 Ctrl+C 结束）\n$ pidstat 1\n...\n04:36:24      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command\n04:36:25        0      6882    1.00    3.00    0.00    0.00    4.00     0  docker-containe\n04:36:25      101      6947    1.00    2.00    0.00    1.00    3.00     1  nginx\n04:36:25        1     14834    1.00    1.00    0.00    1.00    2.00     0  php-fpm\n04:36:25        1     14835    1.00    1.00    0.00    1.00    2.00     0  php-fpm\n04:36:25        1     14845    0.00    2.00    0.00    2.00    2.00     1  php-fpm\n04:36:25        1     14855    0.00    1.00    0.00    1.00    1.00     1  php-fpm\n04:36:25        1     14857    1.00    2.00    0.00    1.00    3.00     0  php-fpm\n04:36:25        0     15006    0.00    1.00    0.00    0.00    1.00     0  dockerd\n04:36:25        0     15801    0.00    1.00    0.00    0.00    1.00     1  pidstat\n04:36:25        1     17084    1.00    0.00    0.00    2.00    1.00     0  stress\n04:36:25        0     31116    0.00    1.00    0.00    0.00    1.00     0  atopacctd\n...\n```\n\n观察一会儿，你是不是发现，所有进程的 CPU 使用率也都不高啊，最高的 Docker 和 Nginx 也只有 4% 和 3%，即使所有进程的 CPU 使用率都加起来，也不过是 21%，离 80% 还差得远呢！\n\n最早的时候，我碰到这种问题就完全懵了：明明用户 CPU 使用率已经高达 80%，但我却怎么都找不到是哪个进程的问题。到这里，你也可以想想，你是不是也遇到过这种情况？还能不能再做进一步的分析呢？\n\n后来我发现，会出现这种情况，很可能是因为前面的分析漏了一些关键信息。你可以先暂停一下，自己往上翻，重新操作检查一遍。或者，我们一起返回去分析 top 的输出，看看能不能有新发现。\n\n现在，我们回到第一个终端，重新运行 top 命令，并观察一会儿：\n\n```yaml\n$ top\ntop - 04:58:24 up 14 days, 15:47,  1 user,  load average: 3.39, 3.82, 2.74\nTasks: 149 total,   6 running,  93 sleeping,   0 stopped,   0 zombie\n%Cpu(s): 77.7 us, 19.3 sy,  0.0 ni,  2.0 id,  0.0 wa,  0.0 hi,  1.0 si,  0.0 st\nKiB Mem :  8169348 total,  2543916 free,   457976 used,  5167456 buff/cache\nKiB Swap:        0 total,        0 free,        0 used.  7363908 avail Mem \n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n 6947 systemd+  20   0   33104   3764   2340 S   4.0  0.0   0:32.69 nginx\n 6882 root      20   0   12108   8360   3884 S   2.0  0.1   0:31.40 docker-containe\n15465 daemon    20   0  336696  15256   7576 S   2.0  0.2   0:00.62 php-fpm\n15466 daemon    20   0  336696  15196   7516 S   2.0  0.2   0:00.62 php-fpm\n15489 daemon    20   0  336696  16200   8520 S   2.0  0.2   0:00.62 php-fpm\n 6948 systemd+  20   0   33104   3764   2340 S   1.0  0.0   0:00.95 nginx\n15006 root      20   0 1168608  65632  37536 S   1.0  0.8   9:51.09 dockerd\n15476 daemon    20   0  336696  16200   8520 S   1.0  0.2   0:00.61 php-fpm\n15477 daemon    20   0  336696  16200   8520 S   1.0  0.2   0:00.61 php-fpm\n24340 daemon    20   0    8184   1616    536 R   1.0  0.0   0:00.01 stress\n24342 daemon    20   0    8196   1580    492 R   1.0  0.0   0:00.01 stress\n24344 daemon    20   0    8188   1056    492 R   1.0  0.0   0:00.01 stress\n24347 daemon    20   0    8184   1356    540 R   1.0  0.0   0:00.01 stress\n...\n```\n\n这次从头开始看 top 的每行输出，咦？Tasks 这一行看起来有点奇怪，就绪队列中居然有 6 个 Running 状态的进程（6 running），是不是有点多呢？\n\n回想一下 ab 测试的参数，并发请求数是 5。再看进程列表里， php-fpm 的数量也是 5，再加上 Nginx，好像同时有 6 个进程也并不奇怪。但真的是这样吗？\n\n再仔细看进程列表，这次主要看 Running（R） 状态的进程。你有没有发现， Nginx 和所有的 php-fpm 都处于 Sleep（S）状态，而真正处于 Running（R）状态的，却是几个 stress 进程。这几个 stress 进程就比较奇怪了，需要我们做进一步的分析。\n\n我们还是使用 pidstat 来分析这几个进程，并且使用 -p 选项指定进程的 PID。首先，从上面 top 的结果中，找到这几个进程的 PID。比如，先随便找一个 24344，然后用 pidstat 命令看一下它的 CPU 使用情况：\n\n```perl\n$ pidstat -p 24344 \n16:14:55      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command\n```\n\n奇怪，居然没有任何输出。难道是 pidstat 命令出问题了吗？之前我说过，**在怀疑性能工具出问题前，最好还是先用其他工具交叉确认一下**。那用什么工具呢？ ps 应该是最简单易用的。我们在终端里运行下面的命令，看看 24344 进程的状态：\n\n```perl\n# 从所有进程中查找 PID 是 24344 的进程\n$ ps aux | grep 24344\nroot      9628  0.0  0.0  14856  1096 pts/0    S+   16:15   0:00 grep --color=auto 24344\n```\n\n还是没有输出。现在终于发现问题，原来这个进程已经不存在了，所以 pidstat 就没有任何输出。既然进程都没了，那性能问题应该也跟着没了吧。我们再用 top 命令确认一下：\n\n```yaml\n$ top\n...\n%Cpu(s): 80.9 us, 14.9 sy,  0.0 ni,  2.8 id,  0.0 wa,  0.0 hi,  1.3 si,  0.0 st\n... \n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n 6882 root      20   0   12108   8360   3884 S   2.7  0.1   0:45.63 docker-containe\n 6947 systemd+  20   0   33104   3764   2340 R   2.7  0.0   0:47.79 nginx\n 3865 daemon    20   0  336696  15056   7376 S   2.0  0.2   0:00.15 php-fpm\n  6779 daemon    20   0    8184   1112    556 R   0.3  0.0   0:00.01 stress\n...\n```\n\n好像又错了。结果还跟原来一样，用户 CPU 使用率还是高达 80.9%，系统 CPU 接近 15%，而空闲 CPU 只有 2.8%，Running 状态的进程有 Nginx、stress 等。\n\n可是，刚刚我们看到 stress 进程不存在了，怎么现在还在运行呢？再细看一下 top 的输出，原来，这次 stress 进程的 PID 跟前面不一样了，原来的 PID 24344 不见了，现在的是 6779。\n\n进程的 PID 在变，这说明什么呢？在我看来，要么是这些进程在不停地重启，要么就是全新的进程，这无非也就两个原因：\n\n- 第一个原因，进程在不停地崩溃重启，比如因为段错误、配置错误等等，这时，进程在退出后可能又被监控系统自动重启了。\n- 第二个原因，这些进程都是短时进程，也就是在其他应用内部通过 exec 调用的外面命令。这些命令一般都只运行很短的时间就会结束，你很难用 top 这种间隔时间比较长的工具发现（上面的案例，我们碰巧发现了）。\n\n至于 stress，我们前面提到过，它是一个常用的压力测试工具。它的 PID 在不断变化中，看起来像是被其他进程调用的短时进程。要想继续分析下去，还得找到它们的父进程。\n\n要怎么查找一个进程的父进程呢？没错，用 pstree 就可以用树状形式显示所有进程之间的关系：\n\n```lua\n$ pstree | grep stress\n        |-docker-containe-+-php-fpm-+-php-fpm---sh---stress\n        |         |-3*[php-fpm---sh---stress---stress]\n```\n\n从这里可以看到，stress 是被 php-fpm 调用的子进程，并且进程数量不止一个（这里是 3 个）。找到父进程后，我们能进入 app 的内部分析了。\n\n首先，当然应该去看看它的源码。运行下面的命令，把案例应用的源码拷贝到 app 目录，然后再执行 grep 查找是不是有代码再调用 stress 命令：\n\n```bash\n# 拷贝源码到本地\n$ docker cp phpfpm:/app . \n# grep 查找看看是不是有代码在调用 stress 命令\n$ grep stress -r app\napp/index.php:// fake I/O with stress (via write()/unlink()).\napp/index.php:$result = exec(\"/usr/local/bin/stress -t 1 -d 1 2\u003e\u00261\", $output, $status);\n```\n\n找到了，果然是 app/index.php 文件中直接调用了 stress 命令。\n\n再来看看[app/index.php](https://github.com/feiskyer/linux-perf-examples/blob/master/nginx-short-process/app/index.php)的源代码：\n\n```php\n$ cat app/index.php\n\u003c?php\n// fake I/O with stress (via write()/unlink()).\n$result = exec(\"/usr/local/bin/stress -t 1 -d 1 2\u003e\u00261\", $output, $status);\nif (isset($_GET[\"verbose\"]) \u0026\u0026 $_GET[\"verbose\"]==1 \u0026\u0026 $status != 0) {\n  echo \"Server internal error: \";\n  print_r($output);\n} else {\n  echo \"It works!\";\n}\n?\u003e\n```\n\n可以看到，源码里对每个请求都会调用一个 stress 命令，模拟 I/O 压力。从注释上看，stress 会通过 write() 和 unlink() 对 I/O 进程进行压测，看来，这应该就是系统 CPU 使用率升高的根源了。\n\n不过，stress 模拟的是 I/O 压力，而之前在 top 的输出中看到的，却一直是用户 CPU 和系统 CPU 升高，并没见到 iowait 升高。这又是怎么回事呢？stress 到底是不是 CPU 使用率升高的原因呢？\n\n我们还得继续往下走。从代码中可以看到，给请求加入 verbose=1 参数后，就可以查看 stress 的输出。你先试试看，在第二个终端运行：\n\n```yaml\n$ curl http://192.168.0.10:10000?verbose=1\nServer internal error: Array\n(\n    [0] =\u003e stress: info: [19607] dispatching hogs: 0 cpu, 0 io, 0 vm, 1 hdd\n    [1] =\u003e stress: FAIL: [19608] (563) mkstemp failed: Permission denied\n    [2] =\u003e stress: FAIL: [19607] (394) \u003c-- worker 19608 returned error 1\n    [3] =\u003e stress: WARN: [19607] (396) now reaping child worker processes\n    [4] =\u003e stress: FAIL: [19607] (400) kill error: No such process\n    [5] =\u003e stress: FAIL: [19607] (451) failed run completed in 0s\n)\n```\n\n看错误消息 mkstemp failed: Permission denied ，以及 failed run completed in 0s。原来 stress 命令并没有成功，它因为权限问题失败退出了。看来，我们发现了一个 PHP 调用外部 stress 命令的 bug：没有权限创建临时文件。\n\n从这里我们可以猜测，正是由于权限错误，大量的 stress 进程在启动时初始化失败，进而导致用户 CPU 使用率的升高。\n\n分析出问题来源，下一步是不是就要开始优化了呢？当然不是！既然只是猜测，那就需要再确认一下，这个猜测到底对不对，是不是真的有大量的 stress 进程。该用什么工具或指标呢？\n\n我们前面已经用了 top、pidstat、pstree 等工具，没有发现大量的 stress 进程。那么，还有什么其他的工具可以用吗？\n\n还记得上一期提到的 perf 吗？它可以用来分析 CPU 性能事件，用在这里就很合适。依旧在第一个终端中运行 perf record -g 命令 ，并等待一会儿（比如 15 秒）后按 Ctrl+C 退出。然后再运行 perf report 查看报告：\n\n```ruby\n# 记录性能事件，等待大约 15 秒后按 Ctrl+C 退出\n$ perf record -g \n# 查看报告\n$ perf report\n```\n\n这样，你就可以看到下图这个性能报告：\n\n![img](c99445b401301147fa41cb2b5739e833.png)\n\n你看，stress 占了所有 CPU 时钟事件的 77%，而 stress 调用调用栈中比例最高的，是随机数生成函数 random()，看来它的确就是 CPU 使用率升高的元凶了。随后的优化就很简单了，只要修复权限问题，并减少或删除 stress 的调用，就可以减轻系统的 CPU 压力。\n\n当然，实际生产环境中的问题一般都要比这个案例复杂，在你找到触发瓶颈的命令行后，却可能发现，这个外部命令的调用过程是应用核心逻辑的一部分，并不能轻易减少或者删除。\n\n这时，你就得继续排查，为什么被调用的命令，会导致 CPU 使用率升高或 I/O 升高等问题。这些复杂场景的案例，我会在后面的综合实战里详细分析。\n\n最后，在案例结束时，不要忘了清理环境，执行下面的 Docker 命令，停止案例中用到的 Nginx 进程：\n\n```shell\n$ docker rm -f nginx phpfpm\n```\n\n## execsnoop\n\n在这个案例中，我们使用了 top、pidstat、pstree 等工具分析了系统 CPU 使用率高的问题，并发现 CPU 升高是短时进程 stress 导致的，但是整个分析过程还是比较复杂的。对于这类问题，有没有更好的方法监控呢？\n\n[execsnoop](https://github.com/brendangregg/perf-tools/blob/master/execsnoop) 就是一个专为短时进程设计的工具。它通过 ftrace 实时监控进程的 exec() 行为，并输出短时进程的基本信息，包括进程 PID、父进程 PID、命令行参数以及执行的结果。\n\n比如，用 execsnoop 监控上述案例，就可以直接得到 stress 进程的父进程 PID 以及它的命令行参数，并可以发现大量的 stress 进程在不停启动：\n\n```bash\n# 按 Ctrl+C 结束\n$ execsnoop\nPCOMM            PID    PPID   RET ARGS\nsh               30394  30393    0\nstress           30396  30394    0 /usr/local/bin/stress -t 1 -d 1\nsh               30398  30393    0\nstress           30399  30398    0 /usr/local/bin/stress -t 1 -d 1\nsh               30402  30400    0\nstress           30403  30402    0 /usr/local/bin/stress -t 1 -d 1\nsh               30405  30393    0\nstress           30407  30405    0 /usr/local/bin/stress -t 1 -d 1\n...\n```\n\nexecsnoop 所用的 ftrace 是一种常用的动态追踪技术，一般用于分析 Linux 内核的运行时行为，后面课程我也会详细介绍并带你使用。\n\n## 小结\n\n碰到常规问题无法解释的 CPU 使用率情况时，首先要想到有可能是短时应用导致的问题，比如有可能是下面这两种情况。\n\n- 第一，**应用里直接调用了其他二进制程序，这些程序通常运行时间比较短，通过 top 等工具也不容易发现**。\n- 第二，**应用本身在不停地崩溃重启，而启动过程的资源初始化，很可能会占用相当多的 CPU**。\n\n对于这类进程，我们可以用 pstree 或者 execsnoop 找到它们的父进程，再从父进程所在的应用入手，排查问题的根源。\n\n## 思考\n\n最后，我想邀请你一起来聊聊，你所碰到的 CPU 性能问题。有没有哪个印象深刻的经历可以跟我分享呢？或者，在今天的案例操作中，你遇到了什么问题，又解决了哪些呢？你可以结合我的讲述，总结自己的思路。\n\n欢迎在留言区和我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 07 案例篇：系统中出现大量不可中断进程和僵尸进程怎么办？（上）\n\n你好，我是倪朋飞。\n\n上一节，我用一个 Nginx+PHP 的案例，给你讲了服务器 CPU 使用率高的分析和应对方法。这里你一定要记得，当碰到无法解释的 CPU 使用率问题时，先要检查一下是不是短时应用在捣鬼。\n\n短时应用的运行时间比较短，很难在 top 或者 ps 这类展示系统概要和进程快照的工具中发现，你需要使用记录事件的工具来配合诊断，比如 execsnoop 或者 perf top。\n\n这些思路你不用刻意去背，多练习几次，多在操作中思考，你便能灵活运用。\n\n另外，我们还讲到 CPU 使用率的类型。除了上一节提到的用户 CPU 之外，它还包括系统 CPU（比如上下文切换）、等待 I/O 的 CPU（比如等待磁盘的响应）以及中断 CPU（包括软中断和硬中断）等。\n\n我们已经在上下文切换的文章中，一起分析了系统 CPU 使用率高的问题，剩下的等待 I/O 的 CPU 使用率（以下简称为 iowait）升高，也是最常见的一个服务器性能问题。今天我们就来看一个多进程 I/O 的案例，并分析这种情况。\n\n## 进程状态\n\n当 iowait 升高时，进程很可能因为得不到硬件的响应，而长时间处于不可中断状态。从 ps 或者 top 命令的输出中，你可以发现它们都处于 D 状态，也就是不可中断状态（Uninterruptible Sleep）。既然说到了进程的状态，进程有哪些状态你还记得吗？我们先来回顾一下。\n\ntop 和 ps 是最常用的查看进程状态的工具，我们就从 top 的输出开始。下面是一个 top 命令输出的示例，S 列（也就是 Status 列）表示进程的状态。从这个示例里，你可以看到 R、D、Z、S、I 等几个状态，它们分别是什么意思呢？\n\n```yaml\n$ top\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n28961 root      20   0   43816   3148   4040 R   3.2  0.0   0:00.01 top\n  620 root      20   0   37280  33676    908 D   0.3  0.4   0:00.01 app\n    1 root      20   0  160072   9416   6752 S   0.0  0.1   0:37.64 systemd\n 1896 root      20   0       0      0      0 Z   0.0  0.0   0:00.00 devapp\n    2 root      20   0       0      0      0 S   0.0  0.0   0:00.10 kthreadd\n    4 root       0 -20       0      0      0 I   0.0  0.0   0:00.00 kworker/0:0H\n    6 root       0 -20       0      0      0 I   0.0  0.0   0:00.00 mm_percpu_wq\n    7 root      20   0       0      0      0 S   0.0  0.0   0:06.37 ksoftirqd/0\n```\n\n我们挨个来看一下：\n\n- **R** 是 Running 或 Runnable 的缩写，表示进程在 CPU 的就绪队列中，正在运行或者正在等待运行。\n- **D** 是 Disk Sleep 的缩写，也就是不可中断状态睡眠（Uninterruptible Sleep），一般表示进程正在跟硬件交互，并且交互过程不允许被其他进程或中断打断。\n- **Z** 是 Zombie 的缩写，如果你玩过“植物大战僵尸”这款游戏，应该知道它的意思。它表示僵尸进程，也就是进程实际上已经结束了，但是父进程还没有回收它的资源（比如进程的描述符、PID 等）。\n- **S** 是 Interruptible Sleep 的缩写，也就是可中断状态睡眠，表示进程因为等待某个事件而被系统挂起。当进程等待的事件发生时，它会被唤醒并进入 R 状态。\n- **I** 是 Idle 的缩写，也就是空闲状态，用在不可中断睡眠的内核线程上。前面说了，硬件交互导致的不可中断进程用 D 表示，但对某些内核线程来说，它们有可能实际上并没有任何负载，用 Idle 正是为了区分这种情况。要注意，D 状态的进程会导致平均负载升高， I 状态的进程却不会。\n\n当然了，上面的示例并没有包括进程的所有状态。除了以上 5 个状态，进程还包括下面这 2 个状态。\n\n第一个是 **T 或者 t**，也就是 Stopped 或 Traced 的缩写，表示进程处于暂停或者跟踪状态。\n\n向一个进程发送 SIGSTOP 信号，它就会因响应这个信号变成暂停状态（Stopped）；再向它发送 SIGCONT 信号，进程又会恢复运行（如果进程是终端里直接启动的，则需要你用 fg 命令，恢复到前台运行）。\n\n而当你用调试器（如 gdb）调试一个进程时，在使用断点中断进程后，进程就会变成跟踪状态，这其实也是一种特殊的暂停状态，只不过你可以用调试器来跟踪并按需要控制进程的运行。\n\n另一个是 **X**，也就是 Dead 的缩写，表示进程已经消亡，所以你不会在 top 或者 ps 命令中看到它。\n\n了解了这些，我们再回到今天的主题。先看不可中断状态，这其实是为了保证进程数据与硬件状态一致，并且正常情况下，不可中断状态在很短时间内就会结束。所以，短时的不可中断状态进程，我们一般可以忽略。\n\n但如果系统或硬件发生了故障，进程可能会在不可中断状态保持很久，甚至导致系统中出现大量不可中断进程。这时，你就得注意下，系统是不是出现了 I/O 等性能问题。\n\n再看僵尸进程，这是多进程应用很容易碰到的问题。正常情况下，当一个进程创建了子进程后，它应该通过系统调用 wait() 或者 waitpid() 等待子进程结束，回收子进程的资源；而子进程在结束时，会向它的父进程发送 SIGCHLD 信号，所以，父进程还可以注册 SIGCHLD 信号的处理函数，异步回收资源。\n\n如果父进程没这么做，或是子进程执行太快，父进程还没来得及处理子进程状态，子进程就已经提前退出，那这时的子进程就会变成僵尸进程。换句话说，父亲应该一直对儿子负责，善始善终，如果不作为或者跟不上，都会导致“问题少年”的出现。\n\n通常，僵尸进程持续的时间都比较短，在父进程回收它的资源后就会消亡；或者在父进程退出后，由 init 进程回收后也会消亡。\n\n一旦父进程没有处理子进程的终止，还一直保持运行状态，那么子进程就会一直处于僵尸状态。大量的僵尸进程会用尽 PID 进程号，导致新进程不能创建，所以这种情况一定要避免。\n\n## 案例分析\n\n接下来，我将用一个多进程应用的案例，带你分析大量不可中断状态和僵尸状态进程的问题。这个应用基于 C 开发，由于它的编译和运行步骤比较麻烦，我把它打包成了一个 [Docker 镜像](https://github.com/feiskyer/linux-perf-examples/tree/master/high-iowait-process)。这样，你只需要运行一个 Docker 容器就可以得到模拟环境。\n\n### 你的准备\n\n下面的案例仍然基于 Ubuntu 18.04，同样适用于其他的 Linux 系统。我使用的案例环境如下所示：\n\n- 机器配置：2 CPU，8GB 内存\n- 预先安装 docker、sysstat、dstat 等工具，如 apt install [docker.io](https://docker.io/) dstat sysstat\n\n这里，dstat 是一个新的性能工具，它吸收了 vmstat、iostat、ifstat 等几种工具的优点，可以同时观察系统的 CPU、磁盘 I/O、网络以及内存使用情况。\n\n接下来，我们打开一个终端，SSH 登录到机器上，并安装上述工具。\n\n注意，以下所有命令都默认以 root 用户运行，如果你用普通用户身份登陆系统，请运行 sudo su root 命令切换到 root 用户。\n\n如果安装过程有问题，你可以先上网搜索解决，实在解决不了的，记得在留言区向我提问。\n\n\u003e 温馨提示：案例应用的核心代码逻辑比较简单，你可能一眼就能看出问题，但实际生产环境中的源码就复杂多了。所以，我依旧建议，操作之前别看源码，避免先入为主，而要把它当成一个黑盒来分析，这样你可以更好地根据现象分析问题。你姑且当成你工作中的一次演练，这样效果更佳。\n\n### 操作和分析\n\n安装完成后，我们首先执行下面的命令运行案例应用：\n\n```shell\n$ docker run --privileged --name=app -itd feisky/app:iowait\n```\n\n然后，输入 ps 命令，确认案例应用已正常启动。如果一切正常，你应该可以看到如下所示的输出：\n\n```yaml\n$ ps aux | grep /app\nroot      4009  0.0  0.0   4376  1008 pts/0    Ss+  05:51   0:00 /app\nroot      4287  0.6  0.4  37280 33660 pts/0    D+   05:54   0:00 /app\nroot      4288  0.6  0.4  37280 33668 pts/0    D+   05:54   0:00 /app\n```\n\n从这个界面，我们可以发现多个 app 进程已经启动，并且它们的状态分别是 Ss+ 和 D+。其中，S 表示可中断睡眠状态，D 表示不可中断睡眠状态，我们在前面刚学过，那后面的 s 和 + 是什么意思呢？不知道也没关系，查一下 man ps 就可以。现在记住，s 表示这个进程是一个会话的领导进程，而 + 表示前台进程组。\n\n这里又出现了两个新概念，**进程组**和**会话**。它们用来管理一组相互关联的进程，意思其实很好理解。\n\n- 进程组表示一组相互关联的进程，比如每个子进程都是父进程所在组的成员；\n- 而会话是指共享同一个控制终端的一个或多个进程组。\n\n比如，我们通过 SSH 登录服务器，就会打开一个控制终端（TTY），这个控制终端就对应一个会话。而我们在终端中运行的命令以及它们的子进程，就构成了一个个的进程组，其中，在后台运行的命令，构成后台进程组；在前台运行的命令，构成前台进程组。\n\n明白了这些，我们再用 top 看一下系统的资源使用情况：\n\n```yaml\n# 按下数字 1 切换到所有 CPU 的使用情况，观察一会儿按 Ctrl+C 结束\n$ top\ntop - 05:56:23 up 17 days, 16:45,  2 users,  load average: 2.00, 1.68, 1.39\nTasks: 247 total,   1 running,  79 sleeping,   0 stopped, 115 zombie\n%Cpu0  :  0.0 us,  0.7 sy,  0.0 ni, 38.9 id, 60.5 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu1  :  0.0 us,  0.7 sy,  0.0 ni,  4.7 id, 94.6 wa,  0.0 hi,  0.0 si,  0.0 st\n... \n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n 4340 root      20   0   44676   4048   3432 R   0.3  0.0   0:00.05 top\n 4345 root      20   0   37280  33624    860 D   0.3  0.0   0:00.01 app\n 4344 root      20   0   37280  33624    860 D   0.3  0.4   0:00.01 app\n    1 root      20   0  160072   9416   6752 S   0.0  0.1   0:38.59 systemd\n...\n```\n\n从这里你能看出什么问题吗？细心一点，逐行观察，别放过任何一个地方。忘了哪行参数意思的话，也要及时返回去复习。\n\n好的，如果你已经有了答案，那就继续往下走，看看跟我找的问题是否一样。这里，我发现了四个可疑的地方。\n\n- 先看第一行的平均负载（ Load Average），过去 1 分钟、5 分钟和 15 分钟内的平均负载在依次减小，说明平均负载正在升高；而 1 分钟内的平均负载已经达到系统的 CPU 个数，说明系统很可能已经有了性能瓶颈。\n- 再看第二行的 Tasks，有 1 个正在运行的进程，但僵尸进程比较多，而且还在不停增加，说明有子进程在退出时没被清理。\n- 接下来看两个 CPU 的使用率情况，用户 CPU 和系统 CPU 都不高，但 iowait 分别是 60.5% 和 94.6%，好像有点儿不正常。\n- 最后再看每个进程的情况， CPU 使用率最高的进程只有 0.3%，看起来并不高；但有两个进程处于 D 状态，它们可能在等待 I/O，但光凭这里并不能确定是它们导致了 iowait 升高。\n\n我们把这四个问题再汇总一下，就可以得到很明确的两点：\n\n- 第一点，iowait 太高了，导致系统的平均负载升高，甚至达到了系统 CPU 的个数。\n- 第二点，僵尸进程在不断增多，说明有程序没能正确清理子进程的资源。\n\n那么，碰到这两个问题该怎么办呢？结合我们前面分析问题的思路，你先自己想想，动手试试，下节课我来继续“分解”。\n\n## 小结\n\n今天我们主要通过简单的操作，熟悉了几个必备的进程状态。用我们最熟悉的 ps 或者 top ，可以查看进程的状态，这些状态包括运行（R）、空闲（I）、不可中断睡眠（D）、可中断睡眠（S）、僵尸（Z）以及暂停（T）等。\n\n其中，不可中断状态和僵尸状态，是我们今天学习的重点。\n\n- 不可中断状态，表示进程正在跟硬件交互，为了保护进程数据和硬件的一致性，系统不允许其他进程或中断打断这个进程。进程长时间处于不可中断状态，通常表示系统有 I/O 性能问题。\n- 僵尸进程表示进程已经退出，但它的父进程还没有回收子进程占用的资源。短暂的僵尸状态我们通常不必理会，但进程长时间处于僵尸状态，就应该注意了，可能有应用程序没有正常处理子进程的退出。\n\n## 思考\n\n最后，我想请你思考一下今天的课后题，案例中发现的这两个问题，你会怎么分析呢？又应该怎么解决呢？你可以结合前面我们做过的案例分析，总结自己的思路，提出自己的问题。\n\n欢迎在留言区和我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 08 案例篇：系统中出现大量不可中断进程和僵尸进程怎么办？（下）\n\n你好，我是倪朋飞。\n\n上一节，我给你讲了 Linux 进程状态的含义，以及不可中断进程和僵尸进程产生的原因，我们先来简单复习下。\n\n使用 ps 或者 top 可以查看进程的状态，这些状态包括运行、空闲、不可中断睡眠、可中断睡眠、僵尸以及暂停等。其中，我们重点学习了不可中断状态和僵尸进程：\n\n- 不可中断状态，一般表示进程正在跟硬件交互，为了保护进程数据与硬件一致，系统不允许其他进程或中断打断该进程。\n- 僵尸进程表示进程已经退出，但它的父进程没有回收该进程所占用的资源。\n\n上一节的最后，我用一个案例展示了处于这两种状态的进程。通过分析 top 命令的输出，我们发现了两个问题：\n\n- 第一，iowait 太高了，导致系统平均负载升高，并且已经达到了系统 CPU 的个数。\n- 第二，僵尸进程在不断增多，看起来是应用程序没有正确清理子进程的资源。\n\n相信你一定认真思考过这两个问题，那么，真相到底是什么呢？接下来，我们一起顺着这两个问题继续分析，找出根源。\n\n首先，请你打开一个终端，登录到上次的机器中。然后执行下面的命令，重新运行这个案例：\n\n```shell\n# 先删除上次启动的案例\n$ docker rm -f app\n# 重新运行案例\n$ docker run --privileged --name=app -itd feisky/app:iowait\n```\n\n## iowait 分析\n\n我们先来看一下 iowait 升高的问题。\n\n我相信，一提到 iowait 升高，你首先会想要查询系统的 I/O 情况。我一般也是这种思路，那么什么工具可以查询系统的 I/O 情况呢？\n\n这里，我推荐的正是上节课要求安装的 dstat ，它的好处是，可以同时查看 CPU 和 I/O 这两种资源的使用情况，便于对比分析。\n\n那么，我们在终端中运行 dstat 命令，观察 CPU 和 I/O 的使用情况：\n\n```csharp\n# 间隔 1 秒输出 10 组数据\n$ dstat 1 10\nYou did not select any stats, using -cdngy by default.\n--total-cpu-usage-- -dsk/total- -net/total- ---paging-- ---system--\nusr sys idl wai stl| read  writ| recv  send|  in   out | int   csw\n  0   0  96   4   0|1219k  408k|   0     0 |   0     0 |  42   885\n  0   0   2  98   0|  34M    0 | 198B  790B|   0     0 |  42   138\n  0   0   0 100   0|  34M    0 |  66B  342B|   0     0 |  42   135\n  0   0  84  16   0|5633k    0 |  66B  342B|   0     0 |  52   177\n  0   3  39  58   0|  22M    0 |  66B  342B|   0     0 |  43   144\n  0   0   0 100   0|  34M    0 | 200B  450B|   0     0 |  46   147\n  0   0   2  98   0|  34M    0 |  66B  342B|   0     0 |  45   134\n  0   0   0 100   0|  34M    0 |  66B  342B|   0     0 |  39   131\n  0   0  83  17   0|5633k    0 |  66B  342B|   0     0 |  46   168\n  0   3  39  59   0|  22M    0 |  66B  342B|   0     0 |  37   134\n```\n\n从 dstat 的输出，我们可以看到，每当 iowait 升高（wai）时，磁盘的读请求（read）都会很大。这说明 iowait 的升高跟磁盘的读请求有关，很可能就是磁盘读导致的。\n\n那到底是哪个进程在读磁盘呢？不知道你还记不记得，上节在 top 里看到的不可中断状态进程，我觉得它就很可疑，我们试着来分析下。\n\n我们继续在刚才的终端中，运行 top 命令，观察 D 状态的进程：\n\n```yaml\n# 观察一会儿按 Ctrl+C 结束\n$ top\n...\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n 4340 root      20   0   44676   4048   3432 R   0.3  0.0   0:00.05 top\n 4345 root      20   0   37280  33624    860 D   0.3  0.0   0:00.01 app\n 4344 root      20   0   37280  33624    860 D   0.3  0.4   0:00.01 app\n... \n```\n\n我们从 top 的输出找到 D 状态进程的 PID，你可以发现，这个界面里有两个 D 状态的进程，PID 分别是 4344 和 4345。\n\n接着，我们查看这些进程的磁盘读写情况。对了，别忘了工具是什么。一般要查看某一个进程的资源使用情况，都可以用我们的老朋友 pidstat，不过这次记得加上 -d 参数，以便输出 I/O 使用情况。\n\n比如，以 4344 为例，我们在终端里运行下面的 pidstat 命令，并用 -p 4344 参数指定进程号：\n\n```makefile\n# -d 展示 I/O 统计数据，-p 指定进程号，间隔 1 秒输出 3 组数据\n$ pidstat -d -p 4344 1 3\n06:38:50      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command\n06:38:51        0      4344      0.00      0.00      0.00       0  app\n06:38:52        0      4344      0.00      0.00      0.00       0  app\n06:38:53        0      4344      0.00      0.00      0.00       0  app\n```\n\n在这个输出中， kB_rd 表示每秒读的 KB 数， kB_wr 表示每秒写的 KB 数，iodelay 表示 I/O 的延迟（单位是时钟周期）。它们都是 0，那就表示此时没有任何的读写，说明问题不是 4344 进程导致的。\n\n可是，用同样的方法分析进程 4345，你会发现，它也没有任何磁盘读写。\n\n那要怎么知道，到底是哪个进程在进行磁盘读写呢？我们继续使用 pidstat，但这次去掉进程号，干脆就来观察所有进程的 I/O 使用情况。\n\n在终端中运行下面的 pidstat 命令：\n\n```bash\n# 间隔 1 秒输出多组数据 (这里是 20 组)\n$ pidstat -d 1 20\n...\n06:48:46      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command\n06:48:47        0      4615      0.00      0.00      0.00       1  kworker/u4:1\n06:48:47        0      6080  32768.00      0.00      0.00     170  app\n06:48:47        0      6081  32768.00      0.00      0.00     184  app \n06:48:47      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command\n06:48:48        0      6080      0.00      0.00      0.00     110  app \n06:48:48      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command\n06:48:49        0      6081      0.00      0.00      0.00     191  app \n06:48:49      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command \n06:48:50      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command\n06:48:51        0      6082  32768.00      0.00      0.00       0  app\n06:48:51        0      6083  32768.00      0.00      0.00       0  app \n06:48:51      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command\n06:48:52        0      6082  32768.00      0.00      0.00     184  app\n06:48:52        0      6083  32768.00      0.00      0.00     175  app \n06:48:52      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command\n06:48:53        0      6083      0.00      0.00      0.00     105  app\n...\n```\n\n观察一会儿可以发现，的确是 app 进程在进行磁盘读，并且每秒读的数据有 32 MB，看来就是 app 的问题。不过，app 进程到底在执行啥 I/O 操作呢？\n\n这里，我们需要回顾一下进程用户态和内核态的区别。进程想要访问磁盘，就必须使用系统调用，所以接下来，重点就是找出 app 进程的系统调用了。\n\nstrace 正是最常用的跟踪进程系统调用的工具。所以，我们从 pidstat 的输出中拿到进程的 PID 号，比如 6082，然后在终端中运行 strace 命令，并用 -p 参数指定 PID 号：\n\n```yaml\n$ strace -p 6082\nstrace: attach: ptrace(PTRACE_SEIZE, 6082): Operation not permitted\n```\n\n这儿出现了一个奇怪的错误，strace 命令居然失败了，并且命令报出的错误是没有权限。按理来说，我们所有操作都已经是以 root 用户运行了，为什么还会没有权限呢？你也可以先想一下，碰到这种情况，你会怎么处理呢？\n\n**一般遇到这种问题时，我会先检查一下进程的状态是否正常**。比如，继续在终端中运行 ps 命令，并使用 grep 找出刚才的 6082 号进程：\n\n```yaml\n$ ps aux | grep 6082\nroot      6082  0.0  0.0      0     0 pts/0    Z+   13:43   0:00 [app] \u003cdefunct\u003e\n```\n\n果然，进程 6082 已经变成了 Z 状态，也就是僵尸进程。僵尸进程都是已经退出的进程，所以就没法儿继续分析它的系统调用。关于僵尸进程的处理方法，我们一会儿再说，现在还是继续分析 iowait 的问题。\n\n到这一步，你应该注意到了，系统 iowait 的问题还在继续，但是 top、pidstat 这类工具已经不能给出更多的信息了。这时，我们就应该求助那些基于事件记录的动态追踪工具了。\n\n你可以用 perf top 看看有没有新发现。再或者，可以像我一样，在终端中运行 perf record，持续一会儿（例如 15 秒），然后按 Ctrl+C 退出，再运行 perf report 查看报告：\n\n```ruby\n$ perf record -g\n$ perf report\n```\n\n接着，找到我们关注的 app 进程，按回车键展开调用栈，你就会得到下面这张调用关系图：\n\n![img](21e79416e946ed049317a4b4c5a576a1.png)\n\n这个图里的 swapper 是内核中的调度进程，你可以先忽略掉。\n\n我们来看其他信息，你可以发现， app 的确在通过系统调用 sys_read() 读取数据。并且从 new_sync_read 和 blkdev_direct_IO 能看出，进程正在对磁盘进行**直接读**，也就是绕过了系统缓存，每个读请求都会从磁盘直接读，这就可以解释我们观察到的 iowait 升高了。\n\n看来，罪魁祸首是 app 内部进行了磁盘的直接 I/O 啊！\n\n下面的问题就容易解决了。我们接下来应该从代码层面分析，究竟是哪里出现了直接读请求。查看源码文件 [app.c](https://github.com/feiskyer/linux-perf-examples/blob/master/high-iowait-process/app.c)，你会发现它果然使用了 O_DIRECT 选项打开磁盘，于是绕过了系统缓存，直接对磁盘进行读写。\n\n```scss\nopen(disk, O_RDONLY|O_DIRECT|O_LARGEFILE, 0755)\n```\n\n直接读写磁盘，对 I/O 敏感型应用（比如数据库系统）是很友好的，因为你可以在应用中，直接控制磁盘的读写。但在大部分情况下，我们最好还是通过系统缓存来优化磁盘 I/O，换句话说，删除 O_DIRECT 这个选项就是了。\n\n[app-fix1.c](https://github.com/feiskyer/linux-perf-examples/blob/master/high-iowait-process/app-fix1.c) 就是修改后的文件，我也打包成了一个镜像文件，运行下面的命令，你就可以启动它了：\n\n```shell\n# 首先删除原来的应用\n$ docker rm -f app\n# 运行新的应用\n$ docker run --privileged --name=app -itd feisky/app:iowait-fix1\n```\n\n最后，再用 top 检查一下：\n\n```yaml\n$ top\ntop - 14:59:32 up 19 min,  1 user,  load average: 0.15, 0.07, 0.05\nTasks: 137 total,   1 running,  72 sleeping,   0 stopped,  12 zombie\n%Cpu0  :  0.0 us,  1.7 sy,  0.0 ni, 98.0 id,  0.3 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu1  :  0.0 us,  1.3 sy,  0.0 ni, 98.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n... \n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n 3084 root      20   0       0      0      0 Z   1.3  0.0   0:00.04 app\n 3085 root      20   0       0      0      0 Z   1.3  0.0   0:00.04 app\n    1 root      20   0  159848   9120   6724 S   0.0  0.1   0:09.03 systemd\n    2 root      20   0       0      0      0 S   0.0  0.0   0:00.00 kthreadd\n    3 root      20   0       0      0      0 I   0.0  0.0   0:00.40 kworker/0:0\n...\n```\n\n你会发现， iowait 已经非常低了，只有 0.3%，说明刚才的改动已经成功修复了 iowait 高的问题，大功告成！不过，别忘了，僵尸进程还在等着你。仔细观察僵尸进程的数量，你会郁闷地发现，僵尸进程还在不断的增长中。\n\n## 僵尸进程\n\n接下来，我们就来处理僵尸进程的问题。既然僵尸进程是因为父进程没有回收子进程的资源而出现的，那么，要解决掉它们，就要找到它们的根儿，**也就是找出父进程，然后在父进程里解决。**\n\n父进程的找法我们前面讲过，最简单的就是运行 pstree 命令：\n\n```bash\n# -a 表示输出命令行选项\n# p 表 PID\n# s 表示指定进程的父进程\n$ pstree -aps 3084\nsystemd,1\n  └─dockerd,15006 -H fd://\n      └─docker-containe,15024 --config /var/run/docker/containerd/containerd.toml\n          └─docker-containe,3991 -namespace moby -workdir...\n              └─app,4009\n                  └─(app,3084)\n```\n\n运行完，你会发现 3084 号进程的父进程是 4009，也就是 app 应用。\n\n所以，我们接着查看 app 应用程序的代码，看看子进程结束的处理是否正确，比如有没有调用 wait() 或 waitpid() ，抑或是，有没有注册 SIGCHLD 信号的处理函数。\n\n现在我们查看修复 iowait 后的源码文件 [app-fix1.c](https://github.com/feiskyer/linux-perf-examples/blob/master/high-iowait-process/app-fix1.c) ，找到子进程的创建和清理的地方：\n\n```perl\nint status = 0;\n  for (;;) {\n    for (int i = 0; i \u003c 2; i++) {\n      if(fork()== 0) {\n        sub_process();\n      }\n    }\n    sleep(5);\n  } \n  while(wait(\u0026status)\u003e0);\n```\n\n循环语句本来就容易出错，你能找到这里的问题吗？这段代码虽然看起来调用了 wait() 函数等待子进程结束，但却错误地把 wait() 放到了 for 死循环的外面，也就是说，wait() 函数实际上并没被调用到，我们把它挪到 for 循环的里面就可以了。\n\n修改后的文件我放到了 [app-fix2.c](https://github.com/feiskyer/linux-perf-examples/blob/master/high-iowait-process/app-fix2.c) 中，也打包成了一个 Docker 镜像，运行下面的命令，你就可以启动它：\n\n```shell\n# 先停止产生僵尸进程的 app\n$ docker rm -f app\n# 然后启动新的 app\n$ docker run --privileged --name=app -itd feisky/app:iowait-fix2\n```\n\n启动后，再用 top 最后来检查一遍：\n\n```shell\n$ top\ntop - 15:00:44 up 20 min,  1 user,  load average: 0.05, 0.05, 0.04\nTasks: 125 total,   1 running,  72 sleeping,   0 stopped,   0 zombie\n%Cpu0  :  0.0 us,  1.7 sy,  0.0 ni, 98.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu1  :  0.0 us,  1.3 sy,  0.0 ni, 98.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n... \n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n 3198 root      20   0    4376    840    780 S   0.3  0.0   0:00.01 app\n    2 root      20   0       0      0      0 S   0.0  0.0   0:00.00 kthreadd\n    3 root      20   0       0      0      0 I   0.0  0.0   0:00.41 kworker/0:0\n...\n```\n\n好了，僵尸进程（Z 状态）没有了， iowait 也是 0，问题终于全部解决了。\n\n## 小结\n\n今天我用一个多进程的案例，带你分析系统等待 I/O 的 CPU 使用率（也就是 iowait%）升高的情况。\n\n虽然这个案例是磁盘 I/O 导致了 iowait 升高，不过， **iowait 高不一定代表 I/O 有性能瓶颈。当系统中只有 I/O 类型的进程在运行时，iowait 也会很高，但实际上，磁盘的读写远没有达到性能瓶颈的程度**。\n\n因此，碰到 iowait 升高时，需要先用 dstat、pidstat 等工具，确认是不是磁盘 I/O 的问题，然后再找是哪些进程导致了 I/O。\n\n等待 I/O 的进程一般是不可中断状态，所以用 ps 命令找到的 D 状态（即不可中断状态）的进程，多为可疑进程。但这个案例中，在 I/O 操作后，进程又变成了僵尸进程，所以不能用 strace 直接分析这个进程的系统调用。\n\n这种情况下，我们用了 perf 工具，来分析系统的 CPU 时钟事件，最终发现是直接 I/O 导致的问题。这时，再检查源码中对应位置的问题，就很轻松了。\n\n而僵尸进程的问题相对容易排查，使用 pstree 找出父进程后，去查看父进程的代码，检查 wait() / waitpid() 的调用，或是 SIGCHLD 信号处理函数的注册就行了。\n\n## 思考\n\n最后，我想邀请你一起来聊聊，你碰到过的不可中断状态进程和僵尸进程问题。你是怎么分析它们的根源？又是怎么解决的？在今天的案例操作中，你又有什么新的发现吗？你可以结合我的讲述，总结自己的思路。\n\n欢迎在留言区和我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 09 基础篇：怎么理解Linux软中断？\n\n你好，我是倪朋飞。\n\n上一期，我用一个不可中断进程的案例，带你学习了 iowait（也就是等待 I/O 的 CPU 使用率）升高时的分析方法。这里你要记住，进程的不可中断状态是系统的一种保护机制，可以保证硬件的交互过程不被意外打断。所以，短时间的不可中断状态是很正常的。\n\n但是，当进程长时间都处于不可中断状态时，你就得当心了。这时，你可以使用 dstat、pidstat 等工具，确认是不是磁盘 I/O 的问题，进而排查相关的进程和磁盘设备。关于磁盘 I/O 的性能问题，你暂且不用专门去背，我会在后续的 I/O 部分详细介绍，到时候理解了也就记住了。\n\n其实除了 iowait，软中断（softirq）CPU 使用率升高也是最常见的一种性能问题。接下来的两节课，我们就来学习软中断的内容，我还会以最常见的反向代理服务器 Nginx 的案例，带你分析这种情况。\n\n## 从“取外卖”看中断\n\n说到中断，我在前面[关于“上下文切换”的文章]，简单说过中断的含义，先来回顾一下。中断是系统用来响应硬件设备请求的一种机制，它会打断进程的正常调度和执行，然后调用内核中的中断处理程序来响应设备的请求。\n\n你可能要问了，为什么要有中断呢？我可以举个生活中的例子，让你感受一下中断的魅力。\n\n比如说你订了一份外卖，但是不确定外卖什么时候送到，也没有别的方法了解外卖的进度，但是，配送员送外卖是不等人的，到了你这儿没人取的话，就直接走人了。所以你只能苦苦等着，时不时去门口看看外卖送到没，而不能干其他事情。\n\n不过呢，如果在订外卖的时候，你就跟配送员约定好，让他送到后给你打个电话，那你就不用苦苦等待了，就可以去忙别的事情，直到电话一响，接电话、取外卖就可以了。\n\n这里的“打电话”，其实就是一个中断。没接到电话的时候，你可以做其他的事情；只有接到了电话（也就是发生中断），你才要进行另一个动作：取外卖。\n\n这个例子你就可以发现，**中断其实是一种异步的事件处理机制，可以提高系统的并发处理能力**。\n\n由于中断处理程序会打断其他进程的运行，所以，**为了减少对正常进程运行调度的影响，中断处理程序就需要尽可能快地运行**。如果中断本身要做的事情不多，那么处理起来也不会有太大问题；但如果中断要处理的事情很多，中断服务程序就有可能要运行很长时间。\n\n特别是，中断处理程序在响应中断时，还会临时关闭中断。这就会导致上一次中断处理完成之前，其他中断都不能响应，也就是说中断有可能会丢失。\n\n那么还是以取外卖为例。假如你订了 2 份外卖，一份主食和一份饮料，并且是由 2 个不同的配送员来配送。这次你不用时时等待着，两份外卖都约定了电话取外卖的方式。但是，问题又来了。\n\n当第一份外卖送到时，配送员给你打了个长长的电话，商量发票的处理方式。与此同时，第二个配送员也到了，也想给你打电话。\n\n但是很明显，因为电话占线（也就是关闭了中断响应），第二个配送员的电话是打不通的。所以，第二个配送员很可能试几次后就走掉了（也就是丢失了一次中断）。\n\n## 软中断\n\n如果你弄清楚了“取外卖”的模式，那对系统的中断机制就很容易理解了。事实上，为了解决中断处理程序执行过长和中断丢失的问题，Linux 将中断处理过程分成了两个阶段，也就是**上半部和下半部**：\n\n- **上半部用来快速处理中断**，它在中断禁止模式下运行，主要处理跟硬件紧密相关的或时间敏感的工作。\n- **下半部用来延迟处理上半部未完成的工作，通常以内核线程的方式运行**。\n\n比如说前面取外卖的例子，上半部就是你接听电话，告诉配送员你已经知道了，其他事儿见面再说，然后电话就可以挂断了；下半部才是取外卖的动作，以及见面后商量发票处理的动作。\n\n这样，第一个配送员不会占用你太多时间，当第二个配送员过来时，照样能正常打通你的电话。\n\n除了取外卖，我再举个最常见的网卡接收数据包的例子，让你更好地理解。\n\n网卡接收到数据包后，会通过**硬件中断**的方式，通知内核有新的数据到了。这时，内核就应该调用中断处理程序来响应它。你可以自己先想一下，这种情况下的上半部和下半部分别负责什么工作呢？\n\n对上半部来说，既然是快速处理，其实就是要把网卡的数据读到内存中，然后更新一下硬件寄存器的状态（表示数据已经读好了），最后再发送一个**软中断**信号，通知下半部做进一步的处理。\n\n而下半部被软中断信号唤醒后，需要从内存中找到网络数据，再按照网络协议栈，对数据进行逐层解析和处理，直到把它送给应用程序。\n\n所以，这两个阶段你也可以这样理解：\n\n- 上半部直接处理硬件请求，也就是我们常说的硬中断，特点是快速执行；\n- 而下半部则是由内核触发，也就是我们常说的软中断，特点是延迟执行。\n\n实际上，上半部会打断 CPU 正在执行的任务，然后立即执行中断处理程序。而下半部以内核线程的方式执行，并且每个 CPU 都对应一个软中断内核线程，名字为 “ksoftirqd/CPU 编号”，比如说， 0 号 CPU 对应的软中断内核线程的名字就是 ksoftirqd/0。\n\n不过要注意的是，软中断不只包括了刚刚所讲的硬件设备中断处理程序的下半部，一些内核自定义的事件也属于软中断，比如内核调度和 RCU 锁（Read-Copy Update 的缩写，RCU 是 Linux 内核中最常用的锁之一）等。\n\n那要怎么知道你的系统里有哪些软中断呢？\n\n## 查看软中断和内核线程\n\n不知道你还记不记得，前面提到过的 proc 文件系统。它是一种内核空间和用户空间进行通信的机制，可以用来查看内核的数据结构，或者用来动态修改内核的配置。其中：\n\n- /proc/softirqs 提供了软中断的运行情况；\n- /proc/interrupts 提供了硬中断的运行情况。\n\n运行下面的命令，查看 /proc/softirqs 文件的内容，你就可以看到各种类型软中断在不同 CPU 上的累积运行次数：\n\n```yaml\n$ cat /proc/softirqs\n                    CPU0       CPU1\n          HI:          0          0\n       TIMER:     811613    1972736\n      NET_TX:         49          7\n      NET_RX:    1136736    1506885\n       BLOCK:          0          0\n    IRQ_POLL:          0          0\n     TASKLET:     304787       3691\n       SCHED:     689718    1897539\n     HRTIMER:          0          0\n         RCU:    1330771    1354737\n```\n\n在查看 /proc/softirqs 文件内容时，你要特别注意以下这两点。\n\n第一，要注意软中断的类型，也就是这个界面中第一列的内容。从第一列你可以看到，软中断包括了 10 个类别，分别对应不同的工作类型。比如 NET_RX 表示网络接收中断，而 NET_TX 表示网络发送中断。\n\n第二，要注意同一种软中断在不同 CPU 上的分布情况，也就是同一行的内容。正常情况下，同一种中断在不同 CPU 上的累积次数应该差不多。比如这个界面中，NET_RX 在 CPU0 和 CPU1 上的中断次数基本是同一个数量级，相差不大。\n\n不过你可能发现，TASKLET 在不同 CPU 上的分布并不均匀。TASKLET 是最常用的软中断实现机制，每个 TASKLET 只运行一次就会结束 ，并且只在调用它的函数所在的 CPU 上运行。\n\n因此，使用 TASKLET 特别简便，当然也会存在一些问题，比如说由于只在一个 CPU 上运行导致的调度不均衡，再比如因为不能在多个 CPU 上并行运行带来了性能限制。\n\n另外，刚刚提到过，软中断实际上是以内核线程的方式运行的，每个 CPU 都对应一个软中断内核线程，这个软中断内核线程就叫做 ksoftirqd/CPU 编号。那要怎么查看这些线程的运行状况呢？\n\n其实用 ps 命令就可以做到，比如执行下面的指令：\n\n```perl\n$ ps aux | grep softirq\nroot         7  0.0  0.0      0     0 ?        S    Oct10   0:01 [ksoftirqd/0]\nroot        16  0.0  0.0      0     0 ?        S    Oct10   0:01 [ksoftirqd/1]\n```\n\n注意，这些线程的名字外面都有中括号，这说明 ps 无法获取它们的命令行参数（cmline）。一般来说，ps 的输出中，名字括在中括号里的，一般都是内核线程。\n\n## 小结\n\nLinux 中的中断处理程序分为上半部和下半部：\n\n- 上半部对应硬件中断，用来快速处理中断。\n- 下半部对应软中断，用来异步处理上半部未完成的工作。\n\nLinux 中的软中断包括网络收发、定时、调度、RCU 锁等各种类型，可以通过查看 /proc/softirqs 来观察软中断的运行情况。\n\n## 思考\n\n最后，我想请你一起聊聊，你是怎么理解软中断的？你有没有碰到过因为软中断出现的性能问题？你又是怎么分析它们的瓶颈的呢？你可以结合今天的内容，总结自己的思路，写下自己的问题。\n\n欢迎在留言区和我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 10 案例篇：系统的软中断CPU使用率升高，我该怎么办\n\n你好，我是倪朋飞。\n\n上一期我给你讲了软中断的基本原理，我们先来简单复习下。\n\n中断是一种异步的事件处理机制，用来提高系统的并发处理能力。中断事件发生，会触发执行中断处理程序，而中断处理程序被分为上半部和下半部这两个部分。\n\n- 上半部对应硬中断，用来快速处理中断；\n- 下半部对应软中断，用来异步处理上半部未完成的工作。\n\nLinux 中的软中断包括网络收发、定时、调度、RCU 锁等各种类型，我们可以查看 proc 文件系统中的 /proc/softirqs ，观察软中断的运行情况。\n\n在 Linux 中，每个 CPU 都对应一个软中断内核线程，名字是 ksoftirqd/CPU 编号。当软中断事件的频率过高时，内核线程也会因为 CPU 使用率过高而导致软中断处理不及时，进而引发网络收发延迟、调度缓慢等性能问题。\n\n软中断 CPU 使用率过高也是一种最常见的性能问题。今天，我就用最常见的反向代理服务器 Nginx 的案例，教你学会分析这种情况。\n\n## 案例\n\n### 你的准备\n\n接下来的案例基于 Ubuntu 18.04，也同样适用于其他的 Linux 系统。我使用的案例环境是这样的：\n\n- 机器配置：2 CPU、8 GB 内存。\n- 预先安装 docker、sysstat、sar 、hping3、tcpdump 等工具，比如 apt-get install [docker.io](https://docker.io/) sysstat hping3 tcpdump。\n\n这里我又用到了三个新工具，sar、 hping3 和 tcpdump，先简单介绍一下：\n\n- sar 是一个系统活动报告工具，既可以实时查看系统的当前活动，又可以配置保存和报告历史统计数据。\n- hping3 是一个可以构造 TCP/IP 协议数据包的工具，可以对系统进行安全审计、防火墙测试等。\n- tcpdump 是一个常用的网络抓包工具，常用来分析各种网络问题。\n\n本次案例用到两台虚拟机，我画了一张图来表示它们的关系。\n\n![img](5f9487847e937f955ebc2ec86d490b96.png)\n\n你可以看到，其中一台虚拟机运行 Nginx ，用来模拟待分析的 Web 服务器；而另一台当作 Web 服务器的客户端，用来给 Nginx 增加压力请求。使用两台虚拟机的目的，是为了相互隔离，避免“交叉感染”。\n\n接下来，我们打开两个终端，分别 SSH 登录到两台机器上，并安装上面提到的这些工具。\n\n同以前的案例一样，下面的所有命令都默认以 root 用户运行，如果你是用普通用户身份登陆系统，请运行 sudo su root 命令切换到 root 用户。\n\n如果安装过程中有什么问题，同样鼓励你先自己搜索解决，解决不了的，可以在留言区向我提问。如果你以前已经安装过了，就可以忽略这一点了。\n\n### 操作和分析\n\n安装完成后，我们先在第一个终端，执行下面的命令运行案例，也就是一个最基本的 Nginx 应用：\n\n```ruby\n# 运行 Nginx 服务并对外开放 80 端口\n$ docker run -itd --name=nginx -p 80:80 nginx\n```\n\n然后，在第二个终端，使用 curl 访问 Nginx 监听的端口，确认 Nginx 正常启动。假设 192.168.0.30 是 Nginx 所在虚拟机的 IP 地址，运行 curl 命令后你应该会看到下面这个输出界面：\n\n```xml\n$ curl http://192.168.0.30/\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n\u003ctitle\u003eWelcome to nginx!\u003c/title\u003e\n...\n```\n\n接着，还是在第二个终端，我们运行 hping3 命令，来模拟 Nginx 的客户端请求：\n\n```ruby\n# -S 参数表示设置 TCP 协议的 SYN（同步序列号），-p 表示目的端口为 80\n# -i u100 表示每隔 100 微秒发送一个网络帧\n# 注：如果你在实践过程中现象不明显，可以尝试把 100 调小，比如调成 10 甚至 1\n$ hping3 -S -p 80 -i u100 192.168.0.30\n```\n\n现在我们再回到第一个终端，你应该发现了异常。是不是感觉系统响应明显变慢了，即便只是在终端中敲几个回车，都得很久才能得到响应？这个时候应该怎么办呢？\n\n虽然在运行 hping3 命令时，我就已经告诉你，这是一个 SYN FLOOD 攻击，你肯定也会想到从网络方面入手，来分析这个问题。不过，在实际的生产环境中，没人直接告诉你原因。\n\n所以，我希望你把 hping3 模拟 SYN FLOOD 这个操作暂时忘掉，然后重新从观察到的问题开始，分析系统的资源使用情况，逐步找出问题的根源。\n\n那么，该从什么地方入手呢？刚才我们发现，简单的 SHELL 命令都明显变慢了，先看看系统的整体资源使用情况应该是个不错的注意，比如执行下 top 看看是不是出现了 CPU 的瓶颈。我们在第一个终端运行 top 命令，看一下系统整体的资源使用情况。\n\n```yaml\n# top 运行后按数字 1 切换到显示所有 CPU\n$ top\ntop - 10:50:58 up 1 days, 22:10,  1 user,  load average: 0.00, 0.00, 0.00\nTasks: 122 total,   1 running,  71 sleeping,   0 stopped,   0 zombie\n%Cpu0  :  0.0 us,  0.0 sy,  0.0 ni, 96.7 id,  0.0 wa,  0.0 hi,  3.3 si,  0.0 st\n%Cpu1  :  0.0 us,  0.0 sy,  0.0 ni, 95.6 id,  0.0 wa,  0.0 hi,  4.4 si,  0.0 st\n... \n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n    7 root      20   0       0      0      0 S   0.3  0.0   0:01.64 ksoftirqd/0\n   16 root      20   0       0      0      0 S   0.3  0.0   0:01.97 ksoftirqd/1\n 2663 root      20   0  923480  28292  13996 S   0.3  0.3   4:58.66 docker-containe\n 3699 root      20   0       0      0      0 I   0.3  0.0   0:00.13 kworker/u4:0\n 3708 root      20   0   44572   4176   3512 R   0.3  0.1   0:00.07 top\n    1 root      20   0  225384   9136   6724 S   0.0  0.1   0:23.25 systemd\n    2 root      20   0       0      0      0 S   0.0  0.0   0:00.03 kthreadd\n...\n```\n\n这里你有没有发现异常的现象？我们从第一行开始，逐个看一下：\n\n- 平均负载全是 0，就绪队列里面只有一个进程（1 running）。\n- 每个 CPU 的使用率都挺低，最高的 CPU1 的使用率也只有 4.4%，并不算高。\n- 再看进程列表，CPU 使用率最高的进程也只有 0.3%，还是不高呀。\n\n那为什么系统的响应变慢了呢？既然每个指标的数值都不大，那我们就再来看看，这些指标对应的更具体的含义。毕竟，哪怕是同一个指标，用在系统的不同部位和场景上，都有可能对应着不同的性能问题。\n\n仔细看 top 的输出，两个 CPU 的使用率虽然分别只有 3.3% 和 4.4%，但都用在了软中断上；而从进程列表上也可以看到，CPU 使用率最高的也是软中断进程 ksoftirqd。看起来，软中断有点可疑了。\n\n根据上一期的内容，既然软中断可能有问题，那你先要知道，究竟是哪类软中断的问题。停下来想想，上一节我们用了什么方法，来判断软中断类型呢？没错，还是 proc 文件系统。观察 /proc/softirqs 文件的内容，你就能知道各种软中断类型的次数。\n\n不过，这里的各类软中断次数，又是什么时间段里的次数呢？它是系统运行以来的**累积中断次数**。所以我们直接查看文件内容，得到的只是累积中断次数，对这里的问题并没有直接参考意义。因为，这些**中断次数的变化速率**才是我们需要关注的。\n\n那什么工具可以观察命令输出的变化情况呢？我想你应该想起来了，在前面案例中用过的 watch 命令，就可以定期运行一个命令来查看输出；如果再加上 -d 参数，还可以高亮出变化的部分，从高亮部分我们就可以直观看出，哪些内容变化得更快。\n\n比如，还是在第一个终端，我们运行下面的命令：\n\n```yaml\n$ watch -d cat /proc/softirqs\n                    CPU0       CPU1\n          HI:          0          0\n       TIMER:    1083906    2368646\n      NET_TX:         53          9\n      NET_RX:    1550643    1916776\n       BLOCK:          0          0\n    IRQ_POLL:          0          0\n     TASKLET:     333637       3930\n       SCHED:     963675    2293171\n     HRTIMER:          0          0\n         RCU:    1542111    1590625\n```\n\n通过 /proc/softirqs 文件内容的变化情况，你可以发现， TIMER（定时中断）、NET_RX（网络接收）、SCHED（内核调度）、RCU（RCU 锁）等这几个软中断都在不停变化。\n\n其中，NET_RX，也就是网络数据包接收软中断的变化速率最快。而其他几种类型的软中断，是保证 Linux 调度、时钟和临界区保护这些正常工作所必需的，所以它们有一定的变化倒是正常的。\n\n那么接下来，我们就从网络接收的软中断着手，继续分析。既然是网络接收的软中断，第一步应该就是观察系统的网络接收情况。这里你可能想起了很多网络工具，不过，我推荐今天的主人公工具 sar 。\n\nsar 可以用来查看系统的网络收发情况，还有一个好处是，不仅可以观察网络收发的吞吐量（BPS，每秒收发的字节数），还可以观察网络收发的 PPS，即每秒收发的网络帧数。\n\n我们在第一个终端中运行 sar 命令，并添加 -n DEV 参数显示网络收发的报告：\n\n```bash\n# -n DEV 表示显示网络收发的报告，间隔 1 秒输出一组数据\n$ sar -n DEV 1\n15:03:46        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil\n15:03:47         eth0  12607.00   6304.00    664.86    358.11      0.00      0.00      0.00      0.01\n15:03:47      docker0   6302.00  12604.00    270.79    664.66      0.00      0.00      0.00      0.00\n15:03:47           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n15:03:47    veth9f6bbcd   6302.00  12604.00    356.95    664.66      0.00      0.00      0.00      0.05\n```\n\n对于 sar 的输出界面，我先来简单介绍一下，从左往右依次是：\n\n- 第一列：表示报告的时间。\n- 第二列：IFACE 表示网卡。\n- 第三、四列：rxpck/s 和 txpck/s 分别表示每秒接收、发送的网络帧数，也就是 PPS。\n- 第五、六列：rxkB/s 和 txkB/s 分别表示每秒接收、发送的千字节数，也就是 BPS。\n- 后面的其他参数基本接近 0，显然跟今天的问题没有直接关系，你可以先忽略掉。\n\n我们具体来看输出的内容，你可以发现：\n\n- 对网卡 eth0 来说，每秒接收的网络帧数比较大，达到了 12607，而发送的网络帧数则比较小，只有 6304；每秒接收的千字节数只有 664 KB，而发送的千字节数更小，只有 358 KB。\n- docker0 和 veth9f6bbcd 的数据跟 eth0 基本一致，只是发送和接收相反，发送的数据较大而接收的数据较小。这是 Linux 内部网桥转发导致的，你暂且不用深究，只要知道这是系统把 eth0 收到的包转发给 Nginx 服务即可。具体工作原理，我会在后面的网络部分详细介绍。\n\n从这些数据，你有没有发现什么异常的地方？\n\n既然怀疑是网络接收中断的问题，我们还是重点来看 eth0 ：接收的 PPS 比较大，达到 12607，而接收的 BPS 却很小，只有 664 KB。直观来看网络帧应该都是比较小的，我们稍微计算一下，664*1024/12607 = 54 字节，说明平均每个网络帧只有 54 字节，这显然是很小的网络帧，也就是我们通常所说的小包问题。\n\n那么，有没有办法知道这是一个什么样的网络帧，以及从哪里发过来的呢？\n\n使用 tcpdump 抓取 eth0 上的包就可以了。我们事先已经知道， Nginx 监听在 80 端口，它所提供的 HTTP 服务是基于 TCP 协议的，所以我们可以指定 TCP 协议和 80 端口精确抓包。\n\n接下来，我们在第一个终端中运行 tcpdump 命令，通过 -i eth0 选项指定网卡 eth0，并通过 tcp port 80 选项指定 TCP 协议的 80 端口：\n\n```bash\n# -i eth0 只抓取 eth0 网卡，-n 不解析协议名和主机名\n# tcp port 80 表示只抓取 tcp 协议并且端口号为 80 的网络帧\n$ tcpdump -i eth0 -n tcp port 80\n15:11:32.678966 IP 192.168.0.2.18238 \u003e 192.168.0.30.80: Flags [S], seq 458303614, win 512, length 0\n...\n```\n\n从 tcpdump 的输出中，你可以发现\n\n- 192.168.0.2.18238 \u003e 192.168.0.30.80 ，表示网络帧从 192.168.0.2 的 18238 端口发送到 192.168.0.30 的 80 端口，也就是从运行 hping3 机器的 18238 端口发送网络帧，目的为 Nginx 所在机器的 80 端口。\n- Flags [S] 则表示这是一个 SYN 包。\n\n再加上前面用 sar 发现的， PPS 超过 12000 的现象，现在我们可以确认，这就是从 192.168.0.2 这个地址发送过来的 SYN FLOOD 攻击。\n\n到这里，我们已经做了全套的性能诊断和分析。从系统的软中断使用率高这个现象出发，通过观察 /proc/softirqs 文件的变化情况，判断出软中断类型是网络接收中断；再通过 sar 和 tcpdump ，确认这是一个 SYN FLOOD 问题。\n\nSYN FLOOD 问题最简单的解决方法，就是从交换机或者硬件防火墙中封掉来源 IP，这样 SYN FLOOD 网络帧就不会发送到服务器中。\n\n至于 SYN FLOOD 的原理和更多解决思路，你暂时不需要过多关注，后面的网络章节里我们都会学到。\n\n案例结束后，也不要忘了收尾，记得停止最开始启动的 Nginx 服务以及 hping3 命令。\n\n在第一个终端中，运行下面的命令就可以停止 Nginx 了：\n\n```shell\n# 停止 Nginx 服务\n$ docker rm -f nginx\n```\n\n然后到第二个终端中按下 Ctrl+C 就可以停止 hping3。\n\n## 小结\n\n软中断 CPU 使用率（softirq）升高是一种很常见的性能问题。虽然软中断的类型很多，但实际生产中，我们遇到的性能瓶颈大多是网络收发类型的软中断，特别是网络接收的软中断。\n\n在碰到这类问题时，你可以借用 sar、tcpdump 等工具，做进一步分析。不要害怕网络性能，后面我会教你更多的分析方法。\n\n## 思考\n\n最后，我想请你一起来聊聊，你所碰到的软中断问题。你所碰到的软中问题是哪种类型，是不是这个案例中的小包问题？你又是怎么分析它们的来源并解决的呢？可以结合今天的案例，总结你自己的思路和感受。如果遇到过其他问题，也可以留言给我一起解决。\n\n欢迎在留言区和我讨论，也欢迎你把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 11 套路篇：如何迅速分析出系统CPU的瓶颈在哪里？\n\n你好，我是倪朋飞。\n\n前几节里，我通过几个案例，带你分析了各种常见的 CPU 性能问题。通过这些，我相信你对 CPU 的性能分析已经不再陌生和恐惧，起码有了基本的思路，也了解了不少 CPU 性能的分析工具。\n\n不过，我猜你可能也碰到了一个我曾有过的困惑： CPU 的性能指标那么多，CPU 性能分析工具也是一抓一大把，如果离开专栏，换成实际的工作场景，我又该观察什么指标、选择哪个性能工具呢？\n\n不要担心，今天我就以多年的性能优化经验，给你总结出一个“又快又准”的瓶颈定位套路，告诉你在不同场景下，指标工具怎么选，性能瓶颈怎么找。\n\n## CPU 性能指标\n\n我们先来回顾下，描述 CPU 的性能指标都有哪些。你可以自己先找张纸，凭着记忆写一写；或者打开前面的文章，自己总结一下。\n\n首先，**最容易想到的应该是 CPU 使用率**，这也是实际环境中最常见的一个性能指标。\n\nCPU 使用率描述了非空闲时间占总 CPU 时间的百分比，根据 CPU 上运行任务的不同，又被分为用户 CPU、系统 CPU、等待 I/O CPU、软中断和硬中断等。\n\n- 用户 CPU 使用率，包括用户态 CPU 使用率（user）和低优先级用户态 CPU 使用率（nice），表示 CPU 在用户态运行的时间百分比。用户 CPU 使用率高，通常说明有应用程序比较繁忙。\n- 系统 CPU 使用率，表示 CPU 在内核态运行的时间百分比（不包括中断）。系统 CPU 使用率高，说明内核比较繁忙。\n- 等待 I/O 的 CPU 使用率，通常也称为 iowait，表示等待 I/O 的时间百分比。iowait 高，通常说明系统与硬件设备的 I/O 交互时间比较长。\n- 软中断和硬中断的 CPU 使用率，分别表示内核调用软中断处理程序、硬中断处理程序的时间百分比。它们的使用率高，通常说明系统发生了大量的中断。\n- 除了上面这些，还有在虚拟化环境中会用到的窃取 CPU 使用率（steal）和客户 CPU 使用率（guest），分别表示被其他虚拟机占用的 CPU 时间百分比，和运行客户虚拟机的 CPU 时间百分比。\n\n**第二个比较容易想到的，应该是平均负载（Load Average）**，也就是系统的平均活跃进程数。它反应了系统的整体负载情况，主要包括三个数值，分别指过去 1 分钟、过去 5 分钟和过去 15 分钟的平均负载。\n\n理想情况下，平均负载等于逻辑 CPU 个数，这表示每个 CPU 都恰好被充分利用。如果平均负载大于逻辑 CPU 个数，就表示负载比较重了。\n\n**第三个，也是在专栏学习前你估计不太会注意到的，进程上下文切换**，包括：\n\n- 无法获取资源而导致的自愿上下文切换；\n- 被系统强制调度导致的非自愿上下文切换。\n\n上下文切换，本身是保证 Linux 正常运行的一项核心功能。但过多的上下文切换，会将原本运行进程的 CPU 时间，消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，缩短进程真正运行的时间，成为性能瓶颈。\n\n除了上面几种，**还有一个指标，CPU 缓存的命中率**。由于 CPU 发展的速度远快于内存的发展，CPU 的处理速度就比内存的访问速度快得多。这样，CPU 在访问内存的时候，免不了要等待内存的响应。为了协调这两者巨大的性能差距，CPU 缓存（通常是多级缓存）就出现了。\n\n![img](aa08816b60e453b52b5fae5e63549e33.png)\n\n就像上面这张图显示的，CPU 缓存的速度介于 CPU 和内存之间，缓存的是热点的内存数据。根据不断增长的热点数据，这些缓存按照大小不同分为 L1、L2、L3 等三级缓存，其中 L1 和 L2 常用在单核中， L3 则用在多核中。\n\n从 L1 到 L3，三级缓存的大小依次增大，相应的，性能依次降低（当然比内存还是好得多）。而它们的命中率，衡量的是 CPU 缓存的复用情况，命中率越高，则表示性能越好。\n\n这些指标都很有用，需要我们熟练掌握，所以我总结成了一张图，帮你分类和记忆。你可以保存打印下来，随时查看复习，也可以当成 CPU 性能分析的“指标筛选”清单。\n\n![img](1e66612e0022cd6c17847f3ab6989007.png)\n\n## 性能工具\n\n掌握了 CPU 的性能指标，我们还需要知道，怎样去获取这些指标，也就是工具的使用。\n\n你还记得前面案例都用了哪些工具吗？这里我们也一起回顾一下 CPU 性能工具。\n\n首先，平均负载的案例。我们先用 uptime， 查看了系统的平均负载；而在平均负载升高后，又用 mpstat 和 pidstat ，分别观察了每个 CPU 和每个进程 CPU 的使用情况，进而找出了导致平均负载升高的进程，也就是我们的压测工具 stress。\n\n第二个，上下文切换的案例。我们先用 vmstat ，查看了系统的上下文切换次数和中断次数；然后通过 pidstat ，观察了进程的自愿上下文切换和非自愿上下文切换情况；最后通过 pidstat ，观察了线程的上下文切换情况，找出了上下文切换次数增多的根源，也就是我们的基准测试工具 sysbench。\n\n第三个，进程 CPU 使用率升高的案例。我们先用 top ，查看了系统和进程的 CPU 使用情况，发现 CPU 使用率升高的进程是 php-fpm；再用 perf top ，观察 php-fpm 的调用链，最终找出 CPU 升高的根源，也就是库函数 sqrt() 。\n\n第四个，系统的 CPU 使用率升高的案例。我们先用 top 观察到了系统 CPU 升高，但通过 top 和 pidstat ，却找不出高 CPU 使用率的进程。于是，我们重新审视 top 的输出，又从 CPU 使用率不高但处于 Running 状态的进程入手，找出了可疑之处，最终通过 perf record 和 perf report ，发现原来是短时进程在捣鬼。\n\n另外，对于短时进程，我还介绍了一个专门的工具 execsnoop，它可以实时监控进程调用的外部命令。\n\n第五个，不可中断进程和僵尸进程的案例。我们先用 top 观察到了 iowait 升高的问题，并发现了大量的不可中断进程和僵尸进程；接着我们用 dstat 发现是这是由磁盘读导致的，于是又通过 pidstat 找出了相关的进程。但我们用 strace 查看进程系统调用却失败了，最终还是用 perf 分析进程调用链，才发现根源在于磁盘直接 I/O 。\n\n最后一个，软中断的案例。我们通过 top 观察到，系统的软中断 CPU 使用率升高；接着查看 /proc/softirqs， 找到了几种变化速率较快的软中断；然后通过 sar 命令，发现是网络小包的问题，最后再用 tcpdump ，找出网络帧的类型和来源，确定是一个 SYN FLOOD 攻击导致的。\n\n到这里，估计你已经晕了吧，原来短短几个案例，我们已经用过十几种 CPU 性能工具了，而且每种工具的适用场景还不同呢！这么多的工具要怎么区分呢？在实际的性能分析中，又该怎么选择呢？\n\n我的经验是，从两个不同的维度来理解它们，做到活学活用。\n\n## 活学活用，把性能指标和性能工具联系起来\n\n**第一个维度，从 CPU 的性能指标出发。也就是说，当你要查看某个性能指标时，要清楚知道哪些工具可以做到**。\n\n根据不同的性能指标，对提供指标的性能工具进行分类和理解。这样，在实际排查性能问题时，你就可以清楚知道，什么工具可以提供你想要的指标，而不是毫无根据地挨个尝试，撞运气。\n\n其实，我在前面的案例中已经多次用到了这个思路。比如用 top 发现了软中断 CPU 使用率高后，下一步自然就想知道具体的软中断类型。那在哪里可以观察各类软中断的运行情况呢？当然是 proc 文件系统中的 /proc/softirqs 这个文件。\n\n紧接着，比如说，我们找到的软中断类型是网络接收，那就要继续往网络接收方向思考。系统的网络接收情况是什么样的？什么工具可以查到网络接收情况呢？在我们案例中，用的正是 dstat。\n\n虽然你不需要把所有工具背下来，但如果能理解每个指标对应的工具的特性，一定更高效、更灵活地使用。这里，我把提供 CPU 性能指标的工具做成了一个表格，方便你梳理关系和理解记忆，当然，你也可以当成一个“指标工具”指南来使用。\n\n![img](596397e1d6335d2990f70427ad4b14ec.png)\n\n下面，我们再来看第二个维度。\n\n**第二个维度，从工具出发。也就是当你已经安装了某个工具后，要知道这个工具能提供哪些指标**。\n\n这在实际环境特别是生产环境中也是非常重要的，因为很多情况下，你并没有权限安装新的工具包，只能最大化地利用好系统中已经安装好的工具，这就需要你对它们有足够的了解。\n\n具体到每个工具的使用方法，一般都支持丰富的配置选项。不过不用担心，这些配置选项并不用背下来。你只要知道有哪些工具、以及这些工具的基本功能是什么就够了。真正要用到的时候， 通过 man 命令，查它们的使用手册就可以了。\n\n同样的，我也将这些常用工具汇总成了一个表格，方便你区分和理解，自然，你也可以当成一个“工具指标”指南使用，需要时查表即可。\n\n![img](b0c67a7196f5ca4cc58f14f959a364ca.png)\n\n## 如何迅速分析 CPU 的性能瓶颈\n\n我相信到这一步，你对 CPU 的性能指标已经非常熟悉，也清楚每种性能指标分别能用什么工具来获取。\n\n那是不是说，每次碰到 CPU 的性能问题，你都要把上面这些工具全跑一遍，然后再把所有的 CPU 性能指标全分析一遍呢？\n\n你估计觉得这种简单查找的方式，就像是在傻找。不过，别笑话，因为最早的时候我就是这么做的。把所有的指标都查出来再统一分析，当然是可以的，也很可能找到系统的潜在瓶颈。\n\n但是这种方法的效率真的太低了！耗时耗力不说，在庞大的指标体系面前，你一不小心可能就忽略了某个细节，导致白干一场。我就吃过好多次这样的苦。\n\n所以，在实际生产环境中，我们通常都希望尽可能**快**地定位系统的瓶颈，然后尽可能**快**地优化性能，也就是要又快又准地解决性能问题。\n\n那有没有什么方法，可以又快又准找出系统瓶颈呢？答案是肯定的。\n\n虽然 CPU 的性能指标比较多，但要知道，既然都是描述系统的 CPU 性能，它们就不会是完全孤立的，很多指标间都有一定的关联。**想弄清楚性能指标的关联性，就要通晓每种性能指标的工作原理**。这也是为什么我在介绍每个性能指标时，都要穿插讲解相关的系统原理，希望你能记住这一点。\n\n举个例子，用户 CPU 使用率高，我们应该去排查进程的用户态而不是内核态。因为用户 CPU 使用率反映的就是用户态的 CPU 使用情况，而内核态的 CPU 使用情况只会反映到系统 CPU 使用率上。\n\n你看，有这样的基本认识，我们就可以缩小排查的范围，省时省力。\n\n所以，为了**缩小排查范围，我通常会先运行几个支持指标较多的工具，如 top、vmstat 和 pidstat** 。为什么是这三个工具呢？仔细看看下面这张图，你就清楚了。\n\n![img](7a445960a4bc0a58a02e1bc75648aa17.png)\n\n这张图里，我列出了 top、vmstat 和 pidstat 分别提供的重要的 CPU 指标，并用虚线表示关联关系，对应出了性能分析下一步的方向。\n\n通过这张图你可以发现，这三个命令，几乎包含了所有重要的 CPU 性能指标，比如：\n\n- 从 top 的输出可以得到各种 CPU 使用率以及僵尸进程和平均负载等信息。\n- 从 vmstat 的输出可以得到上下文切换次数、中断次数、运行状态和不可中断状态的进程数。\n- 从 pidstat 的输出可以得到进程的用户 CPU 使用率、系统 CPU 使用率、以及自愿上下文切换和非自愿上下文切换情况。\n\n另外，这三个工具输出的很多指标是相互关联的，所以，我也用虚线表示了它们的关联关系，举几个例子你可能会更容易理解。\n\n第一个例子，pidstat 输出的进程用户 CPU 使用率升高，会导致 top 输出的用户 CPU 使用率升高。所以，当发现 top 输出的用户 CPU 使用率有问题时，可以跟 pidstat 的输出做对比，观察是否是某个进程导致的问题。\n\n而找出导致性能问题的进程后，就要用进程分析工具来分析进程的行为，比如使用 strace 分析系统调用情况，以及使用 perf 分析调用链中各级函数的执行情况。\n\n第二个例子，top 输出的平均负载升高，可以跟 vmstat 输出的运行状态和不可中断状态的进程数做对比，观察是哪种进程导致的负载升高。\n\n- 如果是不可中断进程数增多了，那么就需要做 I/O 的分析，也就是用 dstat 或 sar 等工具，进一步分析 I/O 的情况。\n- 如果是运行状态进程数增多了，那就需要回到 top 和 pidstat，找出这些处于运行状态的到底是什么进程，然后再用进程分析工具，做进一步分析。\n\n最后一个例子，当发现 top 输出的软中断 CPU 使用率升高时，可以查看 /proc/softirqs 文件中各种类型软中断的变化情况，确定到底是哪种软中断出的问题。比如，发现是网络接收中断导致的问题，那就可以继续用网络分析工具 sar 和 tcpdump 来分析。\n\n注意，我在这个图中只列出了最核心的几个性能工具，并没有列出所有。这么做，一方面是不想用大量的工具列表吓到你。在学习之初就接触所有或核心或小众的工具，不见得是好事。另一方面，是希望你能先把重心放在核心工具上，毕竟熟练掌握它们，就可以解决大多数问题。\n\n所以，你可以保存下这张图，作为 CPU 性能分析的思路图谱。从最核心的这几个工具开始，通过我提供的那些案例，自己在真实环境里实践，拿下它们。\n\n## 小结\n\n今天，我带你回忆了常见的 CPU 性能指标，梳理了常见的 CPU 性能观测工具，最后还总结了快速分析 CPU 性能问题的思路。\n\n虽然 CPU 的性能指标很多，相应的性能分析工具也很多，但熟悉了各种指标的含义之后，你就会发现它们其实都有一定的关联。顺着这个思路，掌握常用的分析套路并不难。\n\n## 思考\n\n由于篇幅限制，我在这里只举了几个最常见的案例，帮你理解 CPU 性能问题的原理和分析方法。你肯定也碰到过很多跟这些案例不同的 CPU 性能问题吧。我想请你一起来聊聊，你碰到过什么不一样的 CPU 性能问题呢？你又是怎么分析出它的瓶颈的呢？\n\n欢迎在留言区和我讨论，也欢迎你把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 12 套路篇：CPU 性能优化的几个思路\n\n你好，我是倪朋飞。\n\n上一节我们一起回顾了常见的 CPU 性能指标，梳理了核心的 CPU 性能观测工具，最后还总结了快速分析 CPU 性能问题的思路。虽然 CPU 的性能指标很多，相应的性能分析工具也很多，但理解了各种指标的含义后，你就会发现它们其实都有一定的关联。\n\n顺着这些关系往下理解，你就会发现，掌握这些常用的瓶颈分析套路，其实并不难。\n\n在找到 CPU 的性能瓶颈后，下一步要做的就是优化了，也就是找出充分利用 CPU 的方法，以便完成更多的工作。\n\n今天，我就来说说，优化 CPU 性能问题的思路和注意事项。\n\n## 性能优化方法论\n\n在我们历经千辛万苦，通过各种性能分析方法，终于找到引发性能问题的瓶颈后，是不是立刻就要开始优化了呢？别急，动手之前，你可以先看看下面这三个问题。\n\n- 首先，既然要做性能优化，那要怎么判断它是不是有效呢？特别是优化后，到底能提升多少性能呢？\n- 第二，性能问题通常不是独立的，如果有多个性能问题同时发生，你应该先优化哪一个呢？\n- 第三，提升性能的方法并不是唯一的，当有多种方法可以选择时，你会选用哪一种呢？是不是总选那个最大程度提升性能的方法就行了呢？\n\n如果你可以轻松回答这三个问题，那么二话不说就可以开始优化。\n\n比如，在前面的不可中断进程案例中，通过性能分析，我们发现是因为一个进程的**直接 I/O**，导致了 iowait 高达 90%。那是不是用“**直接 I/O 换成缓存 I/O**”的方法，就可以立即优化了呢？\n\n按照上面讲的，你可以先自己思考下那三点。如果不能确定，我们一起来看看。\n\n- 第一个问题，直接 I/O 换成缓存 I/O，可以把 iowait 从 90% 降到接近 0，性能提升很明显。\n- 第二个问题，我们没有发现其他性能问题，直接 I/O 是唯一的性能瓶颈，所以不用挑选优化对象。\n- 第三个问题，缓存 I/O 是我们目前用到的最简单的优化方法，而且这样优化并不会影响应用的功能。\n\n好的，这三个问题很容易就能回答，所以立即优化没有任何问题。\n\n但是，很多现实情况，并不像我举的例子那么简单。性能评估可能有多重指标，性能问题可能会多个同时发生，而且，优化某一个指标的性能，可能又导致其他指标性能的下降。\n\n那么，面对这种复杂的情况，我们该怎么办呢？\n\n接下来，我们就来深入分析这三个问题。\n\n### 怎么评估性能优化的效果？\n\n首先，来看第一个问题，怎么评估性能优化的效果。\n\n我们解决性能问题的目的，自然是想得到一个性能提升的效果。为了评估这个效果，我们需要对系统的性能指标进行量化，并且要分别测试出优化前、后的性能指标，用前后指标的变化来对比呈现效果。我把这个方法叫做性能评估“三步走”。\n\n1. 确定性能的量化指标。\n2. 测试优化前的性能指标。\n3. 测试优化后的性能指标。\n\n先看第一步，性能的量化指标有很多，比如 CPU 使用率、应用程序的吞吐量、客户端请求的延迟等，都可以评估性能。那我们应该选择什么指标来评估呢？\n\n我的建议是**不要局限在单一维度的指标上**，你至少要从应用程序和系统资源这两个维度，分别选择不同的指标。比如，以 Web 应用为例：\n\n- 应用程序的维度，我们可以用**吞吐量和请求延迟**来评估应用程序的性能。\n- 系统资源的维度，我们可以用 **CPU 使用率**来评估系统的 CPU 使用情况。\n\n之所以从这两个不同维度选择指标，主要是因为应用程序和系统资源这两者间相辅相成的关系。\n\n- 好的应用程序是性能优化的最终目的和结果，系统优化总是为应用程序服务的。所以，必须要使用应用程序的指标，来评估性能优化的整体效果。\n- 系统资源的使用情况是影响应用程序性能的根源。所以，需要用系统资源的指标，来观察和分析瓶颈的来源。\n\n至于接下来的两个步骤，主要是为了对比优化前后的性能，更直观地呈现效果。如果你的第一步，是从两个不同维度选择了多个指标，那么在性能测试时，你就需要获得这些指标的具体数值。\n\n还是以刚刚的 Web 应用为例，对应上面提到的几个指标，我们可以选择 ab 等工具，测试 Web 应用的并发请求数和响应延迟。而测试的同时，还可以用 vmstat、pidstat 等性能工具，观察系统和进程的 CPU 使用率。这样，我们就同时获得了应用程序和系统资源这两个维度的指标数值。\n\n不过，在进行性能测试时，有两个特别重要的地方你需要注意下。\n\n第一，要避免性能测试工具干扰应用程序的性能。通常，对 Web 应用来说，性能测试工具跟目标应用程序要在不同的机器上运行。\n\n比如，在之前的 Nginx 案例中，我每次都会强调要用两台虚拟机，其中一台运行 Nginx 服务，而另一台运行模拟客户端的工具，就是为了避免这个影响。\n\n第二，避免外部环境的变化影响性能指标的评估。这要求优化前、后的应用程序，都运行在相同配置的机器上，并且它们的外部依赖也要完全一致。\n\n比如还是拿 Nginx 来说，就可以运行在同一台机器上，并用相同参数的客户端工具来进行性能测试。\n\n### 多个性能问题同时存在，要怎么选择？\n\n再来看第二个问题，开篇词里我们就说过，系统性能总是牵一发而动全身，所以性能问题通常也不是独立存在的。那当多个性能问题同时发生的时候，应该先去优化哪一个呢？\n\n在性能测试的领域，流传很广的一个说法是“二八原则”，也就是说 80% 的问题都是由 20% 的代码导致的。只要找出这 20% 的位置，你就可以优化 80% 的性能。所以，我想表达的是，**并不是所有的性能问题都值得优化**。\n\n我的建议是，动手优化之前先动脑，先把所有这些性能问题给分析一遍，找出最重要的、可以最大程度提升性能的问题，从它开始优化。这样的好处是，不仅性能提升的收益最大，而且很可能其他问题都不用优化，就已经满足了性能要求。\n\n那关键就在于，怎么判断出哪个性能问题最重要。这其实还是我们性能分析要解决的核心问题，只不过这里要分析的对象，从原来的一个问题，变成了多个问题，思路其实还是一样的。\n\n所以，你依然可以用我前面讲过的方法挨个分析，分别找出它们的瓶颈。分析完所有问题后，再按照因果等关系，排除掉有因果关联的性能问题。最后，再对剩下的性能问题进行优化。\n\n如果剩下的问题还是好几个，你就得分别进行性能测试了。比较不同的优化效果后，选择能明显提升性能的那个问题进行修复。这个过程通常会花费较多的时间，这里，我推荐两个可以简化这个过程的方法。\n\n第一，如果发现是系统资源达到了瓶颈，比如 CPU 使用率达到了 100%，那么首先优化的一定是系统资源使用问题。完成系统资源瓶颈的优化后，我们才要考虑其他问题。\n\n第二，针对不同类型的指标，首先去优化那些由瓶颈导致的，性能指标变化幅度最大的问题。比如产生瓶颈后，用户 CPU 使用率升高了 10%，而系统 CPU 使用率却升高了 50%，这个时候就应该首先优化系统 CPU 的使用。\n\n### 有多种优化方法时，要如何选择?\n\n接着来看第三个问题，当多种方法都可用时，应该选择哪一种呢？是不是最大提升性能的方法，一定最好呢？\n\n一般情况下，我们当然想选能最大提升性能的方法，这其实也是性能优化的目标。\n\n但要注意，现实情况要考虑的因素却没那么简单。最直观来说，**性能优化并非没有成本**。性能优化通常会带来复杂度的提升，降低程序的可维护性，还可能在优化一个指标时，引发其他指标的异常。也就是说，很可能你优化了一个指标，另一个指标的性能却变差了。\n\n一个很典型的例子是我将在网络部分讲到的 DPDK（Data Plane Development Kit）。DPDK 是一种优化网络处理速度的方法，它通过绕开内核网络协议栈的方法，提升网络的处理能力。\n\n不过它有一个很典型的要求，就是要独占一个 CPU 以及一定数量的内存大页，并且总是以 100% 的 CPU 使用率运行。所以，如果你的 CPU 核数很少，就有点得不偿失了。\n\n所以，在考虑选哪个性能优化方法时，你要综合多方面的因素。切记，不要想着“一步登天”，试图一次性解决所有问题；也不要只会“拿来主义”，把其他应用的优化方法原封不动拿来用，却不经过任何思考和分析。\n\n## CPU 优化\n\n清楚了性能优化最基本的三个问题后，我们接下来从应用程序和系统的角度，分别来看看如何才能降低 CPU 使用率，提高 CPU 的并行处理能力。\n\n### 应用程序优化\n\n首先，从应用程序的角度来说，降低 CPU 使用率的最好方法当然是，排除所有不必要的工作，只保留最核心的逻辑。比如减少循环的层次、减少递归、减少动态内存分配等等。\n\n除此之外，应用程序的性能优化也包括很多种方法，我在这里列出了最常见的几种，你可以记下来。\n\n- **编译器优化**：很多编译器都会提供优化选项，适当开启它们，在编译阶段你就可以获得编译器的帮助，来提升性能。比如， gcc 就提供了优化选项 -O2，开启后会自动对应用程序的代码进行优化。\n- **算法优化**：使用复杂度更低的算法，可以显著加快处理速度。比如，在数据比较大的情况下，可以用 O(nlogn) 的排序算法（如快排、归并排序等），代替 O(n^2) 的排序算法（如冒泡、插入排序等）。\n- **异步处理**：使用异步处理，可以避免程序因为等待某个资源而一直阻塞，从而提升程序的并发处理能力。比如，把轮询替换为事件通知，就可以避免轮询耗费 CPU 的问题。\n- **多线程代替多进程**：前面讲过，相对于进程的上下文切换，线程的上下文切换并不切换进程地址空间，因此可以降低上下文切换的成本。\n- **善用缓存**：经常访问的数据或者计算过程中的步骤，可以放到内存中缓存起来，这样在下次用时就能直接从内存中获取，加快程序的处理速度。\n\n### 系统优化\n\n从系统的角度来说，优化 CPU 的运行，一方面要充分利用 CPU 缓存的本地性，加速缓存访问；另一方面，就是要控制进程的 CPU 使用情况，减少进程间的相互影响。\n\n具体来说，系统层面的 CPU 优化方法也有不少，这里我同样列举了最常见的一些方法，方便你记忆和使用。\n\n- **CPU 绑定**：把进程绑定到一个或者多个 CPU 上，可以提高 CPU 缓存的命中率，减少跨 CPU 调度带来的上下文切换问题。\n- **CPU 独占**：跟 CPU 绑定类似，进一步将 CPU 分组，并通过 CPU 亲和性机制为其分配进程。这样，这些 CPU 就由指定的进程独占，换句话说，不允许其他进程再来使用这些 CPU。\n- **优先级调整**：使用 nice 调整进程的优先级，正值调低优先级，负值调高优先级。优先级的数值含义前面我们提到过，忘了的话及时复习一下。在这里，适当降低非核心应用的优先级，增高核心应用的优先级，可以确保核心应用得到优先处理。\n- **为进程设置资源限制**：使用 Linux cgroups 来设置进程的 CPU 使用上限，可以防止由于某个应用自身的问题，而耗尽系统资源。\n- **NUMA（Non-Uniform Memory Access）优化**：支持 NUMA 的处理器会被划分为多个 node，每个 node 都有自己的本地内存空间。NUMA 优化，其实就是让 CPU 尽可能只访问本地内存。\n- **中断负载均衡**：无论是软中断还是硬中断，它们的中断处理程序都可能会耗费大量的 CPU。开启 irqbalance 服务或者配置 smp_affinity，就可以把中断处理过程自动负载均衡到多个 CPU 上。\n\n## 千万避免过早优化\n\n掌握上面这些优化方法后，我估计，很多人即使没发现性能瓶颈，也会忍不住把各种各样的优化方法带到实际的开发中。\n\n不过，我想你一定听说过高德纳的这句名言， “过早优化是万恶之源”，我也非常赞同这一点，过早优化不可取。\n\n因为，一方面，优化会带来复杂性的提升，降低可维护性；另一方面，需求不是一成不变的。针对当前情况进行的优化，很可能并不适应快速变化的新需求。这样，在新需求出现时，这些复杂的优化，反而可能阻碍新功能的开发。\n\n所以，性能优化最好是逐步完善，动态进行，不追求一步到位，而要首先保证能满足当前的性能要求。当发现性能不满足要求或者出现性能瓶颈时，再根据性能评估的结果，选择最重要的性能问题进行优化。\n\n## 总结\n\n今天，我带你梳理了常见的 CPU 性能优化思路和优化方法。发现性能问题后，不要急于动手优化，而要先找出最重要的、可以获得最大性能提升的问题，然后再从应用程序和系统两个方面入手优化。\n\n这样不仅可以获得最大的性能提升，而且很可能不需要优化其他问题，就已经满足了性能要求。\n\n但是记住，一定要忍住“把 CPU 性能优化到极致”的冲动，因为 CPU 并不是唯一的性能因素。在后续的文章中，我还会介绍更多的性能问题，比如内存、网络、I/O 甚至是架构设计的问题。\n\n如果不做全方位的分析和测试，只是单纯地把某个指标提升到极致，并不一定能带来整体的收益。\n\n## 思考\n\n由于篇幅的限制，我在这里只列举了几个最常见的 CPU 性能优化方法。除了这些，还有很多其他应用程序，或者系统资源角度的性能优化方法。我想请你一起来聊聊，你还知道哪些其他优化方法呢？\n\n欢迎在留言区跟我讨论，也欢迎你把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 13 Linux 性能优化答疑（一）\n\n你好，我是倪朋飞。\n\n专栏更新至今，四大基础模块之一的 CPU 性能篇，我们就已经学完了。很开心过半数同学还没有掉队，仍然在学习、积极实践操作，并且热情地留下了大量的留言。\n\n这些留言中，我非常高兴地看到，很多同学已经做到了活学活用，用学过的案例思路，分析出了线上应用的性能瓶颈，解决了实际工作中的性能问题。 还有同学能够反复推敲思考，指出文章中某些不当或不严谨的叙述，我也十分感谢你，同时很乐意和你探讨。\n\n此外，很多留言提出的问题也很有价值，大部分我都已经在 app 里回复，一些手机上不方便回复的或者很有价值的典型问题，我专门摘了出来，作为今天的答疑内容，集中回复。另一方面，也是为了保证所有人都能不漏掉任何一个重点。\n\n今天是性能优化答疑的第一期。为了便于你学习理解，它们并不是严格按照文章顺序排列的。每个问题，我都附上了留言区提问的截屏。如果你需要回顾内容原文，可以扫描每个问题右下方的二维码查看。\n\n## 问题 1：性能工具版本太低，导致指标不全\n\n![img](19084718d4682168fea4bb6cb27c4fba.png)\n\n这是使用 CentOS 的同学普遍碰到的问题。在文章中，我的 pidstat 输出里有一个 %wait 指标，代表进程等待 CPU 的时间百分比，这是 systat 11.5.5 版本才引入的新指标，旧版本没有这一项。而 CentOS 软件库里的 sysstat 版本刚好比这个低，所以没有这项指标。\n\n不过，你也不用担心。前面我就强调过，工具只是查找分析的手段，指标才是我们重点分析的对象。如果你的 pidstat 里没有显示，自然还有其他手段能找到这个指标。\n\n比如说，在讲解系统原理和性能工具时，我一般会介绍一些 **proc 文件系统**的知识，教你看懂 proc 文件系统提供的各项指标。之所以这么做，一方面，当然是为了让你更直观地理解系统的工作原理；另一方面，其实是想给你展示，性能工具上能看到的各项性能指标的原始数据来源。\n\n这样，在实际生产环境中，即使你很可能需要运行老版本的操作系统，还没有权限安装新的软件包，你也可以查看 proc 文件系统，获取自己想要的指标。\n\n但是，性能分析的学习，我还是建议你要用最新的性能工具来学。新工具有更全面的指标，让你更容易上手分析。这个绝对的优势，可以让你更直观地得到想要的数据，也不容易让你打退堂鼓。\n\n当然，初学时，你最好试着去理解性能工具的原理，或者熟悉了使用方法后，再回过头重新学习原理。这样，即使是在无法安装新工具的环境中，你仍然可以从 proc 文件系统或者其他地方，获得同样的指标，进行有效的分析。\n\n## 问题 2：使用 stress 命令，无法模拟 iowait 高的场景\n\n![img](34a354b22e351571e7f6a532e719fd43.png)![img](e7ffb84e4c22b08c0b2db14e2f61fdc5.jpg)\n\n使用 stress 无法模拟 iowait 升高，但是却看到了 sys 升高。这是因为案例中 的 stress -i 参数，它表示通过系统调用 sync() 来模拟 I/O 的问题，但这种方法实际上并不可靠。\n\n因为 sync() 的本意是刷新内存缓冲区的数据到磁盘中，以确保同步。如果缓冲区内本来就没多少数据，那读写到磁盘中的数据也就不多，也就没法产生 I/O 压力。\n\n这一点，在使用 SSD 磁盘的环境中尤为明显，很可能你的 iowait 总是 0，却单纯因为大量的系统调用，导致了系统 CPU 使用率 sys 升高。\n\n这种情况，我在留言中也回复过，推荐使用 stress-ng 来代替 stress。担心你没有看到留言，所以这里我再强调一遍。\n\n你可以运行下面的命令，来模拟 iowait 的问题。\n\n```shell\n# -i 的含义还是调用 sync，而—hdd 则表示读写临时文件\n$ stress-ng -i 1 --hdd 1 --timeout 600\n```\n\n## 问题 3：无法模拟出 RES 中断的问题\n\n![img](22d09f0924f7ae09a9dbcb7253b5b6be.jpg)\n\n这个问题是说，即使运行了大量的线程，也无法模拟出重调度中断 RES 升高的问题。\n\n其实我在 CPU 上下文切换的案例中已经提到，重调度中断是调度器用来分散任务到不同 CPU 的机制，也就是可以唤醒空闲状态的 CPU ，来调度新任务运行，而这通常借助**处理器间中断**（Inter-Processor Interrupts，IPI）来实现。\n\n所以，这个中断在单核（只有一个逻辑 CPU）的机器上当然就没有意义了，因为压根儿就不会发生重调度的情况。\n\n不过，正如留言所说，上下文切换的问题依然存在，所以你会看到， cs（context switch）从几百增加到十几万，同时 sysbench 线程的自愿上下文切换和非自愿上下文切换也都会大幅上升，特别是非自愿上下文切换，会上升到十几万。根据非自愿上下文的含义，我们都知道，这是过多的线程在争抢 CPU。\n\n其实这个结论也可以从另一个角度获得。比如，你可以在 pidstat 的选项中，加入 -u 和 -t 参数，输出线程的 CPU 使用情况，你会看到下面的界面：\n\n```makefile\n$ pidstat -u -t 1 \n14:24:03      UID      TGID       TID    %usr %system  %guest   %wait    %CPU   CPU  Command\n14:24:04        0         -      2472    0.99    8.91    0.00   77.23    9.90     0  |__sysbench\n14:24:04        0         -      2473    0.99    8.91    0.00   68.32    9.90     0  |__sysbench\n14:24:04        0         -      2474    0.99    7.92    0.00   75.25    8.91     0  |__sysbench\n14:24:04        0         -      2475    2.97    6.93    0.00   70.30    9.90     0  |__sysbench\n14:24:04        0         -      2476    2.97    6.93    0.00   68.32    9.90     0  |__sysbench\n...\n```\n\n从这个 pidstat 的输出界面，你可以发现，每个 stress 线程的 %wait 高达 70%，而 CPU 使用率只有不到 10%。换句话说， stress 线程大部分时间都消耗在了等待 CPU 上，这也表明，确实是过多的线程在争抢 CPU。\n\n在这里顺便提一下，留言中很常见的一个错误。有些同学会拿 pidstat 中的 %wait 跟 top 中的 iowait% （缩写为 wa）对比，其实这是没有意义的，因为它们是完全不相关的两个指标。\n\n- pidstat 中， %wait 表示进程等待 CPU 的时间百分比。\n- top 中 ，iowait% 则表示等待 I/O 的 CPU 时间百分比。\n\n回忆一下我们学过的进程状态，你应该记得，等待 CPU 的进程已经在 CPU 的就绪队列中，处于运行状态；而等待 I/O 的进程则处于不可中断状态。\n\n另外，不同版本的 sysbench 运行参数也不是完全一样的。比如，在案例 Ubuntu 18.04 中，运行 sysbench 的格式为：\n\n```lua\n$ sysbench --threads=10 --max-time=300 threads run\n```\n\n而在 Ubuntu 16.04 中，运行格式则为（感谢 Haku 留言分享的执行命令）：\n\n```shell\n$ sysbench --num-threads=10 --max-time=300 --test=threads run\n```\n\n## 问题 4：无法模拟出 I/O 性能瓶颈，以及 I/O 压力过大的问题\n\n![img](9e235aca4e92b68e84dba03881c591d8.png)\n\n这个问题可以看成是上一个问题的延伸，只是把 stress 命令换成了一个在容器中运行的 app 应用。\n\n事实上，在 I/O 瓶颈案例中，除了上面这个模拟不成功的留言，还有更多留言的内容刚好相反，说的是案例 I/O 压力过大，导致自己的机器出各种问题，甚至连系统都没响应了。\n\n之所以这样，其实还是因为每个人的机器配置不同，既包括了 CPU 和内存配置的不同，更是因为磁盘的巨大差异。比如，机械磁盘（HDD）、低端固态磁盘（SSD）与高端固态磁盘相比，性能差异可能达到数倍到数十倍。\n\n其实，我自己所用的案例机器也只是低端的 SSD，比机械磁盘稍微好一些，但跟高端固态磁盘还是比不了的。所以，相同操作下，我的机器上刚好出现 I/O 瓶颈，但换成一台使用机械磁盘的机器，可能磁盘 I/O 就被压死了（表现为使用率长时间 100%），而换上好一些的 SSD 磁盘，可能又无法产生足够的 I/O 压力。\n\n另外，由于我在案例中只查找了 /dev/xvd 和 /dev/sd 前缀的磁盘，而没有考虑到使用其他前缀磁盘（比如 /dev/nvme）的同学。如果你正好用的是其他前缀，你可能会碰到跟 Vicky 类似的问题，也就是 app 启动后又很快退出，变成 exited 状态。\n\n![img](a30211eeb41194eb9b5aa193cda25238.png)\n\n在这里，berryfl 同学提供了一个不错的建议：可以在案例中增加一个参数指定块设备，这样有需要的同学就不用自己编译和打包案例应用了。\n\n![img](f351f346cbfc2b3c35d010536b23332c.png)\n\n所以，在最新的案例中，我为 app 应用增加了三个选项。\n\n- -d 设置要读取的磁盘，默认前缀为 `/dev/sd` 或者 `/dev/xvd` 的磁盘。\n- -s 设置每次读取的数据量大小，单位为字节，默认为 67108864（也就是 64MB）。\n- -c 设置每个子进程读取的次数，默认为 20 次，也就是说，读取 20*64MB 数据后，子进程退出。\n\n你可以点击 [Github](https://github.com/feiskyer/linux-perf-examples/tree/master/high-iowait-process) 查看它的源码，使用方法我写在了这里：\n\n```shell\n$ docker run --privileged --name=app -itd feisky/app:iowait /app -d /dev/sdb -s 67108864 -c 20\n```\n\n案例运行后，你可以执行 docker logs 查看它的日志。正常情况下，你可以看到下面的输出：\n\n```csharp\n$ docker logs app\nReading data from disk /dev/sdb with buffer size 67108864 and count 20\n```\n\n## 问题 5：性能工具（如 vmstat）输出中，第一行数据跟其他行差别巨大\n\n![img](efa8186b71c474bd40924a9038016e0f.png)\n\n这个问题主要是说，在执行 vmstat 时，第一行数据跟其他行相比较，数值相差特别大。我相信不少同学都注意到了这个现象，这里我简单解释一下。\n\n首先还是要记住，我总强调的那句话，**在碰到直观上解释不了的现象时，要第一时间去查命令手册**。\n\n比如，运行 man vmstat 命令，你可以在手册中发现下面这句话：\n\n```sql\nThe first report produced gives averages since the last reboot.  Additional reports give information on a sam‐ \npling period of length delay.  The process and memory reports are instantaneous in either case. \n```\n\n也就是说，第一行数据是系统启动以来的平均值，其他行才是你在运行 vmstat 命令时，设置的间隔时间的平均值。另外，进程和内存的报告内容都是即时数值。\n\n你看，这并不是什么不得了的事故，但如果我们不清楚这一点，很可能卡住我们的思维，阻止我们进一步的分析。这里我也不得不提一下，文档的重要作用。\n\n授之以鱼，不如授之以渔。我们专栏的学习核心，一定是教会你**性能分析的原理和思路**，性能工具只是我们的路径和手段。所以，在提到各种性能工具时，我并没有详细解释每个工具的各种命令行选项的作用，一方面是因为你很容易通过文档查到这些，另一方面就是不同版本、不同系统中，个别选项的含义可能并不相同。\n\n所以，不管因为哪个因素，自己 man 一下，一定是最快速并且最准确的方式。特别是，当你发现某些工具的输出不符合常识时，一定记住，第一时间查文档弄明白。实在读不懂文档的话，再上网去搜，或者在专栏里向我提问。\n\n学习是一个“从薄到厚再变薄”的过程，我们从细节知识入手开始学习，积累到一定程度，需要整理成一个体系来记忆，这其中还要不断地对这个体系进行细节修补。有疑问、有反思才可以达到最佳的学习效果。\n\n最后，欢迎继续在留言区写下你的疑问，我会持续不断地解答。我的目的仍然不变，希望可以和你一起，把文章的知识变成你的能力，我们不仅仅在实战中演练，也要在交流中进步。\n\n# 14 Linux 性能优化答疑（二）\n\n你好，我是倪朋飞。\n\n今天是我们第二期答疑，这期答疑的主题是我们多次用到的 perf 工具，内容主要包括前面案例中， perf 使用方法的各种疑问。\n\nperf 在性能分析中非常有效，是我们每个人都需要掌握的核心工具。perf 的使用方法也很丰富，不过不用担心，目前你只要会用 perf record 和 perf report 就够了。而对于 perf 显示的调用栈中的某些内核符号，如果你不理解也没有关系，可以暂时跳过，并不影响我们的分析。\n\n同样的，为了便于你学习理解，它们并不是严格按照文章顺序排列的，如果你需要回顾内容原文，可以扫描每个问题右下方的二维码查看。\n\n## 问题 1： 使用 perf 工具时，看到的是 16 进制地址而不是函数名\n\n![img](94f67501d5be157a25a26d852b8c2869.png)\n\n这也是留言比较多的一个问题，在 CentOS 系统中，使用 perf 工具看不到函数名，只能看到一些 16 进制格式的函数地址。\n\n其实，只要你观察一下 perf 界面最下面的那一行，就会发现一个警告信息：\n\n```sql\nFailed to open /opt/bitnami/php/lib/php/extensions/opcache.so, continuing without symbols\n```\n\n这说明，perf 找不到待分析进程依赖的库。当然，实际上这个案例中有很多依赖库都找不到，只不过，perf 工具本身只在最后一行显示警告信息，所以你只能看到这一条警告。\n\n这个问题，其实也是在分析 Docker 容器应用时，我们经常碰到的一个问题，因为容器应用依赖的库都在镜像里面。\n\n针对这种情况，我总结了下面**四个解决方法**。\n\n**第一个方法，在容器外面构建相同路径的依赖库**。这种方法从原理上可行，但是我并不推荐，一方面是因为找出这些依赖库比较麻烦，更重要的是，构建这些路径，会污染容器主机的环境。\n\n**第二个方法，在容器内部运行 perf**。不过，这需要容器运行在特权模式下，但实际的应用程序往往只以普通容器的方式运行。所以，容器内部一般没有权限执行 perf 分析。\n\n比方说，如果你在普通容器内部运行 perf record ，你将会看到下面这个错误提示：\n\n```scss\n$ perf_4.9 record -a -g\nperf_event_open(..., PERF_FLAG_FD_CLOEXEC) failed with unexpected error 1 (Operation not permitted)\nperf_event_open(..., 0) failed unexpectedly with error 1 (Operation not permitted)\n```\n\n当然，其实你还可以通过配置 /proc/sys/kernel/perf_event_paranoid （比如改成 -1），来允许非特权用户执行 perf 事件分析。\n\n不过还是那句话，为了安全起见，这种方法我也不推荐。\n\n**第三个方法，指定符号路径为容器文件系统的路径**。比如对于第 05 讲的应用，你可以执行下面这个命令：\n\n```shell\n$ mkdir /tmp/foo\n$ PID=$(docker inspect --format {{.State.Pid}} phpfpm)\n$ bindfs /proc/$PID/root /tmp/foo\n$ perf report --symfs /tmp/foo \n# 使用完成后不要忘记解除绑定\n$ umount /tmp/foo/\n```\n\n不过这里要注意，bindfs 这个工具需要你额外安装。bindfs 的基本功能是实现目录绑定（类似于 mount --bind），这里需要你安装的是 1.13.10 版本（这也是它的最新发布版）。\n\n如果你安装的是旧版本，你可以到 [GitHub](https://github.com/mpartel/bindfs)上面下载源码，然后编译安装。\n\n**第四个方法，在容器外面把分析纪录保存下来，再去容器里查看结果**。这样，库和符号的路径也就都对了。\n\n比如，你可以这么做。先运行 perf record -g -p \u003c pid\u003e，执行一会儿（比如 15 秒）后，按 Ctrl+C 停止。\n\n然后，把生成的 perf.data 文件，拷贝到容器里面来分析：\n\n```shell\n$ docker cp perf.data phpfpm:/tmp \n$ docker exec -i -t phpfpm bash\n```\n\n接下来，在容器的 bash 中继续运行下面的命令，安装 perf 并使用 perf report 查看报告：\n\n```shell\n$ cd /tmp/ \n$ apt-get update \u0026\u0026 apt-get install -y linux-tools linux-perf procps\n$ perf_4.9 report\n```\n\n不过，这里也有两点需要你注意。\n\n首先是 perf 工具的版本问题。在最后一步中，我们运行的工具是容器内部安装的版本 perf_4.9，而不是普通的 perf 命令。这是因为， perf 命令实际上是一个软连接，会跟内核的版本进行匹配，但镜像里安装的 perf 版本跟虚拟机的内核版本有可能并不一致。\n\n另外，php-fpm 镜像是基于 Debian 系统的，所以安装 perf 工具的命令，跟 Ubuntu 也并不完全一样。比如， Ubuntu 上的安装方法是下面这样：\n\n```ruby\n$ apt-get install -y linux-tools-common linux-tools-generic linux-tools-$(uname -r)）\n```\n\n而在 php-fpm 容器里，你应该执行下面的命令来安装 perf：\n\n```csharp\n$ apt-get install -y linux-perf\n```\n\n当你按照前面这几种方法操作后，你就可以在容器内部看到 sqrt 的堆栈：\n\n![img](76f8d0f36210001e750b0a82026dedaf.png)\n\n事实上，抛开我们的案例来说，即使是在非容器化的应用中，你也可能会碰到这个问题。假如你的应用程序在编译时，使用 strip 删除了 ELF 二进制文件的符号表，那么你同样也只能看到函数的地址。\n\n现在的磁盘空间，其实已经足够大了。保留这些符号，虽然会导致编译后的文件变大，但对整个磁盘空间来说已经不是什么大问题。所以为了调试的方便，建议你还是把它们保留着。\n\n顺便提一下，案例中各种工具的安装方法，可以算是我们专栏学习的基本功，这一点希望你能够熟悉并掌握。还是那句话，不会安装先查文档，还是不行就上网搜索或者在文章里留言提问。\n\n在这里也要表扬一下，很多同学已经把摸索到的方法分享到了留言中。记录并分享，是一个很好的习惯。\n\n## 问题 2：如何用 perf 工具分析 Java 程序\n\n![img](1eb200e2a68da9a00b2ee009f3de94dc.png)![img](97f609aae409bd9840f606c1d9bc7e6d.jpg)\n\n这两个问题，其实是上一个 perf 问题的延伸。 像是 Java 这种通过 JVM 来运行的应用程序，运行堆栈用的都是 JVM 内置的函数和堆栈管理。所以，从系统层面你只能看到 JVM 的函数堆栈，而不能直接得到 Java 应用程序的堆栈。\n\nperf_events 实际上已经支持了 JIT，但还需要一个 /tmp/perf-PID.map 文件，来进行符号翻译。当然，开源项目 [perf-map-agent](https://github.com/jvm-profiling-tools/perf-map-agent)可以帮你生成这个符号表。\n\n此外，为了生成全部调用栈，你还需要开启 JDK 的选项 -XX:+PreserveFramePointer。因为这里涉及到大量的 Java 知识，我就不再详细展开了。如果你的应用刚好基于 Java ，那么你可以参考 NETFLIX 的技术博客 [Java in Flames](https://medium.com/netflix-techblog/java-in-flames-e763b3d32166) （链接为 https://medium.com/netflix-techblog/java-in-flames-e763b3d32166），来查看详细的使用步骤。\n\n说到这里，我也想强调一个问题，那就是学习性能优化时，不要一开始就把自己限定在具体的某个编程语言或者性能工具中，纠结于语言或工具的细节出不来。\n\n掌握整体的分析思路，才是我们首先要做的。因为，性能优化的原理和思路，在任何编程语言中都是相通的。\n\n## 问题 3：为什么 perf 的报告中，很多符号都不显示调用栈\n\n![img](4902af1ff4d710aa7cb150f44e3e3c05.png)\n\nperf report 是一个可视化展示 perf.data 的工具。在第 08 讲的案例中，我直接给出了最终结果，并没有详细介绍它的参数。估计很多同学的机器在运行时，都碰到了跟路过同学一样的问题，看到的是下面这个界面。\n\n![img](335d41ccf24d93aafe0e4d511218b6e9.png)\n\n这个界面可以清楚看到，perf report 的输出中，只有 swapper 显示了调用栈，其他所有符号都不能查看堆栈情况，包括我们案例中的 app 应用。\n\n这种情况我们以前也遇到过，当你发现性能工具的输出无法理解时，应该怎么办呢？当然还是查工具的手册。比如，你可以执行 man perf-report 命令，找到 -g 参数的说明：\n\n```vbnet\n-g, --call-graph=\u003cprint_type,threshold[,print_limit],order,sort_key[,branch],value\u003e \n           Display call chains using type, min percent threshold, print limit, call order, sort key, optional branch and value. Note that \n           ordering is not fixed so any parameter can be given in an arbitrary order. One exception is the print_limit which should be \n           preceded by threshold.  \n               print_type can be either: \n               - flat: single column, linear exposure of call chains. \n               - graph: use a graph tree, displaying absolute overhead rates. (default) \n               - fractal: like graph, but displays relative rates. Each branch of \n                        the tree is considered as a new profiled object. \n               - folded: call chains are displayed in a line, separated by semicolons \n               - none: disable call chain display.  \n               threshold is a percentage value which specifies a minimum percent to be \n               included in the output call graph.  Default is 0.5 (%).  \n               print_limit is only applied when stdio interface is used.  It's to limit \n               number of call graph entries in a single hist entry.  Note that it needs \n               to be given after threshold (but not necessarily consecutive). \n               Default is 0 (unlimited).  \n               order can be either: \n               - callee: callee based call graph. \n               - caller: inverted caller based call graph. \n               Default is 'caller' when --children is used, otherwise 'callee'.  \n               sort_key can be: \n               - function: compare on functions (default) \n               - address: compare on individual code addresses \n               - srcline: compare on source filename and line number  \n               branch can be: \n               - branch: include last branch information in callgraph when available. \n                         Usually more convenient to use --branch-history for this.  \n               value can be: \n               - percent: diplay overhead percent (default) \n               - period: display event period \n               - count: display event count\n```\n\n通过这个说明可以看到，-g 选项等同于 --call-graph，它的参数是后面那些被逗号隔开的选项，意思分别是输出类型、最小阈值、输出限制、排序方法、排序关键词、分支以及值的类型。\n\n我们可以看到，这里默认的参数是 graph,0.5,caller,function,percent，具体含义文档中都有详细讲解，这里我就不再重复了。\n\n现在再回过头来看我们的问题，堆栈显示不全，相关的参数当然就是最小阈值 threshold。通过手册中对 threshold 的说明，我们知道，当一个事件发生比例高于这个阈值时，它的调用栈才会显示出来。\n\nthreshold 的默认值为 0.5%，也就是说，事件比例超过 0.5% 时，调用栈才能被显示。再观察我们案例应用 app 的事件比例，只有 0.34%，低于 0.5%，所以看不到 app 的调用栈就很正常了。\n\n这种情况下，你只需要给 perf report 设置一个小于 0.34% 的阈值，就可以显示我们想看到的调用图了。比如执行下面的命令：\n\n```ruby\n$ perf report -g graph,0.3\n```\n\n你就可以得到下面这个新的输出界面，展开 app 后，就可以看到它的调用栈了。\n\n![img](b34f95617d13088671f4d9c2b9134693.png)\n\n## 问题 4：怎么理解 perf report 报告\n\n![img](42bf6b82da73656d6c3dad20074f57d8.png)![img](b90140f7d41790f74982d431f7e0238b.png)\n\n看到这里，我估计你也曾嘀咕过，为啥不一上来就用 perf 工具解决，还要执行那么多其他工具呢？ 这个问题其实就给出了很好的解释。\n\n在问题 4 的 perf report 界面中，你也一定注意到了， swapper 高达 99% 的比例。直觉来说，我们应该直接观察它才对，为什么没那么做呢？\n\n其实，当你清楚了 swapper 的原理后，就很容易理解我们为什么可以忽略它了。\n\n看到 swapper，你可能首先想到的是 SWAP 分区。实际上， swapper 跟 SWAP 没有任何关系，它只在系统初始化时创建 init 进程，之后，它就成了一个最低优先级的空闲任务。也就是说，当 CPU 上没有其他任务运行时，就会执行 swapper 。所以，你可以称它为“空闲任务”。\n\n回到我们的问题，在 perf report 的界面中，展开它的调用栈，你会看到， swapper 时钟事件都耗费在了 do_idle 上，也就是在执行空闲任务。\n\n![img](121dcefacba1b554accd0a90ef349fbd.png)\n\n所以，分析案例时，我们直接忽略了前面这个 99% 的符号，转而分析后面只有 0.3% 的 app。其实从这里你也能理解，为什么我们一开始不先用 perf 分析。\n\n因为在多任务系统中，次数多的事件，不一定就是性能瓶颈。所以，只观察到一个大数值，并不能说明什么问题。具体有没有瓶颈，还需要你观测多个方面的多个指标，来交叉验证。这也是我在套路篇中不断强调的一点。\n\n另外，关于 Children 和 Self 的含义，手册里其实有详细说明，还很友好地举了一个例子，来说明它们的百分比的计算方法。简单来说，\n\n- Self 是最后一列的符号（可以理解为函数）本身所占比例；\n- Children 是这个符号调用的其他符号（可以理解为子函数，包括直接和间接调用）占用的比例之和。\n\n正如同学留言问到的，很多性能工具确实会对系统性能有一定影响。就拿 perf 来说，它需要在内核中跟踪内核栈的各种事件，那么不可避免就会带来一定的性能损失。这一点，虽然对大部分应用来说，没有太大影响，但对特定的某些应用（比如那些对时钟周期特别敏感的应用），可能就是灾难了。\n\n所以，使用性能工具时，确实应该考虑工具本身对系统性能的影响。而这种情况，就需要你了解这些工具的原理。比如，\n\n- perf 这种动态追踪工具，会给系统带来一定的性能损失。\n- vmstat、pidstat 这些直接读取 proc 文件系统来获取指标的工具，不会带来性能损失。\n\n## 问题 5：性能优化书籍和参考资料推荐\n\n![img](a0be73a43e756da48bdbdd01d71598ba.png)\n\n我很高兴看到留言有这么高的学习热情，其实好多文章后面都有大量留言，希望我能推荐书籍和学习资料。这一点也是我乐意看到的。专栏学习一定不是你性能优化之旅的全部，能够带你入门、帮你解决实际问题、甚至是激发你的学习热情，已经让我非常开心。\n\n在 [如何学习 Linux 性能优化] 的文章中，我曾经介绍过 Brendan Gregg，他是当之无愧的性能优化大师，你在各种 Linux 性能优化的文章中，基本都能看到他的那张性能工具图谱。\n\n所以，关于性能优化的书籍，我最喜欢的其实正是他写的那本 《Systems Performance: Enterprise and the Cloud》。这本书也出了中文版，名字是《性能之巅：洞悉系统、企业与云计算》。\n\n从出版时间来看，这本书确实算一本老书了，英文版的是 2013 年出版的。但是经典之所以成为经典，正是因为不会过时。这本书里的性能分析思路以及很多的性能工具，到今天依然适用。\n\n另外，我也推荐你去关注他的个人网站 http://www.brendangregg.com/，特别是 [Linux Performance](http://www.brendangregg.com/linuxperf.html)这个页面，包含了很多 Linux 性能优化的资料，比如：\n\n- Linux 性能工具图谱 ；\n- 性能分析参考资料；\n- 性能优化的演讲视频 。\n\n不过，这里很多内容会涉及到大量的内核知识，对初学者来说并不友好。但是，如果你想成为高手，辛苦和坚持都是不可避免的。所以，希望你在查看这些资料时，不要一遇到不懂的就打退堂鼓。任何东西的第一遍学习有不懂的地方很正常，忍住恐惧别放弃，继续往后走，前面很多问题可能会一并解决掉，再看第二遍、第三遍就更轻松了。\n\n还是那句话，抓住主线不动摇，先从最基本的原理开始，掌握性能分析的思路，然后再逐步深入，探究细节，不要试图一口吃成个大胖子。\n\n最后，欢迎继续在留言区写下你的疑问，我会持续不断地解答。我的目的仍然不变，希望可以和你一起，把文章的知识变成你的能力，我们不仅仅在实战中演练，也要在交流中进步。\n\n# 15 基础篇：Linux内存是怎么工作的？\n\n你好，我是倪朋飞。\n\n前几节我们一起学习了 CPU 的性能原理和优化方法，接下来，我们将进入另一个板块——内存。\n\n同 CPU 管理一样，内存管理也是操作系统最核心的功能之一。内存主要用来存储系统和应用程序的指令、数据、缓存等。\n\n那么，Linux 到底是怎么管理内存的呢？今天，我就来带你一起来看看这个问题。\n\n## 内存映射\n\n说到内存，你能说出你现在用的这台计算机内存有多大吗？我估计你记得很清楚，因为这是我们购买时，首先考虑的一个重要参数，比方说，我的笔记本电脑内存就是 8GB 的 。\n\n我们通常所说的内存容量，就像我刚刚提到的 8GB，其实指的是物理内存。物理内存也称为主存，大多数计算机用的主存都是动态随机访问内存（DRAM）。只有内核才可以直接访问物理内存。那么，进程要访问内存时，该怎么办呢？\n\nLinux 内核给每个进程都提供了一个独立的虚拟地址空间，并且这个地址空间是连续的。这样，进程就可以很方便地访问内存，更确切地说是访问虚拟内存。\n\n虚拟地址空间的内部又被分为**内核空间和用户空间**两部分，不同字长（也就是单个 CPU 指令可以处理数据的最大长度）的处理器，地址空间的范围也不同。比如最常见的 32 位和 64 位系统，我画了两张图来分别表示它们的虚拟地址空间，如下所示：\n\n![img](ed8824c7a2e4020e2fdd2a104c70ab7b.png)\n\n通过这里可以看出，32 位系统的内核空间占用 1G，位于最高处，剩下的 3G 是用户空间。而 64 位系统的内核空间和用户空间都是 128T，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。\n\n还记得进程的用户态和内核态吗？进程在用户态时，只能访问用户空间内存；只有进入内核态后，才可以访问内核空间内存。虽然每个进程的地址空间都包含了内核空间，但这些内核空间，其实关联的都是相同的物理内存。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。\n\n既然每个进程都有一个这么大的地址空间，那么所有进程的虚拟内存加起来，自然要比实际的物理内存大得多。所以，并不是所有的虚拟内存都会分配物理内存，只有那些实际使用的虚拟内存才分配物理内存，并且分配后的物理内存，是通过**内存映射**来管理的。\n\n内存映射，其实就是将**虚拟内存地址**映射到**物理内存地址**。为了完成内存映射，内核为每个进程都维护了一张页表，记录虚拟地址与物理地址的映射关系，如下图所示：\n\n![img](fcfbe2f8eb7c6090d82bf93ecdc1f0b6.png)\n\n页表实际上存储在 CPU 的内存管理单元 MMU 中，这样，正常情况下，处理器就可以直接通过硬件，找出要访问的内存。\n\n而当进程访问的虚拟地址在页表中查不到时，系统会产生一个**缺页异常**，进入内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。\n\n另外，我在 [CPU 上下文切换的文章中]曾经提到， TLB（Translation Lookaside Buffer，转译后备缓冲器）会影响 CPU 的内存访问性能，在这里其实就可以得到解释。\n\nTLB 其实就是 MMU 中页表的高速缓存。由于进程的虚拟地址空间是独立的，而 TLB 的访问速度又比 MMU 快得多，所以，通过减少进程的上下文切换，减少 TLB 的刷新次数，就可以提高 TLB 缓存的使用率，进而提高 CPU 的内存访问性能。\n\n不过要注意，MMU 并不以字节为单位来管理内存，而是规定了一个内存映射的最小单位，也就是页，通常是 4 KB 大小。这样，每一次内存映射，都需要关联 4 KB 或者 4KB 整数倍的内存空间。\n\n页的大小只有 4 KB ，导致的另一个问题就是，整个页表会变得非常大。比方说，仅 32 位系统就需要 100 多万个页表项（4GB/4KB），才可以实现整个地址空间的映射。为了解决页表项过多的问题，Linux 提供了两种机制，也就是多级页表和大页（HugePage）。\n\n多级页表就是把内存分成区块来管理，将原来的映射关系改成区块索引和区块内的偏移。由于虚拟内存空间通常只用了很少一部分，那么，多级页表就只保存这些使用中的区块，这样就可以大大地减少页表的项数。\n\nLinux 用的正是四级页表来管理内存页，如下图所示，虚拟地址被分为 5 个部分，前 4 个表项用于选择页，而最后一个索引表示页内偏移。\n\n![img](b5c9179ac64eb5c7ca26448065728325.png)\n\n再看大页，顾名思义，就是比普通页更大的内存块，常见的大小有 2MB 和 1GB。大页通常用在使用大量内存的进程上，比如 Oracle、DPDK 等。\n\n通过这些机制，在页表的映射下，进程就可以通过虚拟地址来访问物理内存了。那么具体到一个 Linux 进程中，这些内存又是怎么使用的呢？\n\n## 虚拟内存空间分布\n\n首先，我们需要进一步了解虚拟内存空间的分布情况。最上方的内核空间不用多讲，下方的用户空间内存，其实又被分成了多个不同的段。以 32 位系统为例，我画了一张图来表示它们的关系。\n\n![img](71a754523386cc75f4456a5eabc93c5d.png)\n\n通过这张图你可以看到，用户空间内存，从低到高分别是五种不同的内存段。\n\n1. 只读段，包括代码和常量等。\n2. 数据段，包括全局变量等。\n3. 堆，包括动态分配的内存，从低地址开始向上增长。\n4. 文件映射段，包括动态库、共享内存等，从高地址开始向下增长。\n5. 栈，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 8 MB。\n\n在这五个内存段中，堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 malloc() 或者 mmap() ，就可以分别在堆和文件映射段动态分配内存。\n\n其实 64 位系统的内存分布也类似，只不过内存空间要大得多。那么，更重要的问题来了，内存究竟是怎么分配的呢？\n\n## 内存分配与回收\n\nmalloc() 是 C 标准库提供的内存分配函数，对应到系统调用上，有两种实现方式，即 brk() 和 mmap()。\n\n对小块内存（小于 128K），C 标准库使用 brk() 来分配，也就是通过移动堆顶的位置来分配内存。这些内存释放后并不会立刻归还系统，而是被缓存起来，这样就可以重复使用。\n\n而大块内存（大于 128K），则直接使用内存映射 mmap() 来分配，也就是在文件映射段找一块空闲内存分配出去。\n\n这两种方式，自然各有优缺点。\n\nbrk() 方式的缓存，可以减少缺页异常的发生，提高内存访问效率。不过，由于这些内存没有归还系统，在内存工作繁忙时，频繁的内存分配和释放会造成内存碎片。\n\n而 mmap() 方式分配的内存，会在释放时直接归还系统，所以每次 mmap 都会发生缺页异常。在内存工作繁忙时，频繁的内存分配会导致大量的缺页异常，使内核的管理负担增大。这也是 malloc 只对大块内存使用 mmap 的原因。\n\n了解这两种调用方式后，我们还需要清楚一点，那就是，当这两种调用发生后，其实并没有真正分配内存。这些内存，都只在首次访问时才分配，也就是通过缺页异常进入内核中，再由内核来分配内存。\n\n整体来说，Linux 使用伙伴系统来管理内存分配。前面我们提到过，这些内存在 MMU 中以页为单位进行管理，伙伴系统也一样，以页为单位来管理内存，并且会通过相邻页的合并，减少内存碎片化（比如 brk 方式造成的内存碎片）。\n\n你可能会想到一个问题，如果遇到比页更小的对象，比如不到 1K 的时候，该怎么分配内存呢？\n\n实际系统运行中，确实有大量比页还小的对象，如果为它们也分配单独的页，那就太浪费内存了。\n\n所以，在用户空间，malloc 通过 brk() 分配的内存，在释放时并不立即归还系统，而是缓存起来重复利用。在内核空间，Linux 则通过 slab 分配器来管理小内存。你可以把 slab 看成构建在伙伴系统上的一个缓存，主要作用就是分配并释放内核中的小对象。\n\n对内存来说，如果只分配而不释放，就会造成内存泄漏，甚至会耗尽系统内存。所以，在应用程序用完内存后，还需要调用 free() 或 unmap() ，来释放这些不用的内存。\n\n当然，系统也不会任由某个进程用完所有内存。在发现内存紧张时，系统就会通过一系列机制来回收内存，比如下面这三种方式：\n\n- 回收缓存，比如使用 LRU（Least Recently Used）算法，回收最近使用最少的内存页面；\n- 回收不常访问的内存，把不常用的内存通过交换分区直接写到磁盘中；\n- 杀死进程，内存紧张时系统还会通过 OOM（Out of Memory），直接杀掉占用大量内存的进程。\n\n其中，第二种方式回收不常访问的内存时，会用到交换分区（以下简称 Swap）。Swap 其实就是把一块磁盘空间当成内存来用。它可以把进程暂时不用的数据存储到磁盘中（这个过程称为换出），当进程访问这些内存时，再从磁盘读取这些数据到内存中（这个过程称为换入）。\n\n所以，你可以发现，Swap 把系统的可用内存变大了。不过要注意，通常只在内存不足时，才会发生 Swap 交换。并且由于磁盘读写的速度远比内存慢，Swap 会导致严重的内存性能问题。\n\n第三种方式提到的 OOM（Out of Memory），其实是内核的一种保护机制。它监控进程的内存使用情况，并且使用 oom_score 为每个进程的内存使用情况进行评分：\n\n- 一个进程消耗的内存越大，oom_score 就越大；\n- 一个进程运行占用的 CPU 越多，oom_score 就越小。\n\n这样，进程的 oom_score 越大，代表消耗的内存越多，也就越容易被 OOM 杀死，从而可以更好保护系统。\n\n当然，为了实际工作的需要，管理员可以通过 /proc 文件系统，手动设置进程的 oom_adj ，从而调整进程的 oom_score。\n\noom_adj 的范围是 [-17, 15]，数值越大，表示进程越容易被 OOM 杀死；数值越小，表示进程越不容易被 OOM 杀死，其中 -17 表示禁止 OOM。\n\n比如用下面的命令，你就可以把 sshd 进程的 oom_adj 调小为 -16，这样， sshd 进程就不容易被 OOM 杀死。\n\n```bash\necho -16 \u003e /proc/$(pidof sshd)/oom_adj\n```\n\n## 如何查看内存使用情况\n\n通过了解内存空间的分布，以及内存的分配和回收，我想你对内存的工作原理应该有了大概的认识。当然，系统的实际工作原理更加复杂，也会涉及其他一些机制，这里我只讲了最主要的原理。掌握了这些，你可以对内存的运作有一条主线认识，不至于脑海里只有术语名词的堆砌。\n\n那么在了解内存的工作原理之后，我们又该怎么查看系统内存使用情况呢？\n\n其实前面 CPU 内容的学习中，我们也提到过一些相关工具。在这里，你第一个想到的应该是 free 工具吧。下面是一个 free 的输出示例：\n\n```makefile\n# 注意不同版本的 free 输出可能会有所不同\n$ free\n              total        used        free      shared  buff/cache   available\nMem:        8169348      263524     6875352         668     1030472     7611064\nSwap:             0           0           0\n```\n\n你可以看到，free 输出的是一个表格，其中的数值都默认以字节为单位。表格总共有两行六列，这两行分别是物理内存 Mem 和交换分区 Swap 的使用情况，而六列中，每列数据的含义分别为：\n\n- 第一列，total 是总内存大小；\n- 第二列，used 是已使用内存的大小，包含了共享内存；\n- 第三列，free 是未使用内存的大小；\n- 第四列，shared 是共享内存的大小；\n- 第五列，buff/cache 是缓存和缓冲区的大小；\n- 最后一列，available 是新进程可用内存的大小。\n\n这里尤其注意一下，最后一列的可用内存 available 。available 不仅包含未使用内存，还包括了可回收的缓存，所以一般会比未使用内存更大。不过，并不是所有缓存都可以回收，因为有些缓存可能正在使用中。\n\n不过，我们知道，free 显示的是整个系统的内存使用情况。如果你想查看进程的内存使用情况，可以用 top 或者 ps 等工具。比如，下面是 top 的输出示例：\n\n```yaml\n# 按下 M 切换到内存排序\n$ top\n...\nKiB Mem :  8169348 total,  6871440 free,   267096 used,  1030812 buff/cache\nKiB Swap:        0 total,        0 free,        0 used.  7607492 avail Mem \n \n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n  430 root      19  -1  122360  35588  23748 S   0.0  0.4   0:32.17 systemd-journal\n 1075 root      20   0  771860  22744  11368 S   0.0  0.3   0:38.89 snapd\n 1048 root      20   0  170904  17292   9488 S   0.0  0.2   0:00.24 networkd-dispat\n    1 root      20   0   78020   9156   6644 S   0.0  0.1   0:22.92 systemd\n12376 azure     20   0   76632   7456   6420 S   0.0  0.1   0:00.01 systemd\n12374 root      20   0  107984   7312   6304 S   0.0  0.1   0:00.00 sshd\n...\n```\n\ntop 输出界面的顶端，也显示了系统整体的内存使用情况，这些数据跟 free 类似，我就不再重复解释。我们接着看下面的内容，跟内存相关的几列数据，比如 VIRT、RES、SHR 以及 %MEM 等。\n\n这些数据，包含了进程最重要的几个内存使用情况，我们挨个来看。\n\n- VIRT 是进程虚拟内存的大小，只要是进程申请过的内存，即便还没有真正分配物理内存，也会计算在内。\n- RES 是常驻内存的大小，也就是进程实际使用的物理内存大小，但不包括 Swap 和共享内存。\n- SHR 是共享内存的大小，比如与其他进程共同使用的共享内存、加载的动态链接库以及程序的代码段等。\n- %MEM 是进程使用物理内存占系统总内存的百分比。\n\n除了要认识这些基本信息，在查看 top 输出时，你还要注意两点。\n\n第一，虚拟内存通常并不会全部分配物理内存。从上面的输出，你可以发现每个进程的虚拟内存都比常驻内存大得多。\n\n第二，共享内存 SHR 并不一定是共享的，比方说，程序的代码段、非共享的动态链接库，也都算在 SHR 里。当然，SHR 也包括了进程间真正共享的内存。所以在计算多个进程的内存使用时，不要把所有进程的 SHR 直接相加得出结果。\n\n## 小结\n\n今天，我们梳理了 Linux 内存的工作原理。对普通进程来说，它能看到的其实是内核提供的虚拟内存，这些虚拟内存还需要通过页表，由系统映射为物理内存。\n\n当进程通过 malloc() 申请内存后，内存并不会立即分配，而是在首次访问时，才通过缺页异常陷入内核中分配内存。\n\n由于进程的虚拟地址空间比物理内存大很多，Linux 还提供了一系列的机制，应对内存不足的问题，比如缓存的回收、交换分区 Swap 以及 OOM 等。\n\n当你需要了解系统或者进程的内存使用情况时，可以用 free 和 top 、ps 等性能工具。它们都是分析性能问题时最常用的性能工具，希望你能熟练使用它们，并真正理解各个指标的含义。\n\n## 思考\n\n最后，我想请你来聊聊你所理解的 Linux 内存。你碰到过哪些内存相关的性能瓶颈？你又是怎么样来分析它们的呢？你可以结合今天学到的内存知识和工作原理，提出自己的观点。\n\n欢迎在留言区和我讨论，也欢迎你把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 16 基础篇：怎么理解内存中的Buffer和Cache？\n\n你好，我是倪朋飞。\n\n上一节，我们梳理了 Linux 内存管理的基本原理，并学会了用 free 和 top 等工具，来查看系统和进程的内存使用情况。\n\n内存和 CPU 的关系非常紧密，而内存管理本身也是很复杂的机制，所以感觉知识很硬核、很难啃，都是正常的。但还是那句话，初学时不用非得理解所有内容，继续往后学，多理解相关的概念并配合一定的实践之后，再回头复习往往会容易不少。当然，基本功不容放弃。\n\n在今天的内容开始之前，我们先来回顾一下系统的内存使用情况，比如下面这个 free 输出界面：\n\n```makefile\n# 注意不同版本的 free 输出可能会有所不同\n$ free\n              total        used        free      shared  buff/cache   available\nMem:        8169348      263524     6875352         668     1030472     7611064\nSwap:             0           0           0\n```\n\n显然，这个界面包含了物理内存 Mem 和交换分区 Swap 的具体使用情况，比如总内存、已用内存、缓存、可用内存等。其中缓存是 Buffer 和 Cache 两部分的总和 。\n\n这里的大部分指标都比较容易理解，但 Buffer 和 Cache 可能不太好区分。从字面上来说，Buffer 是缓冲区，而 Cache 是缓存，两者都是数据在内存中的临时存储。那么，你知道这两种“临时存储”有什么区别吗？\n\n注：今天内容接下来的部分，Buffer 和 Cache 我会都用英文来表示，避免跟文中的“缓存”一词混淆。而文中的“缓存”，则通指内存中的临时存储。\n\n## free 数据的来源\n\n在我正式讲解两个概念前，你可以先想想，你有没有什么途径来进一步了解它们？除了中文翻译直接得到概念，别忘了，Buffer 和 Cache 还是我们用 free 获得的指标。\n\n还记得我之前讲过的，碰到看不明白的指标时该怎么办吗？\n\n估计你想起来了，不懂就去查手册。用 man 命令查询 free 的文档，就可以找到对应指标的详细说明。比如，我们执行 man free ，就可以看到下面这个界面。\n\n```bash\nbuffers\n              Memory used by kernel buffers (Buffers in /proc/meminfo) \n       cache  Memory used by the page cache and slabs (Cached and SReclaimable in /proc/meminfo) \n       buff/cache\n              Sum of buffers and cache\n```\n\n从 free 的手册中，你可以看到 buffer 和 cache 的说明。\n\n- Buffers 是内核缓冲区用到的内存，对应的是 /proc/meminfo 中的 Buffers 值。\n- Cache 是内核页缓存和 Slab 用到的内存，对应的是 /proc/meminfo 中的 Cached 与 SReclaimable 之和。\n\n这里的说明告诉我们，这些数值都来自 /proc/meminfo，但更具体的 Buffers、Cached 和 SReclaimable 的含义，还是没有说清楚。\n\n要弄明白它们到底是什么，我估计你第一反应就是去百度或者 Google 一下。虽然大部分情况下，网络搜索能给出一个答案。但是，且不说筛选信息花费的时间精力，对你来说，这个答案的准确性也是很难保证的。\n\n要注意，网上的结论可能是对的，但是很可能跟你的环境并不匹配。最简单来说，同一个指标的具体含义，就可能因为内核版本、性能工具版本的不同而有挺大差别。这也是为什么，我总在专栏中强调通用思路和方法，而不是让你死记结论。对于案例实践来说，机器环境就是我们的最大限制。\n\n那么，有没有更简单、更准确的方法，来查询它们的含义呢？\n\n## proc 文件系统\n\n我在前面 CPU 性能模块就曾经提到过，/proc 是 Linux 内核提供的一种特殊文件系统，是用户跟内核交互的接口。比方说，用户可以从 /proc 中查询内核的运行状态和配置选项，查询进程的运行状态、统计数据等，当然，你也可以通过 /proc 来修改内核的配置。\n\nproc 文件系统同时也是很多性能工具的最终数据来源。比如我们刚才看到的 free ，就是通过读取 /proc/meminfo ，得到内存的使用情况。\n\n继续说回 /proc/meminfo，既然 Buffers、Cached、SReclaimable 这几个指标不容易理解，那我们还得继续查 proc 文件系统，获取它们的详细定义。\n\n执行 man proc ，你就可以得到 proc 文件系统的详细文档。\n\n注意这个文档比较长，你最好搜索一下（比如搜索 meminfo），以便更快定位到内存部分。\n\n```vbnet\nBuffers %lu\n    Relatively temporary storage for raw disk blocks that shouldn't get tremendously large (20MB or so). \nCached %lu\n   In-memory cache for files read from the disk (the page cache).  Doesn't include SwapCached.\n...\nSReclaimable %lu (since Linux 2.6.19)\n    Part of Slab, that might be reclaimed, such as caches.\n    \nSUnreclaim %lu (since Linux 2.6.19)\n    Part of Slab, that cannot be reclaimed on memory pressure.\n```\n\n通过这个文档，我们可以看到：\n\n- Buffers 是对原始磁盘块的临时存储，也就是用来**缓存磁盘的数据**，通常不会特别大（20MB 左右）。这样，内核就可以把分散的写集中起来，统一优化磁盘的写入，比如可以把多次小的写合并成单次大的写等等。\n- Cached 是从磁盘读取文件的页缓存，也就是用来**缓存从文件读取的数据**。这样，下次访问这些文件数据时，就可以直接从内存中快速获取，而不需要再次访问缓慢的磁盘。\n- SReclaimable 是 Slab 的一部分。Slab 包括两部分，其中的可回收部分，用 SReclaimable 记录；而不可回收部分，用 SUnreclaim 记录。\n\n好了，我们终于找到了这三个指标的详细定义。到这里，你是不是长舒一口气，满意地想着，总算弄明白 Buffer 和 Cache 了。不过，知道这个定义就真的理解了吗？这里我给你提了两个问题，你先想想能不能回答出来。\n\n第一个问题，Buffer 的文档没有提到这是磁盘读数据还是写数据的缓存，而在很多网络搜索的结果中都会提到 Buffer 只是对**将要写入磁盘数据**的缓存。那反过来说，它会不会也缓存从磁盘中读取的数据呢？\n\n第二个问题，文档中提到，Cache 是对从文件读取数据的缓存，那么它是不是也会缓存写文件的数据呢？\n\n为了解答这两个问题，接下来，我将用几个案例来展示， Buffer 和 Cache 在不同场景下的使用情况。\n\n## 案例\n\n### 你的准备\n\n跟前面实验一样，今天的案例也是基于 Ubuntu 18.04，当然，其他 Linux 系统也适用。我的案例环境是这样的。\n\n- 机器配置：2 CPU，8GB 内存。\n- 预先安装 sysstat 包，如 apt install sysstat。\n\n之所以要安装 sysstat ，是因为我们要用到 vmstat ，来观察 Buffer 和 Cache 的变化情况。虽然从 /proc/meminfo 里也可以读到相同的结果，但毕竟还是 vmstat 的结果更加直观。\n\n另外，这几个案例使用了 dd 来模拟磁盘和文件的 I/O，所以我们也需要观测 I/O 的变化情况。\n\n上面的工具安装完成后，你可以打开两个终端，连接到 Ubuntu 机器上。\n\n准备环节的最后一步，为了减少缓存的影响，记得在第一个终端中，运行下面的命令来清理系统缓存：\n\n```ruby\n# 清理文件页、目录项、Inodes 等各种缓存\n$ echo 3 \u003e /proc/sys/vm/drop_caches\n```\n\n这里的 /proc/sys/vm/drop_caches ，就是通过 proc 文件系统修改内核行为的一个示例，写入 3 表示清理文件页、目录项、Inodes 等各种缓存。这几种缓存的区别你暂时不用管，后面我们都会讲到。\n\n### 场景 1：磁盘和文件写案例\n\n我们先来模拟第一个场景。首先，在第一个终端，运行下面这个 vmstat 命令：\n\n```scss\n# 每隔 1 秒输出 1 组数据\n$ vmstat 1\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\nr  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n0  0      0 7743608   1112  92168    0    0     0     0   52  152  0  1 100  0  0\n 0  0      0 7743608   1112  92168    0    0     0     0   36   92  0  0 100  0  0\n```\n\n输出界面里， 内存部分的 buff 和 cache ，以及 io 部分的 bi 和 bo 就是我们要关注的重点。\n\n- buff 和 cache 就是我们前面看到的 Buffers 和 Cache，单位是 KB。\n- bi 和 bo 则分别表示块设备读取和写入的大小，单位为块 / 秒。因为 Linux 中块的大小是 1KB，所以这个单位也就等价于 KB/s。\n\n正常情况下，空闲系统中，你应该看到的是，这几个值在多次结果中一直保持不变。\n\n接下来，到第二个终端执行 dd 命令，通过读取随机设备，生成一个 500MB 大小的文件：\n\n```javascript\n$ dd if=/dev/urandom of=/tmp/file bs=1M count=500\n```\n\n然后再回到第一个终端，观察 Buffer 和 Cache 的变化情况：\n\n```yaml\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\nr  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n0  0      0 7499460   1344 230484    0    0     0     0   29  145  0  0 100  0  0\n 1  0      0 7338088   1752 390512    0    0   488     0   39  558  0 47 53  0  0\n 1  0      0 7158872   1752 568800    0    0     0     4   30  376  1 50 49  0  0\n 1  0      0 6980308   1752 747860    0    0     0     0   24  360  0 50 50  0  0\n 0  0      0 6977448   1752 752072    0    0     0     0   29  138  0  0 100  0  0\n 0  0      0 6977440   1760 752080    0    0     0   152   42  212  0  1 99  1  0\n...\n 0  1      0 6977216   1768 752104    0    0     4 122880   33  234  0  1 51 49  0\n 0  1      0 6977440   1768 752108    0    0     0 10240   38  196  0  0 50 50  0\n```\n\n通过观察 vmstat 的输出，我们发现，在 dd 命令运行时， Cache 在不停地增长，而 Buffer 基本保持不变。\n\n再进一步观察 I/O 的情况，你会看到，\n\n- 在 Cache 刚开始增长时，块设备 I/O 很少，bi 只出现了一次 488 KB/s，bo 则只有一次 4KB。而过一段时间后，才会出现大量的块设备写，比如 bo 变成了 122880。\n- 当 dd 命令结束后，Cache 不再增长，但块设备写还会持续一段时间，并且，多次 I/O 写的结果加起来，才是 dd 要写的 500M 的数据。\n\n把这个结果，跟我们刚刚了解到的 Cache 的定义做个对比，你可能会有点晕乎。为什么前面文档上说 Cache 是文件读的页缓存，怎么现在写文件也有它的份？\n\n这个疑问，我们暂且先记下来，接着再来看另一个磁盘写的案例。两个案例结束后，我们再统一进行分析。\n\n不过，对于接下来的案例，我必须强调一点：\n\n下面的命令对环境要求很高，需要你的系统配置多块磁盘，并且磁盘分区 /dev/sdb1 还要处于未使用状态。如果你只有一块磁盘，千万不要尝试，否则将会对你的磁盘分区造成损坏。\n\n如果你的系统符合标准，就可以继续在第二个终端中，运行下面的命令。清理缓存后，向磁盘分区 /dev/sdb1 写入 2GB 的随机数据：\n\n```shell\n# 首先清理缓存\n$ echo 3 \u003e /proc/sys/vm/drop_caches\n# 然后运行 dd 命令向磁盘分区 /dev/sdb1 写入 2G 数据\n$ dd if=/dev/urandom of=/dev/sdb1 bs=1M count=2048\n```\n\n然后，再回到终端一，观察内存和 I/O 的变化情况：\n\n```css\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n1  0      0 7584780 153592  97436    0    0   684     0   31  423  1 48 50  2  0\n 1  0      0 7418580 315384 101668    0    0     0     0   32  144  0 50 50  0  0\n 1  0      0 7253664 475844 106208    0    0     0     0   20  137  0 50 50  0  0\n 1  0      0 7093352 631800 110520    0    0     0     0   23  223  0 50 50  0  0\n 1  1      0 6930056 790520 114980    0    0     0 12804   23  168  0 50 42  9  0\n 1  0      0 6757204 949240 119396    0    0     0 183804   24  191  0 53 26 21  0\n 1  1      0 6591516 1107960 123840    0    0     0 77316   22  232  0 52 16 33  0\n```\n\n从这里你会看到，虽然同是写数据，写磁盘跟写文件的现象还是不同的。写磁盘时（也就是 bo 大于 0 时），Buffer 和 Cache 都在增长，但显然 Buffer 的增长快得多。\n\n这说明，写磁盘用到了大量的 Buffer，这跟我们在文档中查到的定义是一样的。\n\n对比两个案例，我们发现，写文件时会用到 Cache 缓存数据，而写磁盘则会用到 Buffer 来缓存数据。所以，回到刚刚的问题，虽然文档上只提到，Cache 是文件读的缓存，但实际上，Cache 也会缓存写文件时的数据。\n\n### 场景 2：磁盘和文件读案例\n\n了解了磁盘和文件写的情况，我们再反过来想，磁盘和文件读的时候，又是怎样的呢？\n\n我们回到第二个终端，运行下面的命令。清理缓存后，从文件 /tmp/file 中，读取数据写入空设备：\n\n```shell\n# 首先清理缓存\n$ echo 3 \u003e /proc/sys/vm/drop_caches\n# 运行 dd 命令读取文件数据\n$ dd if=/tmp/file of=/dev/null\n```\n\n然后，再回到终端一，观察内存和 I/O 的变化情况：\n\n```css\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 0  1      0 7724164   2380 110844    0    0 16576     0   62  360  2  2 76 21  0\n 0  1      0 7691544   2380 143472    0    0 32640     0   46  439  1  3 50 46  0\n 0  1      0 7658736   2380 176204    0    0 32640     0   54  407  1  4 50 46  0\n 0  1      0 7626052   2380 208908    0    0 32640    40   44  422  2  2 50 46  0\n```\n\n观察 vmstat 的输出，你会发现读取文件时（也就是 bi 大于 0 时），Buffer 保持不变，而 Cache 则在不停增长。这跟我们查到的定义“Cache 是对文件读的页缓存”是一致的。\n\n那么，磁盘读又是什么情况呢？我们再运行第二个案例来看看。\n\n首先，回到第二个终端，运行下面的命令。清理缓存后，从磁盘分区 /dev/sda1 中读取数据，写入空设备：\n\n```shell\n# 首先清理缓存\n$ echo 3 \u003e /proc/sys/vm/drop_caches\n# 运行 dd 命令读取文件\n$ dd if=/dev/sda1 of=/dev/null bs=1M count=1024\n```\n\n然后，再回到终端一，观察内存和 I/O 的变化情况：\n\n```css\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n0  0      0 7225880   2716 608184    0    0     0     0   48  159  0  0 100  0  0\n 0  1      0 7199420  28644 608228    0    0 25928     0   60  252  0  1 65 35  0\n 0  1      0 7167092  60900 608312    0    0 32256     0   54  269  0  1 50 49  0\n 0  1      0 7134416  93572 608376    0    0 32672     0   53  253  0  0 51 49  0\n 0  1      0 7101484 126320 608480    0    0 32748     0   80  414  0  1 50 49  0\n```\n\n观察 vmstat 的输出，你会发现读磁盘时（也就是 bi 大于 0 时），Buffer 和 Cache 都在增长，但显然 Buffer 的增长快很多。这说明读磁盘时，数据缓存到了 Buffer 中。\n\n当然，我想，经过上一个场景中两个案例的分析，你自己也可以对比得出这个结论：读文件时数据会缓存到 Cache 中，而读磁盘时数据会缓存到 Buffer 中。\n\n到这里你应该发现了，虽然文档提供了对 Buffer 和 Cache 的说明，但是仍不能覆盖到所有的细节。比如说，今天我们了解到的这两点：\n\n- Buffer 既可以用作“将要写入磁盘数据的缓存”，也可以用作“从磁盘读取数据的缓存”。\n- Cache 既可以用作“从文件读取数据的页缓存”，也可以用作“写文件的页缓存”。\n\n这样，我们就回答了案例开始前的两个问题。\n\n简单来说，**Buffer 是对磁盘数据的缓存，而 Cache 是文件数据的缓存，它们既会用在读请求中，也会用在写请求中**。\n\n## 小结\n\n今天，我们一起探索了内存性能中 Buffer 和 Cache 的详细含义。Buffer 和 Cache 分别缓存磁盘和文件系统的读写数据。\n\n- 从写的角度来说，不仅可以优化磁盘和文件的写入，对应用程序也有好处，应用程序可以在数据真正落盘前，就返回去做其他工作。\n- 从读的角度来说，既可以加速读取那些需要频繁访问的数据，也降低了频繁 I/O 对磁盘的压力。\n\n除了探索的内容本身，这个探索过程对你应该也有所启发。在排查性能问题时，由于各种资源的性能指标太多，我们不可能记住所有指标的详细含义。那么，准确高效的手段——查文档，就非常重要了。\n\n你一定要养成查文档的习惯，并学会解读这些性能指标的详细含义。此外，proc 文件系统也是我们的好帮手。它为我们呈现了系统内部的运行状态，同时也是很多性能工具的数据来源，是辅助排查性能问题的好方法。\n\n## 思考\n\n最后，我想给你留一个思考题。\n\n我们已经知道，可以使用 ps、top 或者 proc 文件系统，来获取进程的内存使用情况。那么，如何统计出所有进程的物理内存使用量呢？\n\n提示：要避免重复计算多个进程同时占用的内存，像是页缓存、共享内存这类。如果你把 ps、top 得到的数据直接相加，就会出现重复计算的问题。\n\n这里，我推荐从 /proc/\u003c pid \u003e/smaps 入手。前面内容里，我并没有直接讲过 /proc/\u003c pid \u003esmaps 文件中各个指标含义，所以，需要你自己动手查 proc 文件系统的文档，解读并回答这个问题。\n\n欢迎在留言区和我讨论，也欢迎你把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 17 案例篇：如何利用系统缓存优化程序的运行效率？\n\n你好，我是倪朋飞。\n\n上一节，我们学习了内存性能中 Buffer 和 Cache 的概念。简单复习一下，Buffer 和 Cache 的设计目的，是为了提升系统的 I/O 性能。它们利用内存，充当起慢速磁盘与快速 CPU 之间的桥梁，可以加速 I/O 的访问速度。\n\nBuffer 和 Cache 分别缓存的是对磁盘和文件系统的读写数据。\n\n- 从写的角度来说，不仅可以优化磁盘和文件的写入，对应用程序也有好处，应用程序可以在数据真正落盘前，就返回去做其他工作。\n- 从读的角度来说，不仅可以提高那些频繁访问数据的读取速度，也降低了频繁 I/O 对磁盘的压力。\n\n既然 Buffer 和 Cache 对系统性能有很大影响，那我们在软件开发的过程中，能不能利用这一点，来优化 I/O 性能，提升应用程序的运行效率呢？\n\n答案自然是肯定的。今天，我就用几个案例帮助你更好地理解缓存的作用，并学习如何充分利用这些缓存来提高程序效率。\n\n为了方便你理解，Buffer 和 Cache 我仍然用英文表示，避免跟“缓存”一词混淆。而文中的“缓存”，通指数据在内存中的临时存储。\n\n## 缓存命中率\n\n在案例开始前，你应该习惯性地先问自己一个问题，你想要做成某件事情，结果应该怎么评估？比如说，我们想利用缓存来提升程序的运行效率，应该怎么评估这个效果呢？换句话说，有没有哪个指标可以衡量缓存使用的好坏呢？\n\n我估计你已经想到了，**缓存的命中率**。所谓缓存命中率，是指直接通过缓存获取数据的请求次数，占所有数据请求次数的百分比。\n\n**命中率越高，表示使用缓存带来的收益越高，应用程序的性能也就越好。**\n\n实际上，缓存是现在所有高并发系统必需的核心模块，主要作用就是把经常访问的数据（也就是热点数据），提前读入到内存中。这样，下次访问时就可以直接从内存读取数据，而不需要经过硬盘，从而加快应用程序的响应速度。\n\n这些独立的缓存模块通常会提供查询接口，方便我们随时查看缓存的命中情况。不过 Linux 系统中并没有直接提供这些接口，所以这里我要介绍一下，cachestat 和 cachetop ，它们正是查看系统缓存命中情况的工具。\n\n- cachestat 提供了整个操作系统缓存的读写命中情况。\n- cachetop 提供了每个进程的缓存命中情况。\n\n这两个工具都是 [bcc](https://github.com/iovisor/bcc) 软件包的一部分，它们基于 Linux 内核的 eBPF（extended Berkeley Packet Filters）机制，来跟踪内核中管理的缓存，并输出缓存的使用和命中情况。\n\n这里注意，eBPF 的工作原理不是我们今天的重点，记住这个名字即可，后面文章中我们会详细学习。今天要掌握的重点，是这两个工具的使用方法。\n\n使用 cachestat 和 cachetop 前，我们首先要安装 bcc 软件包。比如，在 Ubuntu 系统中，你可以运行下面的命令来安装：\n\n```bash\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDD\necho \"deb https://repo.iovisor.org/apt/xenial xenial main\" | sudo tee /etc/apt/sources.list.d/iovisor.list\nsudo apt-get update\nsudo apt-get install -y bcc-tools libbcc-examples linux-headers-$(uname -r)\n```\n\n\u003e 注意：bcc-tools 需要内核版本为 4.1 或者更新的版本，如果你用的是 CentOS，那就需要手动[升级内核版本后再安装](https://github.com/iovisor/bcc/issues/462)。\n\n操作完这些步骤，bcc 提供的所有工具就都安装到 /usr/share/bcc/tools 这个目录中了。不过这里提醒你，bcc 软件包默认不会把这些工具配置到系统的 PATH 路径中，所以你得自己手动配置：\n\n```ruby\n$ export PATH=$PATH:/usr/share/bcc/tools\n```\n\n配置完，你就可以运行 cachestat 和 cachetop 命令了。比如，下面就是一个 cachestat 的运行界面，它以 1 秒的时间间隔，输出了 3 组缓存统计数据：\n\n```markdown\n$ cachestat 1 3\n   TOTAL   MISSES     HITS  DIRTIES   BUFFERS_MB  CACHED_MB\n       2        0        2        1           17        279\n       2        0        2        1           17        279\n       2        0        2        1           17        279 \n```\n\n你可以看到，cachestat 的输出其实是一个表格。每行代表一组数据，而每一列代表不同的缓存统计指标。这些指标从左到右依次表示：\n\n- TOTAL ，表示总的 I/O 次数；\n- MISSES ，表示缓存未命中的次数；\n- HITS ，表示缓存命中的次数；\n- DIRTIES， 表示新增到缓存中的脏页数；\n- BUFFERS_MB 表示 Buffers 的大小，以 MB 为单位；\n- CACHED_MB 表示 Cache 的大小，以 MB 为单位。\n\n接下来我们再来看一个 cachetop 的运行界面：\n\n```objectivec\n$ cachetop\n11:58:50 Buffers MB: 258 / Cached MB: 347 / Sort: HITS / Order: ascending\nPID      UID      CMD              HITS     MISSES   DIRTIES  READ_HIT%  WRITE_HIT%\n   13029 root     python                  1        0        0     100.0%       0.0%\n```\n\n它的输出跟 top 类似，默认按照缓存的命中次数（HITS）排序，展示了每个进程的缓存命中情况。具体到每一个指标，这里的 HITS、MISSES 和 DIRTIES ，跟 cachestat 里的含义一样，分别代表间隔时间内的缓存命中次数、未命中次数以及新增到缓存中的脏页数。\n\n而 READ_HIT 和 WRITE_HIT ，分别表示读和写的缓存命中率。\n\n## 指定文件的缓存大小\n\n除了缓存的命中率外，还有一个指标你可能也会很感兴趣，那就是指定文件在内存中的缓存大小。你可以使用 [pcstat](https://github.com/tobert/pcstat) 这个工具，来查看文件在内存中的缓存大小以及缓存比例。\n\npcstat 是一个基于 Go 语言开发的工具，所以安装它之前，你首先应该安装 Go 语言，你可以点击[这里](https://golang.org/dl/)下载安装。\n\n安装完 Go 语言，再运行下面的命令安装 pcstat：\n\n```shell\n$ export GOPATH=~/go\n$ export PATH=~/go/bin:$PATH\n$ go get golang.org/x/sys/unix\n$ go get github.com/tobert/pcstat/pcstat\n```\n\n全部安装完成后，你就可以运行 pcstat 来查看文件的缓存情况了。比如，下面就是一个 pcstat 运行的示例，它展示了 /bin/ls 这个文件的缓存情况：\n\n```sql\n$ pcstat /bin/ls\n+---------+----------------+------------+-----------+---------+\n| Name    | Size (bytes)   | Pages      | Cached    | Percent |\n|---------+----------------+------------+-----------+---------|\n| /bin/ls | 133792         | 33         | 0         | 000.000 |\n+---------+----------------+------------+-----------+---------+\n```\n\n这个输出中，Cached 就是 /bin/ls 在缓存中的大小，而 Percent 则是缓存的百分比。你看到它们都是 0，这说明 /bin/ls 并不在缓存中。\n\n接着，如果你执行一下 ls 命令，再运行相同的命令来查看的话，就会发现 /bin/ls 都在缓存中了：\n\n```shell\n$ ls\n$ pcstat /bin/ls\n+---------+----------------+------------+-----------+---------+\n| Name    | Size (bytes)   | Pages      | Cached    | Percent |\n|---------+----------------+------------+-----------+---------|\n| /bin/ls | 133792         | 33         | 33        | 100.000 |\n+---------+----------------+------------+-----------+---------+\n```\n\n知道了缓存相应的指标和查看系统缓存的方法后，接下来，我们就进入今天的正式案例。\n\n跟前面的案例一样，今天的案例也是基于 Ubuntu 18.04，当然同样适用于其他的 Linux 系统。\n\n- 机器配置：2 CPU，8GB 内存。\n- 预先按照上面的步骤安装 bcc 和 pcstat 软件包，并把这些工具的安装路径添加到到 PATH 环境变量中。\n- 预先安装 Docker 软件包，比如 apt-get install [docker.io](https://docker.io/)\n\n## 案例一\n\n第一个案例，我们先来看一下上一节提到的 dd 命令。\n\ndd 作为一个磁盘和文件的拷贝工具，经常被拿来测试磁盘或者文件系统的读写性能。不过，既然缓存会影响到性能，如果用 dd 对同一个文件进行多次读取测试，测试的结果会怎么样呢？\n\n我们来动手试试。首先，打开两个终端，连接到 Ubuntu 机器上，确保 bcc 已经安装配置成功。\n\n然后，使用 dd 命令生成一个临时文件，用于后面的文件读取测试：\n\n```shell\n# 生成一个 512MB 的临时文件\n$ dd if=/dev/sda1 of=file bs=1M count=512\n# 清理缓存\n$ echo 3 \u003e /proc/sys/vm/drop_caches\n```\n\n继续在第一个终端，运行 pcstat 命令，确认刚刚生成的文件不在缓存中。如果一切正常，你会看到 Cached 和 Percent 都是 0:\n\n```sql\n$ pcstat file\n+-------+----------------+------------+-----------+---------+\n| Name  | Size (bytes)   | Pages      | Cached    | Percent |\n|-------+----------------+------------+-----------+---------|\n| file  | 536870912      | 131072     | 0         | 000.000 |\n+-------+----------------+------------+-----------+---------+\n```\n\n还是在第一个终端中，现在运行 cachetop 命令：\n\n```ruby\n# 每隔 5 秒刷新一次数据\n$ cachetop 5\n```\n\n这次是第二个终端，运行 dd 命令测试文件的读取速度：\n\n```bash\n$ dd if=file of=/dev/null bs=1M\n512+0 records in\n512+0 records out\n536870912 bytes (537 MB, 512 MiB) copied, 16.0509 s, 33.4 MB/s\n```\n\n从 dd 的结果可以看出，这个文件的读性能是 33.4 MB/s。由于在 dd 命令运行前我们已经清理了缓存，所以 dd 命令读取数据时，肯定要通过文件系统从磁盘中读取。\n\n不过，这是不是意味着， dd 所有的读请求都能直接发送到磁盘呢？\n\n我们再回到第一个终端， 查看 cachetop 界面的缓存命中情况：\n\n```objectivec\nPID      UID      CMD              HITS     MISSES   DIRTIES  READ_HIT%  WRITE_HIT%\n\\.\\.\\.\n    3264 root     dd                  37077    37330        0      49.8%      50.2%\n```\n\n从 cachetop 的结果可以发现，并不是所有的读都落到了磁盘上，事实上读请求的缓存命中率只有 50% 。\n\n接下来，我们继续尝试相同的测试命令。先切换到第二个终端，再次执行刚才的 dd 命令：\n\n```bash\n$ dd if=file of=/dev/null bs=1M\n512+0 records in\n512+0 records out\n536870912 bytes (537 MB, 512 MiB) copied, 0.118415 s, 4.5 GB/s\n```\n\n看到这次的结果，有没有点小惊讶？磁盘的读性能居然变成了 4.5 GB/s，比第一次的结果明显高了太多。为什么这次的结果这么好呢？\n\n不妨再回到第一个终端，看看 cachetop 的情况：\n\n```objectivec\n10:45:22 Buffers MB: 4 / Cached MB: 719 / Sort: HITS / Order: ascending\nPID      UID      CMD              HITS     MISSES   DIRTIES  READ_HIT%  WRITE_HIT%\n\\.\\.\\.\n   32642 root     dd                 131637        0        0     100.0%       0.0%\n```\n\n显然，cachetop 也有了不小的变化。你可以发现，这次的读的缓存命中率是 100.0%，也就是说这次的 dd 命令全部命中了缓存，所以才会看到那么高的性能。\n\n然后，回到第二个终端，再次执行 pcstat 查看文件 file 的缓存情况：\n\n```sql\n$ pcstat file\n+-------+----------------+------------+-----------+---------+\n| Name  | Size (bytes)   | Pages      | Cached    | Percent |\n|-------+----------------+------------+-----------+---------|\n| file  | 536870912      | 131072     | 131072    | 100.000 |\n+-------+----------------+------------+-----------+---------+\n```\n\n从 pcstat 的结果你可以发现，测试文件 file 已经被全部缓存了起来，这跟刚才观察到的缓存命中率 100% 是一致的。\n\n这两次结果说明，系统缓存对第二次 dd 操作有明显的加速效果，可以大大提高文件读取的性能。\n\n但同时也要注意，如果我们把 dd 当成测试文件系统性能的工具，由于缓存的存在，就会导致测试结果严重失真。\n\n## 案例二\n\n接下来，我们再来看一个文件读写的案例。这个案例类似于前面学过的不可中断状态进程的例子。它的基本功能比较简单，也就是每秒从磁盘分区 /dev/sda1 中读取 32MB 的数据，并打印出读取数据花费的时间。\n\n为了方便你运行案例，我把它打包成了一个 [Docker 镜像](https://github.com/feiskyer/linux-perf-examples/tree/master/io-cached)。 跟前面案例类似，我提供了下面两个选项，你可以根据系统配置，自行调整磁盘分区的路径以及 I/O 的大小。\n\n- -d 选项，设置要读取的磁盘或分区路径，默认是查找前缀为 /dev/sd 或者 /dev/xvd 的磁盘。\n- -s 选项，设置每次读取的数据量大小，单位为字节，默认为 33554432（也就是 32MB）。\n\n这个案例同样需要你开启两个终端。分别 SSH 登录到机器上后，先在第一个终端中运行 cachetop 命令：\n\n```ruby\n# 每隔 5 秒刷新一次数据\n$ cachetop 5 \n```\n\n接着，再到第二个终端，执行下面的命令运行案例：\n\n```shell\n$ docker run --privileged --name=app -itd feisky/app:io-direct\n```\n\n案例运行后，我们还需要运行下面这个命令，来确认案例已经正常启动。如果一切正常，你应该可以看到类似下面的输出：\n\n```python\n$ docker logs app\nReading data from disk /dev/sdb1 with buffer size 33554432\nTime used: 0.929935 s to read 33554432 bytes\nTime used: 0.949625 s to read 33554432 bytes\n```\n\n从这里你可以看到，每读取 32 MB 的数据，就需要花 0.9 秒。这个时间合理吗？我想你第一反应就是，太慢了吧。那这是不是没用系统缓存导致的呢？\n\n我们再来检查一下。回到第一个终端，先看看 cachetop 的输出，在这里，我们找到案例进程 app 的缓存使用情况：\n\n```objectivec\n16:39:18 Buffers MB: 73 / Cached MB: 281 / Sort: HITS / Order: ascending\nPID      UID      CMD              HITS     MISSES   DIRTIES  READ_HIT%  WRITE_HIT%\n   21881 root     app                  1024        0        0     100.0%       0.0% \n```\n\n这个输出似乎有点意思了。1024 次缓存全部命中，读的命中率是 100%，看起来全部的读请求都经过了系统缓存。但是问题又来了，如果真的都是缓存 I/O，读取速度不应该这么慢。\n\n不过，话说回来，我们似乎忽略了另一个重要因素，每秒实际读取的数据大小。HITS 代表缓存的命中次数，那么每次命中能读取多少数据呢？自然是一页。\n\n前面讲过，内存以页为单位进行管理，而每个页的大小是 4KB。所以，在 5 秒的时间间隔里，命中的缓存为 1024*4K/1024 = 4MB，再除以 5 秒，可以得到每秒读的缓存是 0.8MB，显然跟案例应用的 32 MB/s 相差太多。\n\n至于为什么只能看到 0.8 MB 的 HITS，我们后面再解释，这里你先知道怎么根据结果来分析就可以了。\n\n这也进一步验证了我们的猜想，这个案例估计没有充分利用系统缓存。其实前面我们遇到过类似的问题，如果为系统调用设置直接 I/O 的标志，就可以绕过系统缓存。\n\n那么，要判断应用程序是否用了直接 I/O，最简单的方法当然是观察它的系统调用，查找应用程序在调用它们时的选项。使用什么工具来观察系统调用呢？自然还是 strace。\n\n继续在终端二中运行下面的 strace 命令，观察案例应用的系统调用情况。注意，这里使用了 pgrep 命令来查找案例进程的 PID 号：\n\n```perl\n# strace -p $(pgrep app)\nstrace: Process 4988 attached\nrestart_syscall(\u003c\\.\\.\\. resuming interrupted nanosleep \\.\\.\\.\u003e) = 0\nopenat(AT_FDCWD, \"/dev/sdb1\", O_RDONLY|O_DIRECT) = 4\nmmap(NULL, 33558528, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f448d240000\nread(4, \"8vq\\213\\314\\264u\\373\\4\\336K\\224\\25@\\371\\1\\252\\2\\262\\252q\\221\\n0\\30\\225bD\\252\\266@J\"\\.\\.\\., 33554432) = 33554432\nwrite(1, \"Time used: 0.948897 s to read 33\"\\.\\.\\., 45) = 45\nclose(4)                                = 0\n```\n\n从 strace 的结果可以看到，案例应用调用了 openat 来打开磁盘分区 /dev/sdb1，并且传入的参数为 O_RDONLY|O_DIRECT（中间的竖线表示或）。\n\nO_RDONLY 表示以只读方式打开，而 O_DIRECT 则表示以直接读取的方式打开，这会绕过系统的缓存。\n\n验证了这一点，就很容易理解为什么读 32 MB 的数据就都要那么久了。直接从磁盘读写的速度，自然远慢于对缓存的读写。这也是缓存存在的最大意义了。\n\n找出问题后，我们还可以在再看看案例应用的[源代码](https://github.com/feiskyer/linux-perf-examples/blob/master/io-cached/app.c)，再次验证一下：\n\n```perl\nint flags = O_RDONLY | O_LARGEFILE | O_DIRECT; \nint fd = open(disk, flags, 0755);\n```\n\n上面的代码，很清楚地告诉我们：它果然用了直接 I/O。\n\n找出了磁盘读取缓慢的原因，优化磁盘读的性能自然不在话下。修改源代码，删除 O_DIRECT 选项，让应用程序使用缓存 I/O ，而不是直接 I/O，就可以加速磁盘读取速度。\n\n[app-cached.c](https://github.com/feiskyer/linux-perf-examples/blob/master/io-cached/app-cached.c) 就是修复后的源码，我也把它打包成了一个容器镜像。在第二个终端中，按 Ctrl+C 停止刚才的 strace 命令，运行下面的命令，你就可以启动它：\n\n```shell\n# 删除上述案例应用\n$ docker rm -f app \n# 运行修复后的应用\n$ docker run --privileged --name=app -itd feisky/app:io-cached\n```\n\n还是第二个终端，再来运行下面的命令查看新应用的日志，你应该能看到下面这个输出：\n\n```python\n$ docker logs app\nReading data from disk /dev/sdb1 with buffer size 33554432\nTime used: 0.037342 s s to read 33554432 bytes\nTime used: 0.029676 s to read 33554432 bytes\n```\n\n现在，每次只需要 0.03 秒，就可以读取 32MB 数据，明显比之前的 0.9 秒快多了。所以，这次应该用了系统缓存。\n\n我们再回到第一个终端，查看 cachetop 的输出来确认一下：\n\n```objectivec\n16:40:08 Buffers MB: 73 / Cached MB: 281 / Sort: HITS / Order: ascending\nPID      UID      CMD              HITS     MISSES   DIRTIES  READ_HIT%  WRITE_HIT%\n   22106 root     app                 40960        0        0     100.0%       0.0%\n```\n\n果然，读的命中率还是 100%，HITS （即命中数）却变成了 40960，同样的方法计算一下，换算成每秒字节数正好是 32 MB（即 40960*4k/5/1024=32M）。\n\n这个案例说明，在进行 I/O 操作时，充分利用系统缓存可以极大地提升性能。 但在观察缓存命中率时，还要注意结合应用程序实际的 I/O 大小，综合分析缓存的使用情况。\n\n案例的最后，再回到开始的问题，为什么优化前，通过 cachetop 只能看到很少一部分数据的全部命中，而没有观察到大量数据的未命中情况呢？这是因为，cachetop 工具并不把直接 I/O 算进来。这也又一次说明了，了解工具原理的重要。\n\n\u003e cachetop 的计算方法涉及到 I/O 的原理以及一些内核的知识，如果你想了解它的原理的话，可以点击[这里](https://github.com/iovisor/bcc/blob/master/tools/cachetop.py)查看它的源代码。\n\n## 总结\n\nBuffers 和 Cache 可以极大提升系统的 I/O 性能。通常，我们用缓存命中率，来衡量缓存的使用效率。命中率越高，表示缓存被利用得越充分，应用程序的性能也就越好。\n\n你可以用 cachestat 和 cachetop 这两个工具，观察系统和进程的缓存命中情况。其中，\n\n- cachestat 提供了整个系统缓存的读写命中情况。\n- cachetop 提供了每个进程的缓存命中情况。\n\n不过要注意，Buffers 和 Cache 都是操作系统来管理的，应用程序并不能直接控制这些缓存的内容和生命周期。所以，在应用程序开发中，一般要用专门的缓存组件，来进一步提升性能。\n\n比如，程序内部可以使用堆或者栈明确声明内存空间，来存储需要缓存的数据。再或者，使用 Redis 这类外部缓存服务，优化数据的访问效率。\n\n## 思考\n\n最后，我想给你留下一道思考题，帮你更进一步了解缓存的原理。\n\n今天的第二个案例你应该很眼熟，因为前面不可中断进程的文章用的也是直接 I/O 的例子，不过那次，我们是从 CPU 使用率和进程状态的角度来分析的。对比 CPU 和缓存这两个不同角度的分析思路，你有什么样的发现呢？\n\n欢迎在留言区和我讨论，写下你的答案和收获，也欢迎你把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 18 案例篇：内存泄漏了，我该如何定位和处理？\n\n你好，我是倪朋飞。\n\n通过前几节对内存基础的学习，我相信你对 Linux 内存的工作原理，已经有了初步了解。\n\n对普通进程来说，能看到的其实是内核提供的虚拟内存，这些虚拟内存还需要通过页表，由系统映射为物理内存。\n\n当进程通过 malloc() 申请虚拟内存后，系统并不会立即为其分配物理内存，而是在首次访问时，才通过缺页异常陷入内核中分配内存。\n\n为了协调 CPU 与磁盘间的性能差异，Linux 还会使用 Cache 和 Buffer ，分别把文件和磁盘读写的数据缓存到内存中。\n\n对应用程序来说，动态内存的分配和回收，是既核心又复杂的一个逻辑功能模块。管理内存的过程中，也很容易发生各种各样的“事故”，比如，\n\n- 没正确回收分配后的内存，导致了泄漏。\n- 访问的是已分配内存边界外的地址，导致程序异常退出，等等。\n\n今天我就带你来看看，内存泄漏到底是怎么发生的，以及发生内存泄漏之后该如何排查和定位。\n\n说起内存泄漏，这就要先从内存的分配和回收说起了。\n\n## 内存的分配和回收\n\n先回顾一下，你还记得应用程序中，都有哪些方法来分配内存吗？用完后，又该怎么释放还给系统呢？\n\n前面讲进程的内存空间时，我曾经提到过，用户空间内存包括多个不同的内存段，比如只读段、数据段、堆、栈以及文件映射段等。这些内存段正是应用程序使用内存的基本方式。\n\n举个例子，你在程序中定义了一个局部变量，比如一个整数数组 *int data[64]* ，就定义了一个可以存储 64 个整数的内存段。由于这是一个局部变量，它会从内存空间的栈中分配内存。\n\n栈内存由系统自动分配和管理。一旦程序运行超出了这个局部变量的作用域，栈内存就会被系统自动回收，所以不会产生内存泄漏的问题。\n\n再比如，很多时候，我们事先并不知道数据大小，所以你就要用到标准库函数 *malloc()* *，* 在程序中动态分配内存。这时候，系统就会从内存空间的堆中分配内存。\n\n堆内存由应用程序自己来分配和管理。除非程序退出，这些堆内存并不会被系统自动释放，而是需要应用程序明确调用库函数 *free()* 来释放它们。如果应用程序没有正确释放堆内存，就会造成内存泄漏。\n\n这是两个栈和堆的例子，那么，其他内存段是否也会导致内存泄漏呢？经过我们前面的学习，这个问题并不难回答。\n\n- 只读段，包括程序的代码和常量，由于是只读的，不会再去分配新的内存，所以也不会产生内存泄漏。\n- 数据段，包括全局变量和静态变量，这些变量在定义时就已经确定了大小，所以也不会产生内存泄漏。\n- 最后一个内存映射段，包括动态链接库和共享内存，其中共享内存由程序动态分配和管理。所以，如果程序在分配后忘了回收，就会导致跟堆内存类似的泄漏问题。\n\n**内存泄漏的危害非常大，这些忘记释放的内存，不仅应用程序自己不能访问，系统也不能把它们再次分配给其他应用**。内存泄漏不断累积，甚至会耗尽系统内存。\n\n虽然，系统最终可以通过 OOM （Out of Memory）机制杀死进程，但进程在 OOM 前，可能已经引发了一连串的反应，导致严重的性能问题。\n\n比如，其他需要内存的进程，可能无法分配新的内存；内存不足，又会触发系统的缓存回收以及 SWAP 机制，从而进一步导致 I/O 的性能问题等等。\n\n内存泄漏的危害这么大，那我们应该怎么检测这种问题呢？特别是，如果你已经发现了内存泄漏，该如何定位和处理呢。\n\n接下来，我们就用一个计算斐波那契数列的案例，来看看内存泄漏问题的定位和处理方法。\n\n斐波那契数列是一个这样的数列：0、1、1、2、3、5、8…，也就是除了前两个数是 0 和 1，其他数都由前面两数相加得到，用数学公式来表示就是 F(n)=F(n-1)+F(n-2)，（n\u003e=2），F(0)=0, F(1)=1。\n\n## 案例\n\n今天的案例基于 Ubuntu 18.04，当然，同样适用其他的 Linux 系统。\n\n- 机器配置：2 CPU，8GB 内存\n- 预先安装 sysstat、Docker 以及 bcc 软件包，比如：\n\n```bash\n# install sysstat docker\nsudo apt-get install -y sysstat docker.io \n# Install bcc\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDD\necho \"deb https://repo.iovisor.org/apt/bionic bionic main\" | sudo tee /etc/apt/sources.list.d/iovisor.list\nsudo apt-get update\nsudo apt-get install -y bcc-tools libbcc-examples linux-headers-$(uname -r)\n```\n\n其中，sysstat 和 Docker 我们已经很熟悉了。sysstat 软件包中的 vmstat ，可以观察内存的变化情况；而 Docker 可以运行案例程序。\n\n[bcc](https://github.com/iovisor/bcc) 软件包前面也介绍过，它提供了一系列的 Linux 性能分析工具，常用来动态追踪进程和内核的行为。更多工作原理你先不用深究，后面学习我们会逐步接触。这里你只需要记住，按照上面步骤安装完后，它提供的所有工具都位于 /usr/share/bcc/tools 这个目录中。\n\n\u003e 注意：bcc-tools 需要内核版本为 4.1 或者更高，如果你使用的是 CentOS7，或者其他内核版本比较旧的系统，那么你需要手动[升级内核版本后再安装](https://github.com/iovisor/bcc/issues/462)。\n\n打开一个终端，SSH 登录到机器上，安装上述工具。\n\n同以前的案例一样，下面的所有命令都默认以 root 用户运行，如果你是用普通用户身份登陆系统，请运行 sudo su root 命令切换到 root 用户。\n\n如果安装过程中有什么问题，同样鼓励你先自己搜索解决，解决不了的，可以在留言区向我提问。如果你以前已经安装过了，就可以忽略这一点了。\n\n安装完成后，再执行下面的命令来运行案例：\n\n```shell\n$ docker run --name=app -itd feisky/app:mem-leak\n```\n\n案例成功运行后，你需要输入下面的命令，确认案例应用已经正常启动。如果一切正常，你应该可以看到下面这个界面：\n\n```ruby\n$ docker logs app\n2th =\u003e 1\n3th =\u003e 2\n4th =\u003e 3\n5th =\u003e 5\n6th =\u003e 8\n7th =\u003e 13\n```\n\n从输出中，我们可以发现，这个案例会输出斐波那契数列的一系列数值。实际上，这些数值每隔 1 秒输出一次。\n\n知道了这些，我们应该怎么检查内存情况，判断有没有泄漏发生呢？你首先想到的可能是 top 工具，不过，top 虽然能观察系统和进程的内存占用情况，但今天的案例并不适合。内存泄漏问题，我们更应该关注内存使用的变化趋势。\n\n所以，开头我也提到了，今天推荐的是另一个老熟人， vmstat 工具。\n\n运行下面的 vmstat ，等待一段时间，观察内存的变化情况。如果忘了 vmstat 里各指标的含义，记得复习前面内容，或者执行 man vmstat 查询。\n\n```scss\n# 每隔 3 秒输出一组数据\n$ vmstat 3\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\nr  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\nr  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n0  0      0 6601824  97620 1098784    0    0     0     0   62  322  0  0 100  0  0\n0  0      0 6601700  97620 1098788    0    0     0     0   57  251  0  0 100  0  0\n0  0      0 6601320  97620 1098788    0    0     0     3   52  306  0  0 100  0  0\n0  0      0 6601452  97628 1098788    0    0     0    27   63  326  0  0 100  0  0\n2  0      0 6601328  97628 1098788    0    0     0    44   52  299  0  0 100  0  0\n0  0      0 6601080  97628 1098792    0    0     0     0   56  285  0  0 100  0  0 \n```\n\n从输出中你可以看到，内存的 free 列在不停的变化，并且是下降趋势；而 buffer 和 cache 基本保持不变。\n\n未使用内存在逐渐减小，而 buffer 和 cache 基本不变，这说明，系统中使用的内存一直在升高。但这并不能说明有内存泄漏，因为应用程序运行中需要的内存也可能会增大。比如说，程序中如果用了一个动态增长的数组来缓存计算结果，占用内存自然会增长。\n\n那怎么确定是不是内存泄漏呢？或者换句话说，有没有简单方法找出让内存增长的进程，并定位增长内存用在哪儿呢？\n\n根据前面内容，你应该想到了用 top 或 ps 来观察进程的内存使用情况，然后找出内存使用一直增长的进程，最后再通过 pmap 查看进程的内存分布。\n\n但这种方法并不太好用，因为要判断内存的变化情况，还需要你写一个脚本，来处理 top 或者 ps 的输出。\n\n这里，我介绍一个专门用来检测内存泄漏的工具，memleak。memleak 可以跟踪系统或指定进程的内存分配、释放请求，然后定期输出一个未释放内存和相应调用栈的汇总情况（默认 5 秒）。\n\n当然，memleak 是 bcc 软件包中的一个工具，我们一开始就装好了，执行 */usr/share/bcc/tools/memleak* 就可以运行它。比如，我们运行下面的命令：\n\n```yaml\n# -a 表示显示每个内存分配请求的大小以及地址\n# -p 指定案例应用的 PID 号\n$ /usr/share/bcc/tools/memleak -a -p $(pidof app)\nWARNING: Couldn't find .text section in /app\nWARNING: BCC can't handle sym look ups for /app\n    addr = 7f8f704732b0 size = 8192\n    addr = 7f8f704772d0 size = 8192\n    addr = 7f8f704712a0 size = 8192\n    addr = 7f8f704752c0 size = 8192\n    32768 bytes in 4 allocations from stack\n        [unknown] [app]\n        [unknown] [app]\n        start_thread+0xdb [libpthread-2.27.so] \n```\n\n从 memleak 的输出可以看到，案例应用在不停地分配内存，并且这些分配的地址没有被回收。\n\n这里有一个问题，Couldn’t find .text section in /app，所以调用栈不能正常输出，最后的调用栈部分只能看到 [unknown] 的标志。\n\n为什么会有这个错误呢？实际上，这是由于案例应用运行在容器中导致的。memleak 工具运行在容器之外，并不能直接访问进程路径 /app。\n\n比方说，在终端中直接运行 ls 命令，你会发现，这个路径的确不存在：\n\n```bash\n$ ls /app\nls: cannot access '/app': No such file or directory\n```\n\n类似的问题，我在 CPU 模块中的 [perf 使用方法]中已经提到好几个解决思路。最简单的方法，就是在容器外部构建相同路径的文件以及依赖库。这个案例只有一个二进制文件，所以只要把案例应用的二进制文件放到 /app 路径中，就可以修复这个问题。\n\n比如，你可以运行下面的命令，把 app 二进制文件从容器中复制出来，然后重新运行 memleak 工具：\n\n```shell\n$ docker cp app:/app /app\n$ /usr/share/bcc/tools/memleak -p $(pidof app) -a\nAttaching to pid 12512, Ctrl+C to quit.\n[03:00:41] Top 10 stacks with outstanding allocations:\n    addr = 7f8f70863220 size = 8192\n    addr = 7f8f70861210 size = 8192\n    addr = 7f8f7085b1e0 size = 8192\n    addr = 7f8f7085f200 size = 8192\n    addr = 7f8f7085d1f0 size = 8192\n    40960 bytes in 5 allocations from stack\n        fibonacci+0x1f [app]\n        child+0x4f [app]\n        start_thread+0xdb [libpthread-2.27.so] \n```\n\n这一次，我们终于看到了内存分配的调用栈，原来是 fibonacci() 函数分配的内存没释放。\n\n定位了内存泄漏的来源，下一步自然就应该查看源码，想办法修复它。我们一起来看案例应用的源代码 [app.c](https://github.com/feiskyer/linux-perf-examples/blob/master/mem-leak/app.c)：\n\n```cpp\n$ docker exec app cat /app.c\n...\nlong long *fibonacci(long long *n0, long long *n1)\n{\n    // 分配 1024 个长整数空间方便观测内存的变化情况\n    long long *v = (long long *) calloc(1024, sizeof(long long));\n    *v = *n0 + *n1;\n    return v;\n} \n \nvoid *child(void *arg)\n{\n    long long n0 = 0;\n    long long n1 = 1;\n    long long *v = NULL;\n    for (int n = 2; n \u003e 0; n++) {\n        v = fibonacci(\u0026n0, \u0026n1);\n        n0 = n1;\n        n1 = *v;\n        printf(\"%dth =\u003e %lld\\n\", n, *v);\n        sleep(1);\n    }\n}\n... \n```\n\n你会发现， child() 调用了 fibonacci() 函数，但并没有释放 fibonacci() 返回的内存。所以，想要修复泄漏问题，在 child() 中加一个释放函数就可以了，比如：\n\n```cpp\nvoid *child(void *arg)\n{\n    ...\n    for (int n = 2; n \u003e 0; n++) {\n        v = fibonacci(\u0026n0, \u0026n1);\n        n0 = n1;\n        n1 = *v;\n        printf(\"%dth =\u003e %lld\\n\", n, *v);\n        free(v);    // 释放内存\n        sleep(1);\n    }\n} \n```\n\n我把修复后的代码放到了 [app-fix.c](https://github.com/feiskyer/linux-perf-examples/blob/master/mem-leak/app-fix.c)，也打包成了一个 Docker 镜像。你可以运行下面的命令，验证一下内存泄漏是否修复：\n\n```shell\n# 清理原来的案例应用\n$ docker rm -f app \n# 运行修复后的应用\n$ docker run --name=app -itd feisky/app:mem-leak-fix \n# 重新执行 memleak 工具检查内存泄漏情况\n$ /usr/share/bcc/tools/memleak -a -p $(pidof app)\nAttaching to pid 18808, Ctrl+C to quit.\n[10:23:18] Top 10 stacks with outstanding allocations:\n[10:23:23] Top 10 stacks with outstanding allocations:\n```\n\n现在，我们看到，案例应用已经没有遗留内存，证明我们的修复工作成功完成。\n\n## 小结\n\n总结一下今天的内容。\n\n应用程序可以访问的用户内存空间，由只读段、数据段、堆、栈以及文件映射段等组成。其中，堆内存和内存映射，需要应用程序来动态管理内存段，所以我们必须小心处理。不仅要会用标准库函数 *malloc()* 来动态分配内存，还要记得在用完内存后，调用库函数 _free() 来 _ 释放它们。\n\n今天的案例比较简单，只用加一个 *free()* 调用就能修复内存泄漏。不过，实际应用程序就复杂多了。比如说，\n\n- malloc() 和 free() 通常并不是成对出现，而是需要你，在每个异常处理路径和成功路径上都释放内存 。\n- 在多线程程序中，一个线程中分配的内存，可能会在另一个线程中访问和释放。\n- 更复杂的是，在第三方的库函数中，隐式分配的内存可能需要应用程序显式释放。\n\n所以，为了避免内存泄漏，最重要的一点就是养成良好的编程习惯，比如分配内存后，一定要先写好内存释放的代码，再去开发其他逻辑。还是那句话，有借有还，才能高效运转，再借不难。\n\n当然，如果已经完成了开发任务，你还可以用 memleak 工具，检查应用程序的运行中，内存是否泄漏。如果发现了内存泄漏情况，再根据 memleak 输出的应用程序调用栈，定位内存的分配位置，从而释放不再访问的内存。\n\n## 思考\n\n最后，给你留一个思考题。\n\n今天的案例，我们通过增加 *free()* 调用，释放函数 *fibonacci()* 分配的内存，修复了内存泄漏的问题。就这个案例而言，还有没有其他更好的修复方法呢？结合前面学习和你自己的工作经验，相信你一定能有更多更好的方案。\n\n欢迎留言和我讨论 ，写下你的答案和收获，也欢迎你把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 19 案例篇：为什么系统的Swap变高了（上）\n\n你好，我是倪朋飞。\n\n上一节，我通过一个斐波那契数列的案例，带你学习了内存泄漏的分析。如果在程序中直接或间接地分配了动态内存，你一定要记得释放掉它们，否则就会导致内存泄漏，严重时甚至会耗尽系统内存。\n\n不过，反过来讲，当发生了内存泄漏时，或者运行了大内存的应用程序，导致系统的内存资源紧张时，系统又会如何应对呢？\n\n在内存基础篇我们已经学过，这其实会导致两种可能结果，内存回收和 OOM 杀死进程。\n\n我们先来看后一个可能结果，内存资源紧张导致的 OOM（Out Of Memory），相对容易理解，指的是系统杀死占用大量内存的进程，释放这些内存，再分配给其他更需要的进程。\n\n这一点我们前面详细讲过，这里就不再重复了。\n\n接下来再看第一个可能的结果，内存回收，也就是系统释放掉可以回收的内存，比如我前面讲过的缓存和缓冲区，就属于可回收内存。它们在内存管理中，通常被叫做**文件页（**File-backed Page）。\n\n大部分文件页，都可以直接回收，以后有需要时，再从磁盘重新读取就可以了。而那些被应用程序修改过，并且暂时还没写入磁盘的数据（也就是脏页），就得先写入磁盘，然后才能进行内存释放。\n\n这些脏页，一般可以通过两种方式写入磁盘。\n\n- 可以在应用程序中，通过系统调用 fsync ，把脏页同步到磁盘中；\n- 也可以交给系统，由内核线程 pdflush 负责这些脏页的刷新。\n\n除了缓存和缓冲区，通过内存映射获取的文件映射页，也是一种常见的文件页。它也可以被释放掉，下次再访问的时候，从文件重新读取。\n\n除了文件页外，还有没有其他的内存可以回收呢？比如，应用程序动态分配的堆内存，也就是我们在内存管理中说到的**匿名页**（Anonymous Page），是不是也可以回收呢？\n\n我想，你肯定会说，它们很可能还要再次被访问啊，当然不能直接回收了。非常正确，这些内存自然不能直接释放。\n\n但是，如果这些内存在分配后很少被访问，似乎也是一种资源浪费。是不是可以把它们暂时先存在磁盘里，释放内存给其他更需要的进程？\n\n其实，这正是 Linux 的 Swap 机制。Swap 把这些不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。\n\n在前几节的案例中，我们已经分别学过缓存和 OOM 的原理和分析。那 Swap 又是怎么工作的呢？因为内容比较多，接下来，我将用两节课的内容，带你探索 Swap 的工作原理，以及 Swap 升高后的分析方法。\n\n今天我们先来看看，Swap 究竟是怎么工作的。\n\n## Swap 原理\n\n前面提到，Swap 说白了就是把一块磁盘空间或者一个本地文件（以下讲解以磁盘为例），当成内存来使用。它包括换出和换入两个过程。\n\n- 所谓换出，就是把进程暂时不用的内存数据存储到磁盘中，并释放这些数据占用的内存。\n- 而换入，则是在进程再次访问这些内存的时候，把它们从磁盘读到内存中来。\n\n所以你看，Swap 其实是把系统的可用内存变大了。这样，即使服务器的内存不足，也可以运行大内存的应用程序。\n\n还记得我最早学习 Linux 操作系统时，内存实在太贵了，一个普通学生根本就用不起大的内存，那会儿我就是开启了 Swap 来运行 Linux 桌面。当然，现在的内存便宜多了，服务器一般也会配置很大的内存，那是不是说 Swap 就没有用武之地了呢？\n\n当然不是。事实上，内存再大，对应用程序来说，也有不够用的时候。\n\n一个很典型的场景就是，即使内存不足时，有些应用程序也并不想被 OOM 杀死，而是希望能缓一段时间，等待人工介入，或者等系统自动释放其他进程的内存，再分配给它。\n\n除此之外，我们常见的笔记本电脑的休眠和快速开机的功能，也基于 Swap 。休眠时，把系统的内存存入磁盘，这样等到再次开机时，只要从磁盘中加载内存就可以。这样就省去了很多应用程序的初始化过程，加快了开机速度。\n\n话说回来，既然 Swap 是为了回收内存，那么 Linux 到底在什么时候需要回收内存呢？前面一直在说内存资源紧张，又该怎么来衡量内存是不是紧张呢？\n\n一个最容易想到的场景就是，有新的大块内存分配请求，但是剩余内存不足。这个时候系统就需要回收一部分内存（比如前面提到的缓存），进而尽可能地满足新内存请求。这个过程通常被称为**直接内存回收**。\n\n除了直接内存回收，还有一个专门的内核线程用来定期回收内存，也就是**kswapd0**。为了衡量内存的使用情况，kswapd0 定义了三个内存阈值（watermark，也称为水位），分别是\n\n页最小阈值（pages_min）、页低阈值（pages_low）和页高阈值（pages_high）。剩余内存，则使用 pages_free 表示。\n\n这里，我画了一张图表示它们的关系。\n\n![img](c1054f1e71037795c6f290e670b29120.png)\n\nkswapd0 定期扫描内存的使用情况，并根据剩余内存落在这三个阈值的空间位置，进行内存的回收操作。\n\n- 剩余内存小于**页最小阈值**，说明进程可用内存都耗尽了，只有内核才可以分配内存。\n- 剩余内存落在**页最小阈值**和**页低阈值**中间，说明内存压力比较大，剩余内存不多了。这时 kswapd0 会执行内存回收，直到剩余内存大于高阈值为止。\n- 剩余内存落在**页低阈值**和**页高阈值**中间，说明内存有一定压力，但还可以满足新内存请求。\n- 剩余内存大于**页高阈值**，说明剩余内存比较多，没有内存压力。\n\n我们可以看到，一旦剩余内存小于页低阈值，就会触发内存的回收。这个页低阈值，其实可以通过内核选项 /proc/sys/vm/min_free_kbytes 来间接设置。min_free_kbytes 设置了页最小阈值，而其他两个阈值，都是根据页最小阈值计算生成的，计算方法如下 ：\n\n```ini\npages_low = pages_min*5/4\npages_high = pages_min*3/2\n```\n\n## NUMA 与 Swap\n\n很多情况下，你明明发现了 Swap 升高，可是在分析系统的内存使用时，却很可能发现，系统剩余内存还多着呢。为什么剩余内存很多的情况下，也会发生 Swap 呢？\n\n看到上面的标题，你应该已经想到了，这正是处理器的 NUMA （Non-Uniform Memory Access）架构导致的。\n\n关于 NUMA，我在 CPU 模块中曾简单提到过。在 NUMA 架构下，多个处理器被划分到不同 Node 上，且每个 Node 都拥有自己的本地内存空间。\n\n而同一个 Node 内部的内存空间，实际上又可以进一步分为不同的内存域（Zone），比如直接内存访问区（DMA）、普通内存区（NORMAL）、伪内存区（MOVABLE）等，如下图所示：\n\n![img](be6cabdecc2ec98893f67ebd5b9aead9.png)\n\n先不用特别关注这些内存域的具体含义，我们只要会查看阈值的配置，以及缓存、匿名页的实际使用情况就够了。\n\n既然 NUMA 架构下的每个 Node 都有自己的本地内存空间，那么，在分析内存的使用时，我们也应该针对每个 Node 单独分析。\n\n你可以通过 numactl 命令，来查看处理器在 Node 的分布情况，以及每个 Node 的内存使用情况。比如，下面就是一个 numactl 输出的示例：\n\n```yaml\n$ numactl --hardware\navailable: 1 nodes (0)\nnode 0 cpus: 0 1\nnode 0 size: 7977 MB\nnode 0 free: 4416 MB\n...\n```\n\n这个界面显示，我的系统中只有一个 Node，也就是 Node 0 ，而且编号为 0 和 1 的两个 CPU， 都位于 Node 0 上。另外，Node 0 的内存大小为 7977 MB，剩余内存为 4416 MB。\n\n了解了 NUNA 的架构和 NUMA 内存的查看方法后，你可能就要问了这跟 Swap 有什么关系呢？\n\n实际上，前面提到的三个内存阈值（页最小阈值、页低阈值和页高阈值），都可以通过内存域在 proc 文件系统中的接口 /proc/zoneinfo 来查看。\n\n比如，下面就是一个 /proc/zoneinfo 文件的内容示例：\n\n```python-repl\n$ cat /proc/zoneinfo\n...\nNode 0, zone   Normal\n pages free     227894\n       min      14896\n       low      18620\n       high     22344\n...\n     nr_free_pages 227894\n     nr_zone_inactive_anon 11082\n     nr_zone_active_anon 14024\n     nr_zone_inactive_file 539024\n     nr_zone_active_file 923986\n...\n```\n\n这个输出中有大量指标，我来解释一下比较重要的几个。\n\n- pages 处的 min、low、high，就是上面提到的三个内存阈值，而 free 是剩余内存页数，它跟后面的 nr_free_pages 相同。\n- nr_zone_active_anon 和 nr_zone_inactive_anon，分别是活跃和非活跃的匿名页数。\n- nr_zone_active_file 和 nr_zone_inactive_file，分别是活跃和非活跃的文件页数。\n\n从这个输出结果可以发现，剩余内存远大于页高阈值，所以此时的 kswapd0 不会回收内存。\n\n当然，某个 Node 内存不足时，系统可以从其他 Node 寻找空闲内存，也可以从本地内存中回收内存。具体选哪种模式，你可以通过 /proc/sys/vm/zone_reclaim_mode 来调整。它支持以下几个选项：\n\n- 默认的 0 ，也就是刚刚提到的模式，表示既可以从其他 Node 寻找空闲内存，也可以从本地回收内存。\n- 1、2、4 都表示只回收本地内存，2 表示可以回写脏数据回收内存，4 表示可以用 Swap 方式回收内存。\n\n## swappiness\n\n到这里，我们就可以理解内存回收的机制了。这些回收的内存既包括了文件页，又包括了匿名页。\n\n- 对文件页的回收，当然就是直接回收缓存，或者把脏页写回磁盘后再回收。\n- 而对匿名页的回收，其实就是通过 Swap 机制，把它们写入磁盘后再释放内存。\n\n不过，你可能还有一个问题。既然有两种不同的内存回收机制，那么在实际回收内存时，到底该先回收哪一种呢？\n\n其实，Linux 提供了一个 /proc/sys/vm/swappiness 选项，用来调整使用 Swap 的积极程度。\n\nswappiness 的范围是 0-100，数值越大，越积极使用 Swap，也就是更倾向于回收匿名页；数值越小，越消极使用 Swap，也就是更倾向于回收文件页。\n\n虽然 swappiness 的范围是 0-100，不过要注意，这并不是内存的百分比，而是调整 Swap 积极程度的权重，即使你把它设置成 0，当[剩余内存 + 文件页小于页高阈值](https://www.kernel.org/doc/Documentation/sysctl/vm.txt)时，还是会发生 Swap。\n\n清楚了 Swap 原理后，当遇到 Swap 使用变高时，又该怎么定位、分析呢？别急，下一节，我们将用一个案例来探索实践。\n\n## 小结\n\n在内存资源紧张时，Linux 通过直接内存回收和定期扫描的方式，来释放文件页和匿名页，以便把内存分配给更需要的进程使用。\n\n- 文件页的回收比较容易理解，直接清空，或者把脏数据写回磁盘后再释放。\n- 而对匿名页的回收，需要通过 Swap 换出到磁盘中，下次访问时，再从磁盘换入到内存中。\n\n你可以设置 /proc/sys/vm/min_free_kbytes，来调整系统定期回收内存的阈值（也就是页低阈值），还可以设置 /proc/sys/vm/swappiness，来调整文件页和匿名页的回收倾向。\n\n在 NUMA 架构下，每个 Node 都有自己的本地内存空间，而当本地内存不足时，默认既可以从其他 Node 寻找空闲内存，也可以从本地内存回收。\n\n你可以设置 /proc/sys/vm/zone_reclaim_mode ，来调整 NUMA 本地内存的回收策略。\n\n## 思考\n\n最后，我想请你一起来聊聊你理解的 SWAP。我估计你以前已经碰到过 Swap 导致的性能问题，你是怎么分析这些问题的呢？你可以结合今天讲的 Swap 原理，记录自己的操作步骤，总结自己的解决思路。\n\n欢迎在留言区和我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 20 案例篇：为什么系统的Swap变高了？（下）\n\n你好，我是倪朋飞。\n\n上一节我们详细学习了 Linux 内存回收，特别是 Swap 的原理，先简单回顾一下。\n\n在内存资源紧张时，Linux 通过直接内存回收和定期扫描的方式，来释放文件页和匿名页，以便把内存分配给更需要的进程使用。\n\n- 文件页的回收比较容易理解，直接清空缓存，或者把脏数据写回磁盘后，再释放缓存就可以了。\n- 而对不常访问的匿名页，则需要通过 Swap 换出到磁盘中，这样在下次访问的时候，再次从磁盘换入到内存中就可以了。\n\n开启 Swap 后，你可以设置 /proc/sys/vm/min_free_kbytes ，来调整系统定期回收内存的阈值，也可以设置 /proc/sys/vm/swappiness ，来调整文件页和匿名页的回收倾向。\n\n那么，当 Swap 使用升高时，要如何定位和分析呢？下面，我们就来看一个磁盘 I/O 的案例，实战分析和演练。\n\n## 案例\n\n下面案例基于 Ubuntu 18.04，同样适用于其他的 Linux 系统。\n\n- 机器配置：2 CPU，8GB 内存\n- 你需要预先安装 sysstat 等工具，如 apt install sysstat\n\n首先，我们打开两个终端，分别 SSH 登录到两台机器上，并安装上面提到的这些工具。\n\n同以前的案例一样，接下来的所有命令都默认以 root 用户运行，如果你是用普通用户身份登陆系统，请运行 sudo su root 命令切换到 root 用户。\n\n如果安装过程中有什么问题，同样鼓励你先自己搜索解决，解决不了的，可以在留言区向我提问。\n\n然后，在终端中运行 free 命令，查看 Swap 的使用情况。比如，在我的机器中，输出如下：\n\n```makefile\n$ free\n             total        used        free      shared  buff/cache   available\nMem:        8169348      331668     6715972         696     1121708     7522896\nSwap:             0           0           0\n```\n\n从这个 free 输出你可以看到，Swap 的大小是 0，这说明我的机器没有配置 Swap。\n\n为了继续 Swap 的案例， 就需要先配置、开启 Swap。如果你的环境中已经开启了 Swap，那你可以略过下面的开启步骤，继续往后走。\n\n要开启 Swap，我们首先要清楚，Linux 本身支持两种类型的 Swap，即 Swap 分区和 Swap 文件。以 Swap 文件为例，在第一个终端中运行下面的命令开启 Swap，我这里配置 Swap 文件的大小为 8GB：\n\n```shell\n# 创建 Swap 文件\n$ fallocate -l 8G /mnt/swapfile\n# 修改权限只有根用户可以访问\n$ chmod 600 /mnt/swapfile\n# 配置 Swap 文件\n$ mkswap /mnt/swapfile\n# 开启 Swap\n$ swapon /mnt/swapfile\n```\n\n然后，再执行 free 命令，确认 Swap 配置成功：\n\n```makefile\n$ free\n             total        used        free      shared  buff/cache   available\nMem:        8169348      331668     6715972         696     1121708     7522896\nSwap:       8388604           0     8388604\n```\n\n现在，free 输出中，Swap 空间以及剩余空间都从 0 变成了 8GB，说明 Swap 已经**正常开启**。\n\n接下来，我们在第一个终端中，运行下面的 dd 命令，模拟大文件的读取：\n\n```shell\n# 写入空设备，实际上只有磁盘的读请求\n$ dd if=/dev/sda1 of=/dev/null bs=1G count=2048\n```\n\n接着，在第二个终端中运行 sar 命令，查看内存各个指标的变化情况。你可以多观察一会儿，查看这些指标的变化情况。\n\n```makefile\n# 间隔 1 秒输出一组数据\n# -r 表示显示内存使用情况，-S 表示显示 Swap 使用情况\n$ sar -r -S 1\n04:39:56    kbmemfree   kbavail kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty\n04:39:57      6249676   6839824   1919632     23.50    740512     67316   1691736     10.22    815156    841868         4 \n04:39:56    kbswpfree kbswpused  %swpused  kbswpcad   %swpcad\n04:39:57      8388604         0      0.00         0      0.00 \n04:39:57    kbmemfree   kbavail kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty\n04:39:58      6184472   6807064   1984836     24.30    772768     67380   1691736     10.22    847932    874224        20 \n04:39:57    kbswpfree kbswpused  %swpused  kbswpcad   %swpcad\n04:39:58      8388604         0      0.00         0      0.00 \n… \n \n04:44:06    kbmemfree   kbavail kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty\n04:44:07       152780   6525716   8016528     98.13   6530440     51316   1691736     10.22    867124   6869332         0 \n04:44:06    kbswpfree kbswpused  %swpused  kbswpcad   %swpcad\n04:44:07      8384508      4096      0.05        52      1.27\n```\n\n我们可以看到，sar 的输出结果是两个表格，第一个表格表示内存的使用情况，第二个表格表示 Swap 的使用情况。其中，各个指标名称前面的 kb 前缀，表示这些指标的单位是 KB。\n\n去掉前缀后，你会发现，大部分指标我们都已经见过了，剩下的几个新出现的指标，我来简单介绍一下。\n\n- kbcommit，表示当前系统负载需要的内存。它实际上是为了保证系统内存不溢出，对需要内存的估计值。%commit，就是这个值相对总内存的百分比。\n- kbactive，表示活跃内存，也就是最近使用过的内存，一般不会被系统回收。\n- kbinact，表示非活跃内存，也就是不常访问的内存，有可能会被系统回收。\n\n清楚了界面指标的含义后，我们再结合具体数值，来分析相关的现象。你可以清楚地看到，总的内存使用率（%memused）在不断增长，从开始的 23% 一直长到了 98%，并且主要内存都被缓冲区（kbbuffers）占用。具体来说：\n\n- 刚开始，剩余内存（kbmemfree）不断减少，而缓冲区（kbbuffers）则不断增大，由此可知，剩余内存不断分配给了缓冲区。\n- 一段时间后，剩余内存已经很小，而缓冲区占用了大部分内存。这时候，Swap 的使用开始逐渐增大，缓冲区和剩余内存则只在小范围内波动。\n\n你可能困惑了，为什么缓冲区在不停增大？这又是哪些进程导致的呢？\n\n显然，我们还得看看进程缓存的情况。在前面缓存的案例中我们学过， cachetop 正好能满足这一点。那我们就来 cachetop 一下。\n\n在第二个终端中，按下 Ctrl+C 停止 sar 命令，然后运行下面的 cachetop 命令，观察缓存的使用情况：\n\n```objectivec\n$ cachetop 5\n12:28:28 Buffers MB: 6349 / Cached MB: 87 / Sort: HITS / Order: ascending\nPID      UID      CMD              HITS     MISSES   DIRTIES  READ_HIT%  WRITE_HIT%\n   18280 root     python                 22        0        0     100.0%       0.0%\n   18279 root     dd                  41088    41022        0      50.0%      50.0%\n```\n\n通过 cachetop 的输出，我们看到，dd 进程的读写请求只有 50% 的命中率，并且未命中的缓存页数（MISSES）为 41022（单位是页）。这说明，正是案例开始时运行的 dd，导致了缓冲区使用升高。\n\n你可能接着会问，为什么 Swap 也跟着升高了呢？直观来说，缓冲区占了系统绝大部分内存，还属于可回收内存，内存不够用时，不应该先回收缓冲区吗？\n\n这种情况，我们还得进一步通过 /proc/zoneinfo ，观察剩余内存、内存阈值以及匿名页和文件页的活跃情况。\n\n你可以在第二个终端中，按下 Ctrl+C，停止 cachetop 命令。然后运行下面的命令，观察 /proc/zoneinfo 中这几个指标的变化情况：\n\n```ruby\n# -d 表示高亮变化的字段\n# -A 表示仅显示 Normal 行以及之后的 15 行输出\n$ watch -d grep -A 15 'Normal' /proc/zoneinfo\nNode 0, zone   Normal\n  pages free     21328\n        min      14896\n        low      18620\n        high     22344\n        spanned  1835008\n        present  1835008\n        managed  1796710\n        protection: (0, 0, 0, 0, 0)\n      nr_free_pages 21328\n      nr_zone_inactive_anon 79776\n      nr_zone_active_anon 206854\n      nr_zone_inactive_file 918561\n      nr_zone_active_file 496695\n      nr_zone_unevictable 2251\n      nr_zone_write_pending 0\n```\n\n你可以发现，剩余内存（pages_free）在一个小范围内不停地波动。当它小于页低阈值（pages_low) 时，又会突然增大到一个大于页高阈值（pages_high）的值。\n\n再结合刚刚用 sar 看到的剩余内存和缓冲区的变化情况，我们可以推导出，剩余内存和缓冲区的波动变化，正是由于内存回收和缓存再次分配的循环往复。\n\n- 当剩余内存小于页低阈值时，系统会回收一些缓存和匿名内存，使剩余内存增大。其中，缓存的回收导致 sar 中的缓冲区减小，而匿名内存的回收导致了 Swap 的使用增大。\n- 紧接着，由于 dd 还在继续，剩余内存又会重新分配给缓存，导致剩余内存减少，缓冲区增大。\n\n其实还有一个有趣的现象，如果多次运行 dd 和 sar，你可能会发现，在多次的循环重复中，有时候是 Swap 用得比较多，有时候 Swap 很少，反而缓冲区的波动更大。\n\n换句话说，系统回收内存时，有时候会回收更多的文件页，有时候又回收了更多的匿名页。\n\n显然，系统回收不同类型内存的倾向，似乎不那么明显。你应该想到了上节课提到的 swappiness，正是调整不同类型内存回收的配置选项。\n\n还是在第二个终端中，按下 Ctrl+C 停止 watch 命令，然后运行下面的命令，查看 swappiness 的配置：\n\n```shell\n$ cat /proc/sys/vm/swappiness\n60\n```\n\nswappiness 显示的是默认值 60，这是一个相对中和的配置，所以系统会根据实际运行情况，选择合适的回收类型，比如回收不活跃的匿名页，或者不活跃的文件页。\n\n到这里，我们已经找出了 Swap 发生的根源。另一个问题就是，刚才的 Swap 到底影响了哪些应用程序呢？换句话说，Swap 换出的是哪些进程的内存？\n\n这里我还是推荐 proc 文件系统，用来查看进程 Swap 换出的虚拟内存大小，它保存在 /proc/pid/status 中的 VmSwap 中（推荐你执行 man proc 来查询其他字段的含义）。\n\n在第二个终端中运行下面的命令，就可以查看使用 Swap 最多的进程。注意 for、awk、sort 都是最常用的 Linux 命令，如果你还不熟悉，可以用 man 来查询它们的手册，或上网搜索教程来学习。\n\n```shell\n# 按 VmSwap 使用量对进程排序，输出进程名称、进程 ID 以及 SWAP 用量\n$ for file in /proc/*/status ; do awk '/VmSwap|Name|^Pid/{printf $2 \" \" $3}END{ print \"\"}' $file; done | sort -k 3 -n -r | head\ndockerd 2226 10728 kB\ndocker-containe 2251 8516 kB\nsnapd 936 4020 kB\nnetworkd-dispat 911 836 kB\npolkitd 1004 44 kB\n```\n\n从这里你可以看到，使用 Swap 比较多的是 dockerd 和 docker-containe 进程，所以，当 dockerd 再次访问这些换出到磁盘的内存时，也会比较慢。\n\n这也说明了一点，虽然缓存属于可回收内存，但在类似大文件拷贝这类场景下，系统还是会用 Swap 机制来回收匿名内存，而不仅仅是回收占用绝大部分内存的文件页。\n\n最后，如果你在一开始配置了 Swap，不要忘记在案例结束后关闭。你可以运行下面的命令，关闭 Swap：\n\n```ruby\n$ swapoff -a\n```\n\n实际上，关闭 Swap 后再重新打开，也是一种常用的 Swap 空间清理方法，比如：\n\n```ruby\n$ swapoff -a \u0026\u0026 swapon -a \n```\n\n## 小结\n\n在内存资源紧张时，Linux 会通过 Swap ，把不常访问的匿名页换出到磁盘中，下次访问的时候再从磁盘换入到内存中来。你可以设置 /proc/sys/vm/min_free_kbytes，来调整系统定期回收内存的阈值；也可以设置 /proc/sys/vm/swappiness，来调整文件页和匿名页的回收倾向。\n\n当 Swap 变高时，你可以用 sar、/proc/zoneinfo、/proc/pid/status 等方法，查看系统和进程的内存使用情况，进而找出 Swap 升高的根源和受影响的进程。\n\n反过来说，通常，降低 Swap 的使用，可以提高系统的整体性能。要怎么做呢？这里，我也总结了几种常见的降低方法。\n\n- 禁止 Swap，现在服务器的内存足够大，所以除非有必要，禁用 Swap 就可以了。随着云计算的普及，大部分云平台中的虚拟机都默认禁止 Swap。\n- 如果实在需要用到 Swap，可以尝试降低 swappiness 的值，减少内存回收时 Swap 的使用倾向。\n- 响应延迟敏感的应用，如果它们可能在开启 Swap 的服务器中运行，你还可以用库函数 mlock() 或者 mlockall() 锁定内存，阻止它们的内存换出。\n\n## 思考\n\n最后，给你留一个思考题。\n\n今天的案例中，swappiness 使用的是默认配置的 60。如果把它配置成 0 的话，还会发生 Swap 吗？这又是为什么呢？\n\n希望你可以实际操作一下，重点观察 sar 的输出，并结合今天的内容来记录、总结。\n\n欢迎留言和我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 21 套路篇：如何“快准狠”找到系统内存的问题？\n\n你好，我是倪朋飞。\n\n前几节，通过几个案例，我们分析了各种常见的内存性能问题。我相信通过它们，你对内存的性能分析已经有了基本的思路，也熟悉了很多分析内存性能的工具。你肯定会想，有没有迅速定位内存问题的方法？当定位出内存的瓶颈后，又有哪些优化内存的思路呢？\n\n今天，我就来帮你梳理一下，怎样可以快速定位系统内存，并且总结了相关的解决思路。\n\n## 内存性能指标\n\n为了分析内存的性能瓶颈，首先你要知道，怎样衡量内存的性能，也就是性能指标问题。我们先来回顾一下，前几节学过的内存性能指标。\n\n你可以自己先找张纸，凭着记忆写一写；或者打开前面的文章，自己总结一下。\n\n首先，你最容易想到的是系统内存使用情况，比如已用内存、剩余内存、共享内存、可用内存、缓存和缓冲区的用量等。\n\n- 已用内存和剩余内存很容易理解，就是已经使用和还未使用的内存。\n- 共享内存是通过 tmpfs 实现的，所以它的大小也就是 tmpfs 使用的内存大小。tmpfs 其实也是一种特殊的缓存。\n- 可用内存是新进程可以使用的最大内存，它包括剩余内存和可回收缓存。\n- 缓存包括两部分，一部分是磁盘读取文件的页缓存，用来缓存从磁盘读取的数据，可以加快以后再次访问的速度。另一部分，则是 Slab 分配器中的可回收内存。\n- 缓冲区是对原始磁盘块的临时存储，用来缓存将要写入磁盘的数据。这样，内核就可以把分散的写集中起来，统一优化磁盘写入。\n\n第二类很容易想到的，应该是进程内存使用情况，比如进程的虚拟内存、常驻内存、共享内存以及 Swap 内存等。\n\n- 虚拟内存，包括了进程代码段、数据段、共享内存、已经申请的堆内存和已经换出的内存等。这里要注意，已经申请的内存，即使还没有分配物理内存，也算作虚拟内存。\n- 常驻内存是进程实际使用的物理内存，不过，它不包括 Swap 和共享内存。\n- 共享内存，既包括与其他进程共同使用的真实的共享内存，还包括了加载的动态链接库以及程序的代码段等。\n- Swap 内存，是指通过 Swap 换出到磁盘的内存。\n\n当然，这些指标中，常驻内存一般会换算成占系统总内存的百分比，也就是进程的内存使用率。\n\n除了这些很容易想到的指标外，我还想再强调一下，缺页异常。\n\n在内存分配的原理中，我曾经讲到过，系统调用内存分配请求后，并不会立刻为其分配物理内存，而是在请求首次访问时，通过缺页异常来分配。缺页异常又分为下面两种场景。\n\n- 可以直接从物理内存中分配时，被称为次缺页异常。\n- 需要磁盘 I/O 介入（比如 Swap）时，被称为主缺页异常。\n\n显然，主缺页异常升高，就意味着需要磁盘 I/O，那么内存访问也会慢很多。\n\n除了系统内存和进程内存，第三类重要指标就是 Swap 的使用情况，比如 Swap 的已用空间、剩余空间、换入速度和换出速度等。\n\n- 已用空间和剩余空间很好理解，就是字面上的意思，已经使用和没有使用的内存空间。\n- 换入和换出速度，则表示每秒钟换入和换出内存的大小。\n\n这些内存的性能指标都需要我们熟记并且会用，我把它们汇总成了一个思维导图，你可以保存打印出来，或者自己仿照着总结一份。\n\n![img](e28cf90f0b137574bca170984d1e6736.png)\n\n## 内存性能工具\n\n了解了内存的性能指标，我们还得知道，怎么才能获得这些指标，也就是会用性能工具。这里，我们也用同样的方法，回顾一下前面案例中已经用到的各种内存性能工具。 还是鼓励你先自己回忆和总结一下。\n\n首先，你应该注意到了，所有的案例中都用到了 free。这是个最常用的内存工具，可以查看系统的整体内存和 Swap 使用情况。相对应的，你可以用 top 或 ps，查看进程的内存使用情况。\n\n然后，在缓存和缓冲区的原理篇中，我们通过 proc 文件系统，找到了内存指标的来源；并通过 vmstat，动态观察了内存的变化情况。与 free 相比，vmstat 除了可以动态查看内存变化，还可以区分缓存和缓冲区、Swap 换入和换出的内存大小。\n\n接着，在缓存和缓冲区的案例篇中，为了弄清楚缓存的命中情况，我们又用了 cachestat ，查看整个系统缓存的读写命中情况，并用 cachetop 来观察每个进程缓存的读写命中情况。\n\n再接着，在内存泄漏的案例中，我们用 vmstat，发现了内存使用在不断增长，又用 memleak，确认发生了内存泄漏。通过 memleak 给出的内存分配栈，我们找到了内存泄漏的可疑位置。\n\n最后，在 Swap 的案例中，我们用 sar 发现了缓冲区和 Swap 升高的问题。通过 cachetop，我们找到了缓冲区升高的根源；通过对比剩余内存跟 /proc/zoneinfo 的内存阈，我们发现 Swap 升高是内存回收导致的。案例最后，我们还通过 /proc 文件系统，找出了 Swap 所影响的进程。\n\n到这里，你是不是再次感觉到了来自性能世界的“恶意”。性能工具怎么那么多呀？其实，还是那句话，理解内存的工作原理，结合性能指标来记忆，拿下工具的使用方法并不难。\n\n## 性能指标和工具的联系\n\n同 CPU 性能分析一样，我的经验是两个不同维度出发，整理和记忆。\n\n- 从内存指标出发，更容易把工具和内存的工作原理关联起来。\n- 从性能工具出发，可以更快地利用工具，找出我们想观察的性能指标。特别是在工具有限的情况下，我们更得充分利用手头的每一个工具，挖掘出更多的问题。\n\n同样的，根据内存性能指标和工具的对应关系，我做了两个表格，方便你梳理关系和理解记忆。当然，你也可以当成“指标工具”和“工具指标”指南来用，在需要时直接查找。\n\n第一个表格，从内存指标出发，列举了哪些性能工具可以提供这些指标。这样，在实际排查性能问题时，你就可以清楚知道，究竟要用什么工具来辅助分析，提供你想要的指标。\n\n![img](8f477035fc4348a1f80bde3117a7dfed.png)\n\n第二个表格，从性能工具出发，整理了这些常见工具能提供的内存指标。掌握了这个表格，你可以最大化利用已有的工具，尽可能多地找到你要的指标。\n\n这些工具的具体使用方法并不用背，你只要知道有哪些可用的工具，以及这些工具提供的基本指标。真正用到时， man 一下查它们的使用手册就可以了。\n\n![img](52bb55fba133401889206d02c224769b.png)\n\n## 如何迅速分析内存的性能瓶颈\n\n我相信到这一步，你对内存的性能指标已经非常熟悉，也清楚每种性能指标分别能用什么工具来获取。\n\n那是不是说，每次碰到内存性能问题，你都要把上面这些工具全跑一遍，然后再把所有内存性能指标全分析一遍呢？\n\n自然不是。前面的 CPU 性能篇我们就说过，简单查找法，虽然是有用的，也很可能找到某些系统潜在瓶颈。但是这种方法的低效率和大工作量，让我们首先拒绝了这种方法。\n\n还是那句话，在实际生产环境中，我们希望的是，尽可能**快**地定位系统瓶颈，然后尽可能**快**地优化性能，也就是要又快又准地解决性能问题。\n\n那有没有什么方法，可以又快又准地分析出系统的内存问题呢？\n\n方法当然有。还是那个关键词，找关联。其实，虽然内存的性能指标很多，但都是为了描述内存的原理，指标间自然不会完全孤立，一般都会有关联。当然，反过来说，这些关联也正是源于系统的内存原理，这也是我总强调基础原理的重要性，并在文章中穿插讲解。\n\n举个最简单的例子，当你看到系统的剩余内存很低时，是不是就说明，进程一定不能申请分配新内存了呢？当然不是，因为进程可以使用的内存，除了剩余内存，还包括了可回收的缓存和缓冲区。\n\n所以，**为了迅速定位内存问题，我通常会先运行几个覆盖面比较大的性能工具，比如 free、top、vmstat、pidstat 等**。\n\n具体的分析思路主要有这几步。\n\n1. 先用 free 和 top，查看系统整体的内存使用情况。\n2. 再用 vmstat 和 pidstat，查看一段时间的趋势，从而判断出内存问题的类型。\n3. 最后进行详细分析，比如内存分配分析、缓存 / 缓冲区分析、具体进程的内存使用分析等。\n\n同时，我也把这个分析过程画成了一张流程图，你可以保存并打印出来使用。\n\n![img](d79cd017f0c90b84a36e70a3c5dccffe.png)\n\n图中列出了最常用的几个内存工具，和相关的分析流程。其中，箭头表示分析的方向，举几个例子你可能会更容易理解。\n\n第一个例子，当你通过 free，发现大部分内存都被缓存占用后，可以使用 vmstat 或者 sar 观察一下缓存的变化趋势，确认缓存的使用是否还在继续增大。\n\n如果继续增大，则说明导致缓存升高的进程还在运行，那你就能用缓存 / 缓冲区分析工具（比如 cachetop、slabtop 等），分析这些缓存到底被哪里占用。\n\n第二个例子，当你 free 一下，发现系统可用内存不足时，首先要确认内存是否被缓存 / 缓冲区占用。排除缓存 / 缓冲区后，你可以继续用 pidstat 或者 top，定位占用内存最多的进程。\n\n找出进程后，再通过进程内存空间工具（比如 pmap），分析进程地址空间中内存的使用情况就可以了。\n\n第三个例子，当你通过 vmstat 或者 sar 发现内存在不断增长后，可以分析中是否存在内存泄漏的问题。\n\n比如你可以使用内存分配分析工具 memleak ，检查是否存在内存泄漏。如果存在内存泄漏问题，memleak 会为你输出内存泄漏的进程以及调用堆栈。\n\n注意，这个图里我没有列出所有性能工具，只给出了最核心的几个。这么做，一方面，确实不想让大量的工具列表吓到你。\n\n另一方面，希望你能把重心先放在核心工具上，通过我提供的案例和真实环境的实践，掌握使用方法和分析思路。 毕竟熟练掌握它们，你就可以解决大多数的内存问题。\n\n## 小结\n\n在今天的文章中，我带你回顾了常见的内存性能指标，梳理了常见的内存性能分析工具，最后还总结了快速分析内存问题的思路。\n\n虽然内存的性能指标和性能工具都挺多，但理解了内存管理的基本原理后，你会发现它们其实都有一定的关联。梳理出它们的关系，掌握内存分析的套路并不难。\n\n找到内存问题的来源后，下一步就是相应的优化工作了。在我看来，内存调优最重要的就是，保证应用程序的热点数据放到内存中，并尽量减少换页和交换。\n\n常见的优化思路有这么几种。\n\n1. 最好禁止 Swap。如果必须开启 Swap，降低 swappiness 的值，减少内存回收时 Swap 的使用倾向。\n2. 减少内存的动态分配。比如，可以使用内存池、大页（HugePage）等。\n3. 尽量使用缓存和缓冲区来访问数据。比如，可以使用堆栈明确声明内存空间，来存储需要缓存的数据；或者用 Redis 这类的外部缓存组件，优化数据的访问。\n4. 使用 cgroups 等方式限制进程的内存使用情况。这样，可以确保系统内存不会被异常进程耗尽。\n5. 通过 /proc/pid/oom_adj ，调整核心应用的 oom_score。这样，可以保证即使内存紧张，核心应用也不会被 OOM 杀死。\n\n## 思考\n\n由于篇幅限制，我在这里只列举了一些我认为的重要内存指标和分析思路。我想，你肯定也碰到过很多内存相关的性能问题。所以，我想请你来聊一聊，你处理过的内存性能问题，你是怎样分析它的瓶颈并解决的呢？这个过程中，遇到了什么坑，或者有什么重要收获吗？\n\n欢迎在留言区跟我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 22 Linux 性能优化答疑（三）\n\n你好，我是倪朋飞。\n\n专栏更新至今，四大基础模块的第二个模块——内存性能篇，我们就已经学完了。很开心你还没有掉队，仍然在积极学习和实践操作，并且热情地留言与讨论。\n\n这些留言中，我非常高兴看到，很多同学用学过的案例思路，解决了实际工作中的性能问题。我也非常感谢 espzest、大甜菜、Smile 等积极思考的同学，指出了文章中某些不当或者不严谨的地方。另外，还有我来也、JohnT3e、白华等同学，积极在留言区讨论学习和实践中的问题，也分享了宝贵的经验，在这里也非常感谢你们。\n\n今天是性能优化的第三期。照例，我从内存模块的留言中摘出了一些典型问题，作为今天的答疑内容，集中回复。为了便于你学习理解，它们并不是严格按照文章顺序排列的。\n\n每个问题，我都附上了留言区提问的截屏。如果你需要回顾内容原文，可以扫描每个问题右下方的二维码查看。\n\n## 问题 1：内存回收与 OOM\n\n虎虎的这个问题，实际上包括四个子问题，即，\n\n- 怎么理解 LRU 内存回收？\n- 回收后的内存又到哪里去了？\n- OOM 是按照虚拟内存还是实际内存来打分？\n- 怎么估计应用程序的最小内存？\n\n![img](905b15ee0df924038befe0e61ce81436.png)\n\n其实在 Linux [内存的原理篇]和 [Swap 原理篇]中我曾经讲到，一旦发现内存紧张，系统会通过三种方式回收内存。我们来复习一下，这三种方式分别是 ：\n\n- 基于 LRU（Least Recently Used）算法，回收缓存；\n- 基于 Swap 机制，回收不常访问的匿名页；\n- 基于 OOM（Out of Memory）机制，杀掉占用大量内存的进程。\n\n前两种方式，缓存回收和 Swap 回收，实际上都是基于 LRU 算法，也就是优先回收不常访问的内存。LRU 回收算法，实际上维护着 active 和 inactive 两个双向链表，其中：\n\n- active 记录活跃的内存页；\n- inactive 记录非活跃的内存页。\n\n越接近链表尾部，就表示内存页越不常访问。这样，在回收内存时，系统就可以根据活跃程度，优先回收不活跃的内存。\n\n活跃和非活跃的内存页，按照类型的不同，又分别分为文件页和匿名页，对应着缓存回收和 Swap 回收。\n\n当然，你可以从 /proc/meminfo 中，查询它们的大小，比如：\n\n```makefile\n# grep 表示只保留包含 active 的指标（忽略大小写）\n# sort 表示按照字母顺序排序\n$ cat /proc/meminfo | grep -i active | sort\nActive(anon):     167976 kB\nActive(file):     971488 kB\nActive:          1139464 kB\nInactive(anon):      720 kB\nInactive(file):  2109536 kB\nInactive:        2110256 kB\n```\n\n第三种方式，OOM 机制按照 oom_score 给进程排序。oom_score 越大，进程就越容易被系统杀死。\n\n当系统发现内存不足以分配新的内存请求时，就会尝试[直接内存回收]。这种情况下，如果回收完文件页和匿名页后，内存够用了，当然皆大欢喜，把回收回来的内存分配给进程就可以了。但如果内存还是不足，OOM 就要登场了。\n\nOOM 发生时，你可以在 dmesg 中看到 Out of memory 的信息，从而知道是哪些进程被 OOM 杀死了。比如，你可以执行下面的命令，查询 OOM 日志：\n\n```perl\n$ dmesg | grep -i \"Out of memory\"\nOut of memory: Kill process 9329 (java) score 321 or sacrifice child\n```\n\n当然了，如果你不希望应用程序被 OOM 杀死，可以调整进程的 oom_score_adj，减小 OOM 分值，进而降低被杀死的概率。或者，你还可以开启内存的 overcommit，允许进程申请超过物理内存的虚拟内存（这儿实际上假设的是，进程不会用光申请到的虚拟内存）。\n\n这三种方式，我们就复习完了。接下来，我们回到开始的四个问题，相信你自己已经有了答案。\n\n1. LRU 算法的原理刚才已经提到了，这里不再重复。\n2. 内存回收后，会被重新放到未使用内存中。这样，新的进程就可以请求、使用它们。\n3. OOM 触发的时机基于虚拟内存。换句话说，进程在申请内存时，如果申请的虚拟内存加上服务器实际已用的内存之和，比总的物理内存还大，就会触发 OOM。\n4. 要确定一个进程或者容器的最小内存，最简单的方法就是让它运行起来，再通过 ps 或者 smap ，查看它的内存使用情况。不过要注意，进程刚启动时，可能还没开始处理实际业务，一旦开始处理实际业务，就会占用更多内存。所以，要记得给内存留一定的余量。\n\n## 问题 2: 文件系统与磁盘的区别\n\n文件系统和磁盘的原理，我将在下一个模块中讲解，它们跟内存的关系也十分密切。不过，在学习 Buffer 和 Cache 的原理时，我曾提到，Buffer 用于磁盘，而 Cache 用于文件。因此，有不少同学困惑了，比如 JJ 留言中的这两个问题。\n\n- 读写文件最终也是读写磁盘，到底要怎么区分，是读写文件还是读写磁盘呢？\n- 读写磁盘难道可以不经过文件系统吗？\n\n![img](6ac5f2e0bf43098a3ba2d14f057eeeb1.png)\n\n如果你也有相同的疑问，主要还是没搞清楚，磁盘和文件的区别。我在“[怎么理解内存中的 Buffer 和 Cache]”文章的留言区简单回复过，不过担心有同学没有看到，所以在这里重新讲一下。\n\n磁盘是一个存储设备（确切地说是块设备），可以被划分为不同的磁盘分区。而在磁盘或者磁盘分区上，还可以再创建文件系统，并挂载到系统的某个目录中。这样，系统就可以通过这个挂载目录，来读写文件。\n\n换句话说，磁盘是存储数据的块设备，也是文件系统的载体。所以，文件系统确实还是要通过磁盘，来保证数据的持久化存储。\n\n你在很多地方都会看到这句话， Linux 中一切皆文件。换句话说，你可以通过相同的文件接口，来访问磁盘和文件（比如 open、read、write、close 等）。\n\n- 我们通常说的“文件”，其实是指普通文件。\n- 而磁盘或者分区，则是指块设备文件。\n\n你可以执行 “ls -l \u003c 路径 \u003e” 查看它们的区别。如果不懂 ls 输出的含义，别忘了 man 一下就可以。执行 man ls 命令，以及 info ‘(coreutils) ls invocation’ 命令，就可以查到了。\n\n在读写普通文件时，I/O 请求会首先经过文件系统，然后由文件系统负责，来与磁盘进行交互。而在读写块设备文件时，会跳过文件系统，直接与磁盘交互，也就是所谓的“裸 I/O”。\n\n这两种读写方式使用的缓存自然不同。文件系统管理的缓存，其实就是 Cache 的一部分。而裸磁盘的缓存，用的正是 Buffer。\n\n更多关于文件系统、磁盘以及 I/O 的原理，你先不要着急，往后我们都会讲到。\n\n## 问题 3: 如何统计所有进程的物理内存使用量\n\n这其实是 [怎么理解内存中的 Buffer 和 Cache] 的课后思考题，无名老卒、Griffin、JohnT3e 等少数几个同学，都给出了一些思路。\n\n比如，无名老卒同学的方法，是把所有进程的 RSS 全部累加：\n\n![img](baa48809addf1f7b4d7c280f4ce03764.png)\n\n这种方法，实际上导致不少地方会被重复计算。RSS 表示常驻内存，把进程用到的共享内存也算了进去。所以，直接累加会导致共享内存被重复计算，不能得到准确的答案。\n\n留言中好几个同学的答案都有类似问题。你可以重新检查一下自己的方法，弄清楚每个指标的定义和原理，防止重复计算。\n\n当然，也有同学的思路非常正确，比如 JohnT3e 提到的，这个问题的关键在于理解 PSS 的含义。\n\n![img](f5c56462ba5c821de1454a9c021e0f1c.png)\n\n你当然可以通过 stackexchange 上的[链接](https://unix.stackexchange.com/questions/33381/getting-information-about-a-process-memory-usage-from-proc-pid-smaps)找到答案，不过，我还是更推荐，直接查 proc 文件系统的[文档](https://www.kernel.org/doc/Documentation/filesystems/proc.txt)：\n\n\u003e The “proportional set size” (PSS) of a process is the count of pages it has in memory, where each page is divided by the number of processes sharing it. So if a process has 1000 pages all to itself, and 1000 shared with one other process, its PSS will be 1500.\n\n这里我简单解释一下，每个进程的 PSS ，是指把共享内存平分到各个进程后，再加上进程本身的非共享内存大小的和。\n\n就像文档中的这个例子，一个进程的非共享内存为 1000 页，它和另一个进程的共享进程也是 1000 页，那么它的 PSS=1000/2+1000=1500 页。\n\n这样，你就可以直接累加 PSS ，不用担心共享内存重复计算的问题了。\n\n比如，你可以运行下面的命令来计算：\n\n```shell\n# 使用 grep 查找 Pss 指标后，再用 awk 计算累加值\n$ grep Pss /proc/[1-9]*/smaps | awk '{total+=$2}; END {printf \"%d kB\\n\", total }'\n391266 kB\n```\n\n## 问题 4: CentOS 系统中如何安装 bcc-tools\n\n很多同学留言说用的是 CentOS 系统。虽然我在文章中也给出了一个[参考文档](https://github.com/iovisor/bcc/issues/462)，不过 bcc-tools 工具安装起来还是有些困难。\n\n比如白华同学留言表示，网络上的教程不太完整，步骤有些乱：\n\n![img](036cde548f2455e3d80b6b1c50e33c91.png)\n\n不过，白华和渡渡鸟 _linux 同学在探索实践后，留言分享了他们的经验，感谢你们的分享。\n\n![img](8b80a335c3fa543226f42dcb2c506017.png)![img](f34b80fc9f7eefc928959bfb41ce590d.png)\n\n在这里，我也统一回复一下，在 CentOS 中安装 bcc-tools 的步骤。以 CentOS 7 为例，整个安装主要可以分两步。\n\n第一步，升级内核。你可以运行下面的命令来操作：\n\n```bash\n# 升级系统\nyum update -y \n# 安装 ELRepo\nrpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org\nrpm -Uvh https://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm \n# 安装新内核\nyum remove -y kernel-headers kernel-tools kernel-tools-libs\nyum --enablerepo=\"elrepo-kernel\" install -y kernel-ml kernel-ml-devel kernel-ml-headers kernel-ml-tools kernel-ml-tools-libs kernel-ml-tools-libs-devel \n# 更新 Grub 后重启\ngrub2-mkconfig -o /boot/grub2/grub.cfg\ngrub2-set-default 0\nreboot \n# 重启后确认内核版本已升级为 4.20.0-1.el7.elrepo.x86_64\nuname -r\n```\n\n第二步，安装 bcc-tools：\n\n```ruby\n# 安装 bcc-tools\nyum install -y bcc-tools \n# 配置 PATH 路径\nexport PATH=$PATH:/usr/share/bcc/tools \n# 验证安装成功\ncachestat \n```\n\n## 问题 5: 内存泄漏案例的优化方法\n\n这是我在 [内存泄漏了，我该如何定位和处理] 中留的一个思考题。这个问题是这样的：\n\n在内存泄漏案例的最后，我们通过增加 free() 调用，释放了函数 fibonacci() 分配的内存，修复了内存泄漏的问题。就这个案例而言，还有没有其他更好的修复方法呢？\n\n很多同学留言写下了自己的想法，都很不错。这里，我重点表扬下郭江伟同学，他给出的方法非常好：\n\n![img](757c532b561d142306c435a57277cae4.png)\n\n他的思路是不用动态内存分配的方法，而是用数组来暂存计算结果。这样就可以由系统自动管理这些栈内存，也不存在内存泄漏的问题了。\n\n这种减少动态内存分配的思路，除了可以解决内存泄漏问题，其实也是常用的内存优化方法。比如，在需要大量内存的场景中，你就可以考虑用栈内存、内存池、HugePage 等方法，来优化内存的分配和管理。\n\n除了这五个问题，还有一点我也想说一下。很多同学在说工具的版本问题，的确，生产环境中的 Linux 版本往往都比较低，导致很多新工具不能在生产环境中直接使用。\n\n不过，这并不代表我们就无能为力了。毕竟，系统的原理都是大同小异的。这其实也是我一直强调的观点。\n\n- 在学习时，最好先用最新的系统和工具，它们可以为你提供更简单直观的结果，帮你更好的理解系统的原理。\n- 在你掌握了这些原理后，回过头来，再去理解旧版本系统中的工具和原理，你会发现，即便旧版本中的很多工具并不是那么好用，但是原理和指标是类似的，你依然可以轻松掌握它们的使用方法。\n\n最后，欢迎继续在留言区写下你的疑问，我会持续不断地解答。我的目的不变，希望可以和你一起，把文章的知识变成你的能力，我们不仅仅在实战中演练，也要在交流中进步。\n\n# 23 基础篇：Linux 文件系统是怎么工作的？\n\n你好，我是倪朋飞。\n\n通过前面 CPU 和内存模块的学习，我相信，你已经掌握了 CPU 和内存的性能分析以及优化思路。从这一节开始，我们将进入下一个重要模块——文件系统和磁盘的 I/O 性能。\n\n同 CPU、内存一样，磁盘和文件系统的管理，也是操作系统最核心的功能。\n\n- 磁盘为系统提供了最基本的持久化存储。\n- 文件系统则在磁盘的基础上，提供了一个用来管理文件的树状结构。\n\n那么，磁盘和文件系统是怎么工作的呢？又有哪些指标可以衡量它们的性能呢？\n\n今天，我就带你先来看看，Linux 文件系统的工作原理。磁盘的工作原理，我们下一节再来学习。\n\n## 索引节点和目录项\n\n文件系统，本身是对存储设备上的文件，进行组织管理的机制。组织方式不同，就会形成不同的文件系统。\n\n你要记住最重要的一点，在 Linux 中一切皆文件。不仅普通的文件和目录，就连块设备、套接字、管道等，也都要通过统一的文件系统来管理。\n\n为了方便管理，Linux 文件系统为每个文件都分配两个数据结构，索引节点（index node）和目录项（directory entry）。它们主要用来记录文件的元信息和目录结构。\n\n- 索引节点，简称为 inode，用来记录文件的元数据，比如 inode 编号、文件大小、访问权限、修改日期、数据的位置等。索引节点和文件一一对应，它跟文件内容一样，都会被持久化存储到磁盘中。所以记住，索引节点同样占用磁盘空间。\n- 目录项，简称为 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的关联关系。多个关联的目录项，就构成了文件系统的目录结构。不过，不同于索引节点，目录项是由内核维护的一个内存数据结构，所以通常也被叫做目录项缓存。\n\n换句话说，索引节点是每个文件的唯一标志，而目录项维护的正是文件系统的树状结构。目录项和索引节点的关系是多对一，你可以简单理解为，一个文件可以有多个别名。\n\n举个例子，通过硬链接为文件创建的别名，就会对应不同的目录项，不过这些目录项本质上还是链接同一个文件，所以，它们的索引节点相同。\n\n索引节点和目录项纪录了文件的元数据，以及文件间的目录关系，那么具体来说，文件数据到底是怎么存储的呢？是不是直接写到磁盘中就好了呢？\n\n实际上，磁盘读写的最小单位是扇区，然而扇区只有 512B 大小，如果每次都读写这么小的单位，效率一定很低。所以，文件系统又把连续的扇区组成了逻辑块，然后每次都以逻辑块为最小单元，来管理数据。常见的逻辑块大小为 4KB，也就是由连续的 8 个扇区组成。\n\n为了帮助你理解目录项、索引节点以及文件数据的关系，我画了一张示意图。你可以对照着这张图，来回忆刚刚讲过的内容，把知识和细节串联起来。\n\n![img](328d942a38230a973f11bae67307be47.png)\n\n不过，这里有两点需要你注意。\n\n第一，目录项本身就是一个内存缓存，而索引节点则是存储在磁盘中的数据。在前面的 Buffer 和 Cache 原理中，我曾经提到过，为了协调慢速磁盘与快速 CPU 的性能差异，文件内容会缓存到页缓存 Cache 中。\n\n那么，你应该想到，这些索引节点自然也会缓存到内存中，加速文件的访问。\n\n第二，磁盘在执行文件系统格式化时，会被分成三个存储区域，超级块、索引节点区和数据块区。其中，\n\n- 超级块，存储整个文件系统的状态。\n- 索引节点区，用来存储索引节点。\n- 数据块区，则用来存储文件数据。\n\n## 虚拟文件系统\n\n目录项、索引节点、逻辑块以及超级块，构成了 Linux 文件系统的四大基本要素。不过，为了支持各种不同的文件系统，Linux 内核在用户进程和文件系统的中间，又引入了一个抽象层，也就是虚拟文件系统 VFS（Virtual File System）。\n\nVFS 定义了一组所有文件系统都支持的数据结构和标准接口。这样，用户进程和内核中的其他子系统，只需要跟 VFS 提供的统一接口进行交互就可以了，而不需要再关心底层各种文件系统的实现细节。\n\n这里，我画了一张 Linux 文件系统的架构图，帮你更好地理解系统调用、VFS、缓存、文件系统以及块存储之间的关系。\n\n![img](728b7b39252a1e23a7a223cdf4aa1612.png)\n\n通过这张图，你可以看到，在 VFS 的下方，Linux 支持各种各样的文件系统，如 Ext4、XFS、NFS 等等。按照存储位置的不同，这些文件系统可以分为三类。\n\n- 第一类是基于磁盘的文件系统，也就是把数据直接存储在计算机本地挂载的磁盘中。常见的 Ext4、XFS、OverlayFS 等，都是这类文件系统。\n- 第二类是基于内存的文件系统，也就是我们常说的虚拟文件系统。这类文件系统，不需要任何磁盘分配存储空间，但会占用内存。我们经常用到的 /proc 文件系统，其实就是一种最常见的虚拟文件系统。此外，/sys 文件系统也属于这一类，主要向用户空间导出层次化的内核对象。\n- 第三类是网络文件系统，也就是用来访问其他计算机数据的文件系统，比如 NFS、SMB、iSCSI 等。\n\n这些文件系统，要先挂载到 VFS 目录树中的某个子目录（称为挂载点），然后才能访问其中的文件。拿第一类，也就是基于磁盘的文件系统为例，在安装系统时，要先挂载一个根目录（/），在根目录下再把其他文件系统（比如其他的磁盘分区、/proc 文件系统、/sys 文件系统、NFS 等）挂载进来。\n\n## 文件系统 I/O\n\n把文件系统挂载到挂载点后，你就能通过挂载点，再去访问它管理的文件了。VFS 提供了一组标准的文件访问接口。这些接口以系统调用的方式，提供给应用程序使用。\n\n就拿 cat 命令来说，它首先调用 open() ，打开一个文件；然后调用 read() ，读取文件的内容；最后再调用 write() ，把文件内容输出到控制台的标准输出中：\n\n```cpp\nint open(const char *pathname, int flags, mode_t mode); \nssize_t read(int fd, void *buf, size_t count); \nssize_t write(int fd, const void *buf, size_t count); \n```\n\n文件读写方式的各种差异，导致 I/O 的分类多种多样。最常见的有，缓冲与非缓冲 I/O、直接与非直接 I/O、阻塞与非阻塞 I/O、同步与异步 I/O 等。 接下来，我们就详细看这四种分类。\n\n第一种，根据是否利用标准库缓存，可以把文件 I/O 分为缓冲 I/O 与非缓冲 I/O。\n\n- 缓冲 I/O，是指利用标准库缓存来加速文件的访问，而标准库内部再通过系统调度访问文件。\n- 非缓冲 I/O，是指直接通过系统调用来访问文件，不再经过标准库缓存。\n\n注意，这里所说的“缓冲”，是指标准库内部实现的缓存。比方说，你可能见到过，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来。\n\n无论缓冲 I/O 还是非缓冲 I/O，它们最终还是要经过系统调用来访问文件。而根据上一节内容，我们知道，系统调用后，还会通过页缓存，来减少磁盘的 I/O 操作。\n\n第二，根据是否利用操作系统的页缓存，可以把文件 I/O 分为直接 I/O 与非直接 I/O。\n\n- 直接 I/O，是指跳过操作系统的页缓存，直接跟文件系统交互来访问文件。\n- 非直接 I/O 正好相反，文件读写时，先要经过系统的页缓存，然后再由内核或额外的系统调用，真正写入磁盘。\n\n想要实现直接 I/O，需要你在系统调用中，指定 O_DIRECT 标志。如果没有设置过，默认的是非直接 I/O。\n\n不过要注意，直接 I/O、非直接 I/O，本质上还是和文件系统交互。如果是在数据库等场景中，你还会看到，跳过文件系统读写磁盘的情况，也就是我们通常所说的裸 I/O。\n\n第三，根据应用程序是否阻塞自身运行，可以把文件 I/O 分为阻塞 I/O 和非阻塞 I/O：\n\n- 所谓阻塞 I/O，是指应用程序执行 I/O 操作后，如果没有获得响应，就会阻塞当前线程，自然就不能执行其他任务。\n- 所谓非阻塞 I/O，是指应用程序执行 I/O 操作后，不会阻塞当前的线程，可以继续执行其他的任务，随后再通过轮询或者事件通知的形式，获取调用的结果。\n\n比方说，访问管道或者网络套接字时，设置 O_NONBLOCK 标志，就表示用非阻塞方式访问；而如果不做任何设置，默认的就是阻塞访问。\n\n第四，根据是否等待响应结果，可以把文件 I/O 分为同步和异步 I/O：\n\n- 所谓同步 I/O，是指应用程序执行 I/O 操作后，要一直等到整个 I/O 完成后，才能获得 I/O 响应。\n- 所谓异步 I/O，是指应用程序执行 I/O 操作后，不用等待完成和完成后的响应，而是继续执行就可以。等到这次 I/O 完成后，响应会用事件通知的方式，告诉应用程序。\n\n举个例子，在操作文件时，如果你设置了 O_SYNC 或者 O_DSYNC 标志，就代表同步 I/O。如果设置了 O_DSYNC，就要等文件数据写入磁盘后，才能返回；而 O_SYNC，则是在 O_DSYNC 基础上，要求文件元数据也要写入磁盘后，才能返回。\n\n再比如，在访问管道或者网络套接字时，设置了 O_ASYNC 选项后，相应的 I/O 就是异步 I/O。这样，内核会再通过 SIGIO 或者 SIGPOLL，来通知进程文件是否可读写。\n\n你可能发现了，这里的好多概念也经常出现在网络编程中。比如非阻塞 I/O，通常会跟 select/poll 配合，用在网络套接字的 I/O 中。\n\n你也应该可以理解，“Linux 一切皆文件”的深刻含义。无论是普通文件和块设备、还是网络套接字和管道等，它们都通过统一的 VFS 接口来访问。\n\n## 性能观测\n\n学了这么多文件系统的原理，你估计也是迫不及待想上手，观察一下文件系统的性能情况了。\n\n接下来，打开一个终端，SSH 登录到服务器上，然后跟我一起来探索，如何观测文件系统的性能。\n\n### 容量\n\n对文件系统来说，最常见的一个问题就是空间不足。当然，你可能本身就知道，用 df 命令，就能查看文件系统的磁盘空间使用情况。比如：\n\n```bash\n$ df /dev/sda1 \nFilesystem     1K-blocks    Used Available Use% Mounted on \n/dev/sda1       30308240 3167020  27124836  11% / \n```\n\n你可以看到，我的根文件系统只使用了 11% 的空间。这里还要注意，总空间用 1K-blocks 的数量来表示，你可以给 df 加上 -h 选项，以获得更好的可读性：\n\n```bash\n$ df -h /dev/sda1 \nFilesystem      Size  Used Avail Use% Mounted on \n/dev/sda1        29G  3.1G   26G  11% / \n```\n\n不过有时候，明明你碰到了空间不足的问题，可是用 df 查看磁盘空间后，却发现剩余空间还有很多。这是怎么回事呢？\n\n不知道你还记不记得，刚才我强调的一个细节。除了文件数据，索引节点也占用磁盘空间。你可以给 df 命令加上 -i 参数，查看索引节点的使用情况，如下所示：\n\n```bash\n$ df -i /dev/sda1 \nFilesystem      Inodes  IUsed   IFree IUse% Mounted on \n/dev/sda1      3870720 157460 3713260    5% / \n```\n\n索引节点的容量，（也就是 Inode 个数）是在格式化磁盘时设定好的，一般由格式化工具自动生成。当你发现索引节点空间不足，但磁盘空间充足时，很可能就是过多小文件导致的。\n\n所以，一般来说，删除这些小文件，或者把它们移动到索引节点充足的其他磁盘中，就可以解决这个问题。\n\n### 缓存\n\n在前面 Cache 案例中，我已经介绍过，可以用 free 或 vmstat，来观察页缓存的大小。复习一下，free 输出的 Cache，是页缓存和可回收 Slab 缓存的和，你可以从 /proc/meminfo ，直接得到它们的大小：\n\n```makefile\n$ cat /proc/meminfo | grep -E \"SReclaimable|Cached\" \nCached:           748316 kB \nSwapCached:            0 kB \nSReclaimable:     179508 kB \n```\n\n话说回来，文件系统中的目录项和索引节点缓存，又该如何观察呢？\n\n实际上，内核使用 Slab 机制，管理目录项和索引节点的缓存。/proc/meminfo 只给出了 Slab 的整体大小，具体到每一种 Slab 缓存，还要查看 /proc/slabinfo 这个文件。\n\n比如，运行下面的命令，你就可以得到，所有目录项和各种文件系统索引节点的缓存情况：\n\n```yaml\n$ cat /proc/slabinfo | grep -E '^#|dentry|inode' \n# name            \u003cactive_objs\u003e \u003cnum_objs\u003e \u003cobjsize\u003e \u003cobjperslab\u003e \u003cpagesperslab\u003e : tunables \u003climit\u003e \u003cbatchcount\u003e \u003csharedfactor\u003e : slabdata \u003cactive_slabs\u003e \u003cnum_slabs\u003e \u003csharedavail\u003e \nxfs_inode              0      0    960   17    4 : tunables    0    0    0 : slabdata      0      0      0 \n... \next4_inode_cache   32104  34590   1088   15    4 : tunables    0    0    0 : slabdata   2306   2306      0hugetlbfs_inode_cache     13     13    624   13    2 : tunables    0    0    0 : slabdata      1      1      0 \nsock_inode_cache    1190   1242    704   23    4 : tunables    0    0    0 : slabdata     54     54      0 \nshmem_inode_cache   1622   2139    712   23    4 : tunables    0    0    0 : slabdata     93     93      0 \nproc_inode_cache    3560   4080    680   12    2 : tunables    0    0    0 : slabdata    340    340      0 \ninode_cache        25172  25818    608   13    2 : tunables    0    0    0 : slabdata   1986   1986      0 \ndentry             76050 121296    192   21    1 : tunables    0    0    0 : slabdata   5776   5776      0 \n```\n\n这个界面中，dentry 行表示目录项缓存，inode_cache 行，表示 VFS 索引节点缓存，其余的则是各种文件系统的索引节点缓存。\n\n/proc/slabinfo 的列比较多，具体含义你可以查询 man slabinfo。在实际性能分析中，我们更常使用 slabtop ，来找到占用内存最多的缓存类型。\n\n比如，下面就是我运行 slabtop 得到的结果：\n\n```yaml\n# 按下 c 按照缓存大小排序，按下 a 按照活跃对象数排序 \n$ slabtop \nActive / Total Objects (% used)    : 277970 / 358914 (77.4%) \nActive / Total Slabs (% used)      : 12414 / 12414 (100.0%) \nActive / Total Caches (% used)     : 83 / 135 (61.5%) \nActive / Total Size (% used)       : 57816.88K / 73307.70K (78.9%) \nMinimum / Average / Maximum Object : 0.01K / 0.20K / 22.88K  \n  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME \n69804  23094   0%    0.19K   3324       21     13296K dentry \n16380  15854   0%    0.59K   1260       13     10080K inode_cache \n58260  55397   0%    0.13K   1942       30      7768K kernfs_node_cache \n   485    413   0%    5.69K     97        5      3104K task_struct \n  1472   1397   0%    2.00K     92       16      2944K kmalloc-2048 \n```\n\n从这个结果你可以看到，在我的系统中，目录项和索引节点占用了最多的 Slab 缓存。不过它们占用的内存其实并不大，加起来也只有 23MB 左右。\n\n## 小结\n\n今天，我带你梳理了 Linux 文件系统的工作原理。\n\n文件系统，是对存储设备上的文件，进行组织管理的一种机制。为了支持各类不同的文件系统，Linux 在各种文件系统实现上，抽象了一层虚拟文件系统（VFS）。\n\nVFS 定义了一组所有文件系统都支持的数据结构和标准接口。这样，用户进程和内核中的其他子系统，就只需要跟 VFS 提供的统一接口进行交互。\n\n为了降低慢速磁盘对性能的影响，文件系统又通过页缓存、目录项缓存以及索引节点缓存，缓和磁盘延迟对应用程序的影响。\n\n在性能观测方面，今天主要讲了容量和缓存的指标。下一节，我们将会学习 Linux 磁盘 I/O 的工作原理，并掌握磁盘 I/O 的性能观测方法。\n\n## 思考\n\n最后，给你留一个思考题。在实际工作中，我们经常会根据文件名字，查找它所在路径，比如：\n\n```lua\n$ find / -name file-name\n```\n\n今天的问题就是，这个命令，会不会导致系统的缓存升高呢？如果有影响，又会导致哪种类型的缓存升高呢？你可以结合今天内容，自己先去操作和分析，看看观察到的结果跟你分析的是否一样。\n\n欢迎在留言区和我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 24 基础篇：Linux 磁盘IO是怎么工作的（上）\n\n你好，我是倪朋飞。\n\n上一节，我们学习了 Linux 文件系统的工作原理。简单回顾一下，文件系统是对存储设备上的文件，进行组织管理的一种机制。而 Linux 在各种文件系统实现上，又抽象了一层虚拟文件系统 VFS，它定义了一组，所有文件系统都支持的，数据结构和标准接口。\n\n这样，对应用程序来说，只需要跟 VFS 提供的统一接口交互，而不需要关注文件系统的具体实现；对具体的文件系统来说，只需要按照 VFS 的标准，就可以无缝支持各种应用程序。\n\nVFS 内部又通过目录项、索引节点、逻辑块以及超级块等数据结构，来管理文件。\n\n- 目录项，记录了文件的名字，以及文件与其他目录项之间的目录关系。\n- 索引节点，记录了文件的元数据。\n- 逻辑块，是由连续磁盘扇区构成的最小读写单元，用来存储文件数据。\n- 超级块，用来记录文件系统整体的状态，如索引节点和逻辑块的使用情况等。\n\n其中，目录项是一个内存缓存；而超级块、索引节点和逻辑块，都是存储在磁盘中的持久化数据。\n\n那么，进一步想，磁盘又是怎么工作的呢？又有哪些指标可以用来衡量它的性能呢？\n\n接下来，我就带你一起看看， Linux 磁盘 I/O 的工作原理。\n\n## 磁盘\n\n磁盘是可以持久化存储的设备，根据存储介质的不同，常见磁盘可以分为两类：机械磁盘和固态磁盘。\n\n第一类，机械磁盘，也称为硬盘驱动器（Hard Disk Driver），通常缩写为 HDD。机械磁盘主要由盘片和读写磁头组成，数据就存储在盘片的环状磁道中。在读写数据前，需要移动读写磁头，定位到数据所在的磁道，然后才能访问数据。\n\n显然，如果 I/O 请求刚好连续，那就不需要磁道寻址，自然可以获得最佳性能。这其实就是我们熟悉的，连续 I/O 的工作原理。与之相对应的，当然就是随机 I/O，它需要不停地移动磁头，来定位数据位置，所以读写速度就会比较慢。\n\n第二类，固态磁盘（Solid State Disk），通常缩写为 SSD，由固态电子元器件组成。固态磁盘不需要磁道寻址，所以，不管是连续 I/O，还是随机 I/O 的性能，都比机械磁盘要好得多。\n\n其实，无论机械磁盘，还是固态磁盘，相同磁盘的随机 I/O 都要比连续 I/O 慢很多，原因也很明显。\n\n- 对机械磁盘来说，我们刚刚提到过的，由于随机 I/O 需要更多的磁头寻道和盘片旋转，它的性能自然要比连续 I/O 慢。\n- 而对固态磁盘来说，虽然它的随机性能比机械硬盘好很多，但同样存在“先擦除再写入”的限制。随机读写会导致大量的垃圾回收，所以相对应的，随机 I/O 的性能比起连续 I/O 来，也还是差了很多。\n- 此外，连续 I/O 还可以通过预读的方式，来减少 I/O 请求的次数，这也是其性能优异的一个原因。很多性能优化的方案，也都会从这个角度出发，来优化 I/O 性能。\n\n此外，机械磁盘和固态磁盘还分别有一个最小的读写单位。\n\n- 机械磁盘的最小读写单位是扇区，一般大小为 512 字节。\n- 而固态磁盘的最小读写单位是页，通常大小是 4KB、8KB 等。\n\n在上一节中，我也提到过，如果每次都读写 512 字节这么小的单位的话，效率很低。所以，文件系统会把连续的扇区或页，组成逻辑块，然后以逻辑块作为最小单元来管理数据。常见的逻辑块的大小是 4KB，也就是说，连续 8 个扇区，或者单独的一个页，都可以组成一个逻辑块。\n\n除了可以按照存储介质来分类，另一个常见的分类方法，是按照接口来分类，比如可以把硬盘分为 IDE（Integrated Drive Electronics）、SCSI（Small Computer System Interface） 、SAS（Serial Attached SCSI） 、SATA（Serial ATA） 、FC（Fibre Channel） 等。\n\n不同的接口，往往分配不同的设备名称。比如， IDE 设备会分配一个 hd 前缀的设备名，SCSI 和 SATA 设备会分配一个 sd 前缀的设备名。如果是多块同类型的磁盘，就会按照 a、b、c 等的字母顺序来编号。\n\n除了磁盘本身的分类外，当你把磁盘接入服务器后，按照不同的使用方式，又可以把它们划分为多种不同的架构。\n\n最简单的，就是直接作为独立磁盘设备来使用。这些磁盘，往往还会根据需要，划分为不同的逻辑分区，每个分区再用数字编号。比如我们前面多次用到的 /dev/sda ，还可以分成两个分区 /dev/sda1 和 /dev/sda2。\n\n另一个比较常用的架构，是把多块磁盘组合成一个逻辑磁盘，构成冗余独立磁盘阵列，也就是 RAID（Redundant Array of Independent Disks），从而可以提高数据访问的性能，并且增强数据存储的可靠性。\n\n根据容量、性能和可靠性需求的不同，RAID 一般可以划分为多个级别，如 RAID0、RAID1、RAID5、RAID10 等。\n\n- RAID0 有最优的读写性能，但不提供数据冗余的功能。\n- 而其他级别的 RAID，在提供数据冗余的基础上，对读写性能也有一定程度的优化。\n\n最后一种架构，是把这些磁盘组合成一个网络存储集群，再通过 NFS、SMB、iSCSI 等网络存储协议，暴露给服务器使用。\n\n其实在 Linux 中，**磁盘实际上是作为一个块设备来管理的**，也就是以块为单位读写数据，并且支持随机读写。每个块设备都会被赋予两个设备号，分别是主、次设备号。主设备号用在驱动程序中，用来区分设备类型；而次设备号则是用来给多个同类设备编号。\n\n## 通用块层\n\n跟我们上一节讲到的虚拟文件系统 VFS 类似，为了减小不同块设备的差异带来的影响，Linux 通过一个统一的通用块层，来管理各种不同的块设备。\n\n通用块层，其实是处在文件系统和磁盘驱动中间的一个块设备抽象层。它主要有两个功能 。\n\n- 第一个功能跟虚拟文件系统的功能类似。向上，为文件系统和应用程序，提供访问块设备的标准接口；向下，把各种异构的磁盘设备抽象为统一的块设备，并提供统一框架来管理这些设备的驱动程序。\n- 第二个功能，通用块层还会给文件系统和应用程序发来的 I/O 请求排队，并通过重新排序、请求合并等方式，提高磁盘读写的效率。\n\n其中，对 I/O 请求排序的过程，也就是我们熟悉的 I/O 调度。事实上，Linux 内核支持四种 I/O 调度算法，分别是 NONE、NOOP、CFQ 以及 DeadLine。这里我也分别介绍一下。\n\n第一种 NONE ，更确切来说，并不能算 I/O 调度算法。因为它完全不使用任何 I/O 调度器，对文件系统和应用程序的 I/O 其实不做任何处理，常用在虚拟机中（此时磁盘 I/O 调度完全由物理机负责）。\n\n第二种 NOOP ，是最简单的一种 I/O 调度算法。它实际上是一个先入先出的队列，只做一些最基本的请求合并，常用于 SSD 磁盘。\n\n第三种 CFQ（Completely Fair Scheduler），也被称为完全公平调度器，是现在很多发行版的默认 I/O 调度器，它为每个进程维护了一个 I/O 调度队列，并按照时间片来均匀分布每个进程的 I/O 请求。\n\n类似于进程 CPU 调度，CFQ 还支持进程 I/O 的优先级调度，所以它适用于运行大量进程的系统，像是桌面环境、多媒体应用等。\n\n最后一种 DeadLine 调度算法，分别为读、写请求创建了不同的 I/O 队列，可以提高机械磁盘的吞吐量，并确保达到最终期限（deadline）的请求被优先处理。DeadLine 调度算法，多用在 I/O 压力比较重的场景，比如数据库等。\n\n## I/O 栈\n\n清楚了磁盘和通用块层的工作原理，再结合上一期我们讲过的文件系统原理，我们就可以整体来看 Linux 存储系统的 I/O 原理了。\n\n我们可以把 Linux 存储系统的 I/O 栈，由上到下分为三个层次，分别是文件系统层、通用块层和设备层。这三个 I/O 层的关系如下图所示，这其实也是 Linux 存储系统的 I/O 栈全景图。\n\n![img](14bc3d26efe093d3eada173f869146b1.png)（图片来自 [Linux Storage Stack Diagram](https://www.thomas-krenn.com/en/wiki/Linux_Storage_Stack_Diagram) )\n\n根据这张 I/O 栈的全景图，我们可以更清楚地理解，存储系统 I/O 的工作原理。\n\n- 文件系统层，包括虚拟文件系统和其他各种文件系统的具体实现。它为上层的应用程序，提供标准的文件访问接口；对下会通过通用块层，来存储和管理磁盘数据。\n- 通用块层，包括块设备 I/O 队列和 I/O 调度器。它会对文件系统的 I/O 请求进行排队，再通过重新排序和请求合并，然后才要发送给下一级的设备层。\n- 设备层，包括存储设备和相应的驱动程序，负责最终物理设备的 I/O 操作。\n\n存储系统的 I/O ，通常是整个系统中最慢的一环。所以， Linux 通过多种缓存机制来优化 I/O 效率。\n\n比方说，为了优化文件访问的性能，会使用页缓存、索引节点缓存、目录项缓存等多种缓存机制，以减少对下层块设备的直接调用。\n\n同样，为了优化块设备的访问效率，会使用缓冲区，来缓存块设备的数据。\n\n不过，抽象的原理讲了这么多，具体操作起来，应该怎么衡量磁盘的 I/O 性能呢？我先卖个关子，下节课我们一起来看，最常用的磁盘 I/O 性能指标，以及 I/O 性能工具。\n\n## 小结\n\n在今天的文章中，我们梳理了 Linux 磁盘 I/O 的工作原理，并了解了由文件系统层、通用块层和设备层构成的 Linux 存储系统 I/O 栈。\n\n其中，通用块层是 Linux 磁盘 I/O 的核心。向上，它为文件系统和应用程序，提供访问了块设备的标准接口；向下，把各种异构的磁盘设备，抽象为统一的块设备，并会对文件系统和应用程序发来的 I/O 请求进行重新排序、请求合并等，提高了磁盘访问的效率。\n\n## 思考\n\n最后，我想邀请你一起来聊聊，你所理解的磁盘 I/O。我相信你很可能已经碰到过，文件或者磁盘的 I/O 性能问题，你是怎么分析这些问题的呢？你可以结合今天的磁盘 I/O 原理和上一节的文件系统原理，记录你的操作步骤，并总结出自己的思路。\n\n欢迎在留言区和我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 25 基础篇：Linux 磁盘IO是怎么工作的（下）\n\n你好，我是倪朋飞。\n\n上一节我们学习了 Linux 磁盘 I/O 的工作原理，并了解了由文件系统层、通用块层和设备层构成的 Linux 存储系统 I/O 栈。\n\n其中，通用块层是 Linux 磁盘 I/O 的核心。向上，它为文件系统和应用程序，提供访问了块设备的标准接口；向下，把各种异构的磁盘设备，抽象为统一的块设备，并会对文件系统和应用程序发来的 I/O 请求，进行重新排序、请求合并等，提高了磁盘访问的效率。\n\n掌握了磁盘 I/O 的工作原理，你估计迫不及待想知道，怎么才能衡量磁盘的 I/O 性能。\n\n接下来，我们就来看看，磁盘的性能指标，以及观测这些指标的方法。\n\n## 磁盘性能指标\n\n说到磁盘性能的衡量标准，必须要提到五个常见指标，也就是我们经常用到的，使用率、饱和度、IOPS、吞吐量以及响应时间等。这五个指标，是衡量磁盘性能的基本指标。\n\n- 使用率，是指磁盘处理 I/O 的时间百分比。过高的使用率（比如超过 80%），通常意味着磁盘 I/O 存在性能瓶颈。\n- 饱和度，是指磁盘处理 I/O 的繁忙程度。过高的饱和度，意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I/O 请求。\n- IOPS（Input/Output Per Second），是指每秒的 I/O 请求数。\n- 吞吐量，是指每秒的 I/O 请求大小。\n- 响应时间，是指 I/O 请求从发出到收到响应的间隔时间。\n\n这里要注意的是，使用率只考虑有没有 I/O，而不考虑 I/O 的大小。换句话说，当使用率是 100% 的时候，磁盘依然有可能接受新的 I/O 请求。\n\n这些指标，很可能是你经常挂在嘴边的，一讨论磁盘性能必定提起的对象。不过我还是要强调一点，不要孤立地去比较某一指标，而要结合读写比例、I/O 类型（随机还是连续）以及 I/O 的大小，综合来分析。\n\n举个例子，在数据库、大量小文件等这类随机读写比较多的场景中，IOPS 更能反映系统的整体性能；而在多媒体等顺序读写较多的场景中，吞吐量才更能反映系统的整体性能。\n\n一般来说，我们在为应用程序的服务器选型时，要先对磁盘的 I/O 性能进行基准测试，以便可以准确评估，磁盘性能是否可以满足应用程序的需求。\n\n这一方面，我推荐用性能测试工具 fio ，来测试磁盘的 IOPS、吞吐量以及响应时间等核心指标。但还是那句话，因地制宜，灵活选取。在基准测试时，一定要注意根据应用程序 I/O 的特点，来具体评估指标。\n\n当然，这就需要你测试出，不同 I/O 大小（一般是 512B 至 1MB 中间的若干值）分别在随机读、顺序读、随机写、顺序写等各种场景下的性能情况。\n\n用性能工具得到的这些指标，可以作为后续分析应用程序性能的依据。一旦发生性能问题，你就可以把它们作为磁盘性能的极限值，进而评估磁盘 I/O 的使用情况。\n\n了解磁盘的性能指标，只是我们 I/O 性能测试的第一步。接下来，又该用什么方法来观测它们呢？这里，我给你介绍几个常用的 I/O 性能观测方法。\n\n## **磁盘 I/O 观测**\n\n第一个要观测的，是每块磁盘的使用情况。\n\niostat 是最常用的磁盘 I/O 性能观测工具，它提供了每个磁盘的使用率、IOPS、吞吐量等各种常见的性能指标，当然，这些指标实际上来自 /proc/diskstats。\n\niostat 的输出界面如下。\n\n```bash\n# -d -x 表示显示所有磁盘 I/O 的指标\n$ iostat -d -x 1 \nDevice            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util \nloop0            0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00 \nloop1            0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00 \nsda              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00 \nsdb              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00 \n```\n\n从这里你可以看到，iostat 提供了非常丰富的性能指标。第一列的 Device 表示磁盘设备的名字，其他各列指标，虽然数量较多，但是每个指标的含义都很重要。为了方便你理解，我把它们总结成了一个表格。\n\n![img](cff31e715af51c9cb8085ce1bb48318d.png)\n\n这些指标中，你要注意：\n\n- %util ，就是我们前面提到的磁盘 I/O 使用率；\n- r/s+ w/s ，就是 IOPS；\n- rkB/s+wkB/s ，就是吞吐量；\n- r_await+w_await ，就是响应时间。\n\n在观测指标时，也别忘了结合请求的大小（ rareq-sz 和 wareq-sz）一起分析。\n\n你可能注意到，从 iostat 并不能直接得到磁盘饱和度。事实上，饱和度通常也没有其他简单的观测方法，不过，你可以把观测到的，平均请求队列长度或者读写请求完成的等待时间，跟基准测试的结果（比如通过 fio）进行对比，综合评估磁盘的饱和情况。\n\n## **进程 I/O 观测**\n\n除了每块磁盘的 I/O 情况，每个进程的 I/O 情况也是我们需要关注的重点。\n\n上面提到的 iostat 只提供磁盘整体的 I/O 性能数据，缺点在于，并不能知道具体是哪些进程在进行磁盘读写。要观察进程的 I/O 情况，你还可以使用 pidstat 和 iotop 这两个工具。\n\npidstat 是我们的老朋友了，这里我就不再啰嗦它的功能了。给它加上 -d 参数，你就可以看到进程的 I/O 情况，如下所示：\n\n```bash\n$ pidstat -d 1 \n13:39:51      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command \n13:39:52      102       916      0.00      4.00      0.00       0  rsyslogd\n```\n\n从 pidstat 的输出你能看到，它可以实时查看每个进程的 I/O 情况，包括下面这些内容。\n\n- 用户 ID（UID）和进程 ID（PID） 。\n- 每秒读取的数据大小（kB_rd/s） ，单位是 KB。\n- 每秒发出的写请求数据大小（kB_wr/s） ，单位是 KB。\n- 每秒取消的写请求数据大小（kB_ccwr/s） ，单位是 KB。\n- 块 I/O 延迟（iodelay），包括等待同步块 I/O 和换入块 I/O 结束的时间，单位是时钟周期。\n\n除了可以用 pidstat 实时查看，根据 I/O 大小对进程排序，也是性能分析中一个常用的方法。这一点，我推荐另一个工具， iotop。它是一个类似于 top 的工具，你可以按照 I/O 大小对进程排序，然后找到 I/O 较大的那些进程。\n\niotop 的输出如下所示：\n\n```bash\n$ iotop\nTotal DISK READ :       0.00 B/s | Total DISK WRITE :       7.85 K/s \nActual DISK READ:       0.00 B/s | Actual DISK WRITE:       0.00 B/s \n  TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO\u003e    COMMAND \n15055 be/3 root        0.00 B/s    7.85 K/s  0.00 %  0.00 % systemd-journald \n```\n\n从这个输出，你可以看到，前两行分别表示，进程的磁盘读写大小总数和磁盘真实的读写大小总数。因为缓存、缓冲区、I/O 合并等因素的影响，它们可能并不相等。\n\n剩下的部分，则是从各个角度来分别表示进程的 I/O 情况，包括线程 ID、I/O 优先级、每秒读磁盘的大小、每秒写磁盘的大小、换入和等待 I/O 的时钟百分比等。\n\n这两个工具，是我们分析磁盘 I/O 性能时最常用到的。你先了解它们的功能和指标含义，具体的使用方法，接下来的案例实战中我们一起学习。\n\n## 小结\n\n今天，我们梳理了 Linux 磁盘 I/O 的性能指标和性能工具。我们通常用 IOPS、吞吐量、使用率、饱和度以及响应时间等几个指标，来评估磁盘的 I/O 性能。\n\n你可以用 iostat 获得磁盘的 I/O 情况，也可以用 pidstat、iotop 等观察进程的 I/O 情况。不过在分析这些性能指标时，你要注意结合读写比例、I/O 类型以及 I/O 大小等，进行综合分析。\n\n## 思考\n\n最后，我想请你一起来聊聊，你碰到过的磁盘 I/O 问题。在碰到磁盘 I/O 性能问题时，你是怎么分析和定位的呢？你可以结合今天学到的磁盘 I/O 指标和工具，以及上一节学过的磁盘 I/O 原理，来总结你的思路。\n\n欢迎在留言区和我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 26 案例篇：如何找出狂打日志的“内鬼”？\n\n你好，我是倪朋飞。\n\n前两节，我们学了文件系统和磁盘的 I/O 原理，我先带你复习一下。\n\n文件系统，是对存储设备上的文件进行组织管理的一种机制。为了支持各类不同的文件系统，Linux 在各种文件系统上，抽象了一层虚拟文件系统 VFS。\n\n它定义了一组所有文件系统都支持的数据结构和标准接口。这样，应用程序和内核中的其他子系统，就只需要跟 VFS 提供的统一接口进行交互。\n\n在文件系统的下层，为了支持各种不同类型的存储设备，Linux 又在各种存储设备的基础上，抽象了一个通用块层。\n\n通用块层，为文件系统和应用程序提供了访问块设备的标准接口；同时，为各种块设备的驱动程序提供了统一的框架。此外，通用块层还会对文件系统和应用程序发送过来的 I/O 请求进行排队，并通过重新排序、请求合并等方式，提高磁盘读写的效率。\n\n通用块层的下一层，自然就是设备层了，包括各种块设备的驱动程序以及物理存储设备。\n\n文件系统、通用块层以及设备层，就构成了 Linux 的存储 I/O 栈。存储系统的 I/O ，通常是整个系统中最慢的一环。所以，Linux 采用多种缓存机制，来优化 I/O 的效率，比方说，\n\n- 为了优化文件访问的性能，采用页缓存、索引节点缓存、目录项缓存等多种缓存机制，减少对下层块设备的直接调用。\n- 同样的，为了优化块设备的访问效率，使用缓冲区来缓存块设备的数据。\n\n不过，在碰到文件系统和磁盘的 I/O 问题时，具体应该怎么定位和分析呢？今天，我就以一个最常见的应用程序记录大量日志的案例，带你来分析这种情况。\n\n## 案例准备\n\n本次案例还是基于 Ubuntu 18.04，同样适用于其他的 Linux 系统。我使用的案例环境如下所示：\n\n- 机器配置：2 CPU，8GB 内存\n- 预先安装 docker、sysstat 等工具，如 apt install [docker.io](https://docker.io/) sysstat\n\n这里要感谢唯品会资深运维工程师阳祥义帮忙，分担了今天的案例。这个案例，是一个用 Python 开发的小应用，为了方便运行，我把它打包成了一个 Docker 镜像。这样，你只要运行 Docker 命令，就可以启动它。\n\n接下来，打开一个终端，SSH 登录到案例所用的机器中，并安装上述工具。跟以前一样，案例中所有命令，都默认以 root 用户运行。如果你是用普通用户身份登陆系统，请运行 sudo su root 命令，切换到 root 用户。\n\n到这里，准备工作就完成了。接下来，我们正式进入操作环节。\n\n\u003e 温馨提示：案例中 Python 应用的核心逻辑比较简单，你可能一眼就能看出问题，但实际生产环境中的源码就复杂多了。所以，我依旧建议，操作之前别看源码，避免先入为主，要把它当成一个黑盒来分析。这样，你可以更好把握住，怎么从系统的资源使用问题出发，分析出瓶颈所在的应用，以及瓶颈在应用中大概的位置。\n\n## 案例分析\n\n首先，我们在终端中执行下面的命令，运行今天的目标应用：\n\n```shell\n$ docker run -v /tmp:/tmp --name=app -itd feisky/logapp \n```\n\n然后，在终端中运行 ps 命令，确认案例应用正常启动。如果操作无误，你应该可以在 ps 的输出中，看到一个 app.py 的进程：\n\n```bash\n$ ps -ef | grep /app.py \nroot     18940 18921 73 14:41 pts/0    00:00:02 python /app.py \n```\n\n接着，我们来看看系统有没有性能问题。要观察哪些性能指标呢？前面文章中，我们知道 CPU、内存和磁盘 I/O 等系统资源，很容易出现资源瓶颈，这就是我们观察的方向了。我们来观察一下这些资源的使用情况。\n\n当然，动手之前你应该想清楚，要用哪些工具来做，以及工具的使用顺序又是怎样的。你可以先回忆下前面的案例和思路，自己想一想，然后再继续下面的步骤。\n\n我的想法是，我们可以先用 top ，来观察 CPU 和内存的使用情况；然后再用 iostat ，来观察磁盘的 I/O 情况。\n\n所以，接下来，你可以在终端中运行 top 命令，观察 CPU 和内存的使用情况：\n\n```yaml\n# 按 1 切换到每个 CPU 的使用情况 \n$ top \ntop - 14:43:43 up 1 day,  1:39,  2 users,  load average: 2.48, 1.09, 0.63 \nTasks: 130 total,   2 running,  74 sleeping,   0 stopped,   0 zombie \n%Cpu0  :  0.7 us,  6.0 sy,  0.0 ni,  0.7 id, 92.7 wa,  0.0 hi,  0.0 si,  0.0 st \n%Cpu1  :  0.0 us,  0.3 sy,  0.0 ni, 92.3 id,  7.3 wa,  0.0 hi,  0.0 si,  0.0 st \nKiB Mem :  8169308 total,   747684 free,   741336 used,  6680288 buff/cache \nKiB Swap:        0 total,        0 free,        0 used.  7113124 avail Mem  \n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND \n18940 root      20   0  656108 355740   5236 R   6.3  4.4   0:12.56 python \n1312 root      20   0  236532  24116   9648 S   0.3  0.3   9:29.80 python3 \n```\n\n观察 top 的输出，你会发现，CPU0 的使用率非常高，它的系统 CPU 使用率（sys%）为 6%，而 iowait 超过了 90%。这说明 CPU0 上，可能正在运行 I/O 密集型的进程。不过，究竟是什么原因呢？这个疑问先保留着，我们先继续看完。\n\n接着我们来看，进程部分的 CPU 使用情况。你会发现， python 进程的 CPU 使用率已经达到了 6%，而其余进程的 CPU 使用率都比较低，不超过 0.3%。看起来 python 是个可疑进程。记下 python 进程的 PID 号 18940，我们稍后分析。\n\n最后再看内存的使用情况，总内存 8G，剩余内存只有 730 MB，而 Buffer/Cache 占用内存高达 6GB 之多，这说明内存主要被缓存占用。虽然大部分缓存可回收，我们还是得了解下缓存的去处，确认缓存使用都是合理的。\n\n到这一步，你基本可以判断出，CPU 使用率中的 iowait 是一个潜在瓶颈，而内存部分的缓存占比较大，那磁盘 I/O 又是怎么样的情况呢？\n\n我们在终端中按 Ctrl+C ，停止 top 命令，再运行 iostat 命令，观察 I/O 的使用情况：\n\n```bash\n# -d 表示显示 I/O 性能指标，-x 表示显示扩展统计（即所有 I/O 指标） \n$ iostat -x -d 1 \nDevice            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util \nloop0            0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00 \nsdb              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00 \nsda              0.00   64.00      0.00  32768.00     0.00     0.00   0.00   0.00    0.00 7270.44 1102.18     0.00   512.00  15.50  99.20\n```\n\n还记得这些性能指标的含义吗？先自己回忆一下，如果实在想不起来，查看上一节的内容，或者用 man iostat 查询。\n\n观察 iostat 的最后一列，你会看到，磁盘 sda 的 I/O 使用率已经高达 99%，很可能已经接近 I/O 饱和。\n\n再看前面的各个指标，每秒写磁盘请求数是 64 ，写大小是 32 MB，写请求的响应时间为 7 秒，而请求队列长度则达到了 1100。\n\n超慢的响应时间和特长的请求队列长度，进一步验证了 I/O 已经饱和的猜想。此时，sda 磁盘已经遇到了严重的性能瓶颈。\n\n到这里，也就可以理解，为什么前面看到的 iowait 高达 90% 了，这正是磁盘 sda 的 I/O 瓶颈导致的。接下来的重点就是分析 I/O 性能瓶颈的根源了。那要怎么知道，这些 I/O 请求相关的进程呢？\n\n不知道你还记不记得，上一节我曾提到过，可以用 pidstat 或者 iotop ，观察进程的 I/O 情况。这里，我就用 pidstat 来看一下。\n\n使用 pidstat 加上 -d 参数，就可以显示每个进程的 I/O 情况。所以，你可以在终端中运行如下命令来观察：\n\n```bash\n$ pidstat -d 1  \n15:08:35      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command \n15:08:36        0     18940      0.00  45816.00      0.00      96  python  \n15:08:36      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command \n15:08:37        0       354      0.00      0.00      0.00     350  jbd2/sda1-8 \n15:08:37        0     18940      0.00  46000.00      0.00      96  python \n15:08:37        0     20065      0.00      0.00      0.00    1503  kworker/u4:2 \n```\n\n从 pidstat 的输出，你可以发现，只有 python 进程的写比较大，而且每秒写的数据超过 45 MB，比上面 iostat 发现的 32MB 的结果还要大。很明显，正是 python 进程导致了 I/O 瓶颈。\n\n再往下看 iodelay 项。虽然只有 python 在大量写数据，但你应该注意到了，有两个进程 （kworker 和 jbd2 ）的延迟，居然比 python 进程还大很多。\n\n这其中，kworker 是一个内核线程，而 jbd2 是 ext4 文件系统中，用来保证数据完整性的内核线程。他们都是保证文件系统基本功能的内核线程，所以具体细节暂时就不用管了，我们只需要明白，它们延迟的根源还是大量 I/O。\n\n综合 pidstat 的输出来看，还是 python 进程的嫌疑最大。接下来，我们来分析 python 进程到底在写什么。\n\n首先留意一下 python 进程的 PID 号， 18940。看到 18940 ，你有没有觉得熟悉？其实前面在使用 top 时，我们记录过的 CPU 使用率最高的进程，也正是它。不过，虽然在 top 中使用率最高，也不过是 6%，并不算高。所以，以 I/O 问题为分析方向还是正确的。\n\n知道了进程的 PID 号，具体要怎么查看写的情况呢？\n\n其实，我在系统调用的案例中讲过，读写文件必须通过系统调用完成。观察系统调用情况，就可以知道进程正在写的文件。想起 strace 了吗，它正是我们分析系统调用时最常用的工具。\n\n接下来，我们在终端中运行 strace 命令，并通过 -p 18940 指定 python 进程的 PID 号：\n\n```perl\n$ strace -p 18940 \nstrace: Process 18940 attached \n...\nmmap(NULL, 314576896, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f0f7aee9000 \nmmap(NULL, 314576896, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f0f682e8000 \nwrite(3, \"2018-12-05 15:23:01,709 - __main\"..., 314572844 \n) = 314572844 \nmunmap(0x7f0f682e8000, 314576896)       = 0 \nwrite(3, \"\\n\", 1)                       = 1 \nmunmap(0x7f0f7aee9000, 314576896)       = 0 \nclose(3)                                = 0 \nstat(\"/tmp/logtest.txt.1\", {st_mode=S_IFREG|0644, st_size=943718535, ...}) = 0 \n```\n\n从 write() 系统调用上，我们可以看到，进程向文件描述符编号为 3 的文件中，写入了 300MB 的数据。看来，它应该是我们要找的文件。不过，write() 调用中只能看到文件的描述符编号，文件名和路径还是未知的。\n\n再观察后面的 stat() 调用，你可以看到，它正在获取 /tmp/logtest.txt.1 的状态。 这种“点 + 数字格式”的文件，在日志回滚中非常常见。我们可以猜测，这是第一个日志回滚文件，而正在写的日志文件路径，则是 /tmp/logtest.txt。\n\n当然，这只是我们的猜测，自然还需要验证。这里，我再给你介绍一个新的工具 lsof。它专门用来查看进程打开文件列表，不过，这里的“文件”不只有普通文件，还包括了目录、块设备、动态库、网络套接字等。\n\n接下来，我们在终端中运行下面的 lsof 命令，看看进程 18940 都打开了哪些文件：\n\n```bash\n$ lsof -p 18940 \nCOMMAND   PID USER   FD   TYPE DEVICE  SIZE/OFF    NODE NAME \npython  18940 root  cwd    DIR   0,50      4096 1549389 / \npython  18940 root  rtd    DIR   0,50      4096 1549389 / \n… \npython  18940 root    2u   CHR  136,0       0t0       3 /dev/pts/0 \npython  18940 root    3w   REG    8,1 117944320     303 /tmp/logtest.txt \n```\n\n这个输出界面中，有几列我简单介绍一下，FD 表示文件描述符号，TYPE 表示文件类型，NAME 表示文件路径。这也是我们需要关注的重点。\n\n再看最后一行，这说明，这个进程打开了文件 /tmp/logtest.txt，并且它的文件描述符是 3 号，而 3 后面的 w ，表示以写的方式打开。\n\n这跟刚才 strace 完我们猜测的结果一致，看来这就是问题的根源：进程 18940 以每次 300MB 的速度，在“疯狂”写日志，而日志文件的路径是 /tmp/logtest.txt。\n\n既然找出了问题根源，接下来按照惯例，就该查看源代码，然后分析为什么这个进程会狂打日志了。\n\n你可以运行 docker cp 命令，把案例应用的源代码拷贝出来，然后查看它的内容。（你也可以点击[这里](https://github.com/feiskyer/linux-perf-examples/tree/master/logging-app)查看案例应用的源码）：\n\n```python\n# 拷贝案例应用源代码到当前目录\n$ docker cp app:/app.py .  \n# 查看案例应用的源代码\n$ cat app.py  \nlogger = logging.getLogger(__name__) \nlogger.setLevel(level=logging.INFO) \nrHandler = RotatingFileHandler(\"/tmp/logtest.txt\", maxBytes=1024 * 1024 * 1024, backupCount=1) \nrHandler.setLevel(logging.INFO)  \ndef write_log(size): \n  '''Write logs to file''' \n  message = get_message(size) \n  while True: \n    logger.info(message) \n    time.sleep(0.1)  \nif __name__ == '__main__': \n  msg_size = 300 * 1024 * 1024 \n  write_log(msg_size) \n```\n\n分析这个源码，我们发现，它的日志路径是 /tmp/logtest.txt，默认记录 INFO 级别以上的所有日志，而且每次写日志的大小是 300MB。这跟我们上面的分析结果是一致的。\n\n一般来说，生产系统的应用程序，应该有动态调整日志级别的功能。继续查看源码，你会发现，这个程序也可以调整日志级别。如果你给它发送 SIGUSR1 信号，就可以把日志调整为 INFO 级；发送 SIGUSR2 信号，则会调整为 WARNING 级：\n\n```python\ndef set_logging_info(signal_num, frame): \n  '''Set loging level to INFO when receives SIGUSR1''' \n  logger.setLevel(logging.INFO)  \ndef set_logging_warning(signal_num, frame): \n  '''Set loging level to WARNING when receives SIGUSR2''' \n  logger.setLevel(logging.WARNING)  \nsignal.signal(signal.SIGUSR1, set_logging_info) \nsignal.signal(signal.SIGUSR2, set_logging_warning) \n```\n\n根据源码中的日志调用 logger. info(message) ，我们知道，它的日志是 INFO 级，这也正是它的默认级别。那么，只要把默认级别调高到 WARNING 级，日志问题应该就解决了。\n\n接下来，我们就来检查一下，刚刚的分析对不对。在终端中运行下面的 kill 命令，给进程 18940 发送 SIGUSR2 信号：\n\n```shell\n$ kill -SIGUSR2 18940 \n```\n\n然后，再执行 top 和 iostat 观察一下：\n\n```bash\n$ top \n... \n%Cpu(s):  0.3 us,  0.2 sy,  0.0 ni, 99.5 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st \n$ iostat -d -x 1\nDevice            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util \nloop0            0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00 \nsdb              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00 \nsda              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00\n```\n\n观察 top 和 iostat 的输出，你会发现，稍等一段时间后，iowait 会变成 0，而 sda 磁盘的 I/O 使用率也会逐渐减少到 0。\n\n到这里，我们不仅定位了狂打日志的应用程序，并通过调高日志级别的方法，完美解决了 I/O 的性能瓶颈。\n\n案例最后，当然不要忘了运行下面的命令，停止案例应用：\n\n```shell\n$ docker rm -f app \n```\n\n## 小结\n\n日志，是了解应用程序内部运行情况，最常用、也最有效的工具。无论是操作系统，还是应用程序，都会记录大量的运行日志，以便事后查看历史记录。这些日志一般按照不同级别来开启，比如，开发环境通常打开调试级别的日志，而线上环境则只记录警告和错误日志。\n\n在排查应用程序问题时，我们可能需要，在线上环境临时开启应用程序的调试日志。有时候，事后一不小心就忘了调回去。没把线上的日志调高到警告级别，可能会导致 CPU 使用率、磁盘 I/O 等一系列的性能问题，严重时，甚至会影响到同一台服务器上运行的其他应用程序。\n\n今后，在碰到这种“狂打日志”的场景时，你可以用 iostat、strace、lsof 等工具来定位狂打日志的进程，找出相应的日志文件，再通过应用程序的接口，调整日志级别来解决问题。\n\n如果应用程序不能动态调整日志级别，你可能还需要修改应用的配置，并重启应用让配置生效。\n\n## 思考\n\n最后，给你留一个思考题。\n\n在今天的案例开始时，我们用 top 和 iostat 查看了系统资源的使用情况。除了 CPU 和磁盘 I/O 外，剩余内存也比较少，而内存主要被 Buffer/Cache 占用。\n\n那么，今天的问题就是，这些内存到底是被 Buffer 还是 Cache 占用了呢？有没有什么方法来确认你的分析结果呢？\n\n欢迎在留言区和我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 27 案例篇：为什么我的磁盘IO延迟很高？\n\n你好，我是倪朋飞。\n\n上一节，我们研究了一个狂打日志引发 I/O 性能问题的案例，先来简单回顾一下。\n\n日志，是了解应用程序内部运行情况，最常用也是最有效的工具。日志一般会分为调试、信息、警告、错误等多个不同级别。\n\n通常，生产环境只用开启警告级别的日志，这一般不会导致 I/O 问题。但在偶尔排查问题时，可能需要我们开启调试日志。调试结束后，很可能忘了把日志级别调回去。这时，大量的调试日志就可能会引发 I/O 性能问题。\n\n你可以用 iostat ，确认是否有 I/O 性能瓶颈。再用 strace 和 lsof ，来定位应用程序以及它正在写入的日志文件路径。最后通过应用程序的接口调整日志级别，完美解决 I/O 问题。\n\n不过，如果应用程序没有动态调整日志级别的功能，你还需要修改应用配置并重启应用，以便让配置生效。\n\n今天，我们再来看一个新的案例。这次案例是一个基于 Python Flask 框架的 Web 应用，它提供了一个查询单词热度的 API，但是 API 的响应速度并不让人满意。\n\n非常感谢携程系统研发部资深后端工程师董国星，帮助提供了今天的案例。\n\n## **案例准备**\n\n本次案例还是基于 Ubuntu 18.04，同样适用于其他的 Linux 系统。我使用的案例环境如下所示：\n\n- 机器配置：2 CPU，8GB 内存\n- 预先安装 docker、sysstat 等工具，如 apt install [docker.io](https://docker.io/) sysstat\n\n为了方便你运行今天的案例，我把它打包成了一个 Docker 镜像。这样，你就只需要运行 Docker 命令就可以启动它。\n\n今天的案例需要两台虚拟机，其中一台是案例分析的目标机器，运行 Flask 应用，它的 IP 地址是 192.168.0.10；而另一台作为客户端，请求单词的热度。我画了一张图表示它们的关系，如下所示：\n\n![img](a8cc1b02b8c896380d2c53b8018bddbf.png)\n\n接下来，打开两个终端，分别 SSH 登录到这两台虚拟机中，并在第一台虚拟机中，安装上述工具。\n\n跟以前一样，案例中所有命令都默认以 root 用户运行，如果你是用普通用户身份登陆系统，请运行 sudo su root 命令切换到 root 用户。\n\n到这里，准备工作就完成了。接下来，我们正式进入操作环节。\n\n\u003e 温馨提示：案例中 Python 应用的核心逻辑比较简单，你可能一眼就能看出问题，但实际生产环境中的源码就复杂多了。所以，我依旧建议，操作之前别看源码，避免先入为主，而要把它当成一个黑盒来分析。这样，你可以更好把握，怎么从系统的资源使用问题出发，分析出瓶颈所在的应用，以及瓶颈在应用中大概的位置。\n\n## **案例分析**\n\n首先，我们在第一个终端中执行下面的命令，运行本次案例要分析的目标应用：\n\n```shell\n$ docker run --name=app -p 10000:80 -itd feisky/word-pop \n```\n\n然后，在第二个终端中运行 curl 命令，访问 http://192.168.0.10:1000/，确认案例正常启动。你应该可以在 curl 的输出界面里，看到一个 hello world 的输出：\n\n```ruby\n$ curl http://192.168.0.10:10000/ \nhello world \n```\n\n接下来，在第二个终端中，访问案例应用的单词热度接口，也就是 http://192.168.0.10:1000/popularity/word。\n\n```shell\n$ curl http://192.168.0.10:1000/popularity/word \n```\n\n稍等一会儿，你会发现，这个接口居然这么长时间都没响应，究竟是怎么回事呢？我们先回到终端一来分析一下。\n\n我们试试在第一个终端里，随便执行一个命令，比如执行 df 命令，查看一下文件系统的使用情况。奇怪的是，这么简单的命令，居然也要等好久才有输出。\n\n```bash\n$ df \nFilesystem     1K-blocks    Used Available Use% Mounted on \nudev             4073376       0   4073376   0% /dev \ntmpfs             816932    1188    815744   1% /run \n/dev/sda1       30308240 8713640  21578216  29% / \n```\n\n通过 df 我们知道，系统还有足够多的磁盘空间。那为什么响应会变慢呢？看来还是得观察一下，系统的资源使用情况，像是 CPU、内存和磁盘 I/O 等的具体使用情况。\n\n这里的思路其实跟上一个案例比较类似，我们可以先用 top 来观察 CPU 和内存的使用情况，然后再用 iostat 来观察磁盘的 I/O 情况。\n\n为了避免分析过程中 curl 请求突然结束，我们回到终端二，按 Ctrl+C 停止刚才的应用程序；然后，把 curl 命令放到一个循环里执行；这次我们还要加一个 time 命令，观察每次的执行时间：\n\n```shell\n$ while true; do time curl http://192.168.0.10:10000/popularity/word; sleep 1; done \n```\n\n继续回到终端一来分析性能。我们在终端一中运行 top 命令，观察 CPU 和内存的使用情况：\n\n```yaml\n$ top \ntop - 14:27:02 up 10:30,  1 user,  load average: 1.82, 1.26, 0.76 \nTasks: 129 total,   1 running,  74 sleeping,   0 stopped,   0 zombie \n%Cpu0  :  3.5 us,  2.1 sy,  0.0 ni,  0.0 id, 94.4 wa,  0.0 hi,  0.0 si,  0.0 st \n%Cpu1  :  2.4 us,  0.7 sy,  0.0 ni, 70.4 id, 26.5 wa,  0.0 hi,  0.0 si,  0.0 st \nKiB Mem :  8169300 total,  3323248 free,   436748 used,  4409304 buff/cache \nKiB Swap:        0 total,        0 free,        0 used.  7412556 avail Mem  \n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND \n12280 root      20   0  103304  28824   7276 S  14.0  0.4   0:08.77 python \n   16 root      20   0       0      0      0 S   0.3  0.0   0:09.22 ksoftirqd/1 \n1549 root      20   0  236712  24480   9864 S   0.3  0.3   3:31.38 python3 \n```\n\n观察 top 的输出可以发现，两个 CPU 的 iowait 都非常高。特别是 CPU0， iowait 已经高达 94 %，而剩余内存还有 3GB，看起来也是充足的。\n\n再往下看，进程部分有一个 python 进程的 CPU 使用率稍微有点高，达到了 14%。虽然 14% 并不能成为性能瓶颈，不过有点嫌疑——可能跟 iowait 的升高有关。\n\n那这个 PID 号为 12280 的 python 进程，到底是不是我们的案例应用呢？\n\n我们在第一个终端中，按下 Ctrl+C，停止 top 命令；然后执行下面的 ps 命令，查找案例应用 [app.py](http://app.py/) 的 PID 号：\n\n```bash\n$ ps aux | grep app.py \nroot     12222  0.4  0.2  96064 23452 pts/0    Ss+  14:37   0:00 python /app.py \nroot     12280 13.9  0.3 102424 27904 pts/0    Sl+  14:37   0:09 /usr/local/bin/python /app.py \n```\n\n从 ps 的输出，你可以看到，这个 CPU 使用率较高的进程，正是我们的案例应用。不过先别着急分析 CPU 问题，毕竟 iowait 已经高达 94%， I/O 问题才是我们首要解决的。\n\n接下来，我们在终端一中，运行下面的 iostat 命令，其中:\n\n- -d 选项是指显示出 I/O 的性能指标；\n- -x 选项是指显示出扩展统计信息（即显示所有 I/O 指标）。\n\n```bash\n$ iostat -d -x 1\nDevice            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util \nloop0            0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00 \nsda              0.00   71.00      0.00  32912.00     0.00     0.00   0.00   0.00    0.00 18118.31 241.89     0.00   463.55  13.86  98.40 \n```\n\n再次看到 iostat 的输出，你还记得这个界面中的性能指标含义吗？先自己回忆一下，如果实在想不起来，一定要先查看上节内容，或者用 man iostat 查明白。\n\n明白了指标含义，再来具体观察 iostat 的输出。你可以发现，磁盘 sda 的 I/O 使用率已经达到 98% ，接近饱和了。而且，写请求的响应时间高达 18 秒，每秒的写数据为 32 MB，显然写磁盘碰到了瓶颈。\n\n那要怎么知道，这些 I/O 请求到底是哪些进程导致的呢？我想，你已经还记得上一节我们用到的 pidstat。\n\n在终端一中，运行下面的 pidstat 命令，观察进程的 I/O 情况：\n\n```bash\n$ pidstat -d 1 \n14:39:14      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command \n14:39:15        0     12280      0.00 335716.00      0.00       0  python \n```\n\n从 pidstat 的输出，我们再次看到了 PID 号为 12280 的结果。这说明，正是案例应用引发 I/O 的性能瓶颈。\n\n走到这一步，你估计觉得，接下来就很简单了，上一个案例不刚刚学过吗？无非就是，先用 strace 确认它是不是在写文件，再用 lsof 找出文件描述符对应的文件即可。\n\n到底是不是这样呢？我们不妨来试试。还是在终端一中，执行下面的 strace 命令：\n\n```cpp\n$ strace -p 12280 \nstrace: Process 12280 attached \nselect(0, NULL, NULL, NULL, {tv_sec=0, tv_usec=567708}) = 0 (Timeout) \nstat(\"/usr/local/lib/python3.7/importlib/_bootstrap.py\", {st_mode=S_IFREG|0644, st_size=39278, ...}) = 0 \nstat(\"/usr/local/lib/python3.7/importlib/_bootstrap.py\", {st_mode=S_IFREG|0644, st_size=39278, ...}) = 0 \n```\n\n从 strace 中，你可以看到大量的 stat 系统调用，并且大都为 python 的文件，但是，请注意，这里并没有任何 write 系统调用。\n\n由于 strace 的输出比较多，我们可以用 grep ，来过滤一下 write，比如：\n\n```perl\n$ strace -p 12280 2\u003e\u00261 | grep write \n```\n\n遗憾的是，这里仍然没有任何输出。\n\n难道此时已经没有性能问题了吗？重新执行刚才的 top 和 iostat 命令，你会不幸地发现，性能问题仍然存在。\n\n我们只好综合 strace、pidstat 和 iostat 这三个结果来分析了。很明显，你应该发现了这里的矛盾：iostat 已经证明磁盘 I/O 有性能瓶颈，而 pidstat 也证明了，这个瓶颈是由 12280 号进程导致的，但 strace 跟踪这个进程，却没有找到任何 write 系统调用。\n\n这就奇怪了。难道因为案例使用的编程语言是 Python ，而 Python 是解释型的，所以找不到？还是说，因为案例运行在 Docker 中呢？这里留个悬念，你自己想想。\n\n文件写，明明应该有相应的 write 系统调用，但用现有工具却找不到痕迹，这时就该想想换工具的问题了。怎样才能知道哪里在写文件呢？\n\n这里我给你介绍一个新工具， [filetop](https://github.com/iovisor/bcc/blob/master/tools/filetop.py)。它是 [bcc](https://github.com/iovisor/bcc) 软件包的一部分，基于 Linux 内核的 eBPF（extended Berkeley Packet Filters）机制，主要跟踪内核中文件的读写情况，并输出线程 ID（TID）、读写大小、读写类型以及文件名称。\n\neBPF 的工作原理，你暂时不用深究，后面内容我们会逐渐接触到，先会使用就可以了。\n\n至于老朋友 bcc 的安装方法，可以参考它的 Github 网站 https://github.com/iovisor/bcc。比如在 Ubuntu 16 以上的版本中，你可以运行下面的命令来安装它：\n\n```bash\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDD \necho \"deb https://repo.iovisor.org/apt/$(lsb_release -cs) $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/iovisor.list \nsudo apt-get update \nsudo apt-get install bcc-tools libbcc-examples linux-headers-$(uname -r)\n```\n\n安装后，bcc 提供的所有工具，就全部安装到了 /usr/share/bcc/tools 这个目录中。接下来我们就用这个工具，观察一下文件的读写情况。\n\n首先，在终端一中运行下面的命令：\n\n```yaml\n# 切换到工具目录 \n$ cd /usr/share/bcc/tools  \n# -C 选项表示输出新内容时不清空屏幕 \n$ ./filetop -C  \nTID    COMM             READS  WRITES R_Kb    W_Kb    T FILE \n514    python           0      1      0       2832    R 669.txt \n514    python           0      1      0       2490    R 667.txt \n514    python           0      1      0       2685    R 671.txt \n514    python           0      1      0       2392    R 670.txt \n514    python           0      1      0       2050    R 672.txt  \n... \nTID    COMM             READS  WRITES R_Kb    W_Kb    T FILE \n514    python           2      0      5957    0       R 651.txt \n514    python           2      0      5371    0       R 112.txt \n514    python           2      0      4785    0       R 861.txt \n514    python           2      0      4736    0       R 213.txt \n514    python           2      0      4443    0       R 45.txt  \n```\n\n你会看到，filetop 输出了 8 列内容，分别是线程 ID、线程命令行、读写次数、读写的大小（单位 KB）、文件类型以及读写的文件名称。\n\n这些内容里，你可能会看到很多动态链接库，不过这不是我们的重点，暂且忽略即可。我们的重点，是一个 python 应用，所以要特别关注 python 相关的内容。\n\n多观察一会儿，你就会发现，每隔一段时间，线程号为 514 的 python 应用就会先写入大量的 txt 文件，再大量地读。\n\n线程号为 514 的线程，属于哪个进程呢？我们可以用 ps 命令查看。先在终端一中，按下 Ctrl+C ，停止 filetop ；然后，运行下面的 ps 命令。这个输出的第二列内容，就是我们想知道的进程号：\n\n```bash\n$ ps -efT | grep 514\nroot     12280  514 14626 33 14:47 pts/0    00:00:05 /usr/local/bin/python /app.py \n```\n\n我们看到，这个线程正是案例应用 12280 的线程。终于可以先松一口气，不过还没完，filetop 只给出了文件名称，却没有文件路径，还得继续找啊。\n\n我再介绍一个好用的工具，opensnoop 。它同属于 bcc 软件包，可以动态跟踪内核中的 open 系统调用。这样，我们就可以找出这些 txt 文件的路径。\n\n接下来，在终端一中，运行下面的 opensnoop 命令：\n\n```bash\n$ opensnoop \n12280  python              6   0 /tmp/9046db9e-fe25-11e8-b13f-0242ac110002/650.txt \n12280  python              6   0 /tmp/9046db9e-fe25-11e8-b13f-0242ac110002/651.txt \n12280  python              6   0 /tmp/9046db9e-fe25-11e8-b13f-0242ac110002/652.txt \n```\n\n这次，通过 opensnoop 的输出，你可以看到，这些 txt 路径位于 /tmp 目录下。你还能看到，它打开的文件数量，按照数字编号，从 0.txt 依次增大到 999.txt，这可远多于前面用 filetop 看到的数量。\n\n综合 filetop 和 opensnoop ，我们就可以进一步分析了。我们可以大胆猜测，案例应用在写入 1000 个 txt 文件后，又把这些内容读到内存中进行处理。我们来检查一下，这个目录中是不是真的有 1000 个文件：\n\n```bash\n$ ls /tmp/9046db9e-fe25-11e8-b13f-0242ac110002 | wc -l \nls: cannot access '/tmp/9046db9e-fe25-11e8-b13f-0242ac110002': No such file or directory \n0 \n```\n\n操作后却发现，目录居然不存在了。怎么回事呢？我们回到 opensnoop 再观察一会儿：\n\n```bash\n$ opensnoop \n12280  python              6   0 /tmp/defee970-fe25-11e8-b13f-0242ac110002/261.txt \n12280  python              6   0 /tmp/defee970-fe25-11e8-b13f-0242ac110002/840.txt \n12280  python              6   0 /tmp/defee970-fe25-11e8-b13f-0242ac110002/136.txt \n```\n\n原来，这时的路径已经变成了另一个目录。这说明，这些目录都是应用程序动态生成的，用完就删了。\n\n结合前面的所有分析，我们基本可以判断，案例应用会动态生成一批文件，用来临时存储数据，用完就会删除它们。但不幸的是，正是这些文件读写，引发了 I/O 的性能瓶颈，导致整个处理过程非常慢。\n\n当然，我们还需要验证这个猜想。老办法，还是查看应用程序的源码 [app.py](https://github.com/feiskyer/linux-perf-examples/blob/master/io-latency/app.py)，\n\n```python\n@app.route(\"/popularity/\u003cword\u003e\") \ndef word_popularity(word): \n  dir_path = '/tmp/{}'.format(uuid.uuid1()) \n  count = 0 \n  sample_size = 1000 \n   \n  def save_to_file(file_name, content): \n    with open(file_name, 'w') as f: \n    f.write(content)  \n  try: \n    # initial directory firstly \n    os.mkdir(dir_path)  \n    # save article to files \n    for i in range(sample_size): \n        file_name = '{}/{}.txt'.format(dir_path, i) \n        article = generate_article() \n        save_to_file(file_name, article)  \n    # count word popularity \n    for root, dirs, files in os.walk(dir_path): \n        for file_name in files: \n            with open('{}/{}'.format(dir_path, file_name)) as f: \n                if validate(word, f.read()): \n                    count += 1 \n    finally: \n        # clean files \n        shutil.rmtree(dir_path, ignore_errors=True)  \n    return jsonify({'popularity': count / sample_size * 100, 'word': word}) \n```\n\n源码中可以看到，这个案例应用，在每个请求的处理过程中，都会生成一批临时文件，然后读入内存处理，最后再把整个目录删除掉。\n\n这是一种常见的利用磁盘空间处理大量数据的技巧，不过，本次案例中的 I/O 请求太重，导致磁盘 I/O 利用率过高。\n\n要解决这一点，其实就是算法优化问题了。比如在内存充足时，就可以把所有数据都放到内存中处理，这样就能避免 I/O 的性能问题。\n\n你可以检验一下，在终端二中分别访问 http://192.168.0.10:10000/popularity/word 和 http://192.168.0.10:10000/popular/word ，对比前后的效果：\n\n```go\n$ time curl http://192.168.0.10:10000/popularity/word\n{ \n  \"popularity\": 0.0, \n  \"word\": \"word\" \n} \nreal    2m43.172s \nuser    0m0.004s \nsys    0m0.007s\n$ time curl http://192.168.0.10:10000/popular/word\n{\n  \"popularity\": 0.0,\n  \"word\": \"word\"\n} \nreal    0m8.810s\nuser    0m0.010s\nsys    0m0.000s \n```\n\n新的接口只要 8 秒就可以返回，明显比一开始的 3 分钟好很多。\n\n当然，这只是优化的第一步，并且方法也不算完善，还可以做进一步的优化。不过，在实际系统中，我们大都是类似的做法，先用最简单的方法，尽早解决线上问题，然后再继续思考更好的优化方法。\n\n## 小结\n\n今天，我们分析了一个响应过慢的单词热度案例。\n\n首先，我们用 top、iostat，分析了系统的 CPU 和磁盘使用情况。我们发现了磁盘 I/O 瓶颈，也知道了这个瓶颈是案例应用导致的。\n\n接着，我们试着照搬上一节案例的方法，用 strace 来观察进程的系统调用，不过这次很不走运，没找到任何 write 系统调用。\n\n于是，我们又用了新的工具，借助动态追踪工具包 bcc 中的 filetop 和 opensnoop ，找出了案例应用的问题，发现这个根源是大量读写临时文件。\n\n找出问题后，优化方法就相对比较简单了。如果内存充足时，最简单的方法，就是把数据都放在速度更快的内存中，这样就没有磁盘 I/O 的瓶颈了。当然，再进一步，你可以还可以利用 Trie 树等各种算法，进一步优化单词处理的效率。\n\n## 思考\n\n最后，给你留一个思考题，也是我在文章中提到过的，让你思考的问题。\n\n今天的案例中，iostat 已经证明，磁盘 I/O 出现了性能瓶颈， pidstat 也证明了这个瓶颈是由 12280 号进程导致的。但是，strace 跟踪这个进程，却没有发现任何 write 系统调用。\n\n这究竟是怎么回事？难道是因为案例使用的编程语言 Python 本身是解释型？还是说，因为案例运行在 Docker 中呢？\n\n这里我小小提示一下。当你发现性能工具的输出无法解释时，最好返回去想想，是不是分析中漏掉了什么线索，或者去翻翻工具手册，看看是不是某些默认选项导致的。\n\n欢迎在留言区和我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 28 案例篇：一个SQL查询要15秒，这是怎么回事？\n\n你好，我是倪朋飞。\n\n上一节，我们分析了一个单词热度应用响应过慢的案例。当用 top、iostat 分析了系统的 CPU 和磁盘 I/O 使用情况后，我们发现系统出现了磁盘的 I/O 瓶颈，而且正是案例应用导致的。\n\n接着，在使用 strace 却没有任何发现后，我又给你介绍了两个新的工具 filetop 和 opensnoop，分析它们对系统调用 write() 和 open() 的追踪结果。\n\n我们发现，案例应用正在读写大量的临时文件，因此产生了性能瓶颈。找出瓶颈后，我们又用把文件数据都放在内存的方法，解决了磁盘 I/O 的性能问题。\n\n当然，你可能会说，在实际应用中，大量数据肯定是要存入数据库的，而不会直接用文本文件的方式存储。不过，数据库也不是万能的。当数据库出现性能问题时，又该如何分析和定位它的瓶颈呢？\n\n今天我们就来一起分析一个数据库的案例。这是一个基于 Python Flask 的商品搜索应用，商品信息存在 MySQL 中。这个应用可以通过 MySQL 接口，根据客户端提供的商品名称，去数据库表中查询商品信息。\n\n非常感谢唯品会资深运维工程师阳祥义，帮助提供了今天的案例。\n\n## 案例准备\n\n本次案例还是基于 Ubuntu 18.04，同样适用于其他的 Linux 系统。我使用的案例环境如下所示：\n\n- 机器配置：2 CPU，8GB 内存\n- 预先安装 docker、sysstat 、git、make 等工具，如 apt install [docker.io](https://docker.io/) sysstat make git\n\n其中，docker 和 sysstat 已经用过很多次，这里不再赘述；git 用来拉取本次案例所需脚本，这些脚本存储在 Github 代码仓库中；最后的 make 则是一个常用构建工具，这里用来运行今天的案例。\n\n案例总共由三个容器组成，包括一个 MySQL 数据库应用、一个商品搜索应用以及一个数据处理的应用。其中，商品搜索应用以 HTTP 的形式提供了一个接口：\n\n- /：返回 Index Page；\n- /db/insert/products/：插入指定数量的商品信息；\n- /products/：查询指定商品的信息，并返回处理时间。\n\n由于应用比较多，为了方便你运行它们，我把它们同样打包成了几个 Docker 镜像，并推送到了 Github 上。这样，你只需要运行几条命令，就可以启动了。\n\n今天的案例需要两台虚拟机，其中一台作为案例分析的目标机器，运行 Flask 应用，它的 IP 地址是 192.168.0.10；另一台则是作为客户端，请求单词的热度。我画了一张图表示它们的关系。\n\n![img](8c954570f6e46193505c2598a06cbc5d.png)\n\n接下来，打开两个终端，分别 SSH 登录到这两台虚拟机中，并在第一台虚拟机中安装上述工具。\n\n跟以前一样，案例中所有命令都默认以 root 用户运行，如果你是用普通用户身份登陆系统，请运行 sudo su root 命令切换到 root 用户。\n\n到这里，准备工作就完成了。接下来，我们正式进入操作环节。\n\n## 案例分析\n\n首先，我们在第一个终端中执行下面命令，拉取本次案例所需脚本：\n\n```shell\n$ git clone https://github.com/feiskyer/linux-perf-examples\n$ cd linux-perf-examples/mysql-slow\n```\n\n接着，执行下面的命令，运行本次的目标应用。正常情况下，你应该可以看到下面的输出：\n\n```bash\n# 注意下面的随机字符串是容器 ID，每次运行均会不同，并且你不需要关注它，因为我们只会用到名字\n$ make run\ndocker run --name=mysql -itd -p 10000:80 -m 800m feisky/mysql:5.6\nWARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.\n4156780da5be0b9026bcf27a3fa56abc15b8408e358fa327f472bcc5add4453f\ndocker run --name=dataservice -itd --privileged feisky/mysql-dataservice\nf724d0816d7e47c0b2b1ff701e9a39239cb9b5ce70f597764c793b68131122bb\ndocker run --name=app --network=container:mysql -itd feisky/mysql-slow\n81d3392ba25bb8436f6151662a13ff6182b6bc6f2a559fc2e9d873cd07224ab6\n```\n\n然后，再运行 docker ps 命令，确认三个容器都处在运行（Up）状态：\n\n```bash\n$ docker ps\nCONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS              PORTS                             NAMES\n9a4e3c580963        feisky/mysql-slow          \"python /app.py\"         42 seconds ago      Up 36 seconds                                         app\n2a47aab18082        feisky/mysql-dataservice   \"python /dataservice…\"   46 seconds ago      Up 41 seconds                                         dataservice\n4c3ff7b24748        feisky/mysql:5.6           \"docker-entrypoint.s…\"   47 seconds ago      Up 46 seconds       3306/tcp, 0.0.0.0:10000-\u003e80/tcp   mysql\n```\n\nMySQL 数据库的启动过程，需要做一些初始化工作，这通常需要花费几分钟时间。你可以运行 docker logs 命令，查看它的启动过程。\n\n当你看到下面这个输出时，说明 MySQL 初始化完成，可以接收外部请求了：\n\n```yaml\n$ docker logs -f mysql\n...\n... [Note] mysqld: ready for connections.\nVersion: '5.6.42-log'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server (GPL)\n```\n\n而商品搜索应用则是在 10000 端口监听。你可以按 Ctrl+C ，停止 docker logs 命令；然后，执行下面的命令，确认它也已经正常运行。如果一切正常，你会看到 Index Page 的输出：\n\n```ruby\n$ curl http://127.0.0.1:10000/\nIndex Page\n```\n\n接下来，运行 make init 命令，初始化数据库，并插入 10000 条商品信息。这个过程比较慢，比如在我的机器中，就花了十几分钟时间。耐心等待一段时间后，你会看到如下的输出：\n\n```lua\n$ make init\ndocker exec -i mysql mysql -uroot -P3306 \u003c tables.sql\ncurl http://127.0.0.1:10000/db/insert/products/10000\ninsert 10000 lines\n```\n\n接着，我们切换到第二个终端，访问一下商品搜索的接口，看看能不能找到想要的商品。执行如下的 curl 命令：\n\n```kotlin\n$ curl http://192.168.0.10:10000/products/geektime\nGot data: () in 15.364538192749023 sec\n```\n\n稍等一会儿，你会发现，这个接口返回的是空数据，而且处理时间超过 15 秒。这么慢的响应速度让人无法忍受，到底出了什么问题呢？\n\n既然今天用了 MySQL，你估计会猜到是慢查询的问题。\n\n不过别急，在具体分析前，为了避免在分析过程中客户端的请求结束，我们把 curl 命令放到一个循环里执行。同时，为了避免给系统过大压力，我们设置在每次查询后，都先等待 5 秒，然后再开始新的请求。\n\n所以，你可以在终端二中，继续执行下面的命令：\n\n```shell\n$ while true; do curl http://192.168.0.10:10000/products/geektime; sleep 5; done\n```\n\n接下来，重新回到终端一中，分析接口响应速度慢的原因。不过，重回终端一后，你会发现系统响应也明显变慢了，随便执行一个命令，都得停顿一会儿才能看到输出。\n\n这跟上一节的现象很类似，看来，我们还是得观察一下系统的资源使用情况，比如 CPU、内存和磁盘 I/O 等的情况。\n\n首先，我们在终端一执行 top 命令，分析系统的 CPU 使用情况：\n\n```yaml\n$ top\ntop - 12:02:15 up 6 days,  8:05,  1 user,  load average: 0.66, 0.72, 0.59\nTasks: 137 total,   1 running,  81 sleeping,   0 stopped,   0 zombie\n%Cpu0  :  0.7 us,  1.3 sy,  0.0 ni, 35.9 id, 62.1 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu1  :  0.3 us,  0.7 sy,  0.0 ni, 84.7 id, 14.3 wa,  0.0 hi,  0.0 si,  0.0 st\nKiB Mem :  8169300 total,  7238472 free,   546132 used,   384696 buff/cache\nKiB Swap:        0 total,        0 free,        0 used.  7316952 avail Mem \n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n27458 999       20   0  833852  57968  13176 S   1.7  0.7   0:12.40 mysqld\n27617 root      20   0   24348   9216   4692 S   1.0  0.1   0:04.40 python\n 1549 root      20   0  236716  24568   9864 S   0.3  0.3  51:46.57 python3\n22421 root      20   0       0      0      0 I   0.3  0.0   0:01.16 kworker/u\n```\n\n观察 top 的输出，我们发现，两个 CPU 的 iowait 都比较高，特别是 CPU0，iowait 已经超过 60%。而具体到各个进程， CPU 使用率并不高，最高的也只有 1.7%。\n\n既然 CPU 的嫌疑不大，那问题应该还是出在了 I/O 上。我们仍然在第一个终端，按下 Ctrl+C，停止 top 命令；然后，执行下面的 iostat 命令，看看有没有 I/O 性能问题：\n\n```bash\n$ iostat -d -x 1\nDevice            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util\n...\nsda            273.00    0.00  32568.00      0.00     0.00     0.00   0.00   0.00    7.90    0.00   1.16   119.30     0.00   3.56  97.20\n```\n\niostat 的输出你应该非常熟悉。观察这个界面，我们发现，磁盘 sda 每秒的读数据为 32 MB， 而 I/O 使用率高达 97% ，接近饱和，这说明，磁盘 sda 的读取确实碰到了性能瓶颈。\n\n那要怎么知道，这些 I/O 请求到底是哪些进程导致的呢？当然可以找我们的老朋友， pidstat。接下来，在终端一中，按下 Ctrl+C 停止 iostat 命令，然后运行下面的 pidstat 命令，观察进程的 I/O 情况：\n\n```makefile\n# -d 选项表示展示进程的 I/O 情况\n$ pidstat -d 1\n12:04:11      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command\n12:04:12      999     27458  32640.00      0.00      0.00       0  mysqld\n12:04:12        0     27617      4.00      4.00      0.00       3  python\n12:04:12        0     27864      0.00      4.00      0.00       0  systemd-journal\n```\n\n从 pidstat 的输出可以看到，PID 为 27458 的 mysqld 进程正在进行大量的读，而且读取速度是 32 MB/s，跟刚才 iostat 的发现一致。两个结果一对比，我们自然就找到了磁盘 I/O 瓶颈的根源，即 mysqld 进程。\n\n不过，这事儿还没完。我们自然要怀疑一下，为什么 mysqld 会去读取大量的磁盘数据呢？按照前面猜测，我们提到过，这有可能是个慢查询问题。\n\n可是，回想一下，慢查询的现象大多是 CPU 使用率高（比如 100% ），但这里看到的却是 I/O 问题。看来，这并不是一个单纯的慢查询问题，我们有必要分析一下 MySQL 读取的数据。\n\n要分析进程的数据读取，当然还要靠上一节用到过的 strace+ lsof 组合。\n\n接下来，还是在终端一中，执行 strace 命令，并且指定 mysqld 的进程号 27458。我们知道，MySQL 是一个多线程的数据库应用，为了不漏掉这些线程的数据读取情况，你要记得在执行 stace 命令时，加上 -f 参数：\n\n```bash\n$ strace -f -p 27458\n[pid 28014] read(38, \"934EiwT363aak7VtqF1mHGa4LL4Dhbks\"..., 131072) = 131072\n[pid 28014] read(38, \"hSs7KBDepBqA6m4ce6i6iUfFTeG9Ot9z\"..., 20480) = 20480\n[pid 28014] read(38, \"NRhRjCSsLLBjTfdqiBRLvN9K6FRfqqLm\"..., 131072) = 131072\n[pid 28014] read(38, \"AKgsik4BilLb7y6OkwQUjjqGeCTQTaRl\"..., 24576) = 24576\n[pid 28014] read(38, \"hFMHx7FzUSqfFI22fQxWCpSnDmRjamaW\"..., 131072) = 131072\n[pid 28014] read(38, \"ajUzLmKqivcDJSkiw7QWf2ETLgvQIpfC\"..., 20480) = 20480\n```\n\n观察一会，你会发现，线程 28014 正在读取大量数据，且读取文件的描述符编号为 38。这儿的 38 又对应着哪个文件呢？我们可以执行下面的 lsof 命令，并且指定线程号 28014 ，具体查看这个可疑线程和可疑文件：\n\n```ruby\n$ lsof -p 28014\n```\n\n奇怪的是，lsof 并没有给出任何输出。实际上，如果你查看 lsof 命令的返回值，就会发现，这个命令的执行失败了。\n\n我们知道，在 SHELL 中，特殊标量 $? 表示上一条命令退出时的返回值。查看这个特殊标量，你会发现它的返回值是 1。可是别忘了，在 Linux 中，返回值为 0 ，才表示命令执行成功。返回值为 1，显然表明执行失败。\n\n```ruby\n$ echo $?\n1\n```\n\n为什么 lsof 命令执行失败了呢？这里希望你暂停往下，自己先思考一下原因。记住我的那句话，遇到现象解释不了，先去查查工具文档。\n\n事实上，通过查询 lsof 的文档，你会发现，-p 参数需要指定进程号，而我们刚才传入的是线程号，所以 lsof 失败了。你看，任何一个细节都可能成为性能分析的“拦路虎”。\n\n回过头我们看，mysqld 的进程号是 27458，而 28014 只是它的一个线程。而且，如果你观察 一下 mysqld 进程的线程，你会发现，mysqld 其实还有很多正在运行的其他线程：\n\n```csharp\n# -t 表示显示线程，-a 表示显示命令行参数\n$ pstree -t -a -p 27458\nmysqld,27458 --log_bin=on --sync_binlog=1\n...\n  ├─{mysqld},27922\n  ├─{mysqld},27923\n  └─{mysqld},28014\n```\n\n找到了原因，lsof 的问题就容易解决了。把线程号换成进程号，继续执行 lsof 命令：\n\n```python-repl\n$ lsof -p 27458\nCOMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n...\nmysqld  27458      999   38u   REG    8,1 512440000 2601895 /var/lib/mysql/test/products.MYD\n```\n\n这次我们得到了 lsof 的输出。从输出中可以看到， mysqld 进程确实打开了大量文件，而根据文件描述符（FD）的编号，我们知道，描述符为 38 的是一个路径为 /var/lib/mysql/test/products.MYD 的文件。这里注意， 38 后面的 u 表示， mysqld 以读写的方式访问文件。\n\n看到这个文件，熟悉 MySQL 的你可能笑了：\n\n- MYD 文件，是 MyISAM 引擎用来存储表数据的文件；\n- 文件名就是数据表的名字；\n- 而这个文件的父目录，也就是数据库的名字。\n\n换句话说，这个文件告诉我们，mysqld 在读取数据库 test 中的 products 表。\n\n实际上，你可以执行下面的命令，查看 mysqld 在管理数据库 test 时的存储文件。不过要注意，由于 MySQL 运行在容器中，你需要通过 docker exec 到容器中查看：\n\n```shell\n$ docker exec -it mysql ls /var/lib/mysql/test/\ndb.opt    products.MYD  products.MYI  products.frm\n```\n\n从这里你可以发现，/var/lib/mysql/test/ 目录中有四个文件，每个文件的作用分别是：\n\n- MYD 文件用来存储表的数据；\n- MYI 文件用来存储表的索引；\n- frm 文件用来存储表的元信息（比如表结构）；\n- opt 文件则用来存储数据库的元信息（比如字符集、字符校验规则等）。\n\n当然，看到这些，你可能还有一个疑问，那就是，这些文件到底是不是 mysqld 正在使用的数据库文件呢？有没有可能是不再使用的旧数据呢？其实，这个很容易确认，查一下 mysqld 配置的数据路径即可。\n\n你可以在终端一中，继续执行下面的命令：\n\n```sql\n$ docker exec -i -t mysql mysql -e 'show global variables like \"%datadir%\";'\n+---------------+-----------------+\n| Variable_name | Value           |\n+---------------+-----------------+\n| datadir       | /var/lib/mysql/ |\n+---------------+-----------------+\n```\n\n这里可以看到，/var/lib/mysql/ 确实是 mysqld 正在使用的数据存储目录。刚才分析得出的数据库 test 和数据表 products ，都是正在使用。\n\n\u003e 注：其实 lsof 的结果已经可以确认，它们都是 mysqld 正在访问的文件。再查询 datadir ，只是想换一个思路，进一步确认一下。\n\n既然已经找出了数据库和表，接下来要做的，就是弄清楚数据库中正在执行什么样的 SQL 了。我们继续在终端一中，运行下面的 docker exec 命令，进入 MySQL 的命令行界面：\n\n```python\n$ docker exec -i -t mysql mysql\n...\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql\u003e\n```\n\n下一步你应该可以想到，那就是在 MySQL 命令行界面中，执行 show processlist 命令，来查看当前正在执行的 SQL 语句。\n\n不过，为了保证 SQL 语句不截断，这里我们可以执行 show full processlist 命令。如果一切正常，你应该可以看到如下输出：\n\n```sql\nmysql\u003e show full processlist;\n+----+------+-----------------+------+---------+------+--------------+-----------------------------------------------------+\n| Id | User | Host            | db   | Command | Time | State        | Info                                                |\n+----+------+-----------------+------+---------+------+--------------+-----------------------------------------------------+\n| 27 | root | localhost       | test | Query   |    0 | init         | show full processlist                               |\n| 28 | root | 127.0.0.1:42262 | test | Query   |    1 | Sending data | select * from products where productName='geektime' |\n+----+------+-----------------+------+---------+------+--------------+-----------------------------------------------------+\n2 rows in set (0.00 sec)\n```\n\n这个输出中，\n\n- db 表示数据库的名字；\n- Command 表示 SQL 类型；\n- Time 表示执行时间；\n- State 表示状态；\n- 而 Info 则包含了完整的 SQL 语句。\n\n多执行几次 show full processlist 命令，你可看到 select * from products where productName=‘geektime’ 这条 SQL 语句的执行时间比较长。\n\n再回忆一下，案例开始时，我们在终端二查询的产品名称 http://192.168.0.10:10000/products/geektime，其中的 geektime 也符合这条查询语句的条件。\n\n我们知道，MySQL 的慢查询问题，很可能是没有利用好索引导致的，那这条查询语句是不是这样呢？我们又该怎么确认，查询语句是否利用了索引呢？\n\n其实，MySQL 内置的 explain 命令，就可以帮你解决这个问题。继续在 MySQL 终端中，运行下面的 explain 命令：\n\n```sql\n# 切换到 test 库\nmysql\u003e use test;\n# 执行 explain 命令\nmysql\u003e explain select * from products where productName='geektime';\n+----+-------------+----------+------+---------------+------+---------+------+-------+-------------+\n| id | select_type | table    | type | possible_keys | key  | key_len | ref  | rows  | Extra       |\n+----+-------------+----------+------+---------------+------+---------+------+-------+-------------+\n|  1 | SIMPLE      | products | ALL  | NULL          | NULL | NULL    | NULL | 10000 | Using where |\n+----+-------------+----------+------+---------------+------+---------+------+-------+-------------+\n1 row in set (0.00 sec)\n```\n\n观察这次的输出。这个界面中，有几个比较重要的字段需要你注意，我就以这个输出为例，分别解释一下：\n\n- select_type 表示查询类型，而这里的 SIMPLE 表示此查询不包括 UNION 查询或者子查询；\n- table 表示数据表的名字，这里是 products；\n- type 表示查询类型，这里的 ALL 表示全表查询，但索引查询应该是 index 类型才对；\n- possible_keys 表示可能选用的索引，这里是 NULL；\n- key 表示确切会使用的索引，这里也是 NULL；\n- rows 表示查询扫描的行数，这里是 10000。\n\n根据这些信息，我们可以确定，这条查询语句压根儿没有使用索引，所以查询时，会扫描全表，并且扫描行数高达 10000 行。响应速度那么慢也就难怪了。\n\n走到这一步，你应该很容易想到优化方法，没有索引那我们就自己建立，给 productName 建立索引就可以了。不过，增加索引前，你需要先弄清楚，这个表结构到底长什么样儿。\n\n执行下面的 MySQL 命令，查询 products 表的结构，你会看到，它只有一个 id 主键，并不包括 productName 的索引：\n\n```sql\nmysql\u003e show create table products;\n...\n| products | CREATE TABLE `products` (\n  `id` int(11) NOT NULL,\n  `productCode` text NOT NULL COMMENT '产品代码',\n  `productName` text NOT NULL COMMENT '产品名称',\n...\n  PRIMARY KEY (`id`)\n) ENGINE=MyISAM DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC |\n...\n```\n\n接下来，我们就可以给 productName 建立索引了，也就是执行下面的 CREATE INDEX 命令：\n\n```vbnet\nmysql\u003e CREATE INDEX products_index ON products (productName);\nERROR 1170 (42000): BLOB/TEXT column 'productName' used in key specification without a key length\n```\n\n不过，醒目的 ERROR 告诉我们，这条命令运行失败了。根据错误信息，productName 是一个 BLOB/TEXT 类型，需要设置一个长度。所以，想要创建索引，就必须为 productName 指定一个前缀长度。\n\n那前缀长度设置为多大比较合适呢？这里其实有专门的算法，即通过计算前缀长度的选择性，来确定索引的长度。不过，我们可以稍微简化一下，直接使用一个固定数值（比如 64），执行下面的命令创建索引：\n\n```java\nmysql\u003e CREATE INDEX products_index ON products (productName(64));\nQuery OK, 10000 rows affected (14.45 sec)\nRecords: 10000  Duplicates: 0  Warnings: 0\n```\n\n现在可以看到，索引已经建好了。能做的都做完了，最后就该检查一下，性能问题是否已经解决了。\n\n我们切换到终端二中，查看还在执行的 curl 命令的结果：\n\n```kotlin\nGot data: ()in 15.383180141448975 sec\nGot data: ()in 15.384996891021729 sec\nGot data: ()in 0.0021054744720458984 sec\nGot data: ()in 0.003951072692871094 sec\n```\n\n显然，查询时间已经从 15 秒缩短到了 3 毫秒。看来，没有索引果然就是这次性能问题的罪魁祸首，解决了索引，就解决了查询慢的问题。\n\n## 案例思考\n\n到这里，商品搜索应用查询慢的问题已经完美解决了。但是，对于这个案例，我还有一点想说明一下。\n\n不知道你还记不记得，案例开始时，我们启动的几个容器应用。除了 MySQL 和商品搜索应用外，还有一个 DataService 应用。为什么这个案例开始时，要运行一个看起来毫不相关的应用呢？\n\n实际上，DataService 是一个严重影响 MySQL 性能的干扰应用。抛开上述索引优化方法不说，这个案例还有一种优化方法，也就是停止 DataService 应用。\n\n接下来，我们就删除数据库索引，回到原来的状态；然后停止 DataService 应用，看看优化效果如何。\n\n首先，我们在终端二中停止 curl 命令，然后回到终端一中，执行下面的命令删除索引：\n\n```shell\n# 删除索引\n$ docker exec -i -t mysql mysql\n\nmysql\u003e use test;\nmysql\u003e DROP INDEX products_index ON products;\n```\n\n接着，在终端二中重新运行 curl 命令。当然，这次你会发现，处理时间又变慢了：\n\n```shell\n$ while true; do curl http://192.168.0.10:10000/products/geektime; sleep 5; done\nGot data: ()in 16.884345054626465 sec\n```\n\n接下来，再次回到终端一中，执行下面的命令，停止 DataService 应用：\n\n```shell\n# 停止 DataService 应用\n$ docker rm -f dataservice\n```\n\n最后，我们回到终端二中，观察 curl 的结果：\n\n```kotlin\nGot data: ()in 16.884345054626465 sec\nGot data: ()in 15.238174200057983 sec\nGot data: ()in 0.12604427337646484 sec\nGot data: ()in 0.1101069450378418 sec\nGot data: ()in 0.11235237121582031 sec\n```\n\n果然，停止 DataService 后，处理时间从 15 秒缩短到了 0.1 秒，虽然比不上增加索引后的 3 毫秒，但相对于 15 秒来说，优化效果还是非常明显的。\n\n那么，这种情况下，还有没有 I/O 瓶颈了呢？\n\n我们切换到终端一中，运行下面的 vmstat 命令（注意不是 iostat，稍后解释原因），观察 I/O 的变化情况：\n\n```yaml\n$ vmstat 1\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 0  1      0 6809304   1368 856744    0    0 32640     0   52  478  1  0 50 49  0\n 0  1      0 6776620   1368 889456    0    0 32640     0   33  490  0  0 50 49  0\n 0  0      0 6747540   1368 918576    0    0 29056     0   42  568  0  0 56 44  0\n 0  0      0 6747540   1368 918576    0    0     0     0   40  141  1  0 100  0  0\n 0  0      0 6747160   1368 918576    0    0     0     0   40  148  0  1 99  0  0\n```\n\n你可以看到，磁盘读（bi）和 iowait（wa）刚开始还是挺大的，但没过多久，就都变成了 0 。换句话说，I/O 瓶颈消失了。\n\n这是为什么呢？原因先留个悬念，作为今天的思考题。\n\n回过头来解释一下刚刚的操作，在查看 I/O 情况时，我并没用 iostat 命令，而是用了 vmstat。其实，相对于 iostat 来说，vmstat 可以同时提供 CPU、内存和 I/O 的使用情况。\n\n在性能分析过程中，能够综合多个指标，并结合系统的工作原理进行分析，对解释性能现象通常会有意想不到的帮助。\n\n## 小结\n\n今天我们分析了一个商品搜索的应用程序。我们先是通过 top、iostat 分析了系统的 CPU 和磁盘使用情况，发现了磁盘的 I/O 瓶颈。\n\n接着，我们借助 pidstat ，发现瓶颈是 mysqld 导致的。紧接着，我们又通过 strace、lsof，找出了 mysqld 正在读的文件。同时，根据文件的名字和路径，我们找出了 mysqld 正在操作的数据库和数据表。综合这些信息，我们判断，这是一个没有利用索引导致的慢查询问题。\n\n于是，我们登录到 MySQL 命令行终端，用数据库分析工具进行验证，发现 MySQL 查询语句访问的字段，果然没有索引。所以，增加索引，就可以解决案例的性能问题了。\n\n## 思考\n\n最后，给你留一个思考题，也是我在案例最后部分提到过的，停止 DataService 后，商品搜索应用的处理时间，从 15 秒缩短到了 0.1 秒。这是为什么呢？\n\n我给个小小的提示。你可以先查看 [dataservice.py](http://dataservice.py/) 的[源码](https://github.com/feiskyer/linux-perf-examples/blob/master/mysql-slow/dataservice.py)，你会发现，DataService 实际上是在读写一个仅包括 “data” 字符串的小文件。不过在读取文件前，它会先把 /proc/sys/vm/drop_caches 改成 1。\n\n还记得这个操作有什么作用吗？如果不记得，可以用 man 查询 proc 文件系统的文档。\n\n欢迎在留言区和我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 29 案例篇：Redis响应严重延迟，如何解决？\n\n你好，我是倪朋飞。\n\n上一节，我们一起分析了一个基于 MySQL 的商品搜索案例，先来回顾一下。\n\n在访问商品搜索接口时，我们发现接口的响应特别慢。通过对系统 CPU、内存和磁盘 I/O 等资源使用情况的分析，我们发现这时出现了磁盘的 I/O 瓶颈，并且正是案例应用导致的。\n\n接着，我们借助 pidstat，发现罪魁祸首是 mysqld 进程。我们又通过 strace、lsof，找出了 mysqld 正在读的文件。根据文件的名字和路径，我们找出了 mysqld 正在操作的数据库和数据表。综合这些信息，我们猜测这是一个没利用索引导致的慢查询问题。\n\n为了验证猜测，我们到 MySQL 命令行终端，使用数据库分析工具发现，案例应用访问的字段果然没有索引。既然猜测是正确的，那增加索引后，问题就自然解决了。\n\n从这个案例你会发现，MySQL 的 MyISAM 引擎，主要依赖系统缓存加速磁盘 I/O 的访问。可如果系统中还有其他应用同时运行， MyISAM 引擎很难充分利用系统缓存。缓存可能会被其他应用程序占用，甚至被清理掉。\n\n所以，一般我并不建议，把应用程序的性能优化完全建立在系统缓存上。最好能在应用程序的内部分配内存，构建完全自主控制的缓存；或者使用第三方的缓存应用，比如 Memcached、Redis 等。\n\nRedis 是最常用的键值存储系统之一，常用作数据库、高速缓存和消息队列代理等。Redis 基于内存来存储数据，不过，为了保证在服务器异常时数据不丢失，很多情况下，我们要为它配置持久化，而这就可能会引发磁盘 I/O 的性能问题。\n\n今天，我就带你一起来分析一个利用 Redis 作为缓存的案例。这同样是一个基于 Python Flask 的应用程序，它提供了一个 查询缓存的接口，但接口的响应时间比较长，并不能满足线上系统的要求。\n\n非常感谢携程系统研发部资深后端工程师董国星，帮助提供了今天的案例。\n\n## 案例准备\n\n本次案例还是基于 Ubuntu 18.04，同样适用于其他的 Linux 系统。我使用的案例环境如下所示：\n\n- 机器配置：2 CPU，8GB 内存\n- 预先安装 docker、sysstat 、git、make 等工具，如 apt install [docker.io](https://docker.io/) sysstat\n\n今天的案例由 Python 应用 +Redis 两部分组成。其中，Python 应用是一个基于 Flask 的应用，它会利用 Redis ，来管理应用程序的缓存，并对外提供三个 HTTP 接口：\n\n- /：返回 hello redis；\n- /init/：插入指定数量的缓存数据，如果不指定数量，默认的是 5000 条；\n- 缓存的键格式为 uuid:\n- 缓存的值为 good、bad 或 normal 三者之一\n- /get_cache/\u003ctype_name\u003e：查询指定值的缓存数据，并返回处理时间。其中，type_name 参数只支持 good, bad 和 normal（也就是找出具有相同 value 的 key 列表）。\n\n由于应用比较多，为了方便你运行，我把它们打包成了两个 Docker 镜像，并推送到了 [Github](https://github.com/feiskyer/linux-perf-examples/tree/master/redis-slow) 上。这样你就只需要运行几条命令，就可以启动了。\n\n今天的案例需要两台虚拟机，其中一台用作案例分析的目标机器，运行 Flask 应用，它的 IP 地址是 192.168.0.10；而另一台作为客户端，请求缓存查询接口。我画了一张图来表示它们的关系。\n\n![img](c8e0ca06d70a1c7f1520d103a3edfc87.png)\n\n接下来，打开两个终端，分别 SSH 登录到这两台虚拟机中，并在第一台虚拟机中安装上述工具。\n\n跟以前一样，案例中所有命令都默认以 root 用户运行，如果你是用普通用户身份登陆系统，请运行 sudo su root 命令切换到 root 用户。\n\n到这里，准备工作就完成了。接下来，我们正式进入操作环节。\n\n## 案例分析\n\n首先，我们在第一个终端中，执行下面的命令，运行本次案例要分析的目标应用。正常情况下，你应该可以看到下面的输出：\n\n```shell\n# 注意下面的随机字符串是容器 ID，每次运行均会不同，并且你不需要关注它\n$ docker run --name=redis -itd -p 10000:80 feisky/redis-server\nec41cb9e4dd5cb7079e1d9f72b7cee7de67278dbd3bd0956b4c0846bff211803\n$ docker run --name=app --network=container:redis -itd feisky/redis-app\n2c54eb252d0552448320d9155a2618b799a1e71d7289ec7277a61e72a9de5fd0\n```\n\n然后，再运行 docker ps 命令，确认两个容器都处于运行（Up）状态：\n\n```bash\n$ docker ps\nCONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                             NAMES\n2c54eb252d05        feisky/redis-app      \"python /app.py\"         48 seconds ago      Up 47 seconds                                         app\nec41cb9e4dd5        feisky/redis-server   \"docker-entrypoint.s…\"   49 seconds ago      Up 48 seconds       6379/tcp, 0.0.0.0:10000-\u003e80/tcp   redis \n```\n\n今天的应用在 10000 端口监听，所以你可以通过 [http://192.168.0.10:10000](http://192.168.0.10:10000/) ，来访问前面提到的三个接口。\n\n比如，我们切换到第二个终端，使用 curl 工具，访问应用首页。如果你看到 `hello redis`的输出，说明应用正常启动：\n\n```ruby\n$ curl http://192.168.0.10:10000/\nhello redis\n```\n\n接下来，继续在终端二中，执行下面的 curl 命令，来调用应用的 /init 接口，初始化 Redis 缓存，并且插入 5000 条缓存信息。这个过程比较慢，比如我的机器就花了十几分钟时间。耐心等一会儿后，你会看到下面这行输出：\n\n```bash\n# 案例插入 5000 条数据，在实践时可以根据磁盘的类型适当调整，比如使用 SSD 时可以调大，而 HDD 可以适当调小\n$ curl http://192.168.0.10:10000/init/5000\n{\"elapsed_seconds\":30.26814079284668,\"keys_initialized\":5000}\n```\n\n继续执行下一个命令，访问应用的缓存查询接口。如果一切正常，你会看到如下输出：\n\n```ruby\n$ curl http://192.168.0.10:10000/get_cache\n{\"count\":1677,\"data\":[\"d97662fa-06ac-11e9-92c7-0242ac110002\",...],\"elapsed_seconds\":10.545469760894775,\"type\":\"good\"}\n```\n\n我们看到，这个接口调用居然要花 10 秒！这么长的响应时间，显然不能满足实际的应用需求。\n\n到底出了什么问题呢？我们还是要用前面学过的性能工具和原理，来找到这个瓶颈。\n\n不过别急，同样为了避免分析过程中客户端的请求结束，在进行性能分析前，我们先要把 curl 命令放到一个循环里来执行。你可以在终端二中，继续执行下面的命令：\n\n```shell\n$ while true; do curl http://192.168.0.10:10000/get_cache; done\n```\n\n接下来，再重新回到终端一，查找接口响应慢的“病因”。\n\n最近几个案例的现象都是响应很慢，这种情况下，我们自然先会怀疑，是不是系统资源出现了瓶颈。所以，先观察 CPU、内存和磁盘 I/O 等的使用情况肯定不会错。\n\n我们先在终端一中执行 top 命令，分析系统的 CPU 使用情况：\n\n```yaml\n$ top\ntop - 12:46:18 up 11 days,  8:49,  1 user,  load average: 1.36, 1.36, 1.04\nTasks: 137 total,   1 running,  79 sleeping,   0 stopped,   0 zombie\n%Cpu0  :  6.0 us,  2.7 sy,  0.0 ni,  5.7 id, 84.7 wa,  0.0 hi,  1.0 si,  0.0 st\n%Cpu1  :  1.0 us,  3.0 sy,  0.0 ni, 94.7 id,  0.0 wa,  0.0 hi,  1.3 si,  0.0 st\nKiB Mem :  8169300 total,  7342244 free,   432912 used,   394144 buff/cache\nKiB Swap:        0 total,        0 free,        0 used.  7478748 avail Mem \n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n 9181 root      20   0  193004  27304   8716 S   8.6  0.3   0:07.15 python\n 9085 systemd+  20   0   28352   9760   1860 D   5.0  0.1   0:04.34 redis-server\n  368 root      20   0       0      0      0 D   1.0  0.0   0:33.88 jbd2/sda1-8\n  149 root       0 -20       0      0      0 I   0.3  0.0   0:10.63 kworker/0:1H\n 1549 root      20   0  236716  24576   9864 S   0.3  0.3  91:37.30 python3\n```\n\n观察 top 的输出可以发现，CPU0 的 iowait 比较高，已经达到了 84%；而各个进程的 CPU 使用率都不太高，最高的 python 和 redis-server ，也分别只有 8% 和 5%。再看内存，总内存 8GB，剩余内存还有 7GB 多，显然内存也没啥问题。\n\n综合 top 的信息，最有嫌疑的就是 iowait。所以，接下来还是要继续分析，是不是 I/O 问题。\n\n还在第一个终端中，先按下 Ctrl+C，停止 top 命令；然后，执行下面的 iostat 命令，查看有没有 I/O 性能问题：\n\n```bash\n$ iostat -d -x 1\nDevice            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util\n...\nsda              0.00  492.00      0.00   2672.00     0.00   176.00   0.00  26.35    0.00    1.76   0.00     0.00     5.43   0.00   0.00\n```\n\n观察 iostat 的输出，我们发现，磁盘 sda 每秒的写数据（wkB/s）为 2.5MB，I/O 使用率（%util）是 0。看来，虽然有些 I/O 操作，但并没导致磁盘的 I/O 瓶颈。\n\n排查一圈儿下来，CPU 和内存使用没问题，I/O 也没有瓶颈，接下来好像就没啥分析方向了？\n\n碰到这种情况，还是那句话，反思一下，是不是又漏掉什么有用线索了。你可以先自己思考一下，从分析对象（案例应用）、系统原理和性能工具这三个方向下功夫，回忆它们的特性，查找现象的异常，再继续往下走。\n\n回想一下，今天的案例问题是从 Redis 缓存中查询数据慢。对查询来说，对应的 I/O 应该是磁盘的读操作，但刚才我们用 iostat 看到的却是写操作。虽说 I/O 本身并没有性能瓶颈，但这里的磁盘写也是比较奇怪的。为什么会有磁盘写呢？那我们就得知道，到底是哪个进程在写磁盘。\n\n要知道 I/O 请求来自哪些进程，还是要靠我们的老朋友 pidstat。在终端一中运行下面的 pidstat 命令，观察进程的 I/O 情况：\n\n```bash\n$ pidstat -d 1\n12:49:35      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command\n12:49:36        0       368      0.00     16.00      0.00      86  jbd2/sda1-8\n12:49:36      100      9085      0.00    636.00      0.00       1  redis-server\n```\n\n从 pidstat 的输出，我们看到，I/O 最多的进程是 PID 为 9085 的 redis-server，并且它也刚好是在写磁盘。这说明，确实是 redis-server 在进行磁盘写。\n\n当然，光找到读写磁盘的进程还不够，我们还要再用 strace+lsof 组合，看看 redis-server 到底在写什么。\n\n接下来，还是在终端一中，执行 strace 命令，并且指定 redis-server 的进程号 9085：\n\n```swift\n# -f 表示跟踪子进程和子线程，-T 表示显示系统调用的时长，-tt 表示显示跟踪时间\n$ strace -f -T -tt -p 9085\n[pid  9085] 14:20:16.826131 epoll_pwait(5, [{EPOLLIN, {u32=8, u64=8}}], 10128, 65, NULL, 8) = 1 \u003c0.000055\u003e\n[pid  9085] 14:20:16.826301 read(8, \"*2\\r\\n$3\\r\\nGET\\r\\n$41\\r\\nuuid:5b2e76cc-\"..., 16384) = 61 \u003c0.000071\u003e\n[pid  9085] 14:20:16.826477 read(3, 0x7fff366a5747, 1) = -1 EAGAIN (Resource temporarily unavailable) \u003c0.000063\u003e\n[pid  9085] 14:20:16.826645 write(8, \"$3\\r\\nbad\\r\\n\", 9) = 9 \u003c0.000173\u003e\n[pid  9085] 14:20:16.826907 epoll_pwait(5, [{EPOLLIN, {u32=8, u64=8}}], 10128, 65, NULL, 8) = 1 \u003c0.000032\u003e\n[pid  9085] 14:20:16.827030 read(8, \"*2\\r\\n$3\\r\\nGET\\r\\n$41\\r\\nuuid:55862ada-\"..., 16384) = 61 \u003c0.000044\u003e\n[pid  9085] 14:20:16.827149 read(3, 0x7fff366a5747, 1) = -1 EAGAIN (Resource temporarily unavailable) \u003c0.000043\u003e\n[pid  9085] 14:20:16.827285 write(8, \"$3\\r\\nbad\\r\\n\", 9) = 9 \u003c0.000141\u003e\n[pid  9085] 14:20:16.827514 epoll_pwait(5, [{EPOLLIN, {u32=8, u64=8}}], 10128, 64, NULL, 8) = 1 \u003c0.000049\u003e\n[pid  9085] 14:20:16.827641 read(8, \"*2\\r\\n$3\\r\\nGET\\r\\n$41\\r\\nuuid:53522908-\"..., 16384) = 61 \u003c0.000043\u003e\n[pid  9085] 14:20:16.827784 read(3, 0x7fff366a5747, 1) = -1 EAGAIN (Resource temporarily unavailable) \u003c0.000034\u003e\n[pid  9085] 14:20:16.827945 write(8, \"$4\\r\\ngood\\r\\n\", 10) = 10 \u003c0.000288\u003e\n[pid  9085] 14:20:16.828339 epoll_pwait(5, [{EPOLLIN, {u32=8, u64=8}}], 10128, 63, NULL, 8) = 1 \u003c0.000057\u003e\n[pid  9085] 14:20:16.828486 read(8, \"*3\\r\\n$4\\r\\nSADD\\r\\n$4\\r\\ngood\\r\\n$36\\r\\n535\"..., 16384) = 67 \u003c0.000040\u003e\n[pid  9085] 14:20:16.828623 read(3, 0x7fff366a5747, 1) = -1 EAGAIN (Resource temporarily unavailable) \u003c0.000052\u003e\n[pid  9085] 14:20:16.828760 write(7, \"*3\\r\\n$4\\r\\nSADD\\r\\n$4\\r\\ngood\\r\\n$36\\r\\n535\"..., 67) = 67 \u003c0.000060\u003e\n[pid  9085] 14:20:16.828970 fdatasync(7) = 0 \u003c0.005415\u003e\n[pid  9085] 14:20:16.834493 write(8, \":1\\r\\n\", 4) = 4 \u003c0.000250\u003e\n```\n\n观察一会儿，有没有发现什么有趣的现象呢？\n\n事实上，从系统调用来看， epoll_pwait、read、write、fdatasync 这些系统调用都比较频繁。那么，刚才观察到的写磁盘，应该就是 write 或者 fdatasync 导致的了。\n\n接着再来运行 lsof 命令，找出这些系统调用的操作对象：\n\n```yaml\n$ lsof -p 9085\nredis-ser 9085 systemd-network    3r     FIFO   0,12      0t0 15447970 pipe\nredis-ser 9085 systemd-network    4w     FIFO   0,12      0t0 15447970 pipe\nredis-ser 9085 systemd-network    5u  a_inode   0,13        0    10179 [eventpoll]\nredis-ser 9085 systemd-network    6u     sock    0,9      0t0 15447972 protocol: TCP\nredis-ser 9085 systemd-network    7w      REG    8,1  8830146  2838532 /data/appendonly.aof\nredis-ser 9085 systemd-network    8u     sock    0,9      0t0 15448709 protocol: TCP\n```\n\n现在你会发现，描述符编号为 3 的是一个 pipe 管道，5 号是 eventpoll，7 号是一个普通文件，而 8 号是一个 TCP socket。\n\n结合磁盘写的现象，我们知道，只有 7 号普通文件才会产生磁盘写，而它操作的文件路径是 /data/appendonly.aof，相应的系统调用包括 write 和 fdatasync。\n\n如果你对 Redis 的持久化配置比较熟，看到这个文件路径以及 fdatasync 的系统调用，你应该能想到，这对应着正是 Redis 持久化配置中的 appendonly 和 appendfsync 选项。很可能是因为它们的配置不合理，导致磁盘写比较多。\n\n接下来就验证一下这个猜测，我们可以通过 Redis 的命令行工具，查询这两个选项的配置。\n\n继续在终端一中，运行下面的命令，查询 appendonly 和 appendfsync 的配置：\n\n```sql\n$ docker exec -it redis redis-cli config get 'append*'\n1) \"appendfsync\"\n2) \"always\"\n3) \"appendonly\"\n4) \"yes\"\n```\n\n从这个结果你可以发现，appendfsync 配置的是 always，而 appendonly 配置的是 yes。这两个选项的详细含义，你可以从 [Redis Persistence](https://redis.io/topics/persistence) 的文档中查到，这里我做一下简单介绍。\n\nRedis 提供了两种数据持久化的方式，分别是快照和追加文件。\n\n**快照方式**，会按照指定的时间间隔，生成数据的快照，并且保存到磁盘文件中。为了避免阻塞主进程，Redis 还会 fork 出一个子进程，来负责快照的保存。这种方式的性能好，无论是备份还是恢复，都比追加文件好很多。\n\n不过，它的缺点也很明显。在数据量大时，fork 子进程需要用到比较大的内存，保存数据也很耗时。所以，你需要设置一个比较长的时间间隔来应对，比如至少 5 分钟。这样，如果发生故障，你丢失的就是几分钟的数据。\n\n**追加文件**，则是用在文件末尾追加记录的方式，对 Redis 写入的数据，依次进行持久化，所以它的持久化也更安全。\n\n此外，它还提供了一个用 appendfsync 选项设置 fsync 的策略，确保写入的数据都落到磁盘中，具体选项包括 always、everysec、no 等。\n\n- always 表示，每个操作都会执行一次 fsync，是最为安全的方式；\n- everysec 表示，每秒钟调用一次 fsync ，这样可以保证即使是最坏情况下，也只丢失 1 秒的数据；\n- 而 no 表示交给操作系统来处理。\n\n回忆一下我们刚刚看到的配置，appendfsync 配置的是 always，意味着每次写数据时，都会调用一次 fsync，从而造成比较大的磁盘 I/O 压力。\n\n当然，你还可以用 strace ，观察这个系统调用的执行情况。比如通过 -e 选项指定 fdatasync 后，你就会得到下面的结果：\n\n```yaml\n$ strace -f -p 9085 -T -tt -e fdatasync\nstrace: Process 9085 attached with 4 threads\n[pid  9085] 14:22:52.013547 fdatasync(7) = 0 \u003c0.007112\u003e\n[pid  9085] 14:22:52.022467 fdatasync(7) = 0 \u003c0.008572\u003e\n[pid  9085] 14:22:52.032223 fdatasync(7) = 0 \u003c0.006769\u003e\n...\n[pid  9085] 14:22:52.139629 fdatasync(7) = 0 \u003c0.008183\u003e\n```\n\n从这里你可以看到，每隔 10ms 左右，就会有一次 fdatasync 调用，并且每次调用本身也要消耗 7~8ms。\n\n不管哪种方式，都可以验证我们的猜想，配置确实不合理。这样，我们就找出了 Redis 正在进行写入的文件，也知道了产生大量 I/O 的原因。\n\n不过，回到最初的疑问，为什么查询时会有磁盘写呢？按理来说不应该只有数据的读取吗？这就需要我们再来审查一下 strace -f -T -tt -p 9085 的结果。\n\n```swift\nread(8, \"*2\\r\\n$3\\r\\nGET\\r\\n$41\\r\\nuuid:53522908-\"..., 16384)\nwrite(8, \"$4\\r\\ngood\\r\\n\", 10)\nread(8, \"*3\\r\\n$4\\r\\nSADD\\r\\n$4\\r\\ngood\\r\\n$36\\r\\n535\"..., 16384)\nwrite(7, \"*3\\r\\n$4\\r\\nSADD\\r\\n$4\\r\\ngood\\r\\n$36\\r\\n535\"..., 67)\nwrite(8, \":1\\r\\n\", 4)\n```\n\n细心的你应该记得，根据 lsof 的分析，文件描述符编号为 7 的是一个普通文件 /data/appendonly.aof，而编号为 8 的是 TCP socket。而观察上面的内容，8 号对应的 TCP 读写，是一个标准的“请求 - 响应”格式，即：\n\n- 从 socket 读取 GET uuid:53522908-… 后，响应 good；\n- 再从 socket 读取 SADD good 535… 后，响应 1。\n\n对 Redis 来说，SADD 是一个写操作，所以 Redis 还会把它保存到用于持久化的 appendonly.aof 文件中。\n\n观察更多的 strace 结果，你会发现，每当 GET 返回 good 时，随后都会有一个 SADD 操作，这也就导致了，明明是查询接口，Redis 却有大量的磁盘写。\n\n到这里，我们就找出了 Redis 写磁盘的原因。不过，在下最终结论前，我们还是要确认一下，8 号 TCP socket 对应的 Redis 客户端，到底是不是我们的案例应用。\n\n我们可以给 lsof 命令加上 -i 选项，找出 TCP socket 对应的 TCP 连接信息。不过，由于 Redis 和 Python 应用都在容器中运行，我们需要进入容器的网络命名空间内部，才能看到完整的 TCP 连接。\n\n\u003e 注意：下面的命令用到的 [nsenter](http://man7.org/linux/man-pages/man1/nsenter.1.html) 工具，可以进入容器命名空间。如果你的系统没有安装，请运行下面命令安装 nsenter： docker run --rm -v /usr/local/bin:/target jpetazzo/nsenter\n\n还是在终端一中，运行下面的命令：\n\n```ruby\n# 由于这两个容器共享同一个网络命名空间，所以我们只需要进入 app 的网络命名空间即可\n$ PID=$(docker inspect --format {{.State.Pid}} app)\n# -i 表示显示网络套接字信息\n$ nsenter --target $PID --net -- lsof -i\nCOMMAND    PID            USER   FD   TYPE   DEVICE SIZE/OFF NODE NAME\nredis-ser 9085 systemd-network    6u  IPv4 15447972      0t0  TCP localhost:6379 (LISTEN)\nredis-ser 9085 systemd-network    8u  IPv4 15448709      0t0  TCP localhost:6379-\u003elocalhost:32996 (ESTABLISHED)\npython    9181            root    3u  IPv4 15448677      0t0  TCP *:http (LISTEN)\npython    9181            root    5u  IPv4 15449632      0t0  TCP localhost:32996-\u003elocalhost:6379 (ESTABLISHED) \n```\n\n这次我们可以看到，redis-server 的 8 号文件描述符，对应 TCP 连接 localhost:6379-\u003elocalhost:32996。其中， localhost:6379 是 redis-server 自己的监听端口，自然 localhost:32996 就是 redis 的客户端。再观察最后一行，localhost:32996 对应的，正是我们的 Python 应用程序（进程号为 9181）。\n\n历经各种波折，我们总算找出了 Redis 响应延迟的潜在原因。总结一下，我们找到两个问题。\n\n第一个问题，Redis 配置的 appendfsync 是 always，这就导致 Redis 每次的写操作，都会触发 fdatasync 系统调用。今天的案例，没必要用这么高频的同步写，使用默认的 1s 时间间隔，就足够了。\n\n第二个问题，Python 应用在查询接口中会调用 Redis 的 SADD 命令，这很可能是不合理使用缓存导致的。\n\n对于第一个配置问题，我们可以执行下面的命令，把 appendfsync 改成 everysec：\n\n```shell\n$ docker exec -it redis redis-cli config set appendfsync everysec\nOK\n```\n\n改完后，切换到终端二中查看，你会发现，现在的请求时间，已经缩短到了 0.9s：\n\n```bash\n{..., \"elapsed_seconds\":0.9368953704833984,\"type\":\"good\"}\n```\n\n而第二个问题，就要查看应用的源码了。点击 [Github](https://github.com/feiskyer/linux-perf-examples/blob/master/redis-slow/app.py) ，你就可以查看案例应用的源代码：\n\n```python\ndef get_cache(type_name):\n    '''handler for /get_cache'''\n    for key in redis_client.scan_iter(\"uuid:*\"):\n        value = redis_client.get(key)\n        if value == type_name:\n            redis_client.sadd(type_name, key[5:])\n    data = list(redis_client.smembers(type_name))\n    redis_client.delete(type_name)\n    return jsonify({\"type\": type_name, 'count': len(data), 'data': data})\n```\n\n果然，Python 应用把 Redis 当成临时空间，用来存储查询过程中找到的数据。不过我们知道，这些数据放内存中就可以了，完全没必要再通过网络调用存储到 Redis 中。\n\n基于这个思路，我把修改后的代码也推送到了相同的源码文件中，你可以通过 http://192.168.0.10:10000/get_cache_data 这个接口来访问它。\n\n我们切换到终端二，按 Ctrl+C 停止之前的 curl 命令；然后执行下面的 curl 命令，调用 http://192.168.0.10:10000/get_cache_data 新接口：\n\n```bash\n$ while true; do curl http://192.168.0.10:10000/get_cache_data; done\n{...,\"elapsed_seconds\":0.16034674644470215,\"type\":\"good\"}\n```\n\n你可以发现，解决第二个问题后，新接口的性能又有了进一步的提升，从刚才的 0.9s ，再次缩短成了不到 0.2s。\n\n当然，案例最后，不要忘记清理案例应用。你可以切换到终端一中，执行下面的命令进行清理：\n\n```shell\n$ docker rm -f app redis\n```\n\n## 小结\n\n今天我带你一起分析了一个 Redis 缓存的案例。\n\n我们先用 top、iostat ，分析了系统的 CPU 、内存和磁盘使用情况，不过却发现，系统资源并没有出现瓶颈。这个时候想要进一步分析的话，该从哪个方向着手呢？\n\n通过今天的案例你会发现，为了进一步分析，就需要你对系统和应用程序的工作原理有一定的了解。\n\n比如，今天的案例中，虽然磁盘 I/O 并没有出现瓶颈，但从 Redis 的原理来说，查询缓存时不应该出现大量的磁盘 I/O 写操作。\n\n顺着这个思路，我们继续借助 pidstat、strace、lsof、nsenter 等一系列的工具，找出了两个潜在问题，一个是 Redis 的不合理配置，另一个是 Python 应用对 Redis 的滥用。找到瓶颈后，相应的优化工作自然就比较轻松了。\n\n## 思考\n\n最后给你留一个思考题。从上一节 MySQL 到今天 Redis 的案例分析，你有没有发现 I/O 性能问题的分析规律呢？如果你有任何想法或心得，都可以记录下来。\n\n当然，这两个案例这并不能涵盖所有的 I/O 性能问题。你在实际工作中，还碰到过哪些 I/O 性能问题吗？你又是怎么分析的呢？\n\n欢迎在留言区和我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 30 套路篇：如何迅速分析出系统IO的瓶颈在哪里？\n\n你好，我是倪朋飞。\n\n前几节学习中，我们通过几个案例，分析了各种常见的 I/O 性能问题。通过这些实战操作，你应该已经熟悉了 I/O 性能问题的分析和定位思路，也掌握了很多 I/O 性能分析的工具。\n\n不过，我想你可能还是会困惑，如果离开专栏，换成其他的实际工作场景，案例中提到的各种性能指标和工具，又该如何选择呢？\n\n上一节最后，我留下了作业，让你自己整理思路。今天，我就带你一起复习，总结一下，如何“快准狠”定位系统的 I/O 瓶颈；并且梳理清楚，在不同场景下，指标工具怎么选，性能瓶颈又该如何定位。\n\n## 性能指标\n\n老规矩，我们先来回顾一下，描述 I/O 的性能指标有哪些？你可以先回想一下文件系统和磁盘 I/O 的原理，结合下面这张 Linux 系统的 I/O 栈图，凭着记忆和理解自己写一写。或者，你也可以打开前面的文章，挨个复习总结一下。\n\n![img](9e42aaf53ff4a544b9a7b03b6ce63f38.png)\n\n学了这么久的 I/O 性能知识，一说起 I/O 指标，你应该首先会想到分类描述。我们要区分开文件系统和磁盘，分别用不同指标来描述它们的性能。\n\n### 文件系统 I/O 性能指标\n\n我们先来看文件系统的情况。\n\n**首先，最容易想到的是存储空间的使用情况，包括容量、使用量以及剩余空间等**。我们通常也称这些为磁盘空间的使用量，因为文件系统的数据最终还是存储在磁盘上。\n\n不过要注意，这些只是文件系统向外展示的空间使用，而非在磁盘空间的真实用量，因为文件系统的元数据也会占用磁盘空间。\n\n而且，如果你配置了 RAID，从文件系统看到的使用量跟实际磁盘的占用空间，也会因为 RAID 级别的不同而不一样。比方说，配置 RAID10 后，你从文件系统最多也只能看到所有磁盘容量的一半。\n\n除了数据本身的存储空间，还有一个**容易忽略的是索引节点的使用情况，它也包括容量、使用量以及剩余量等三个指标**。如果文件系统中存储过多的小文件，就可能碰到索引节点容量已满的问题。\n\n**其次，你应该想到的是前面多次提到过的缓存使用情况，包括页缓存、目录项缓存、索引节点缓存以及各个具体文件系统（如 ext4、XFS 等）的缓存**。这些缓存会使用速度更快的内存，用来临时存储文件数据或者文件系统的元数据，从而可以减少访问慢速磁盘的次数。\n\n除了以上这两点，文件 I/O 也是很重要的性能指标，包括 IOPS（包括 r/s 和 w/s）、响应时间（延迟）以及吞吐量（B/s）等。在考察这类指标时，通常还要考虑实际文件的读写情况。比如，结合文件大小、文件数量、I/O 类型等，综合分析文件 I/O 的性能。\n\n诚然，这些性能指标非常重要，但不幸的是，Linux 文件系统并没提供，直接查看这些指标的方法。我们只能通过系统调用、动态跟踪或者基准测试等方法，间接进行观察、评估。\n\n不过，实际上，这些指标在我们考察磁盘性能时更容易见到，因为 Linux 为磁盘性能提供了更详细的数据。\n\n### 磁盘 I/O 性能指标\n\n接下来，我们就来具体看看，哪些性能指标可以衡量磁盘 I/O 的性能。\n\n在磁盘 I/O 原理的文章中，我曾提到过四个核心的磁盘 I/O 指标。\n\n- **使用率**，是指磁盘忙处理 I/O 请求的百分比。过高的使用率（比如超过 60%）通常意味着磁盘 I/O 存在性能瓶颈。\n- **IOPS**（Input/Output Per Second），是指每秒的 I/O 请求数。\n- **吞吐量**，是指每秒的 I/O 请求大小。\n- **响应时间**，是指从发出 I/O 请求到收到响应的间隔时间。\n\n考察这些指标时，一定要注意综合 I/O 的具体场景来分析，比如读写类型（顺序还是随机）、读写比例、读写大小、存储类型（有无 RAID 以及 RAID 级别、本地存储还是网络存储）等。\n\n不过，这里有个大忌，就是把不同场景的 I/O 性能指标，直接进行分析对比。这是很常见的一个误区，你一定要避免。\n\n除了这些指标外，在前面 Cache 和 Buffer 原理的文章中，我曾多次提到，**缓冲区（Buffer）**也是要重点掌握的指标，它经常出现在内存和磁盘问题的分析中。\n\n文件系统和磁盘 I/O 的这些指标都很有用，需要我们熟练掌握，所以我总结成了一张图，帮你分类和记忆。你可以保存并打印出来，方便随时查看复习，也可以把它当成 I/O 性能分析的“指标筛选”清单使用。\n\n![img](b6d67150e471e1340a6f3c3dc3ba0120.png)\n\n## 性能工具\n\n掌握文件系统和磁盘 I/O 的性能指标后，我们还要知道，怎样去获取这些指标，也就是搞明白工具的使用问题。\n\n你还记得前面的基础篇和案例篇中，都分别用了哪些工具吗？我们一起回顾下这些内容。\n\n第一，在文件系统的原理中，我介绍了查看文件系统容量的工具 df。它既可以查看文件系统数据的空间容量，也可以查看索引节点的容量。至于文件系统缓存，我们通过 /proc/meminfo、/proc/slabinfo 以及 slabtop 等各种来源，观察页缓存、目录项缓存、索引节点缓存以及具体文件系统的缓存情况。\n\n第二，在磁盘 I/O 的原理中，我们分别用 iostat 和 pidstat 观察了磁盘和进程的 I/O 情况。它们都是最常用的 I/O 性能分析工具。通过 iostat ，我们可以得到磁盘的 I/O 使用率、吞吐量、响应时间以及 IOPS 等性能指标；而通过 pidstat ，则可以观察到进程的 I/O 吞吐量以及块设备 I/O 的延迟等。\n\n第三，在狂打日志的案例中，我们先用 top 查看系统的 CPU 使用情况，发现 iowait 比较高；然后，又用 iostat 发现了磁盘的 I/O 使用率瓶颈，并用 pidstat 找出了大量 I/O 的进程；最后，通过 strace 和 lsof，我们找出了问题进程正在读写的文件，并最终锁定性能问题的来源——原来是进程在狂打日志。\n\n第四，在磁盘 I/O 延迟的单词热度案例中，我们同样先用 top、iostat ，发现磁盘有 I/O 瓶颈，并用 pidstat 找出了大量 I/O 的进程。可接下来，想要照搬上次操作的我们失败了。在随后的 strace 命令中，我们居然没看到 write 系统调用。于是，我们换了一个思路，用新工具 filetop 和 opensnoop ，从内核中跟踪系统调用，最终找出瓶颈的来源。\n\n最后，在 MySQL 和 Redis 的案例中，同样的思路，我们先用 top、iostat 以及 pidstat ，确定并找出 I/O 性能问题的瓶颈来源，它们正是 mysqld 和 redis-server。随后，我们又用 strace+lsof 找出了它们正在读写的文件。\n\n关于 MySQL 案例，根据 mysqld 正在读写的文件路径，再结合 MySQL 数据库引擎的原理，我们不仅找出了数据库和数据表的名称，还进一步发现了慢查询的问题，最终通过优化索引解决了性能瓶颈。\n\n至于 Redis 案例，根据 redis-server 读写的文件，以及正在进行网络通信的 TCP Socket，再结合 Redis 的工作原理，我们发现 Redis 持久化选项配置有问题；从 TCP Socket 通信的数据中，我们还发现了客户端的不合理行为。于是，我们修改 Redis 配置选项，并优化了客户端使用 Redis 的方式，从而减少网络通信次数，解决性能问题。\n\n一下子复习了这么多，你是不是觉得头昏脑胀，再次想感叹性能工具的繁杂呀！其实，只要把相应的系统工作原理捋明白，工具使用并不难\n\n## 性能指标和工具的联系\n\n同前面 CPU 和内存板块的学习一样，我建议从指标和工具两个不同维度出发，整理记忆。\n\n- 从 I/O 指标出发，你更容易把性能工具同系统工作原理关联起来，对性能问题有宏观的认识和把握。\n- 而从性能工具出发，可以让你更快上手使用工具，迅速找出我们想观察的性能指标。特别是在工具有限的情况下，我们更要充分利用好手头的每一个工具，少量工具也要尽力挖掘出大量信息。\n\n**第一个维度，从文件系统和磁盘 I/O 的性能指标出发。换句话说，当你想查看某个性能指标时，要清楚知道，哪些工具可以做到。**\n\n根据不同的性能指标，对提供指标的性能工具进行分类和理解。这样，在实际排查性能问题时，你就可以清楚知道，什么工具可以提供你想要的指标，而不是毫无根据地挨个尝试，撞运气。\n\n虽然你不需要把所有相关的工具背下来，但如果能记清楚每个指标对应的工具特性，实际操作起来，一定能更高效、灵活。\n\n这里，我把提供 I/O 性能指标的工具做成了一个表格，方便你梳理关系和理解记忆。你可以把它保存并打印出来，随时记忆。当然，你也可以把它当成一个“指标工具”指南来使用。\n\n![img](6f26fa18a73458764fcda00212006698.png)\n\n下面，我们再来看第二个维度。\n\n**第二个维度，从工具出发。也就是当你已经安装了某个工具后，要知道这个工具能提供哪些指标。**\n\n这在实际环境中，特别是生产环境中也是非常重要的。因为很多情况下，你并没有权限安装新的工具包，只能最大化地利用好系统已有的工具，而这就需要你对它们有足够的了解。\n\n具体到每个工具的使用方法，一般都支持丰富的配置选项。不过不用担心，这些配置选项并不用背下来。你只要知道有哪些工具，以及这些工具的基本功能是什么就够了。真正要用到的时候， 通过 man 命令，查它们的使用手册就可以了。\n\n同样的，我也将这些常用工具汇总成了一个表格，方便你区分和理解。自然，你也可以当成一个“工具指标”指南使用，需要时查表即可。\n\n![img](c48b6664c6d334695ed881d5047446e9.png)\n\n## 如何迅速分析 I/O 的性能瓶颈\n\n到这里，相信你对内存的性能指标已经非常熟悉，也清楚每种性能指标分别能用什么工具来获取。\n\n你应该发现了，比起前两个板块，虽然文件系统和磁盘的 I/O 性能指标仍比较多，但核心的性能工具，其实就是那么几个。熟练掌握它们，再根据实际系统的现象，并配合系统和应用程序的原理， I/O 性能分析就很清晰了。\n\n不过，不管怎么说，如果每次一碰到 I/O 的性能问题，就把上面提到的所有工具跑一遍，肯定是不现实的。\n\n在实际生产环境中，我们希望的是，尽可能**快**地定位系统的瓶颈，然后尽可能**快**地优化性能，也就是要又快又准地解决性能问题。\n\n那有没有什么方法，可以又快又准地找出系统的 I/O 瓶颈呢？答案是肯定的。\n\n还是那句话，找关联。多种性能指标间都有一定的关联性，不要完全孤立的看待他们。**想弄清楚性能指标的关联性，就要通晓每种性能指标的工作原理**。这也是为什么我在介绍每个性能指标时，都要穿插讲解相关的系统原理，再次希望你能记住这一点。\n\n以我们前面几期的案例为例，如果你仔细对比前面的几个案例，从 I/O 延迟的案例到 MySQL 和 Redis 的案例，就会发现，虽然这些问题千差万别，但从 I/O 角度来分析，最开始的分析思路基本上类似，都是：\n\n1. 先用 iostat 发现磁盘 I/O 性能瓶颈；\n2. 再借助 pidstat ，定位出导致瓶颈的进程；\n3. 随后分析进程的 I/O 行为；\n4. 最后，结合应用程序的原理，分析这些 I/O 的来源。\n\n**所以，为了缩小排查范围，我通常会先运行那几个支持指标较多的工具，如 iostat、vmstat、pidstat 等。**然后再根据观察到的现象，结合系统和应用程序的原理，寻找下一步的分析方向。我把这个过程画成了一张图，你可以保存下来参考使用。\n\n![img](1802a35475ee2755fb45aec55ed2d98a.png)\n\n图中列出了最常用的几个文件系统和磁盘 I/O 性能分析工具，以及相应的分析流程，箭头则表示分析方向。这其中，iostat、vmstat、pidstat 是最核心的几个性能工具，它们也提供了最重要的 I/O 性能指标。举几个例子你可能更容易理解。\n\n例如，在前面讲过的 MySQL 和 Redis 案例中，我们就是通过 iostat 确认磁盘出现 I/O 性能瓶颈，然后用 pidstat 找出 I/O 最大的进程，接着借助 strace 找出该进程正在读写的文件，最后结合应用程序的原理，找出大量 I/O 的原因。\n\n再如，当你用 iostat 发现磁盘有 I/O 性能瓶颈后，再用 pidstat 和 vmstat 检查，可能会发现 I/O 来自内核线程，如 Swap 使用大量升高。这种情况下，你就得进行内存分析了，先找出占用大量内存的进程，再设法减少内存的使用。\n\n另外注意，我在这个图中只列出了最核心的几个性能工具，并没有列出前面表格中的所有工具。这么做，一方面是不想用大量的工具列表吓到你。在学习之初就接触所有核心或小众的工具，不见得是好事。另一方面，也是希望你能先把重心放在核心工具上，毕竟熟练掌握它们，就可以解决大多数问题。\n\n所以，你可以保存下这张图，作为文件系统和磁盘 I/O 性能分析的思路图谱。从最核心的这几个工具开始，通过我提供的那些案例，自己在真实环境里实践，拿下它们。\n\n## 小结\n\n今天，我们一起复习了常见的文件系统和磁盘 I/O 性能指标，梳理了常见的 I/O 性能观测工具，并建立了性能指标和工具的关联。最后，我们还总结了快速分析 I/O 性能问题的思路。\n\n还是那句话，虽然 I/O 的性能指标很多，相应的性能分析工具也有不少，但熟悉了各指标含义后，你就会自然找到它们的关联。顺着这个思路往下走，掌握常用的分析套路也并不难。\n\n## 思考\n\n专栏学习中，我只列举了几个最常见的案例，帮你理解文件系统和磁盘 I/O 性能的原理和分析方法。你肯定也碰到过不少其他 I/O 性能问题吧。我想请你一起聊聊，你碰到过哪些 I/O 性能问题呢？你又是怎么分析出它的瓶颈呢？\n\n欢迎在留言区和我讨论，也欢迎你把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 31 套路篇：磁盘 IO 性能优化的几个思路\n\n你好，我是倪朋飞。\n\n上一节，我们一起回顾了常见的文件系统和磁盘 I/O 性能指标，梳理了核心的 I/O 性能观测工具，最后还总结了快速分析 I/O 性能问题的思路。\n\n虽然 I/O 的性能指标很多，相应的性能分析工具也有好几个，但理解了各种指标的含义后，你就会发现它们其实都有一定的关联。\n\n顺着这些关系往下理解，你就会发现，掌握这些常用的瓶颈分析思路，其实并不难。\n\n找出了 I/O 的性能瓶颈后，下一步要做的就是优化了，也就是如何以最快的速度完成 I/O 操作，或者换个思路，减少甚至避免磁盘的 I/O 操作。\n\n今天，我就来说说，优化 I/O 性能问题的思路和注意事项。\n\n## I/O 基准测试\n\n按照我的习惯，优化之前，我会先问自己， I/O 性能优化的目标是什么？换句话说，我们观察的这些 I/O 性能指标（比如 IOPS、吞吐量、延迟等），要达到多少才合适呢？\n\n事实上，I/O 性能指标的具体标准，每个人估计会有不同的答案，因为我们每个人的应用场景、使用的文件系统和物理磁盘等，都有可能不一样。\n\n为了更客观合理地评估优化效果，我们首先应该对磁盘和文件系统进行基准测试，得到文件系统或者磁盘 I/O 的极限性能。\n\n[fio](https://github.com/axboe/fio)（Flexible I/O Tester）正是最常用的文件系统和磁盘 I/O 性能基准测试工具。它提供了大量的可定制化选项，可以用来测试，裸盘或者文件系统在各种场景下的 I/O 性能，包括了不同块大小、不同 I/O 引擎以及是否使用缓存等场景。\n\nfio 的安装比较简单，你可以执行下面的命令来安装它：\n\n```csharp\n# Ubuntu\napt-get install -y fio \n# CentOS\nyum install -y fio \n```\n\n安装完成后，就可以执行 man fio 查询它的使用方法。\n\nfio 的选项非常多， 我会通过几个常见场景的测试方法，介绍一些最常用的选项。这些常见场景包括随机读、随机写、顺序读以及顺序写等，你可以执行下面这些命令来测试：\n\n```perl\n# 随机读\nfio -name=randread -direct=1 -iodepth=64 -rw=randread -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb \n# 随机写\nfio -name=randwrite -direct=1 -iodepth=64 -rw=randwrite -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb \n# 顺序读\nfio -name=read -direct=1 -iodepth=64 -rw=read -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb \n# 顺序写\nfio -name=write -direct=1 -iodepth=64 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb \n```\n\n在这其中，有几个参数需要你重点关注一下。\n\n- direct，表示是否跳过系统缓存。上面示例中，我设置的 1 ，就表示跳过系统缓存。\n- iodepth，表示使用异步 I/O（asynchronous I/O，简称 AIO）时，同时发出的 I/O 请求上限。在上面的示例中，我设置的是 64。\n- rw，表示 I/O 模式。我的示例中， read/write 分别表示顺序读 / 写，而 randread/randwrite 则分别表示随机读 / 写。\n- ioengine，表示 I/O 引擎，它支持同步（sync）、异步（libaio）、内存映射（mmap）、网络（net）等各种 I/O 引擎。上面示例中，我设置的 libaio 表示使用异步 I/O。\n- bs，表示 I/O 的大小。示例中，我设置成了 4K（这也是默认值）。\n- filename，表示文件路径，当然，它可以是磁盘路径（测试磁盘性能），也可以是文件路径（测试文件系统性能）。示例中，我把它设置成了磁盘 /dev/sdb。不过注意，用磁盘路径测试写，会破坏这个磁盘中的文件系统，所以在使用前，你一定要事先做好数据备份。\n\n下面就是我使用 fio 测试顺序读的一个报告示例。\n\n```yaml\nread: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64\nfio-3.1\nStarting 1 process\nJobs: 1 (f=1): [R(1)][100.0%][r=16.7MiB/s,w=0KiB/s][r=4280,w=0 IOPS][eta 00m:00s]\nread: (groupid=0, jobs=1): err= 0: pid=17966: Sun Dec 30 08:31:48 2018\n   read: IOPS=4257, BW=16.6MiB/s (17.4MB/s)(1024MiB/61568msec)\n    slat (usec): min=2, max=2566, avg= 4.29, stdev=21.76\n    clat (usec): min=228, max=407360, avg=15024.30, stdev=20524.39\n     lat (usec): min=243, max=407363, avg=15029.12, stdev=20524.26\n    clat percentiles (usec):\n     |  1.00th=[   498],  5.00th=[  1020], 10.00th=[  1319], 20.00th=[  1713],\n     | 30.00th=[  1991], 40.00th=[  2212], 50.00th=[  2540], 60.00th=[  2933],\n     | 70.00th=[  5407], 80.00th=[ 44303], 90.00th=[ 45351], 95.00th=[ 45876],\n     | 99.00th=[ 46924], 99.50th=[ 46924], 99.90th=[ 48497], 99.95th=[ 49021],\n     | 99.99th=[404751]\n   bw (  KiB/s): min= 8208, max=18832, per=99.85%, avg=17005.35, stdev=998.94, samples=123\n   iops        : min= 2052, max= 4708, avg=4251.30, stdev=249.74, samples=123\n  lat (usec)   : 250=0.01%, 500=1.03%, 750=1.69%, 1000=2.07%\n  lat (msec)   : 2=25.64%, 4=37.58%, 10=2.08%, 20=0.02%, 50=29.86%\n  lat (msec)   : 100=0.01%, 500=0.02%\n  cpu          : usr=1.02%, sys=2.97%, ctx=33312, majf=0, minf=75\n  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, \u003e=64=100.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u003e=64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, \u003e=64=0.0%\n     issued rwt: total=262144,0,0, short=0,0,0, dropped=0,0,0\n     latency   : target=0, window=0, percentile=100.00%, depth=64 \nRun status group 0 (all jobs):\n   READ: bw=16.6MiB/s (17.4MB/s), 16.6MiB/s-16.6MiB/s (17.4MB/s-17.4MB/s), io=1024MiB (1074MB), run=61568-61568msec \nDisk stats (read/write):\n  sdb: ios=261897/0, merge=0/0, ticks=3912108/0, in_queue=3474336, util=90.09% \n```\n\n这个报告中，需要我们重点关注的是， slat、clat、lat ，以及 bw 和 iops 这几行。\n\n先来看刚刚提到的前三个参数。事实上，slat、clat、lat 都是指 I/O 延迟（latency）。不同之处在于：\n\n- slat ，是指从 I/O 提交到实际执行 I/O 的时长（Submission latency）；\n- clat ，是指从 I/O 提交到 I/O 完成的时长（Completion latency）；\n- 而 lat ，指的是从 fio 创建 I/O 到 I/O 完成的总时长。\n\n这里需要注意的是，对同步 I/O 来说，由于 I/O 提交和 I/O 完成是一个动作，所以 slat 实际上就是 I/O 完成的时间，而 clat 是 0。而从示例可以看到，使用异步 I/O（libaio）时，lat 近似等于 slat + clat 之和。\n\n再来看 bw ，它代表吞吐量。在我上面的示例中，你可以看到，平均吞吐量大约是 16 MB（17005 KiB/1024）。\n\n最后的 iops ，其实就是每秒 I/O 的次数，上面示例中的平均 IOPS 为 4250。\n\n通常情况下，应用程序的 I/O 都是读写并行的，而且每次的 I/O 大小也不一定相同。所以，刚刚说的这几种场景，并不能精确模拟应用程序的 I/O 模式。那怎么才能精确模拟应用程序的 I/O 模式呢？\n\n幸运的是，fio 支持 I/O 的重放。借助前面提到过的 blktrace，再配合上 fio，就可以实现对应用程序 I/O 模式的基准测试。你需要先用 blktrace ，记录磁盘设备的 I/O 访问情况；然后使用 fio ，重放 blktrace 的记录。\n\n比如你可以运行下面的命令来操作：\n\n```shell\n# 使用 blktrace 跟踪磁盘 I/O，注意指定应用程序正在操作的磁盘\n$ blktrace /dev/sdb \n# 查看 blktrace 记录的结果\n# ls\nsdb.blktrace.0  sdb.blktrace.1 \n# 将结果转化为二进制文件\n$ blkparse sdb -d sdb.bin \n# 使用 fio 重放日志\n$ fio --name=replay --filename=/dev/sdb --direct=1 --read_iolog=sdb.bin \n```\n\n这样，我们就通过 blktrace+fio 的组合使用，得到了应用程序 I/O 模式的基准测试报告。\n\n## I/O 性能优化\n\n得到 I/O 基准测试报告后，再用上我们上一节总结的性能分析套路，找出 I/O 的性能瓶颈并优化，就是水到渠成的事情了。当然， 想要优化 I/O 性能，肯定离不开 Linux 系统的 I/O 栈图的思路辅助。你可以结合下面的 I/O 栈图再回顾一下。\n\n![img](9e42aaf53ff4a544b9a7b03b6ce63f38-1550567461955.png)\n\n下面，我就带你从应用程序、文件系统以及磁盘角度，分别看看 I/O 性能优化的基本思路。\n\n### 应用程序优化\n\n首先，我们来看一下，从应用程序的角度有哪些优化 I/O 的思路。\n\n应用程序处于整个 I/O 栈的最上端，它可以通过系统调用，来调整 I/O 模式（如顺序还是随机、同步还是异步）， 同时，它也是 I/O 数据的最终来源。在我看来，可以有这么几种方式来优化应用程序的 I/O 性能。\n\n第一，可以用追加写代替随机写，减少寻址开销，加快 I/O 写的速度。\n\n第二，可以借助缓存 I/O ，充分利用系统缓存，降低实际 I/O 的次数。\n\n第三，可以在应用程序内部构建自己的缓存，或者用 Redis 这类外部缓存系统。这样，一方面，能在应用程序内部，控制缓存的数据和生命周期；另一方面，也能降低其他应用程序使用缓存对自身的影响。\n\n比如，在前面的 MySQL 案例中，我们已经见识过，只是因为一个干扰应用清理了系统缓存，就会导致 MySQL 查询有数百倍的性能差距（0.1s vs 15s）。\n\n再如， C 标准库提供的 fopen、fread 等库函数，都会利用标准库的缓存，减少磁盘的操作。而你直接使用 open、read 等系统调用时，就只能利用操作系统提供的页缓存和缓冲区等，而没有库函数的缓存可用。\n\n第四，在需要频繁读写同一块磁盘空间时，可以用 mmap 代替 read/write，减少内存的拷贝次数。\n\n第五，在需要同步写的场景中，尽量将写请求合并，而不是让每个请求都同步写入磁盘，即可以用 fsync() 取代 O_SYNC。\n\n第六，在多个应用程序共享相同磁盘时，为了保证 I/O 不被某个应用完全占用，推荐你使用 cgroups 的 I/O 子系统，来限制进程 / 进程组的 IOPS 以及吞吐量。\n\n最后，在使用 CFQ 调度器时，可以用 ionice 来调整进程的 I/O 调度优先级，特别是提高核心应用的 I/O 优先级。ionice 支持三个优先级类：Idle、Best-effort 和 Realtime。其中， Best-effort 和 Realtime 还分别支持 0-7 的级别，数值越小，则表示优先级别越高。\n\n### 文件系统优化\n\n应用程序访问普通文件时，实际是由文件系统间接负责，文件在磁盘中的读写。所以，跟文件系统中相关的也有很多优化 I/O 性能的方式。\n\n第一，你可以根据实际负载场景的不同，选择最适合的文件系统。比如 Ubuntu 默认使用 ext4 文件系统，而 CentOS 7 默认使用 xfs 文件系统。\n\n相比于 ext4 ，xfs 支持更大的磁盘分区和更大的文件数量，如 xfs 支持大于 16TB 的磁盘。但是 xfs 文件系统的缺点在于无法收缩，而 ext4 则可以。\n\n第二，在选好文件系统后，还可以进一步优化文件系统的配置选项，包括文件系统的特性（如 ext_attr、dir_index）、日志模式（如 journal、ordered、writeback）、挂载选项（如 noatime）等等。\n\n比如， 使用 tune2fs 这个工具，可以调整文件系统的特性（tune2fs 也常用来查看文件系统超级块的内容）。 而通过 /etc/fstab ，或者 mount 命令行参数，我们可以调整文件系统的日志模式和挂载选项等。\n\n第三，可以优化文件系统的缓存。\n\n比如，你可以优化 pdflush 脏页的刷新频率（比如设置 dirty_expire_centisecs 和 dirty_writeback_centisecs）以及脏页的限额（比如调整 dirty_background_ratio 和 dirty_ratio 等）。\n\n再如，你还可以优化内核回收目录项缓存和索引节点缓存的倾向，即调整 vfs_cache_pressure（/proc/sys/vm/vfs_cache_pressure，默认值 100），数值越大，就表示越容易回收。\n\n最后，在不需要持久化时，你还可以用内存文件系统 tmpfs，以获得更好的 I/O 性能 。tmpfs 把数据直接保存在内存中，而不是磁盘中。比如 /dev/shm/ ，就是大多数 Linux 默认配置的一个内存文件系统，它的大小默认为总内存的一半。\n\n### 磁盘优化\n\n数据的持久化存储，最终还是要落到具体的物理磁盘中，同时，磁盘也是整个 I/O 栈的最底层。从磁盘角度出发，自然也有很多有效的性能优化方法。\n\n第一，最简单有效的优化方法，就是换用性能更好的磁盘，比如用 SSD 替代 HDD。\n\n第二，我们可以使用 RAID ，把多块磁盘组合成一个逻辑磁盘，构成冗余独立磁盘阵列。这样做既可以提高数据的可靠性，又可以提升数据的访问性能。\n\n第三，针对磁盘和应用程序 I/O 模式的特征，我们可以选择最适合的 I/O 调度算法。比方说，SSD 和虚拟机中的磁盘，通常用的是 noop 调度算法。而数据库应用，我更推荐使用 deadline 算法。\n\n第四，我们可以对应用程序的数据，进行磁盘级别的隔离。比如，我们可以为日志、数据库等 I/O 压力比较重的应用，配置单独的磁盘。\n\n第五，在顺序读比较多的场景中，我们可以增大磁盘的预读数据，比如，你可以通过下面两种方法，调整 /dev/sdb 的预读大小。\n\n- 调整内核选项 /sys/block/sdb/queue/read_ahead_kb，默认大小是 128 KB，单位为 KB。\n- 使用 blockdev 工具设置，比如 blockdev --setra 8192 /dev/sdb，注意这里的单位是 512B（0.5KB），所以它的数值总是 read_ahead_kb 的两倍。\n\n第六，我们可以优化内核块设备 I/O 的选项。比如，可以调整磁盘队列的长度 /sys/block/sdb/queue/nr_requests，适当增大队列长度，可以提升磁盘的吞吐量（当然也会导致 I/O 延迟增大）。\n\n最后，要注意，磁盘本身出现硬件错误，也会导致 I/O 性能急剧下降，所以发现磁盘性能急剧下降时，你还需要确认，磁盘本身是不是出现了硬件错误。\n\n比如，你可以查看 dmesg 中是否有硬件 I/O 故障的日志。 还可以使用 badblocks、smartctl 等工具，检测磁盘的硬件问题，或用 e2fsck 等来检测文件系统的错误。如果发现问题，你可以使用 fsck 等工具来修复。\n\n## 小结\n\n今天，我们一起梳理了常见的文件系统和磁盘 I/O 的性能优化思路和方法。发现 I/O 性能问题后，不要急于动手优化，而要先找出最重要的、可以最大程度提升性能的问题，然后再从 I/O 栈的不同层入手，考虑具体的优化方法。\n\n记住，磁盘和文件系统的 I/O ，通常是整个系统中最慢的一个模块。所以，在优化 I/O 问题时，除了可以优化 I/O 的执行流程，还可以借助更快的内存、网络、CPU 等，减少 I/O 调用。\n\n比如，你可以充分利用系统提供的 Buffer、Cache ，或是应用程序内部缓存， 再或者 Redis 这类的外部缓存系统。\n\n## 思考\n\n在整个板块的学习中，我只列举了最常见的几个 I/O 性能优化思路。除此之外，还有很多从应用程序、系统再到磁盘硬件的优化方法。我想请你一起来聊聊，你还知道哪些其他优化方法吗？\n\n欢迎在留言区跟我讨论，也欢迎你把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 32 Linux 性能优化答疑（四）\n\n你好，我是倪朋飞。\n\n专栏更新至今，四大基础模块的第三个模块——文件系统和磁盘 I/O 篇，我们就已经学完了。很开心你还没有掉队，仍然在积极学习思考和实践操作，并且热情地留言与讨论。\n\n今天是性能优化的第四期。照例，我从 I/O 模块的留言中摘出了一些典型问题，作为今天的答疑内容，集中回复。同样的，为了便于你学习理解，它们并不是严格按照文章顺序排列的。\n\n每个问题，我都附上了留言区提问的截屏。如果你需要回顾内容原文，可以扫描每个问题右下方的二维码查看。\n\n## 问题 1：阻塞、非阻塞 I/O 与同步、异步 I/O 的区别和联系\n\n![img](1c3237118d1c55792ac0d9cc23f14bb0.png)\n\n在[文件系统的工作原理]篇中，我曾经介绍了阻塞、非阻塞 I/O 以及同步、异步 I/O 的含义，这里我们再简单回顾一下。\n\n首先我们来看阻塞和非阻塞 I/O。根据应用程序是否阻塞自身运行，可以把 I/O 分为阻塞 I/O 和非阻塞 I/O。\n\n- 所谓阻塞 I/O，是指应用程序在执行 I/O 操作后，如果没有获得响应，就会阻塞当前线程，不能执行其他任务。\n- 所谓非阻塞 I/O，是指应用程序在执行 I/O 操作后，不会阻塞当前的线程，可以继续执行其他的任务。\n\n再来看同步 I/O 和异步 I/O。根据 I/O 响应的通知方式的不同，可以把文件 I/O 分为同步 I/O 和异步 I/O。\n\n- 所谓同步 I/O，是指收到 I/O 请求后，系统不会立刻响应应用程序；等到处理完成，系统才会通过系统调用的方式，告诉应用程序 I/O 结果。\n- 所谓异步 I/O，是指收到 I/O 请求后，系统会先告诉应用程序 I/O 请求已经收到，随后再去异步处理；等处理完成后，系统再通过事件通知的方式，告诉应用程序结果。\n\n你可以看出，阻塞 / 非阻塞和同步 / 异步，其实就是两个不同角度的 I/O 划分方式。它们描述的对象也不同，阻塞 / 非阻塞针对的是 I/O 调用者（即应用程序），而同步 / 异步针对的是 I/O 执行者（即系统）。\n\n我举个例子来进一步解释下。比如在 Linux I/O 调用中，\n\n- 系统调用 read 是同步读，所以，在没有得到磁盘数据前，read 不会响应应用程序。\n- 而 aio_read 是异步读，系统收到 AIO 读请求后不等处理就返回了，而具体的 read 结果，再通过回调异步通知应用程序。\n\n再如，在网络套接字的接口中，\n\n- 使用 send() 直接向套接字发送数据时，如果套接字没有设置 O_NONBLOCK 标识，那么 send() 操作就会一直阻塞，当前线程也没法去做其他事情。\n- 当然，如果你用了 epoll，系统会告诉你这个套接字的状态，那就可以用非阻塞的方式使用。当这个套接字不可写的时候，你可以去做其他事情，比如读写其他套接字。\n\n## 问题 2：“文件系统”课后思考\n\n![img](40c924ea4b11e12d6d34181a00f292a6.jpg)\n\n在[文件系统原理]的最后，我给你留了一道思考题，那就是执行 find 命令时，会不会导致系统的缓存升高呢？如果会导致，升高的又是哪种类型的缓存呢？\n\n关于这个问题，白华和 coyang 的答案已经很准确了。通过学习 Linux 文件系统的原理，我们知道，文件名以及文件之间的目录关系，都放在目录项缓存中。而这是一个基于内存的数据结构，会根据需要动态构建。所以，查找文件时，Linux 就会动态构建不在缓存中的目录项结构，导致 dentry 缓存升高。\n\n![img](488110263a9c7ff801a3e04c010f0bc5.png)![img](57e4cf5a42a91392ebebf106f992a858.png)\n\n事实上，除了目录项缓存增加，Buffer 的使用也会增加。如果你用 vmstat 观察一下，会发现 Buffer 和 Cache 都在增长：\n\n```yaml\n$ vmstat 1\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 0  1      0 7563744   6024 225944    0    0  3736     0  574 3249  3  5 89  3  0\n 1  0      0 7542792  14736 236856    0    0  8708     0 13494 32335  8 19 66  7  0\n 0  1      0 7494452  27280 272284    0    0 12544     0 4550 17084  5 15 68 13  0\n 0  1      0 7475084  42380 276320    0    0 15096     0 2541 14253  2  6 78 13  0\n 0  1      0 7455728  57600 280436    0    0 15220     0 2025 14518  2  6 70 22  0\n```\n\n这里，Buffer 的增长是因为，构建目录项缓存所需的元数据（比如文件名称、索引节点等），需要从文件系统中读取。\n\n## 问题 3：“磁盘 I/O 延迟”课后思考\n\n在[磁盘 I/O 延迟案例]的最后，我给你留了一道思考题。\n\n我们通过 iostat ，确认磁盘 I/O 已经出现了性能瓶颈，还用 pidstat 找出了大量磁盘 I/O 的进程。但是，随后使用 strace 跟踪这个进程，却找不到任何 write 系统调用。这是为什么呢？\n\n![img](6408b3aa2aa9a98a930d1a5b2e2fef09.jpg)\n\n很多同学的留言都准确回答了这个问题。比如，划时代和 jeff 的留言都指出，在这个场景中，我们需要加 -f 选项，以便跟踪多进程和多线程的系统调用情况。\n\n![img](e4e9a070022f7b49cb8d5554b9a60055.png)![img](71a6df4144ce59d9e1a01c26453acf05.png)\n\n你看，仅仅是不恰当的选项，都可能会导致性能工具“犯错”，呈现这种看起来不合逻辑的结果。非常高兴看到，这么多同学已经掌握了性能工具使用的核心思路——弄清楚工具本身的原理和问题。\n\n## 问题 4：“MySQL 案例”课后思考\n\n在 [MySQL 案例]的最后，我给你留了一个思考题。\n\n为什么 DataService 应用停止后，即使仍没有索引，MySQL 的查询速度还是快了很多，并且磁盘 I/O 瓶颈也消失了呢？\n\n![img](924fbc974313b1e0fe6b8d14e7a44178.png)\n\nninuxer 的留言基本解释了这个问题，不过还不够完善。\n\n事实上，当你看到 DataService 在修改 */proc/sys/vm/drop_caches* 时，就应该想到前面学过的 Cache 的作用。\n\n我们知道，案例应用访问的数据表，基于 MyISAM 引擎，而 MyISAM 的一个特点，就是只在内存中缓存索引，并不缓存数据。所以，在查询语句无法使用索引时，就需要数据表从数据库文件读入内存，然后再进行处理。\n\n所以，如果你用 vmstat 工具，观察缓存和 I/O 的变化趋势，就会发现下面这样的结果：\n\n```yaml\n$ vmstat 1 \nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st \n# 备注： DataService 正在运行\n0  1      0 7293416    132 366704    0    0 32516    12   36  546  1  3 49 48  0\n 0  1      0 7260772    132 399256    0    0 32640     0   37  463  1  1 49 48  0\n 0  1      0 7228088    132 432088    0    0 32640     0   30  477  0  1 49 49  0\n 0  0      0 7306560    132 353084    0    0 20572     4   90  574  1  4 69 27  0\n 0  2      0 7282300    132 368536    0    0 15468     0   32  304  0  0 79 20  0 \n# 备注：DataService 从这里开始停止\n 0  0      0 7241852   1360 424164    0    0   864   320  133 1266  1  1 94  5  0\n 0  1      0 7228956   1368 437400    0    0 13328     0   45  366  0  0 83 17  0\n 0  1      0 7196320   1368 470148    0    0 32640     0   33  413  1  1 50 49  0\n...\n 0  0      0 6747540   1368 918576    0    0 29056     0   42  568  0  0 56 44  0\n 0  0      0 6747540   1368 918576    0    0     0     0   40  141  1  0 100  0  0\n```\n\n在 DataService 停止前，cache 会连续增长三次后再降回去，这正是因为 DataService 每隔 3 秒清理一次页缓存。而 DataService 停止后，cache 就会不停地增长，直到增长为 918576 后，就不再变了。\n\n这时，磁盘的读（bi）降低到 0，同时，iowait（wa）也降低到 0，这说明，此时的所有数据都已经在系统的缓存中了。我们知道，缓存是内存的一部分，它的访问速度比磁盘快得多，这也就能解释，为什么 MySQL 的查询速度变快了很多。\n\n从这个案例，你会发现，MySQL 的 MyISAM 引擎，本身并不缓存数据，而要依赖系统缓存来加速磁盘 I/O 的访问。一旦系统中还有其他应用同时运行，MyISAM 引擎就很难充分利用系统缓存。因为系统缓存可能被其他应用程序占用，甚至直接被清理掉。\n\n所以，一般来说，我并不建议，把应用程序的性能优化完全建立在系统缓存上。还是那句话，最好能在应用程序的内部分配内存，构建完全自主控制的缓存，比如 MySQL 的 InnoDB 引擎，就同时缓存了索引和数据；或者，可以使用第三方的缓存应用，比如 Memcached、Redis 等。\n\n今天主要回答这些问题，同时也欢迎你继续在留言区写下疑问和感想，我会持续不断地解答。希望借助每一次的答疑，可以和你一起，把文章知识内化为你的能力，我们不仅在实战中演练，也要在交流中进步。\n\n# 33 关于 Linux 网络，你必须知道这些（上）\n\n你好，我是倪朋飞。\n\n前几节，我们一起学习了文件系统和磁盘 I/O 的工作原理，以及相应的性能分析和优化方法。接下来，我们将进入下一个重要模块—— Linux 的网络子系统。\n\n由于网络处理的流程最复杂，跟我们前面讲到的进程调度、中断处理、内存管理以及 I/O 等都密不可分，所以，我把网络模块作为最后一个资源模块来讲解。\n\n同 CPU、内存以及 I/O 一样，网络也是 Linux 系统最核心的功能。网络是一种把不同计算机或网络设备连接到一起的技术，它本质上是一种进程间通信方式，特别是跨系统的进程间通信，必须要通过网络才能进行。随着高并发、分布式、云计算、微服务等技术的普及，网络的性能也变得越来越重要。\n\n那么，Linux 网络又是怎么工作的呢？又有哪些指标衡量网络的性能呢？接下来的两篇文章，我将带你一起学习 Linux 网络的工作原理和性能指标。\n\n## 网络模型\n\n说到网络，我想你肯定经常提起七层负载均衡、四层负载均衡，或者三层设备、二层设备等等。那么，这里说的二层、三层、四层、七层又都是什么意思呢？\n\n实际上，这些层都来自国际标准化组织制定的**开放式系统互联通信参考模型**（Open System Interconnection Reference Model），简称为 OSI 网络模型。\n\n为了解决网络互联中异构设备的兼容性问题，并解耦复杂的网络包处理流程，OSI 模型把网络互联的框架分为应用层、表示层、会话层、传输层、网络层、数据链路层以及物理层等七层，每个层负责不同的功能。其中，\n\n- 应用层，负责为应用程序提供统一的接口。\n- 表示层，负责把数据转换成兼容接收系统的格式。\n- 会话层，负责维护计算机之间的通信连接。\n- 传输层，负责为数据加上传输表头，形成数据包。\n- 网络层，负责数据的路由和转发。\n- 数据链路层，负责 MAC 寻址、错误侦测和改错。\n- 物理层，负责在物理网络中传输数据帧。\n\n但是 OSI 模型还是太复杂了，也没能提供一个可实现的方法。所以，在 Linux 中，我们实际上使用的是另一个更实用的四层模型，即 TCP/IP 网络模型。\n\nTCP/IP 模型，把网络互联的框架分为应用层、传输层、网络层、网络接口层等四层，其中，\n\n- 应用层，负责向用户提供一组应用程序，比如 HTTP、FTP、DNS 等。\n- 传输层，负责端到端的通信，比如 TCP、UDP 等。\n- 网络层，负责网络包的封装、寻址和路由，比如 IP、ICMP 等。\n- 网络接口层，负责网络包在物理网络中的传输，比如 MAC 寻址、错误侦测以及通过网卡传输网络帧等。\n\n为了帮你更形象理解 TCP/IP 与 OSI 模型的关系，我画了一张图，如下所示：\n\n![img](f2dbfb5500c2aa7c47de6216ee7098bd.png)\n\n当然了，虽说 Linux 实际按照 TCP/IP 模型，实现了网络协议栈，但在平时的学习交流中，我们习惯上还是用 OSI 七层模型来描述。比如，说到七层和四层负载均衡，对应的分别是 OSI 模型中的应用层和传输层（而它们对应到 TCP/IP 模型中，实际上是四层和三层）。\n\nTCP/IP 模型包括了大量的网络协议，这些协议的原理，也是我们每个人必须掌握的核心基础知识。如果你不太熟练，推荐你去学《TCP/IP 详解》的卷一和卷二，或者学习极客时间出品的《[趣谈网络协议]》专栏。\n\n## Linux 网络栈\n\n有了 TCP/IP 模型后，在进行网络传输时，数据包就会按照协议栈，对上一层发来的数据进行逐层处理；然后封装上该层的协议头，再发送给下一层。\n\n当然，网络包在每一层的处理逻辑，都取决于各层采用的网络协议。比如在应用层，一个提供 REST API 的应用，可以使用 HTTP 协议，把它需要传输的 JSON 数据封装到 HTTP 协议中，然后向下传递给 TCP 层。\n\n而封装做的事情就很简单了，只是在原来的负载前后，增加固定格式的元数据，原始的负载数据并不会被修改。\n\n比如，以通过 TCP 协议通信的网络包为例，通过下面这张图，我们可以看到，应用程序数据在每个层的封装格式。\n\n![img](c8dfe80acc44ba1aa9df327c54349e79-1550567543306.png)\n\n其中：\n\n- 传输层在应用程序数据前面增加了 TCP 头；\n- 网络层在 TCP 数据包前增加了 IP 头；\n- 而网络接口层，又在 IP 数据包前后分别增加了帧头和帧尾。\n\n这些新增的头部和尾部，都按照特定的协议格式填充，想了解具体格式，你可以查看协议的文档。 比如，你可以查看[这里](https://zh.wikipedia.org/wiki/传输控制协议#封包結構)，了解 TCP 头的格式。\n\n这些新增的头部和尾部，增加了网络包的大小，但我们都知道，物理链路中并不能传输任意大小的数据包。网络接口配置的最大传输单元（MTU），就规定了最大的 IP 包大小。在我们最常用的以太网中，MTU 默认值是 1500（这也是 Linux 的默认值）。\n\n一旦网络包超过 MTU 的大小，就会在网络层分片，以保证分片后的 IP 包不大于 MTU 值。显然，MTU 越大，需要的分包也就越少，自然，网络吞吐能力就越好。\n\n理解了 TCP/IP 网络模型和网络包的封装原理后，你很容易能想到，Linux 内核中的网络栈，其实也类似于 TCP/IP 的四层结构。如下图所示，就是 Linux 通用 IP 网络栈的示意图：\n\n![img](c7b5b16539f90caabb537362ee7c27ac.png)\n\n（图片参考《性能之巅》图 10.7 通用 IP 网络栈绘制）\n\n我们从上到下来看这个网络栈，你可以发现，\n\n- 最上层的应用程序，需要通过系统调用，来跟套接字接口进行交互；\n- 套接字的下面，就是我们前面提到的传输层、网络层和网络接口层；\n- 最底层，则是网卡驱动程序以及物理网卡设备。\n\n这里我简单说一下网卡。网卡是发送和接收网络包的基本设备。在系统启动过程中，网卡通过内核中的网卡驱动程序注册到系统中。而在网络收发过程中，内核通过中断跟网卡进行交互。\n\n再结合前面提到的 Linux 网络栈，可以看出，网络包的处理非常复杂。所以，网卡硬中断只处理最核心的网卡数据读取或发送，而协议栈中的大部分逻辑，都会放到软中断中处理。\n\n## Linux 网络收发流程\n\n了解了 Linux 网络栈后，我们再来看看， Linux 到底是怎么收发网络包的。\n\n\u003e 注意，以下内容都以物理网卡为例。事实上，Linux 还支持众多的虚拟网络设备，而它们的网络收发流程会有一些差别。\n\n### 网络包的接收流程\n\n我们先来看网络包的接收流程。\n\n当一个网络帧到达网卡后，网卡会通过 DMA 方式，把这个网络包放到收包队列中；然后通过硬中断，告诉中断处理程序已经收到了网络包。\n\n接着，网卡中断处理程序会为网络帧分配内核数据结构（sk_buff），并将其拷贝到 sk_buff 缓冲区中；然后再通过软中断，通知内核收到了新的网络帧。\n\n接下来，内核协议栈从缓冲区中取出网络帧，并通过网络协议栈，从下到上逐层处理这个网络帧。比如，\n\n- 在链路层检查报文的合法性，找出上层协议的类型（比如 IPv4 还是 IPv6），再去掉帧头、帧尾，然后交给网络层。\n- 网络层取出 IP 头，判断网络包下一步的走向，比如是交给上层处理还是转发。当网络层确认这个包是要发送到本机后，就会取出上层协议的类型（比如 TCP 还是 UDP），去掉 IP 头，再交给传输层处理。\n- 传输层取出 TCP 头或者 UDP 头后，根据 \u003c 源 IP、源端口、目的 IP、目的端口 \u003e 四元组作为标识，找出对应的 Socket，并把数据拷贝到 Socket 的接收缓存中。\n\n最后，应用程序就可以使用 Socket 接口，读取到新接收到的数据了。\n\n为了更清晰表示这个流程，我画了一张图，这张图的左半部分表示接收流程，而图中的粉色箭头则表示网络包的处理路径。\n\n![img](3af644b6d463869ece19786a4634f765.png)\n\n### 网络包的发送流程\n\n了解网络包的接收流程后，就很容易理解网络包的发送流程。网络包的发送流程就是上图的右半部分，很容易发现，网络包的发送方向，正好跟接收方向相反。\n\n首先，应用程序调用 Socket API（比如 sendmsg）发送网络包。\n\n由于这是一个系统调用，所以会陷入到内核态的套接字层中。套接字层会把数据包放到 Socket 发送缓冲区中。\n\n接下来，网络协议栈从 Socket 发送缓冲区中，取出数据包；再按照 TCP/IP 栈，从上到下逐层处理。比如，传输层和网络层，分别为其增加 TCP 头和 IP 头，执行路由查找确认下一跳的 IP，并按照 MTU 大小进行分片。\n\n分片后的网络包，再送到网络接口层，进行物理地址寻址，以找到下一跳的 MAC 地址。然后添加帧头和帧尾，放到发包队列中。这一切完成后，会有软中断通知驱动程序：发包队列中有新的网络帧需要发送。\n\n最后，驱动程序通过 DMA ，从发包队列中读出网络帧，并通过物理网卡把它发送出去。\n\n## **小结**\n\n在今天的文章中，我带你一起梳理了 Linux 网络的工作原理。\n\n多台服务器通过网卡、交换机、路由器等网络设备连接到一起，构成了相互连接的网络。由于网络设备的异构性和网络协议的复杂性，国际标准化组织定义了一个七层的 OSI 网络模型，但是这个模型过于复杂，实际工作中的事实标准，是更为实用的 TCP/IP 模型。\n\nTCP/IP 模型，把网络互联的框架，分为应用层、传输层、网络层、网络接口层等四层，这也是 Linux 网络栈最核心的构成部分。\n\n- 应用程序通过套接字接口发送数据包，先要在网络协议栈中从上到下进行逐层处理，最终再送到网卡发送出去。\n- 而接收时，同样先经过网络栈从下到上的逐层处理，最终才会送到应用程序。\n\n了解了 Linux 网络的基本原理和收发流程后，你肯定迫不及待想知道，如何去观察网络的性能情况。那么，具体来说，哪些指标可以衡量 Linux 的网络性能呢？别急，我将在下一节中为你详细讲解。\n\n## 思考\n\n最后，我想请你来聊聊你所理解的 Linux 网络。你碰到过哪些网络相关的性能瓶颈？你又是怎么样来分析它们的呢？你可以结合今天学到的网络知识，提出自己的观点。\n\n欢迎在留言区和我讨论，也欢迎你把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 34 关于 Linux 网络，你必须知道这些（下）\n\n你好，我是倪朋飞。\n\n上一节，我带你学习了 Linux 网络的基础原理。简单回顾一下，Linux 网络根据 TCP/IP 模型，构建其网络协议栈。TCP/IP 模型由应用层、传输层、网络层、网络接口层等四层组成，这也是 Linux 网络栈最核心的构成部分。\n\n应用程序通过套接字接口发送数据包时，先要在网络协议栈中从上到下逐层处理，然后才最终送到网卡发送出去；而接收数据包时，也要先经过网络栈从下到上的逐层处理，最后送到应用程序。\n\n了解 Linux 网络的基本原理和收发流程后，你肯定迫不及待想知道，如何去观察网络的性能情况。具体而言，哪些指标可以用来衡量 Linux 的网络性能呢？\n\n## 性能指标\n\n实际上，我们通常用带宽、吞吐量、延时、PPS（Packet Per Second）等指标衡量网络的性能。\n\n- **带宽**，表示链路的最大传输速率，单位通常为 b/s （比特 / 秒）。\n- **吞吐量**，表示单位时间内成功传输的数据量，单位通常为 b/s（比特 / 秒）或者 B/s（字节 / 秒）。吞吐量受带宽限制，而吞吐量 / 带宽，也就是该网络的使用率。\n- **延时**，表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟。在不同场景中，这一指标可能会有不同含义。比如，它可以表示，建立连接需要的时间（比如 TCP 握手延时），或一个数据包往返所需的时间（比如 RTT）。\n- **PPS**，是 Packet Per Second（包 / 秒）的缩写，表示以网络包为单位的传输速率。PPS 通常用来评估网络的转发能力，比如硬件交换机，通常可以达到线性转发（即 PPS 可以达到或者接近理论最大值）。而基于 Linux 服务器的转发，则容易受网络包大小的影响。\n\n除了这些指标，**网络的可用性**（网络能否正常通信）、**并发连接数**（TCP 连接数量）、**丢包率**（丢包百分比）、**重传率**（重新传输的网络包比例）等也是常用的性能指标。\n\n接下来，请你打开一个终端，SSH 登录到服务器上，然后跟我一起来探索、观测这些性能指标。\n\n## **网络配置**\n\n分析网络问题的第一步，通常是查看网络接口的配置和状态。你可以使用 ifconfig 或者 ip 命令，来查看网络的配置。我个人更推荐使用 ip 工具，因为它提供了更丰富的功能和更易用的接口。\n\n\u003e ifconfig 和 ip 分别属于软件包 net-tools 和 iproute2，iproute2 是 net-tools 的下一代。通常情况下它们会在发行版中默认安装。但如果你找不到 ifconfig 或者 ip 命令，可以安装这两个软件包。\n\n以网络接口 eth0 为例，你可以运行下面的两个命令，查看它的配置和状态：\n\n```yaml\n$ ifconfig eth0\neth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500\n      inet 10.240.0.30 netmask 255.240.0.0 broadcast 10.255.255.255\n      inet6 fe80::20d:3aff:fe07:cf2a prefixlen 64 scopeid 0x20\u003clink\u003e\n      ether 78:0d:3a:07:cf:3a txqueuelen 1000 (Ethernet)\n      RX packets 40809142 bytes 9542369803 (9.5 GB)\n      RX errors 0 dropped 0 overruns 0 frame 0\n      TX packets 32637401 bytes 4815573306 (4.8 GB)\n      TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\n\n$ ip -s addr show dev eth0\n2: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000\n  link/ether 78:0d:3a:07:cf:3a brd ff:ff:ff:ff:ff:ff\n  inet 10.240.0.30/12 brd 10.255.255.255 scope global eth0\n      valid_lft forever preferred_lft forever\n  inet6 fe80::20d:3aff:fe07:cf2a/64 scope link\n      valid_lft forever preferred_lft forever\n  RX: bytes packets errors dropped overrun mcast\n   9542432350 40809397 0       0       0       193\n  TX: bytes packets errors dropped carrier collsns\n   4815625265 32637658 0       0       0       0\n```\n\n你可以看到，ifconfig 和 ip 命令输出的指标基本相同，只是显示格式略微不同。比如，它们都包括了网络接口的状态标志、MTU 大小、IP、子网、MAC 地址以及网络包收发的统计信息。\n\n这些具体指标的含义，在文档中都有详细的说明，不过，这里有几个跟网络性能密切相关的指标，需要你特别关注一下。\n\n第一，网络接口的状态标志。ifconfig 输出中的 RUNNING ，或 ip 输出中的 LOWER_UP ，都表示物理网络是连通的，即网卡已经连接到了交换机或者路由器中。如果你看不到它们，通常表示网线被拔掉了。\n\n第二，MTU 的大小。MTU 默认大小是 1500，根据网络架构的不同（比如是否使用了 VXLAN 等叠加网络），你可能需要调大或者调小 MTU 的数值。\n\n第三，网络接口的 IP 地址、子网以及 MAC 地址。这些都是保障网络功能正常工作所必需的，你需要确保配置正确。\n\n第四，网络收发的字节数、包数、错误数以及丢包情况，特别是 TX 和 RX 部分的 errors、dropped、overruns、carrier 以及 collisions 等指标不为 0 时，通常表示出现了网络 I/O 问题。其中：\n\n- errors 表示发生错误的数据包数，比如校验错误、帧同步错误等；\n- dropped 表示丢弃的数据包数，即数据包已经收到了 Ring Buffer，但因为内存不足等原因丢包；\n- overruns 表示超限数据包数，即网络 I/O 速度过快，导致 Ring Buffer 中的数据包来不及处理（队列满）而导致的丢包；\n- carrier 表示发生 carrirer 错误的数据包数，比如双工模式不匹配、物理电缆出现问题等；\n- collisions 表示碰撞数据包数。\n\n## **套接字信息**\n\nifconfig 和 ip 只显示了网络接口收发数据包的统计信息，但在实际的性能问题中，网络协议栈中的统计信息，我们也必须关注。你可以用 netstat 或者 ss ，来查看套接字、网络栈、网络接口以及路由表的信息。\n\n我个人更推荐，使用 ss 来查询网络的连接信息，因为它比 netstat 提供了更好的性能（速度更快）。\n\n比如，你可以执行下面的命令，查询套接字信息：\n\n```bash\n# head -n 3 表示只显示前面 3 行\n# -l 表示只显示监听套接字\n# -n 表示显示数字地址和端口 (而不是名字)\n# -p 表示显示进程信息\n$ netstat -nlp | head -n 3\nActive Internet connections (only servers)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\ntcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      840/systemd-resolve \n# -l 表示只显示监听套接字\n# -t 表示只显示 TCP 套接字\n# -n 表示显示数字地址和端口 (而不是名字)\n# -p 表示显示进程信息\n$ ss -ltnp | head -n 3\nState    Recv-Q    Send-Q        Local Address:Port        Peer Address:Port\nLISTEN   0         128           127.0.0.53%lo:53               0.0.0.0:*        users:((\"systemd-resolve\",pid=840,fd=13))\nLISTEN   0         128                 0.0.0.0:22               0.0.0.0:*        users:((\"sshd\",pid=1459,fd=3))\n```\n\nnetstat 和 ss 的输出也是类似的，都展示了套接字的状态、接收队列、发送队列、本地地址、远端地址、进程 PID 和进程名称等。\n\n其中，接收队列（Recv-Q）和发送队列（Send-Q）需要你特别关注，它们通常应该是 0。当你发现它们不是 0 时，说明有网络包的堆积发生。当然还要注意，在不同套接字状态下，它们的含义不同。\n\n当套接字处于连接状态（Established）时，\n\n- Recv-Q 表示套接字缓冲还没有被应用程序取走的字节数（即接收队列长度）。\n- 而 Send-Q 表示还没有被远端主机确认的字节数（即发送队列长度）。\n\n当套接字处于监听状态（Listening）时，\n\n- Recv-Q 表示 syn backlog 的当前值。\n- 而 Send-Q 表示最大的 syn backlog 值。\n\n而 syn backlog 是 TCP 协议栈中的半连接队列长度，相应的也有一个全连接队列（accept queue），它们都是维护 TCP 状态的重要机制。\n\n顾名思义，所谓半连接，就是还没有完成 TCP 三次握手的连接，连接只进行了一半，而服务器收到了客户端的 SYN 包后，就会把这个连接放到半连接队列中，然后再向客户端发送 SYN+ACK 包。\n\n而全连接，则是指服务器收到了客户端的 ACK，完成了 TCP 三次握手，然后就会把这个连接挪到全连接队列中。这些全连接中的套接字，还需要再被 accept() 系统调用取走，这样，服务器就可以开始真正处理客户端的请求了。\n\n## **协议栈统计信息**\n\n类似的，使用 netstat 或 ss ，也可以查看协议栈的信息：\n\n```yaml\n$ netstat -s\n...\nTcp:\n    3244906 active connection openings\n    23143 passive connection openings\n    115732 failed connection attempts\n    2964 connection resets received\n    1 connections established\n    13025010 segments received\n    17606946 segments sent out\n    44438 segments retransmitted\n    42 bad segments received\n    5315 resets sent\n    InCsumErrors: 42\n... \n$ ss -s\nTotal: 186 (kernel 1446)\nTCP:   4 (estab 1, closed 0, orphaned 0, synrecv 0, timewait 0/0), ports 0 \nTransport Total     IP        IPv6\n*\t  1446      -         -\nRAW\t  2         1         1\nUDP\t  2         2         0\nTCP\t  4         3         1\n...\n```\n\n这些协议栈的统计信息都很直观。ss 只显示已经连接、关闭、孤儿套接字等简要统计，而 netstat 则提供的是更详细的网络协议栈信息。\n\n比如，上面 netstat 的输出示例，就展示了 TCP 协议的主动连接、被动连接、失败重试、发送和接收的分段数量等各种信息。\n\n## **网络吞吐和 PPS**\n\n接下来，我们再来看看，如何查看系统当前的网络吞吐量和 PPS。在这里，我推荐使用我们的老朋友 sar，在前面的 CPU、内存和 I/O 模块中，我们已经多次用到它。\n\n给 sar 增加 -n 参数就可以查看网络的统计信息，比如网络接口（DEV）、网络接口错误（EDEV）、TCP、UDP、ICMP 等等。执行下面的命令，你就可以得到网络接口统计信息：\n\n```bash\n# 数字 1 表示每隔 1 秒输出一组数据\n$ sar -n DEV 1\nLinux 4.15.0-1035-azure (ubuntu) \t01/06/19 \t_x86_64_\t(2 CPU) \n13:21:40        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil\n13:21:41         eth0     18.00     20.00      5.79      4.25      0.00      0.00      0.00      0.00\n13:21:41      docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n13:21:41           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n```\n\n这儿输出的指标比较多，我来简单解释下它们的含义。\n\n- rxpck/s 和 txpck/s 分别是接收和发送的 PPS，单位为包 / 秒。\n- rxkB/s 和 txkB/s 分别是接收和发送的吞吐量，单位是 KB/ 秒。\n- rxcmp/s 和 txcmp/s 分别是接收和发送的压缩数据包数，单位是包 / 秒。\n- %ifutil 是网络接口的使用率，即半双工模式下为 (rxkB/s+txkB/s)/Bandwidth，而全双工模式下为 max(rxkB/s, txkB/s)/Bandwidth。\n\n其中，Bandwidth 可以用 ethtool 来查询，它的单位通常是 Gb/s 或者 Mb/s，不过注意这里小写字母 b ，表示比特而不是字节。我们通常提到的千兆网卡、万兆网卡等，单位也都是比特。如下你可以看到，我的 eth0 网卡就是一个千兆网卡：\n\n```bash\n$ ethtool eth0 | grep Speed\n\tSpeed: 1000Mb/s\n```\n\n## **连通性和延时**\n\n最后，我们通常使用 ping ，来测试远程主机的连通性和延时，而这基于 ICMP 协议。比如，执行下面的命令，你就可以测试本机到 114.114.114.114 这个 IP 地址的连通性和延时：\n\n```python\n# -c3 表示发送三次 ICMP 包后停止\n$ ping -c3 114.114.114.114\nPING 114.114.114.114 (114.114.114.114) 56(84) bytes of data.\n64 bytes from 114.114.114.114: icmp_seq=1 ttl=54 time=244 ms\n64 bytes from 114.114.114.114: icmp_seq=2 ttl=47 time=244 ms\n64 bytes from 114.114.114.114: icmp_seq=3 ttl=67 time=244 ms \n--- 114.114.114.114 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2001ms\nrtt min/avg/max/mdev = 244.023/244.070/244.105/0.034 ms\n```\n\nping 的输出，可以分为两部分。\n\n- 第一部分，是每个 ICMP 请求的信息，包括 ICMP 序列号（icmp_seq）、TTL（生存时间，或者跳数）以及往返延时。\n- 第二部分，则是三次 ICMP 请求的汇总。\n\n比如上面的示例显示，发送了 3 个网络包，并且接收到 3 个响应，没有丢包发生，这说明测试主机到 114.114.114.114 是连通的；平均往返延时（RTT）是 244ms，也就是从发送 ICMP 开始，到接收到 114.114.114.114 回复的确认，总共经历 244ms。\n\n## 小结\n\n我们通常使用带宽、吞吐量、延时等指标，来衡量网络的性能；相应的，你可以用 ifconfig、netstat、ss、sar、ping 等工具，来查看这些网络的性能指标。\n\n在下一节中，我将以经典的 C10K 和 C100K 问题，带你进一步深入 Linux 网络的工作原理。\n\n## 思考\n\n最后，我想请你来聊聊，你理解的 Linux 网络性能。你常用什么指标来衡量网络的性能？又用什么思路分析相应性能问题呢？你可以结合今天学到的知识，提出自己的观点。\n\n欢迎在留言区和我讨论，也欢迎你把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 35 基础篇：C10K 和 C1000K 回顾\n\n你好，我是倪朋飞。\n\n前面内容，我们学习了 Linux 网络的基础原理以及性能观测方法。简单回顾一下，Linux 网络基于 TCP/IP 模型，构建了其网络协议栈，把繁杂的网络功能划分为应用层、传输层、网络层、网络接口层等四个不同的层次，既解决了网络环境中设备异构的问题，也解耦了网络协议的复杂性。\n\n基于 TCP/IP 模型，我们还梳理了 Linux 网络收发流程和相应的性能指标。在应用程序通过套接字接口发送或者接收网络包时，这些网络包都要经过协议栈的逐层处理。我们通常用带宽、吞吐、延迟、PPS 等来衡量网络性能。\n\n今天，我们主要来回顾下经典的 C10K 和 C1000K 问题，以更好理解 Linux 网络的工作原理，并进一步分析，如何做到单机支持 C10M。\n\n注意，C10K 和 C1000K 的首字母 C 是 Client 的缩写。C10K 就是单机同时处理 1 万个请求（并发连接 1 万）的问题，而 C1000K 也就是单机支持处理 100 万个请求（并发连接 100 万）的问题。\n\n## C10K\n\n[C10K 问题](http://www.kegel.com/c10k.html)最早由 Dan Kegel 在 1999 年提出。那时的服务器还只是 32 位系统，运行着 Linux 2.2 版本（后来又升级到了 2.4 和 2.6，而 2.6 才支持 x86_64），只配置了很少的内存（2GB）和千兆网卡。\n\n怎么在这样的系统中支持并发 1 万的请求呢？\n\n从资源上来说，对 2GB 内存和千兆网卡的服务器来说，同时处理 10000 个请求，只要每个请求处理占用不到 200KB（2GB/10000）的内存和 100Kbit （1000Mbit/10000）的网络带宽就可以。所以，物理资源是足够的，接下来自然是软件的问题，特别是网络的 I/O 模型问题。\n\n说到 I/O 的模型，我在文件系统的原理中，曾经介绍过文件 I/O，其实网络 I/O 模型也类似。在 C10K 以前，Linux 中网络处理都用同步阻塞的方式，也就是每个请求都分配一个进程或者线程。请求数只有 100 个时，这种方式自然没问题，但增加到 10000 个请求时，10000 个进程或线程的调度、上下文切换乃至它们占用的内存，都会成为瓶颈。\n\n既然每个请求分配一个线程的方式不合适，那么，为了支持 10000 个并发请求，这里就有两个问题需要我们解决。\n\n第一，怎样在一个线程内处理多个请求，也就是要在一个线程内响应多个网络 I/O。以前的同步阻塞方式下，一个线程只能处理一个请求，到这里不再适用，是不是可以用非阻塞 I/O 或者异步 I/O 来处理多个网络请求呢？\n\n第二，怎么更节省资源地处理客户请求，也就是要用更少的线程来服务这些请求。是不是可以继续用原来的 100 个或者更少的线程，来服务现在的 10000 个请求呢？\n\n当然，事实上，现在 C10K 的问题早就解决了，在继续学习下面的内容前，你可以先自己思考一下这两个问题。结合前面学过的内容，你是不是已经有了解决思路呢？\n\n### I/O 模型优化\n\n异步、非阻塞 I/O 的解决思路，你应该听说过，其实就是我们在网络编程中经常用到的 I/O 多路复用（I/O Multiplexing）。I/O 多路复用是什么意思呢？\n\n别急，详细了解前，我先来讲两种 I/O 事件通知的方式：水平触发和边缘触发，它们常用在套接字接口的文件描述符中。\n\n- 水平触发：只要文件描述符可以非阻塞地执行 I/O ，就会触发通知。也就是说，应用程序可以随时检查文件描述符的状态，然后再根据状态，进行 I/O 操作。\n- 边缘触发：只有在文件描述符的状态发生改变（也就是 I/O 请求达到）时，才发送一次通知。这时候，应用程序需要尽可能多地执行 I/O，直到无法继续读写，才可以停止。如果 I/O 没执行完，或者因为某种原因没来得及处理，那么这次通知也就丢失了。\n\n接下来，我们再回过头来看 I/O 多路复用的方法。这里其实有很多实现方法，我带你来逐个分析一下。\n\n**第一种，使用非阻塞 I/O 和水平触发通知，比如使用 select 或者 poll。**\n\n根据刚才水平触发的原理，select 和 poll 需要从文件描述符列表中，找出哪些可以执行 I/O ，然后进行真正的网络 I/O 读写。由于 I/O 是非阻塞的，一个线程中就可以同时监控一批套接字的文件描述符，这样就达到了单线程处理多请求的目的。\n\n所以，这种方式的最大优点，是对应用程序比较友好，它的 API 非常简单。\n\n但是，应用软件使用 select 和 poll 时，需要对这些文件描述符列表进行轮询，这样，请求数多的时候就会比较耗时。并且，select 和 poll 还有一些其他的限制。\n\nselect 使用固定长度的位相量，表示文件描述符的集合，因此会有最大描述符数量的限制。比如，在 32 位系统中，默认限制是 1024。并且，在 select 内部，检查套接字状态是用轮询的方法，再加上应用软件使用时的轮询，就变成了一个 O(n^2) 的关系。\n\n而 poll 改进了 select 的表示方法，换成了一个没有固定长度的数组，这样就没有了最大描述符数量的限制（当然还会受到系统文件描述符限制）。但应用程序在使用 poll 时，同样需要对文件描述符列表进行轮询，这样，处理耗时跟描述符数量就是 O(N) 的关系。\n\n除此之外，应用程序每次调用 select 和 poll 时，还需要把文件描述符的集合，从用户空间传入内核空间，由内核修改后，再传出到用户空间中。这一来一回的内核空间与用户空间切换，也增加了处理成本。\n\n有没有什么更好的方式来处理呢？答案自然是肯定的。\n\n**第二种，使用非阻塞 I/O 和边缘触发通知，比如 epoll**。\n\n既然 select 和 poll 有那么多的问题，就需要继续对其进行优化，而 epoll 就很好地解决了这些问题。\n\n- epoll 使用红黑树，在内核中管理文件描述符的集合，这样，就不需要应用程序在每次操作时都传入、传出这个集合。\n- epoll 使用事件驱动的机制，只关注有 I/O 事件发生的文件描述符，不需要轮询扫描整个集合。\n\n不过要注意，epoll 是在 Linux 2.6 中才新增的功能（2.4 虽然也有，但功能不完善）。由于边缘触发只在文件描述符可读或可写事件发生时才通知，那么应用程序就需要尽可能多地执行 I/O，并要处理更多的异常事件。\n\n**第三种，使用异步 I/O（Asynchronous I/O，简称为 AIO）**。在前面文件系统原理的内容中，我曾介绍过异步 I/O 与同步 I/O 的区别。异步 I/O 允许应用程序同时发起很多 I/O 操作，而不用等待这些操作完成。而在 I/O 完成后，系统会用事件通知（比如信号或者回调函数）的方式，告诉应用程序。这时，应用程序才会去查询 I/O 操作的结果。\n\n异步 I/O 也是到了 Linux 2.6 才支持的功能，并且在很长时间里都处于不完善的状态，比如 glibc 提供的异步 I/O 库，就一直被社区诟病。同时，由于异步 I/O 跟我们的直观逻辑不太一样，想要使用的话，一定要小心设计，其使用难度比较高。\n\n### 工作模型优化\n\n了解了 I/O 模型后，请求处理的优化就比较直观了。使用 I/O 多路复用后，就可以在一个进程或线程中处理多个请求，其中，又有下面两种不同的工作模型。\n\n**第一种，主进程 + 多个 worker 子进程，这也是最常用的一种模型**。这种方法的一个通用工作模式就是：\n\n- 主进程执行 bind() + listen() 后，创建多个子进程；\n- 然后，在每个子进程中，都通过 accept() 或 epoll_wait() ，来处理相同的套接字。\n\n比如，最常用的反向代理服务器 Nginx 就是这么工作的。它也是由主进程和多个 worker 进程组成。主进程主要用来初始化套接字，并管理子进程的生命周期；而 worker 进程，则负责实际的请求处理。我画了一张图来表示这个关系。\n\n![img](451a24fb8f096729ed6822b1615b097e.png)\n\n这里要注意，accept() 和 epoll_wait() 调用，还存在一个惊群的问题。换句话说，当网络 I/O 事件发生时，多个进程被同时唤醒，但实际上只有一个进程来响应这个事件，其他被唤醒的进程都会重新休眠。\n\n- 其中，accept() 的惊群问题，已经在 Linux 2.6 中解决了；\n- 而 epoll 的问题，到了 Linux 4.5 ，才通过 EPOLLEXCLUSIVE 解决。\n\n为了避免惊群问题， Nginx 在每个 worker 进程中，都增加一个了全局锁（accept_mutex）。这些 worker 进程需要首先竞争到锁，只有竞争到锁的进程，才会加入到 epoll 中，这样就确保只有一个 worker 子进程被唤醒。\n\n不过，根据前面 CPU 模块的学习，你应该还记得，进程的管理、调度、上下文切换的成本非常高。那为什么使用多进程模式的 Nginx ，却具有非常好的性能呢？\n\n这里最主要的一个原因就是，这些 worker 进程，实际上并不需要经常创建和销毁，而是在没任务时休眠，有任务时唤醒。只有在 worker 由于某些异常退出时，主进程才需要创建新的进程来代替它。\n\n当然，你也可以用线程代替进程：主线程负责套接字初始化和子线程状态的管理，而子线程则负责实际的请求处理。由于线程的调度和切换成本比较低，实际上你可以进一步把 epoll_wait() 都放到主线程中，保证每次事件都只唤醒主线程，而子线程只需要负责后续的请求处理。\n\n**第二种，监听到相同端口的多进程模型**。在这种方式下，所有的进程都监听相同的接口，并且开启 SO_REUSEPORT 选项，由内核负责将请求负载均衡到这些监听进程中去。这一过程如下图所示。\n\n![img](90df0945f6ce5c910ae361bf2b135bbd.png)\n\n由于内核确保了只有一个进程被唤醒，就不会出现惊群问题了。比如，Nginx 在 1.9.1 中就已经支持了这种模式。\n\n![img](af2e6c3a19a6e90098772b5df0605b38.png)（图片来自 [Nginx 官网博客](https://www.nginx.com/blog/socket-sharding-nginx-release-1-9-1/))\n\n不过要注意，想要使用 SO_REUSEPORT 选项，需要用 Linux 3.9 以上的版本才可以。\n\n## C1000K\n\n基于 I/O 多路复用和请求处理的优化，C10K 问题很容易就可以解决。不过，随着摩尔定律带来的服务器性能提升，以及互联网的普及，你并不难想到，新兴服务会对性能提出更高的要求。\n\n很快，原来的 C10K 已经不能满足需求，所以又有了 C100K 和 C1000K，也就是并发从原来的 1 万增加到 10 万、乃至 100 万。从 1 万到 10 万，其实还是基于 C10K 的这些理论，epoll 配合线程池，再加上 CPU、内存和网络接口的性能和容量提升。大部分情况下，C100K 很自然就可以达到。\n\n那么，再进一步，C1000K 是不是也可以很容易就实现呢？这其实没有那么简单了。\n\n首先从物理资源使用上来说，100 万个请求需要大量的系统资源。比如，\n\n- 假设每个请求需要 16KB 内存的话，那么总共就需要大约 15 GB 内存。\n- 而从带宽上来说，假设只有 20% 活跃连接，即使每个连接只需要 1KB/s 的吞吐量，总共也需要 1.6 Gb/s 的吞吐量。千兆网卡显然满足不了这么大的吞吐量，所以还需要配置万兆网卡，或者基于多网卡 Bonding 承载更大的吞吐量。\n\n其次，从软件资源上来说，大量的连接也会占用大量的软件资源，比如文件描述符的数量、连接状态的跟踪（CONNTRACK）、网络协议栈的缓存大小（比如套接字读写缓存、TCP 读写缓存）等等。\n\n最后，大量请求带来的中断处理，也会带来非常高的处理成本。这样，就需要多队列网卡、中断负载均衡、CPU 绑定、RPS/RFS（软中断负载均衡到多个 CPU 核上），以及将网络包的处理卸载（Offload）到网络设备（如 TSO/GSO、LRO/GRO、VXLAN OFFLOAD）等各种硬件和软件的优化。\n\nC1000K 的解决方法，本质上还是构建在 epoll 的非阻塞 I/O 模型上。只不过，除了 I/O 模型之外，还需要从应用程序到 Linux 内核、再到 CPU、内存和网络等各个层次的深度优化，特别是需要借助硬件，来卸载那些原来通过软件处理的大量功能。\n\n## C10M\n\n显然，人们对于性能的要求是无止境的。再进一步，有没有可能在单机中，同时处理 1000 万的请求呢？这也就是 [C10M](http://c10m.robertgraham.com/p/blog-page.html) 问题。\n\n实际上，在 C1000K 问题中，各种软件、硬件的优化很可能都已经做到头了。特别是当升级完硬件（比如足够多的内存、带宽足够大的网卡、更多的网络功能卸载等）后，你可能会发现，无论你怎么优化应用程序和内核中的各种网络参数，想实现 1000 万请求的并发，都是极其困难的。\n\n究其根本，还是 Linux 内核协议栈做了太多太繁重的工作。从网卡中断带来的硬中断处理程序开始，到软中断中的各层网络协议处理，最后再到应用程序，这个路径实在是太长了，就会导致网络包的处理优化，到了一定程度后，就无法更进一步了。\n\n要解决这个问题，最重要就是跳过内核协议栈的冗长路径，把网络包直接送到要处理的应用程序那里去。这里有两种常见的机制，DPDK 和 XDP。\n\n第一种机制，DPDK，是用户态网络的标准。它跳过内核协议栈，直接由用户态进程通过轮询的方式，来处理网络接收。\n\n![img](998fd2f52f0a48a910517ada9f2bb23a.png)（图片来自 https://blog.selectel.com/introduction-dpdk-architecture-principles/)\n\n说起轮询，你肯定会下意识认为它是低效的象征，但是进一步反问下自己，它的低效主要体现在哪里呢？是查询时间明显多于实际工作时间的情况下吧！那么，换个角度来想，如果每时每刻都有新的网络包需要处理，轮询的优势就很明显了。比如：\n\n- 在 PPS 非常高的场景中，查询时间比实际工作时间少了很多，绝大部分时间都在处理网络包；\n- 而跳过内核协议栈后，就省去了繁杂的硬中断、软中断再到 Linux 网络协议栈逐层处理的过程，应用程序可以针对应用的实际场景，有针对性地优化网络包的处理逻辑，而不需要关注所有的细节。\n\n此外，DPDK 还通过大页、CPU 绑定、内存对齐、流水线并发等多种机制，优化网络包的处理效率。\n\n第二种机制，XDP（eXpress Data Path），则是 Linux 内核提供的一种高性能网络数据路径。它允许网络包，在进入内核协议栈之前，就进行处理，也可以带来更高的性能。XDP 底层跟我们之前用到的 bcc-tools 一样，都是基于 Linux 内核的 eBPF 机制实现的。\n\nXDP 的原理如下图所示：\n\n![img](067ef9df4212cd4ede3cffcdac7001be.png)（图片来自 https://www.iovisor.org/technology/xdp)\n\n你可以看到，XDP 对内核的要求比较高，需要的是 Linux [4.8 以上版本](https://github.com/iovisor/bcc/blob/master/docs/kernel-versions.md#xdp)，并且它也不提供缓存队列。基于 XDP 的应用程序通常是专用的网络应用，常见的有 IDS（入侵检测系统）、DDoS 防御、 [cilium](https://github.com/cilium/cilium) 容器网络插件等。\n\n## 小结\n\n今天我带你回顾了经典的 C10K 问题，并进一步延伸到了 C1000K 和 C10M 问题。\n\nC10K 问题的根源，一方面在于系统有限的资源；另一方面，也是更重要的因素，是同步阻塞的 I/O 模型以及轮询的套接字接口，限制了网络事件的处理效率。Linux 2.6 中引入的 epoll ，完美解决了 C10K 的问题，现在的高性能网络方案都基于 epoll。\n\n从 C10K 到 C100K ，可能只需要增加系统的物理资源就可以满足；但从 C100K 到 C1000K ，就不仅仅是增加物理资源就能解决的问题了。这时，就需要多方面的优化工作了，从硬件的中断处理和网络功能卸载、到网络协议栈的文件描述符数量、连接状态跟踪、缓存队列等内核的优化，再到应用程序的工作模型优化，都是考虑的重点。\n\n再进一步，要实现 C10M ，就不只是增加物理资源，或者优化内核和应用程序可以解决的问题了。这时候，就需要用 XDP 的方式，在内核协议栈之前处理网络包；或者用 DPDK 直接跳过网络协议栈，在用户空间通过轮询的方式直接处理网络包。\n\n当然了，实际上，在大多数场景中，我们并不需要单机并发 1000 万的请求。通过调整系统架构，把这些请求分发到多台服务器中来处理，通常是更简单和更容易扩展的方案。\n\n## 思考\n\n最后，我想请你来聊聊，你所理解的 C10K 和 C1000K 问题。你碰到过哪些网络并发相关的性能瓶颈？你又是怎么样来分析它们的呢？你可以结合今天学到的网络知识，提出自己的观点。\n\n欢迎在留言区和我讨论，也欢迎你把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 36 套路篇：怎么评估系统的网络性能？\n\n你好，我是倪朋飞。\n\n上一节，我们回顾了经典的 C10K 和 C1000K 问题。简单回顾一下，C10K 是指如何单机同时处理 1 万个请求（并发连接 1 万）的问题，而 C1000K 则是单机支持处理 100 万个请求（并发连接 100 万）的问题。\n\nI/O 模型的优化，是解决 C10K 问题的最佳良方。Linux 2.6 中引入的 epoll，完美解决了 C10K 的问题，并一直沿用至今。今天的很多高性能网络方案，仍都基于 epoll。\n\n自然，随着互联网技术的普及，催生出更高的性能需求。从 C10K 到 C100K，我们只需要增加系统的物理资源，就可以满足要求；但从 C100K 到 C1000K ，光增加物理资源就不够了。\n\n这时，就要对系统的软硬件进行统一优化，从硬件的中断处理，到网络协议栈的文件描述符数量、连接状态跟踪、缓存队列，再到应用程序的工作模型等的整个网络链路，都需要深入优化。\n\n再进一步，要实现 C10M，就不是增加物理资源、调优内核和应用程序可以解决的问题了。这时内核中冗长的网络协议栈就成了最大的负担。\n\n- 需要用 XDP 方式，在内核协议栈之前，先处理网络包。\n- 或基于 DPDK ，直接跳过网络协议栈，在用户空间通过轮询的方式处理。\n\n其中，DPDK 是目前最主流的高性能网络方案，不过，这需要能支持 DPDK 的网卡配合使用。\n\n当然，实际上，在大多数场景中，我们并不需要单机并发 1000 万请求。通过调整系统架构，把请求分发到多台服务器中并行处理，才是更简单、扩展性更好的方案。\n\n不过，这种情况下，就需要我们评估系统的网络性能，以便考察系统的处理能力，并为容量规划提供基准数据。\n\n那么，到底该怎么评估网络的性能呢？今天，我就带你一起来看看这个问题。\n\n## 性能指标回顾\n\n在评估网络性能前，我们先来回顾一下，衡量网络性能的指标。在 Linux 网络基础篇中，我们曾经说到，带宽、吞吐量、延时、PPS 等，都是最常用的网络性能指标。还记得它们的具体含义吗？你可以先思考一下，再继续下面的内容。\n\n首先，**带宽**，表示链路的最大传输速率，单位是 b/s（比特 / 秒）。在你为服务器选购网卡时，带宽就是最核心的参考指标。常用的带宽有 1000M、10G、40G、100G 等。\n\n第二，**吞吐量**，表示没有丢包时的最大数据传输速率，单位通常为 b/s （比特 / 秒）或者 B/s（字节 / 秒）。吞吐量受带宽的限制，吞吐量 / 带宽也就是该网络链路的使用率。\n\n第三，**延时**，表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟。这个指标在不同场景中可能会有不同的含义。它可以表示建立连接需要的时间（比如 TCP 握手延时），或者一个数据包往返所需时间（比如 RTT）。\n\n最后，**PPS**，是 Packet Per Second（包 / 秒）的缩写，表示以网络包为单位的传输速率。PPS 通常用来评估网络的转发能力，而基于 Linux 服务器的转发，很容易受到网络包大小的影响（交换机通常不会受到太大影响，即交换机可以线性转发）。\n\n这四个指标中，带宽跟物理网卡配置是直接关联的。一般来说，网卡确定后，带宽也就确定了（当然，实际带宽会受限于整个网络链路中最小的那个模块）。\n\n另外，你可能在很多地方听说过“网络带宽测试”，这里测试的实际上不是带宽，而是是网络吞吐量。Linux 服务器的网络吞吐量一般会比带宽小，而对交换机等专门的网络设备来说，吞吐量一般会接近带宽。\n\n最后的 PPS，则是以网络包为单位的网络传输速率，通常用在需要大量转发的场景中。而对 TCP 或者 Web 服务来说，更多会用并发连接数和每秒请求数（QPS，Query per Second）等指标，它们更能反应实际应用程序的性能。\n\n## 网络基准测试\n\n熟悉了网络的性能指标后，接下来，我们再来看看，如何通过性能测试来确定这些指标的基准值。\n\n你可以先思考一个问题。我们已经知道，Linux 网络基于 TCP/IP 协议栈，而不同协议层的行为显然不同。那么，测试之前，你应该弄清楚，你要评估的网络性能，究竟属于协议栈的哪一层？换句话说，你的应用程序基于协议栈的哪一层呢？\n\n根据前面学过的 TCP/IP 协议栈的原理，这个问题应该不难回答。比如：\n\n- 基于 HTTP 或者 HTTPS 的 Web 应用程序，显然属于应用层，需要我们测试 HTTP/HTTPS 的性能；\n- 而对大多数游戏服务器来说，为了支持更大的同时在线人数，通常会基于 TCP 或 UDP ，与客户端进行交互，这时就需要我们测试 TCP/UDP 的性能；\n- 当然，还有一些场景，是把 Linux 作为一个软交换机或者路由器来用的。这种情况下，你更关注网络包的处理能力（即 PPS），重点关注网络层的转发性能。\n\n接下来，我就带你从下往上，了解不同协议层的网络性能测试方法。不过要注意，低层协议是其上的各层网络协议的基础。自然，低层协议的性能，也就决定了高层的网络性能。\n\n注意，以下所有的测试方法，都需要两台 Linux 虚拟机。其中一台，可以当作待测试的目标机器；而另一台，则可以当作正在运行网络服务的客户端，用来运行测试工具。\n\n## 各协议层的性能测试\n\n### 转发性能\n\n我们首先来看，网络接口层和网络层，它们主要负责网络包的封装、寻址、路由以及发送和接收。在这两个网络协议层中，每秒可处理的网络包数 PPS，就是最重要的性能指标。特别是 64B 小包的处理能力，值得我们特别关注。那么，如何来测试网络包的处理能力呢？\n\n说到网络包相关的测试，你可能会觉得陌生。不过，其实在专栏开头的 CPU 性能篇中，我们就接触过一个相关工具，也就是软中断案例中的 hping3。\n\n在那个案例中，hping3 作为一个 SYN 攻击的工具来使用。实际上， hping3 更多的用途，是作为一个测试网络包处理能力的性能工具。\n\n今天我再来介绍另一个更常用的工具，Linux 内核自带的高性能网络测试工具 [pktgen](https://wiki.linuxfoundation.org/networking/pktgen)。pktgen 支持丰富的自定义选项，方便你根据实际需要构造所需网络包，从而更准确地测试出目标服务器的性能。\n\n不过，在 Linux 系统中，你并不能直接找到 pktgen 命令。因为 pktgen 作为一个内核线程来运行，需要你加载 pktgen 内核模块后，再通过 /proc 文件系统来交互。下面就是 pktgen 启动的两个内核线程和 /proc 文件系统的交互文件：\n\n```shell\n$ modprobe pktgen\n$ ps -ef | grep pktgen | grep -v grep\nroot     26384     2  0 06:17 ?        00:00:00 [kpktgend_0]\nroot     26385     2  0 06:17 ?        00:00:00 [kpktgend_1]\n$ ls /proc/net/pktgen/\nkpktgend_0  kpktgend_1  pgctrl\n```\n\npktgen 在每个 CPU 上启动一个内核线程，并可以通过 /proc/net/pktgen 下面的同名文件，跟这些线程交互；而 pgctrl 则主要用来控制这次测试的开启和停止。\n\n\u003e 如果 modprobe 命令执行失败，说明你的内核没有配置 CONFIG_NET_PKTGEN 选项。这就需要你配置 pktgen 内核模块（即 CONFIG_NET_PKTGEN=m）后，重新编译内核，才可以使用。\n\n在使用 pktgen 测试网络性能时，需要先给每个内核线程 kpktgend_X 以及测试网卡，配置 pktgen 选项，然后再通过 pgctrl 启动测试。\n\n以发包测试为例，假设发包机器使用的网卡是 eth0，而目标机器的 IP 地址为 192.168.0.30，MAC 地址为 11:11:11:11:11:11。\n\n![img](f01dc79465e7f1d03b6fbdabbe4ad109.png)\n\n接下来，就是一个发包测试的示例。\n\n```bash\n# 定义一个工具函数，方便后面配置各种测试选项\nfunction pgset() {\n    local result\n    echo $1 \u003e $PGDEV \n    result=`cat $PGDEV | fgrep \"Result: OK:\"`\n    if [ \"$result\" = \"\" ]; then\n         cat $PGDEV | fgrep Result:\n    fi\n} \n# 为 0 号线程绑定 eth0 网卡\nPGDEV=/proc/net/pktgen/kpktgend_0\npgset \"rem_device_all\"   # 清空网卡绑定\npgset \"add_device eth0\"  # 添加 eth0 网卡 \n# 配置 eth0 网卡的测试选项\nPGDEV=/proc/net/pktgen/eth0\npgset \"count 1000000\"    # 总发包数量\npgset \"delay 5000\"       # 不同包之间的发送延迟 (单位纳秒)\npgset \"clone_skb 0\"      # SKB 包复制\npgset \"pkt_size 64\"      # 网络包大小\npgset \"dst 192.168.0.30\" # 目的 IP\npgset \"dst_mac 11:11:11:11:11:11\"  # 目的 MAC \n# 启动测试\nPGDEV=/proc/net/pktgen/pgctrl\npgset \"start\"\n```\n\n稍等一会儿，测试完成后，结果可以从 /proc 文件系统中获取。通过下面代码段中的内容，我们可以查看刚才的测试报告：\n\n```yaml\n$ cat /proc/net/pktgen/eth0\nParams: count 1000000  min_pkt_size: 64  max_pkt_size: 64\n     frags: 0  delay: 0  clone_skb: 0  ifname: eth0\n     flows: 0 flowlen: 0\n...\nCurrent:\n     pkts-sofar: 1000000  errors: 0\n     started: 1534853256071us  stopped: 1534861576098us idle: 70673us\n...\nResult: OK: 8320027(c8249354+d70673) usec, 1000000 (64byte,0frags)\n  120191pps 61Mb/sec (61537792bps) errors: 0\n```\n\n你可以看到，测试报告主要分为三个部分：\n\n- 第一部分的 Params 是测试选项；\n- 第二部分的 Current 是测试进度，其中， packts so far（pkts-sofar）表示已经发送了 100 万个包，也就表明测试已完成。\n- 第三部分的 Result 是测试结果，包含测试所用时间、网络包数量和分片、PPS、吞吐量以及错误数。\n\n根据上面的结果，我们发现，PPS 为 12 万，吞吐量为 61 Mb/s，没有发生错误。那么，12 万的 PPS 好不好呢？\n\n作为对比，你可以计算一下千兆交换机的 PPS。交换机可以达到线速（满负载时，无差错转发），它的 PPS 就是 1000Mbit 除以以太网帧的大小，即 1000Mbps/((64+20)*8bit) = 1.5 Mpps（其中 20B 为以太网帧的头部大小）。\n\n你看，即使是千兆交换机的 PPS，也可以达到 150 万 PPS，比我们测试得到的 12 万大多了。所以，看到这个数值你并不用担心，现在的多核服务器和万兆网卡已经很普遍了，稍做优化就可以达到数百万的 PPS。而且，如果你用了上节课讲到的 DPDK 或 XDP ，还能达到千万数量级。\n\n### TCP/UDP 性能\n\n掌握了 PPS 的测试方法，接下来，我们再来看 TCP 和 UDP 的性能测试方法。说到 TCP 和 UDP 的测试，我想你已经很熟悉了，甚至可能一下子就能想到相应的测试工具，比如 iperf 或者 netperf。\n\n特别是现在的云计算时代，在你刚拿到一批虚拟机时，首先要做的，应该就是用 iperf ，测试一下网络性能是否符合预期。\n\niperf 和 netperf 都是最常用的网络性能测试工具，测试 TCP 和 UDP 的吞吐量。它们都以客户端和服务器通信的方式，测试一段时间内的平均吞吐量。\n\n接下来，我们就以 iperf 为例，看一下 TCP 性能的测试方法。目前，iperf 的最新版本为 iperf3，你可以运行下面的命令来安装：\n\n```csharp\n# Ubuntu\napt-get install iperf3\n# CentOS\nyum install iperf3\n```\n\n然后，在目标机器上启动 iperf 服务端：\n\n```ruby\n# -s 表示启动服务端，-i 表示汇报间隔，-p 表示监听端口\n$ iperf3 -s -i 1 -p 10000\n```\n\n接着，在另一台机器上运行 iperf 客户端，运行测试：\n\n```shell\n# -c 表示启动客户端，192.168.0.30 为目标服务器的 IP\n# -b 表示目标带宽 (单位是 bits/s)\n# -t 表示测试时间\n# -P 表示并发数，-p 表示目标服务器监听端口\n$ iperf3 -c 192.168.0.30 -b 1G -t 15 -P 2 -p 10000\n```\n\n稍等一会儿（15 秒）测试结束后，回到目标服务器，查看 iperf 的报告：\n\n```css\n[ ID] Interval           Transfer     Bandwidth\n...\n[SUM]   0.00-15.04  sec  0.00 Bytes  0.00 bits/sec                  sender\n[SUM]   0.00-15.04  sec  1.51 GBytes   860 Mbits/sec                  receiver\n```\n\n最后的 SUM 行就是测试的汇总结果，包括测试时间、数据传输量以及带宽等。按照发送和接收，这一部分又分为了 sender 和 receiver 两行。\n\n从测试结果你可以看到，这台机器 TCP 接收的带宽（吞吐量）为 860 Mb/s， 跟目标的 1Gb/s 相比，还是有些差距的。\n\n### HTTP 性能\n\n从传输层再往上，到了应用层。有的应用程序，会直接基于 TCP 或 UDP 构建服务。当然，也有大量的应用，基于应用层的协议来构建服务，HTTP 就是最常用的一个应用层协议。比如，常用的 Apache、Nginx 等各种 Web 服务，都是基于 HTTP。\n\n要测试 HTTP 的性能，也有大量的工具可以使用，比如 ab、webbench 等，都是常用的 HTTP 压力测试工具。其中，ab 是 Apache 自带的 HTTP 压测工具，主要测试 HTTP 服务的每秒请求数、请求延迟、吞吐量以及请求延迟的分布情况等。\n\n运行下面的命令，你就可以安装 ab 工具：\n\n```ruby\n# Ubuntu\n$ apt-get install -y apache2-utils\n# CentOS\n$ yum install -y httpd-tools\n```\n\n接下来，在目标机器上，使用 Docker 启动一个 Nginx 服务，然后用 ab 来测试它的性能。首先，在目标机器上运行下面的命令：\n\n```ruby\n$ docker run -p 80:80 -itd nginx\n```\n\n而在另一台机器上，运行 ab 命令，测试 Nginx 的性能：\n\n```yaml\n# -c 表示并发请求数为 1000，-n 表示总的请求数为 10000\n$ ab -c 1000 -n 10000 http://192.168.0.30/\n...\nServer Software:        nginx/1.15.8\nServer Hostname:        192.168.0.30\nServer Port:            80 \n... \nRequests per second:    1078.54 [#/sec] (mean)\nTime per request:       927.183 [ms] (mean)\nTime per request:       0.927 [ms] (mean, across all concurrent requests)\nTransfer rate:          890.00 [Kbytes/sec] received \nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        0   27 152.1      1    1038\nProcessing:     9  207 843.0     22    9242\nWaiting:        8  207 843.0     22    9242\nTotal:         15  233 857.7     23    9268 \nPercentage of the requests served within a certain time (ms)\n  50%     23\n  66%     24\n  75%     24\n  80%     26\n  90%    274\n  95%   1195\n  98%   2335\n  99%   4663\n 100%   9268 (longest request)\n```\n\n可以看到，ab 的测试结果分为三个部分，分别是请求汇总、连接时间汇总还有请求延迟汇总。以上面的结果为例，我们具体来看。\n\n在请求汇总部分，你可以看到：\n\n- Requests per second 为 1074；\n- 每个请求的延迟（Time per request）分为两行，第一行的 927 ms 表示平均延迟，包括了线程运行的调度时间和网络请求响应时间，而下一行的 0.927ms ，则表示实际请求的响应时间；\n- Transfer rate 表示吞吐量（BPS）为 890 KB/s。\n\n连接时间汇总部分，则是分别展示了建立连接、请求、等待以及汇总等的各类时间，包括最小、最大、平均以及中值处理时间。\n\n最后的请求延迟汇总部分，则给出了不同时间段内处理请求的百分比，比如， 90% 的请求，都可以在 274ms 内完成。\n\n### 应用负载性能\n\n当你用 iperf 或者 ab 等测试工具，得到 TCP、HTTP 等的性能数据后，这些数据是否就能表示应用程序的实际性能呢？我想，你的答案应该是否定的。\n\n比如，你的应用程序基于 HTTP 协议，为最终用户提供一个 Web 服务。这时，使用 ab 工具，可以得到某个页面的访问性能，但这个结果跟用户的实际请求，很可能不一致。因为用户请求往往会附带着各种各种的负载（payload），而这些负载会影响 Web 应用程序内部的处理逻辑，从而影响最终性能。\n\n那么，为了得到应用程序的实际性能，就要求性能工具本身可以模拟用户的请求负载，而 iperf、ab 这类工具就无能为力了。幸运的是，我们还可以用 wrk、TCPCopy、Jmeter 或者 LoadRunner 等实现这个目标。\n\n以 [wrk](https://github.com/wg/wrk) 为例，它是一个 HTTP 性能测试工具，内置了 LuaJIT，方便你根据实际需求，生成所需的请求负载，或者自定义响应的处理方法。\n\nwrk 工具本身不提供 yum 或 apt 的安装方法，需要通过源码编译来安装。比如，你可以运行下面的命令，来编译和安装 wrk：\n\n```shell\n$ https://github.com/wg/wrk\n$ cd wrk\n$ apt-get install build-essential -y\n$ make\n$ sudo cp wrk /usr/local/bin/\n```\n\nwrk 的命令行参数比较简单。比如，我们可以用 wrk ，来重新测一下前面已经启动的 Nginx 的性能。\n\n```bash\n# -c 表示并发连接数 1000，-t 表示线程数为 2\n$ wrk -c 1000 -t 2 http://192.168.0.30/\nRunning 10s test @ http://192.168.0.30/\n  2 threads and 1000 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    65.83ms  174.06ms   1.99s    95.85%\n    Req/Sec     4.87k   628.73     6.78k    69.00%\n  96954 requests in 10.06s, 78.59MB read\n  Socket errors: connect 0, read 0, write 0, timeout 179\nRequests/sec:   9641.31\nTransfer/sec:      7.82MB\n```\n\n这里使用 2 个线程、并发 1000 连接，重新测试了 Nginx 的性能。你可以看到，每秒请求数为 9641，吞吐量为 7.82MB，平均延迟为 65ms，比前面 ab 的测试结果要好很多。\n\n这也说明，性能工具本身的性能，对性能测试也是至关重要的。不合适的性能工具，并不能准确测出应用程序的最佳性能。\n\n当然，wrk 最大的优势，是其内置的 LuaJIT，可以用来实现复杂场景的性能测试。wrk 在调用 Lua 脚本时，可以将 HTTP 请求分为三个阶段，即 setup、running、done，如下图所示：\n\n![img](d02b845aa308b7a38a5735f3db8d9682.png)\n\n（图片来自[网易云博客](https://sq.163yun.com/blog/article/200008406328934400)）\n\n比如，你可以在 setup 阶段，为请求设置认证参数（来自于 wrk 官方[示例](https://github.com/wg/wrk/blob/master/scripts/auth.lua)）：\n\n```lua\n-- example script that demonstrates response handling and\n-- retrieving an authentication token to set on all future\n-- requests \ntoken = nil\npath  = \"/authenticate\" \nrequest = function()\n   return wrk.format(\"GET\", path)\nend \nresponse = function(status, headers, body)\n   if not token and status == 200 then\n      token = headers[\"X-Token\"]\n      path  = \"/resource\"\n      wrk.headers[\"X-Token\"] = token\n   end\nend\n```\n\n而在执行测试时，通过 -s 选项，执行脚本的路径：\n\n```ruby\n$ wrk -c 1000 -t 2 -s auth.lua http://192.168.0.30/\n```\n\nwrk 需要你用 Lua 脚本，来构造请求负载。这对于大部分场景来说，可能已经足够了 。不过，它的缺点也正是，所有东西都需要代码来构造，并且工具本身不提供 GUI 环境。\n\n像 Jmeter 或者 LoadRunner（商业产品），则针对复杂场景提供了脚本录制、回放、GUI 等更丰富的功能，使用起来也更加方便。\n\n## 小结\n\n今天，我带你一起回顾了网络的性能指标，并学习了网络性能的评估方法。\n\n性能评估是优化网络性能的前提，只有在你发现网络性能瓶颈时，才需要进行网络性能优化。根据 TCP/IP 协议栈的原理，不同协议层关注的性能重点不完全一样，也就对应不同的性能测试方法。比如，\n\n- 在应用层，你可以使用 wrk、Jmeter 等模拟用户的负载，测试应用程序的每秒请求数、处理延迟、错误数等；\n- 而在传输层，则可以使用 iperf 等工具，测试 TCP 的吞吐情况；\n- 再向下，你还可以用 Linux 内核自带的 pktgen ，测试服务器的 PPS。\n\n由于低层协议是高层协议的基础。所以，一般情况下，我们需要从上到下，对每个协议层进行性能测试，然后根据性能测试的结果，结合 Linux 网络协议栈的原理，找出导致性能瓶颈的根源，进而优化网络性能。\n\n## 思考\n\n最后，我想请你来聊一聊。\n\n- 你是如何评估网络性能的？\n- 在评估网络性能时，你会从哪个协议层、选择哪些指标，作为性能测试最核心的目标？\n- 你又会用哪些工具，测试并分析网络的性能呢？\n\n你可以结合今天学到的网络知识，总结自己的思路。\n\n欢迎在留言区和我讨论，也欢迎你把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 37 案例篇：DNS 解析时快时慢，我该怎么办？\n\n你好，我是倪朋飞。\n\n上一节，我带你一起学习了网络性能的评估方法。简单回顾一下，Linux 网络基于 TCP/IP 协议栈构建，而在协议栈的不同层，我们所关注的网络性能也不尽相同。\n\n在应用层，我们关注的是应用程序的并发连接数、每秒请求数、处理延迟、错误数等，可以使用 wrk、Jmeter 等工具，模拟用户的负载，得到想要的测试结果。\n\n而在传输层，我们关注的是 TCP、UDP 等传输层协议的工作状况，比如 TCP 连接数、 TCP 重传、TCP 错误数等。此时，你可以使用 iperf、netperf 等，来测试 TCP 或 UDP 的性能。\n\n再向下到网络层，我们关注的则是网络包的处理能力，即 PPS。Linux 内核自带的 pktgen，就可以帮你测试这个指标。\n\n由于低层协议是高层协议的基础，所以一般情况下，我们所说的网络优化，实际上包含了整个网络协议栈的所有层的优化。当然，性能要求不同，具体需要优化的位置和目标并不完全相同。\n\n前面在评估网络性能（比如 HTTP 性能）时，我们在测试工具中指定了网络服务的 IP 地址。IP 地址是 TCP/IP 协议中，用来确定通信双方的一个重要标识。每个 IP 地址又包括了主机号和网络号两部分。相同网络号的主机组成一个子网；不同子网再通过路由器连接，组成一个庞大的网络。\n\n然而，IP 地址虽然方便了机器的通信，却给访问这些服务的人们，带来了很重的记忆负担。我相信，没几个人能记得住 Github 所在的 IP 地址，因为这串字符，对人脑来说并没有什么含义，不符合我们的记忆逻辑。\n\n不过，这并不妨碍我们经常使用这个服务。为什么呢？当然是因为还有更简单、方便的方式。我们可以通过域名 github.com 访问，而不是必须依靠具体的 IP 地址，这其实正是域名系统 DNS 的由来。\n\nDNS（Domain Name System），即域名系统，是互联网中最基础的一项服务，主要提供域名和 IP 地址之间映射关系的查询服务。\n\nDNS 不仅方便了人们访问不同的互联网服务，更为很多应用提供了，动态服务发现和全局负载均衡（Global Server Load Balance，GSLB）的机制。这样，DNS 就可以选择离用户最近的 IP 来提供服务。即使后端服务的 IP 地址发生变化，用户依然可以用相同域名来访问。\n\nDNS 显然是我们工作中基础而重要的一个环节。那么，DNS 出现问题时，又该如何分析和排查呢？今天，我就带你一起来看看这个问题。\n\n## 域名与 DNS 解析\n\n域名我们本身都比较熟悉，由一串用点分割开的字符组成，被用作互联网中的某一台或某一组计算机的名称，目的就是为了方便识别，互联网中提供各种服务的主机位置。\n\n要注意，域名是全球唯一的，需要通过专门的域名注册商才可以申请注册。为了组织全球互联网中的众多计算机，域名同样用点来分开，形成一个分层的结构。而每个被点分割开的字符串，就构成了域名中的一个层级，并且位置越靠后，层级越高。\n\n我们以极客时间的网站 time.geekbang.org 为例，来理解域名的含义。这个字符串中，最后面的 org 是顶级域名，中间的 geekbang 是二级域名，而最左边的 time 则是三级域名。\n\n如下图所示，注意点（.）是所有域名的根，也就是说所有域名都以点作为后缀，也可以理解为，在域名解析的过程中，所有域名都以点结束。\n\n![img](1b509317968f3f73810ac1d313ced982.png)\n\n通过理解这几个概念，你可以看出，域名主要是为了方便让人记住，而 IP 地址是机器间的通信的真正机制。把域名转换为 IP 地址的服务，也就是我们开头提到的，域名解析服务（DNS），而对应的服务器就是域名服务器，网络协议则是 DNS 协议。\n\n这里注意，DNS 协议在 TCP/IP 栈中属于应用层，不过实际传输还是基于 UDP 或者 TCP 协议（UDP 居多） ，并且域名服务器一般监听在端口 53 上。\n\n既然域名以分层的结构进行管理，相对应的，域名解析其实也是用递归的方式（从顶级开始，以此类推），发送给每个层级的域名服务器，直到得到解析结果。\n\n不过不要担心，递归查询的过程并不需要你亲自操作，DNS 服务器会替你完成，你要做的，只是预先配置一个可用的 DNS 服务器就可以了。\n\n当然，我们知道，通常来说，每级 DNS 服务器，都会有最近解析记录的缓存。当缓存命中时，直接用缓存中的记录应答就可以了。如果缓存过期或者不存在，才需要用刚刚提到的递归方式查询。\n\n所以，系统管理员在配置 Linux 系统的网络时，除了需要配置 IP 地址，还需要给它配置 DNS 服务器，这样它才可以通过域名来访问外部服务。\n\n比如，我的系统配置的就是 114.114.114.114 这个域名服务器。你可以执行下面的命令，来查询你的系统配置：\n\n```shell\n$ cat /etc/resolv.conf\nnameserver 114.114.114.114\n```\n\n另外，DNS 服务通过资源记录的方式，来管理所有数据，它支持 A、CNAME、MX、NS、PTR 等多种类型的记录。比如：\n\n- A 记录，用来把域名转换成 IP 地址；\n- CNAME 记录，用来创建别名；\n- 而 NS 记录，则表示该域名对应的域名服务器地址。\n\n简单来说，当我们访问某个网址时，就需要通过 DNS 的 A 记录，查询该域名对应的 IP 地址，然后再通过该 IP 来访问 Web 服务。\n\n比如，还是以极客时间的网站 time.geekbang.org 为例，执行下面的 nslookup 命令，就可以查询到这个域名的 A 记录，可以看到，它的 IP 地址是 39.106.233.176：\n\n```yaml\n$ nslookup time.geekbang.org\n# 域名服务器及端口信息\nServer:\t\t114.114.114.114\nAddress:\t114.114.114.114#53 \n# 非权威查询结果\nNon-authoritative answer:\nName:\ttime.geekbang.org\nAddress: 39.106.233.17\n```\n\n这里要注意，由于 114.114.114.114 并不是直接管理 time.geekbang.org 的域名服务器，所以查询结果是非权威的。使用上面的命令，你只能得到 114.114.114.114 查询的结果。\n\n前面还提到了，如果没有命中缓存，DNS 查询实际上是一个递归过程，那有没有方法可以知道整个递归查询的执行呢？\n\n其实除了 nslookup，另外一个常用的 DNS 解析工具 dig ，就提供了 trace 功能，可以展示递归查询的整个过程。比如你可以执行下面的命令，得到查询结果：\n\n```python\n# +trace 表示开启跟踪查询\n# +nodnssec 表示禁止 DNS 安全扩展\n$ dig +trace +nodnssec time.geekbang.org \n; \u003c\u003c\u003e\u003e DiG 9.11.3-1ubuntu1.3-Ubuntu \u003c\u003c\u003e\u003e +trace +nodnssec time.geekbang.org\n;; global options: +cmd\n.\t\t\t322086\tIN\tNS\tm.root-servers.net.\n.\t\t\t322086\tIN\tNS\ta.root-servers.net.\n.\t\t\t322086\tIN\tNS\ti.root-servers.net.\n.\t\t\t322086\tIN\tNS\td.root-servers.net.\n.\t\t\t322086\tIN\tNS\tg.root-servers.net.\n.\t\t\t322086\tIN\tNS\tl.root-servers.net.\n.\t\t\t322086\tIN\tNS\tc.root-servers.net.\n.\t\t\t322086\tIN\tNS\tb.root-servers.net.\n.\t\t\t322086\tIN\tNS\th.root-servers.net.\n.\t\t\t322086\tIN\tNS\te.root-servers.net.\n.\t\t\t322086\tIN\tNS\tk.root-servers.net.\n.\t\t\t322086\tIN\tNS\tj.root-servers.net.\n.\t\t\t322086\tIN\tNS\tf.root-servers.net.\n;; Received 239 bytes from 114.114.114.114#53(114.114.114.114) in 1340 ms \norg.\t\t\t172800\tIN\tNS\ta0.org.afilias-nst.info.\norg.\t\t\t172800\tIN\tNS\ta2.org.afilias-nst.info.\norg.\t\t\t172800\tIN\tNS\tb0.org.afilias-nst.org.\norg.\t\t\t172800\tIN\tNS\tb2.org.afilias-nst.org.\norg.\t\t\t172800\tIN\tNS\tc0.org.afilias-nst.info.\norg.\t\t\t172800\tIN\tNS\td0.org.afilias-nst.org.\n;; Received 448 bytes from 198.97.190.53#53(h.root-servers.net) in 708 ms \ngeekbang.org.\t\t86400\tIN\tNS\tdns9.hichina.com.\ngeekbang.org.\t\t86400\tIN\tNS\tdns10.hichina.com.\n;; Received 96 bytes from 199.19.54.1#53(b0.org.afilias-nst.org) in 1833 ms \ntime.geekbang.org.\t600\tIN\tA\t39.106.233.176\n;; Received 62 bytes from 140.205.41.16#53(dns10.hichina.com) in 4 ms\n```\n\ndig trace 的输出，主要包括四部分。\n\n- 第一部分，是从 114.114.114.114 查到的一些根域名服务器（.）的 NS 记录。\n- 第二部分，是从 NS 记录结果中选一个（h.root-servers.net），并查询顶级域名 org. 的 NS 记录。\n- 第三部分，是从 org. 的 NS 记录中选择一个（b0.org.afilias-nst.org），并查询二级域名 geekbang.org. 的 NS 服务器。\n- 最后一部分，就是从 geekbang.org. 的 NS 服务器（dns10.hichina.com）查询最终主机 time.geekbang.org. 的 A 记录。\n\n这个输出里展示的各级域名的 NS 记录，其实就是各级域名服务器的地址，可以让你更清楚 DNS 解析的过程。 为了帮你更直观理解递归查询，我把这个过程整理成了一张流程图，你可以保存下来理解。\n\n![img](5ffda41ec62fc3c9e0de3fa3443c9cd3.png)\n\n当然，不仅仅是发布到互联网的服务需要域名，很多时候，我们也希望能对局域网内部的主机进行域名解析（即内网域名，大多数情况下为主机名）。Linux 也支持这种行为。\n\n所以，你可以把主机名和 IP 地址的映射关系，写入本机的 /etc/hosts 文件中。这样，指定的主机名就可以在本地直接找到目标 IP。比如，你可以执行下面的命令来操作：\n\n```shell\n$ cat /etc/hosts\n127.0.0.1   localhost localhost.localdomain\n::1         localhost6 localhost6.localdomain6\n192.168.0.100 domain.com\n```\n\n或者，你还可以在内网中，搭建自定义的 DNS 服务器，专门用来解析内网中的域名。而内网 DNS 服务器，一般还会设置一个或多个上游 DNS 服务器，用来解析外网的域名。\n\n清楚域名与 DNS 解析的基本原理后，接下来，我就带你一起来看几个案例，实战分析 DNS 解析出现问题时，该如何定位。\n\n## 案例准备\n\n本次案例还是基于 Ubuntu 18.04，同样适用于其他的 Linux 系统。我使用的案例环境如下所示：\n\n- 机器配置：2 CPU，8GB 内存。\n- 预先安装 docker 等工具，如 apt install docker.io。\n\n你可以先打开一个终端，SSH 登录到 Ubuntu 机器中，然后执行下面的命令，拉取案例中使用的 Docker 镜像：\n\n```bash\n$ docker pull feisky/dnsutils\nUsing default tag: latest\n...\nStatus: Downloaded newer image for feisky/dnsutils:latest\n```\n\n然后，运行下面的命令，查看主机当前配置的 DNS 服务器：\n\n```shell\n$ cat /etc/resolv.conf\nnameserver 114.114.114.114\n```\n\n可以看到，我这台主机配置的 DNS 服务器是 114.114.114.114。\n\n到这里，准备工作就完成了。接下来，我们正式进入操作环节。\n\n## 案例分析\n\n### 案例 1：DNS 解析失败\n\n首先，执行下面的命令，进入今天的第一个案例。如果一切正常，你将可以看到下面这个输出：\n\n```ruby\n# 进入案例环境的 SHELL 终端中\n$ docker run -it --rm -v $(mktemp):/etc/resolv.conf feisky/dnsutils bash\nroot@7e9ed6ed4974:/#\n```\n\n注意，这儿 root 后面的 7e9ed6ed4974，是 Docker 生成容器的 ID 前缀，你的环境中很可能是不同的 ID，所以直接忽略这一项就可以了。\n\n\u003e 注意：下面的代码段中， /# 开头的命令都表示在容器内部运行的命令。\n\n接着，继续在容器终端中，执行 DNS 查询命令，我们还是查询 time.geekbang.org 的 IP 地址：\n\n```csharp\n/# nslookup time.geekbang.org\n;; connection timed out; no servers could be reached\n```\n\n你可以发现，这个命令阻塞很久后，还是失败了，报了 connection timed out 和 no servers could be reached 错误。\n\n看到这里，估计你的第一反应就是网络不通了，到底是不是这样呢？我们用 ping 工具检查试试。执行下面的命令，就可以测试本地到 114.114.114.114 的连通性：\n\n```python\n/# ping -c3 114.114.114.114\nPING 114.114.114.114 (114.114.114.114): 56 data bytes\n64 bytes from 114.114.114.114: icmp_seq=0 ttl=56 time=31.116 ms\n64 bytes from 114.114.114.114: icmp_seq=1 ttl=60 time=31.245 ms\n64 bytes from 114.114.114.114: icmp_seq=2 ttl=68 time=31.128 ms\n--- 114.114.114.114 ping statistics ---\n3 packets transmitted, 3 packets received, 0% packet loss\nround-trip min/avg/max/stddev = 31.116/31.163/31.245/0.058 ms\n```\n\n这个输出中，你可以看到网络是通的。那要怎么知道 nslookup 命令失败的原因呢？这里其实有很多方法，最简单的一种，就是开启 nslookup 的调试输出，查看查询过程中的详细步骤，排查其中是否有异常。\n\n比如，我们可以继续在容器终端中，执行下面的命令：\n\n```css\n/# nslookup -debug time.geekbang.org\n;; Connection to 127.0.0.1#53(127.0.0.1) for time.geekbang.org failed: connection refused.\n;; Connection to ::1#53(::1) for time.geekbang.org failed: address not available.\n```\n\n从这次的输出可以看到，nslookup 连接环回地址（127.0.0.1 和 ::1）的 53 端口失败。这里就有问题了，为什么会去连接环回地址，而不是我们的先前看到的 114.114.114.114 呢？\n\n你可能已经想到了症结所在——有可能是因为容器中没有配置 DNS 服务器。那我们就执行下面的命令确认一下：\n\n```shell\n/# cat /etc/resolv.conf\n```\n\n果然，这个命令没有任何输出，说明容器里的确没有配置 DNS 服务器。到这一步，很自然的，我们就知道了解决方法。在 /etc/resolv.conf 文件中，配置上 DNS 服务器就可以了。\n\n你可以执行下面的命令，在配置好 DNS 服务器后，重新执行 nslookup 命令。自然，我们现在发现，这次可以正常解析了：\n\n```makefile\n/# echo \"nameserver 114.114.114.114\" \u003e /etc/resolv.conf\n/# nslookup time.geekbang.org\nServer:\t\t114.114.114.114\nAddress:\t114.114.114.114#53 \nNon-authoritative answer:\nName:\ttime.geekbang.org\nAddress: 39.106.233.176\n```\n\n到这里，第一个案例就轻松解决了。最后，在终端中执行 exit 命令退出容器，Docker 就会自动清理刚才运行的容器。\n\n### 案例 2：DNS 解析不稳定\n\n接下来，我们再来看第二个案例。执行下面的命令，启动一个新的容器，并进入它的终端中：\n\n```ruby\n$ docker run -it --rm --cap-add=NET_ADMIN --dns 8.8.8.8 feisky/dnsutils bash\nroot@0cd3ee0c8ecb:/#\n```\n\n然后，跟上一个案例一样，还是运行 nslookup 命令，解析 time.geekbang.org 的 IP 地址。不过，这次要加一个 time 命令，输出解析所用时间。如果一切正常，你可能会看到如下输出：\n\n```yaml\n/# time nslookup time.geekbang.org\nServer:\t\t8.8.8.8\nAddress:\t8.8.8.8#53 \nNon-authoritative answer:\nName:\ttime.geekbang.org\nAddress: 39.106.233.176 \nreal\t0m10.349s\nuser\t0m0.004s\nsys\t0m0.0\n```\n\n可以看到，这次解析非常慢，居然用了 10 秒。如果你多次运行上面的 nslookup 命令，可能偶尔还会碰到下面这种错误：\n\n```sql\n/# time nslookup time.geekbang.org\n;; connection timed out; no servers could be reached \nreal\t0m15.011s\nuser\t0m0.006s\nsys\t0m0.006s\n```\n\n换句话说，跟上一个案例类似，也会出现解析失败的情况。综合来看，现在 DNS 解析的结果不但比较慢，而且还会发生超时失败的情况。\n\n这是为什么呢？碰到这种问题该怎么处理呢？\n\n其实，根据前面的讲解，我们知道，DNS 解析，说白了就是客户端与服务器交互的过程，并且这个过程还使用了 UDP 协议。\n\n那么，对于整个流程来说，解析结果不稳定，就有很多种可能的情况了。比方说：\n\n- DNS 服务器本身有问题，响应慢并且不稳定；\n- 或者是，客户端到 DNS 服务器的网络延迟比较大；\n- 再或者，DNS 请求或者响应包，在某些情况下被链路中的网络设备弄丢了。\n\n根据上面 nslookup 的输出，你可以看到，现在客户端连接的 DNS 是 8.8.8.8，这是 Google 提供的 DNS 服务。对 Google 我们还是比较放心的，DNS 服务器出问题的概率应该比较小。基本排除了 DNS 服务器的问题，那是不是第二种可能，本机到 DNS 服务器的延迟比较大呢？\n\n前面讲过，ping 可以用来测试服务器的延迟。比如，你可以运行下面的命令：\n\n```python\n/# ping -c3 8.8.8.8\nPING 8.8.8.8 (8.8.8.8): 56 data bytes\n64 bytes from 8.8.8.8: icmp_seq=0 ttl=31 time=137.637 ms\n64 bytes from 8.8.8.8: icmp_seq=1 ttl=31 time=144.743 ms\n64 bytes from 8.8.8.8: icmp_seq=2 ttl=31 time=138.576 ms\n--- 8.8.8.8 ping statistics ---\n3 packets transmitted, 3 packets received, 0% packet loss\nround-trip min/avg/max/stddev = 137.637/140.319/144.743/3.152 ms\n```\n\n从 ping 的输出可以看到，这里的延迟已经达到了 140ms，这也就可以解释，为什么解析这么慢了。实际上，如果你多次运行上面的 ping 测试，还会看到偶尔出现的丢包现象。\n\n```python\n$ ping -c3 8.8.8.8\nPING 8.8.8.8 (8.8.8.8): 56 data bytes\n64 bytes from 8.8.8.8: icmp_seq=0 ttl=30 time=134.032 ms\n64 bytes from 8.8.8.8: icmp_seq=1 ttl=30 time=431.458 ms\n--- 8.8.8.8 ping statistics ---\n3 packets transmitted, 2 packets received, 33% packet loss\nround-trip min/avg/max/stddev = 134.032/282.745/431.458/148.713 ms\n```\n\n这也进一步解释了，为什么 nslookup 偶尔会失败，正是网络链路中的丢包导致的。\n\n碰到这种问题该怎么办呢？显然，既然延迟太大，那就换一个延迟更小的 DNS 服务器，比如电信提供的 114.114.114.114。\n\n配置之前，我们可以先用 ping 测试看看，它的延迟是不是真的比 8.8.8.8 好。执行下面的命令，你就可以看到，它的延迟只有 31ms：\n\n```python\n/# ping -c3 114.114.114.114\nPING 114.114.114.114 (114.114.114.114): 56 data bytes\n64 bytes from 114.114.114.114: icmp_seq=0 ttl=67 time=31.130 ms\n64 bytes from 114.114.114.114: icmp_seq=1 ttl=56 time=31.302 ms\n64 bytes from 114.114.114.114: icmp_seq=2 ttl=56 time=31.250 ms\n--- 114.114.114.114 ping statistics ---\n3 packets transmitted, 3 packets received, 0% packet loss\nround-trip min/avg/max/stddev = 31.130/31.227/31.302/0.072 ms\n```\n\n这个结果表明，延迟的确小了很多。我们继续执行下面的命令，更换 DNS 服务器，然后，再次执行 nslookup 解析命令：\n\n```makefile\n/# echo nameserver 114.114.114.114 \u003e /etc/resolv.conf\n/# time nslookup time.geekbang.org\nServer:\t\t114.114.114.114\nAddress:\t114.114.114.114#53 \nNon-authoritative answer:\nName:\ttime.geekbang.org\nAddress: 39.106.233.176 \nreal    0m0.064s\nuser    0m0.007s\nsys     0m0.006s\n```\n\n你可以发现，现在只需要 64ms 就可以完成解析，比刚才的 10s 要好很多。\n\n到这里，问题看似就解决了。不过，如果你多次运行 nslookup 命令，估计就不是每次都有好结果了。比如，在我的机器中，就经常需要 1s 甚至更多的时间。\n\n```yaml\n/# time nslookup time.geekbang.org\nServer:\t\t114.114.114.114\nAddress:\t114.114.114.114#53 \nNon-authoritative answer:\nName:\ttime.geekbang.org\nAddress: 39.106.233.176 \nreal\t0m1.045s\nuser\t0m0.007s\nsys\t0m0.004s\n```\n\n1s 的 DNS 解析时间还是太长了，对很多应用来说也是不可接受的。那么，该怎么解决这个问题呢？我想你一定已经想到了，那就是使用 DNS 缓存。这样，只有第一次查询时需要去 DNS 服务器请求，以后的查询，只要 DNS 记录不过期，使用缓存中的记录就可以了。\n\n不过要注意，我们使用的主流 Linux 发行版，除了最新版本的 Ubuntu （如 18.04 或者更新版本）外，其他版本并没有自动配置 DNS 缓存。\n\n所以，想要为系统开启 DNS 缓存，就需要你做额外的配置。比如，最简单的方法，就是使用 dnsmasq。\n\ndnsmasq 是最常用的 DNS 缓存服务之一，还经常作为 DHCP 服务来使用。它的安装和配置都比较简单，性能也可以满足绝大多数应用程序对 DNS 缓存的需求。\n\n我们继续在刚才的容器终端中，执行下面的命令，就可以启动 dnsmasq：\n\n```perl\n/# /etc/init.d/dnsmasq start\n * Starting DNS forwarder and DHCP server dnsmasq                    [ OK ]\n```\n\n然后，修改 /etc/resolv.conf，将 DNS 服务器改为 dnsmasq 的监听地址，这儿是 127.0.0.1。接着，重新执行多次 nslookup 命令：\n\n```yaml\n/# echo nameserver 127.0.0.1 \u003e /etc/resolv.conf\n/# time nslookup time.geekbang.org\nServer:\t\t127.0.0.1\nAddress:\t127.0.0.1#53 \nNon-authoritative answer:\nName:\ttime.geekbang.org\nAddress: 39.106.233.176 \nreal\t0m0.492s\nuser\t0m0.007s\nsys\t0m0.006s \n/# time nslookup time.geekbang.org\nServer:\t\t127.0.0.1\nAddress:\t127.0.0.1#53 \nNon-authoritative answer:\nName:\ttime.geekbang.org\nAddress: 39.106.233.176 \nreal\t0m0.011s\nuser\t0m0.008s\nsys\t0m0.003s\n```\n\n现在我们可以看到，只有第一次的解析很慢，需要 0.5s，以后的每次解析都很快，只需要 11ms。并且，后面每次 DNS 解析需要的时间也都很稳定。\n\n案例的最后，还是别忘了执行 exit，退出容器终端，Docker 会自动清理案例容器。\n\n## 小结\n\n今天，我带你一起学习了 DNS 的基本原理，并通过几个案例，带你一起掌握了，发现 DNS 解析问题时的分析和解决思路。\n\nDNS 是互联网中最基础的一项服务，提供了域名和 IP 地址间映射关系的查询服务。很多应用程序在最初开发时，并没考虑 DNS 解析的问题，后续出现问题后，排查好几天才能发现，其实是 DNS 解析慢导致的。\n\n试想，假如一个 Web 服务的接口，每次都需要 1s 时间来等待 DNS 解析，那么，无论你怎么优化应用程序的内在逻辑，对用户来说，这个接口的响应都太慢，因为响应时间总是会大于 1 秒的。\n\n所以，在应用程序的开发过程中，我们必须考虑到 DNS 解析可能带来的性能问题，掌握常见的优化方法。这里，我总结了几种常见的 DNS 优化方法。\n\n- 对 DNS 解析的结果进行缓存。缓存是最有效的方法，但要注意，一旦缓存过期，还是要去 DNS 服务器重新获取新记录。不过，这对大部分应用程序来说都是可接受的。\n- 对 DNS 解析的结果进行预取。这是浏览器等 Web 应用中最常用的方法，也就是说，不等用户点击页面上的超链接，浏览器就会在后台自动解析域名，并把结果缓存起来。\n- 使用 HTTPDNS 取代常规的 DNS 解析。这是很多移动应用会选择的方法，特别是如今域名劫持普遍存在，使用 HTTP 协议绕过链路中的 DNS 服务器，就可以避免域名劫持的问题。\n- 基于 DNS 的全局负载均衡（GSLB）。这不仅为服务提供了负载均衡和高可用的功能，还可以根据用户的位置，返回距离最近的 IP 地址。\n\n## 思考\n\n最后，我想请你来聊一聊，你所碰到的 DNS 问题。你都碰到过哪些类型的 DNS 问题？你是通过哪些方法来排查的，又通过哪些方法解决的呢？你可以结合今天学到的知识，总结自己的思路。\n\n欢迎在留言区和我讨论，也欢迎你把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# 38 案例篇：怎么使用 tcpdump 和 Wireshark 分析网络流量？\n\n你好，我是倪朋飞。\n\n上一节，我们学习了 DNS 性能问题的分析和优化方法。简单回顾一下，DNS 可以提供域名和 IP 地址的映射关系，也是一种常用的全局负载均衡（GSLB）实现方法。\n\n通常，需要暴露到公网的服务，都会绑定一个域名，既方便了人们记忆，也避免了后台服务 IP 地址的变更影响到用户。\n\n不过要注意，DNS 解析受到各种网络状况的影响，性能可能不稳定。比如公网延迟增大，缓存过期导致要重新去上游服务器请求，或者流量高峰时 DNS 服务器性能不足等，都会导致 DNS 响应的延迟增大。\n\n此时，可以借助 nslookup 或者 dig 的调试功能，分析 DNS 的解析过程，再配合 ping 等工具调试 DNS 服务器的延迟，从而定位出性能瓶颈。通常，你可以用缓存、预取、HTTPDNS 等方法，优化 DNS 的性能。\n\n上一节我们用到的 ping，是一个最常用的测试服务延迟的工具。很多情况下，ping 可以帮我们定位出延迟问题，不过有时候， ping 本身也会出现意想不到的问题。这时，就需要我们抓取 ping 命令执行时收发的网络包，然后分析这些网络包，进而找出问题根源。\n\ntcpdump 和 Wireshark 就是最常用的网络抓包和分析工具，更是分析网络性能必不可少的利器。\n\n- tcpdump 仅支持命令行格式使用，常用在服务器中抓取和分析网络包。\n- Wireshark 除了可以抓包外，还提供了强大的图形界面和汇总分析工具，在分析复杂的网络情景时，尤为简单和实用。\n\n因而，在实际分析网络性能时，先用 tcpdump 抓包，后用 Wireshark 分析，也是一种常用的方法。\n\n今天，我就带你一起看看，怎么使用 tcpdump 和 Wireshark ，来分析网络的性能问题。\n\n## 案例准备\n\n本次案例还是基于 Ubuntu 18.04，同样适用于其他的 Linux 系统。我使用的案例环境是这样的：\n\n- 机器配置：2 CPU，8GB 内存。\n- 预先安装 tcpdump、Wireshark 等工具，如：\n\n```csharp\n# Ubuntu\napt-get install tcpdump wireshark \n# CentOS\nyum install -y tcpdump wireshark\n```\n\n由于 Wireshark 的图形界面，并不能通过 SSH 使用，所以我推荐你在本地机器（比如 Windows）中安装。你可以到 https://www.wireshark.org/ 下载并安装 Wireshark。\n\n\u003e 跟以前一样，案例中所有命令，都默认以 root 用户（在 Windows 中，运行 Wireshark 时除外）运行。如果你是用普通用户身份登陆系统，请运行 sudo su root 命令切换到 root 用户。\n\n## 再探 ping\n\n前面讲过，ping 是一种最常用的网络工具，常用来探测网络主机之间的连通性以及延迟。关于 ping 的原理和使用方法，我在前面的 [Linux 网络基础篇] 已经简单介绍过，而 DNS 缓慢的案例中，也多次用到了 ping 测试 DNS 服务器的延迟（RTT）。\n\n不过，虽然 ping 比较简单，但有时候你会发现，ping 工具本身也可能出现异常，比如运行缓慢，但实际网络延迟却并不大的情况。\n\n接下来，我们打开一个终端，SSH 登录到案例机器中，执行下面的命令，来测试案例机器与极客邦科技官网的连通性和延迟。如果一切正常，你会看到下面这个输出：\n\n```python\n# ping 3 次（默认每次发送间隔 1 秒）\n# 假设 DNS 服务器还是上一期配置的 114.114.114.114\n$ ping -c3 geektime.org\nPING geektime.org (35.190.27.188) 56(84) bytes of data.\n64 bytes from 35.190.27.188 (35.190.27.188): icmp_seq=1 ttl=43 time=36.8 ms\n64 bytes from 35.190.27.188 (35.190.27.188): icmp_seq=2 ttl=43 time=31.1 ms\n64 bytes from 35.190.27.188 (35.190.27.188): icmp_seq=3 ttl=43 time=31.2 ms \n--- geektime.org ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 11049ms\nrtt min/avg/max/mdev = 31.146/33.074/36.809/2.649 ms\n```\n\nping 的输出界面， [Linux 网络基础篇] 中我们已经学过，你可以先复习一下，自己解读并且分析这次的输出。\n\n不过要注意，假如你运行时发现 ping 很快就结束了，那就执行下面的命令，再重试一下。至于这条命令的含义，稍后我们再做解释。\n\n```csharp\n# 禁止接收从 DNS 服务器发送过来并包含 googleusercontent 的包\n$ iptables -I INPUT -p udp --sport 53 -m string --string googleusercontent --algo bm -j DROP\n```\n\n根据 ping 的输出，你可以发现，geektime.org 解析后的 IP 地址是 35.190.27.188，而后三次 ping 请求都得到了响应，延迟（RTT）都是 30ms 多一点。\n\n但汇总的地方，就有点儿意思了。3 次发送，收到 3 次响应，没有丢包，但三次发送和接受的总时间居然超过了 11s（11049ms），这就有些不可思议了吧。\n\n会想起上一节的 DNS 解析问题，你可能会怀疑，这可能是 DNS 解析缓慢的问题。但到底是不是呢？\n\n再回去看 ping 的输出，三次 ping 请求中，用的都是 IP 地址，说明 ping 只需要在最开始运行时，解析一次得到 IP，后面就可以只用 IP 了。\n\n我们再用 nslookup 试试。在终端中执行下面的 nslookup 命令，注意，这次我们同样加了 time 命令，输出 nslookup 的执行时间：\n\n```yaml\n$ time nslookup geektime.org\nServer:\t\t114.114.114.114\nAddress:\t114.114.114.114#53 \nNon-authoritative answer:\nName:\tgeektime.org\nAddress: 35.190.27.188 \n \nreal\t0m0.044s\nuser\t0m0.006s\nsys\t0m0.003s\n```\n\n可以看到，域名解析还是很快的，只需要 44ms，显然比 11s 短了很多。\n\n到这里，再往后该怎么分析呢？其实，这时候就可以用 tcpdump 抓包，查看 ping 在收发哪些网络包。\n\n我们再打开另一个终端（终端二），SSH 登录案例机器后，执行下面的命令：\n\n```ruby\n$ tcpdump -nn udp port 53 or host 35.190.27.188\n```\n\n当然，你可以直接用 tcpdump 不加任何参数来抓包，但那样的话，就可能抓取到很多不相干的包。由于我们已经执行过 ping 命令，知道了 geekbang.org 的 IP 地址是 35.190.27.188，也知道 ping 命令会执行 DNS 查询。所以，上面这条命令，就是基于这个规则进行过滤。\n\n我来具体解释一下这条命令。\n\n- -nn ，表示不解析抓包中的域名（即不反向解析）、协议以及端口号。\n- udp port 53 ，表示只显示 UDP 协议的端口号（包括源端口和目的端口）为 53 的包。\n- host 35.190.27.188 ，表示只显示 IP 地址（包括源地址和目的地址）为 35.190.27.188 的包。\n- 这两个过滤条件中间的“ or ”，表示或的关系，也就是说，只要满足上面两个条件中的任一个，就可以展示出来。\n\n接下来，回到终端一，执行相同的 ping 命令：\n\n```lua\n$ ping -c3 geektime.org\n...\n--- geektime.org ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 11095ms\nrtt min/avg/max/mdev = 81.473/81.572/81.757/0.130 ms\n```\n\n命令结束后，再回到终端二中，查看 tcpdump 的输出：\n\n```bash\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes\n14:02:31.100564 IP 172.16.3.4.56669 \u003e 114.114.114.114.53: 36909+ A? geektime.org. (30)\n14:02:31.507699 IP 114.114.114.114.53 \u003e 172.16.3.4.56669: 36909 1/0/0 A 35.190.27.188 (46)\n14:02:31.508164 IP 172.16.3.4 \u003e 35.190.27.188: ICMP echo request, id 4356, seq 1, length 64\n14:02:31.539667 IP 35.190.27.188 \u003e 172.16.3.4: ICMP echo reply, id 4356, seq 1, length 64\n14:02:31.539995 IP 172.16.3.4.60254 \u003e 114.114.114.114.53: 49932+ PTR? 188.27.190.35.in-addr.arpa. (44)\n14:02:36.545104 IP 172.16.3.4.60254 \u003e 114.114.114.114.53: 49932+ PTR? 188.27.190.35.in-addr.arpa. (44)\n14:02:41.551284 IP 172.16.3.4 \u003e 35.190.27.188: ICMP echo request, id 4356, seq 2, length 64\n14:02:41.582363 IP 35.190.27.188 \u003e 172.16.3.4: ICMP echo reply, id 4356, seq 2, length 64\n14:02:42.552506 IP 172.16.3.4 \u003e 35.190.27.188: ICMP echo request, id 4356, seq 3, length 64\n14:02:42.583646 IP 35.190.27.188 \u003e 172.16.3.4: ICMP echo reply, id 4356, seq 3, length 64\n```\n\n这次输出中，前两行，表示 tcpdump 的选项以及接口的基本信息；从第三行开始，就是抓取到的网络包的输出。这些输出的格式，都是 `时间戳 协议 源地址. 源端口 \u003e 目的地址. 目的端口 网络包详细信息`（这是最基本的格式，可以通过选项增加其他字段）。\n\n前面的字段，都比较好理解。但网络包的详细信息，本身根据协议的不同而不同。所以，要理解这些网络包的详细含义，就要对常用网络协议的基本格式以及交互原理，有基本的了解。\n\n当然，实际上，这些内容都会记录在 IETF（ 互联网工程任务组）发布的 [RFC](https://tools.ietf.org/rfc/index)（请求意见稿）中。\n\n比如，第一条就表示，从本地 IP 发送到 114.114.114.114 的 A 记录查询请求，它的报文格式记录在 RFC1035 中，你可以点击[这里](https://www.ietf.org/rfc/rfc1035.txt)查看。在这个 tcpdump 的输出中，\n\n- 36909+ 表示查询标识值，它也会出现在响应中，加号表示启用递归查询。\n- A? 表示查询 A 记录。\n- geektime.org. 表示待查询的域名。\n- 30 表示报文长度。\n\n接下来的一条，则是从 114.114.114.114 发送回来的 DNS 响应——域名 geektime.org. 的 A 记录值为 35.190.27.188。\n\n第三条和第四条，是 ICMP echo request 和 ICMP echo reply，响应包的时间戳 14:02:31.539667，减去请求包的时间戳 14:02:31.508164 ，就可以得到，这次 ICMP 所用时间为 30ms。这看起来并没有问题。\n\n但随后的两条反向地址解析 PTR 请求，就比较可疑了。因为我们只看到了请求包，却没有应答包。仔细观察它们的时间，你会发现，这两条记录都是发出后 5s 才出现下一个网络包，两条 PTR 记录就消耗了 10s。\n\n再往下看，最后的四个包，则是两次正常的 ICMP 请求和响应，根据时间戳计算其延迟，也是 30ms。\n\n到这里，其实我们也就找到了 ping 缓慢的根源，正是两次 PTR 请求没有得到响应而超时导致的。PTR 反向地址解析的目的，是从 IP 地址反查出域名，但事实上，并非所有 IP 地址都会定义 PTR 记录，所以 PTR 查询很可能会失败。\n\n所以，在你使用 ping 时，如果发现结果中的延迟并不大，而 ping 命令本身却很慢，不要慌，有可能是背后的 PTR 在搞鬼。\n\n知道问题后，解决起来就比较简单了，只要禁止 PTR 就可以。还是老路子，执行 man ping 命令，查询使用手册，就可以找出相应的方法，即加上 -n 选项禁止名称解析。比如，我们可以在终端中执行如下命令：\n\n```python\n$ ping -n -c3 geektime.org\nPING geektime.org (35.190.27.188) 56(84) bytes of data.\n64 bytes from 35.190.27.188: icmp_seq=1 ttl=43 time=33.5 ms\n64 bytes from 35.190.27.188: icmp_seq=2 ttl=43 time=39.0 ms\n64 bytes from 35.190.27.188: icmp_seq=3 ttl=43 time=32.8 ms \n--- geektime.org ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2002ms\nrtt min/avg/max/mdev = 32.879/35.160/39.030/2.755 ms\n```\n\n你可以发现，现在只需要 2s 就可以结束，比刚才的 11s 可是快多了。\n\n到这里， 我就带你一起使用 tcpdump ，解决了一个最常见的 ping 工作缓慢的问题。\n\n案例最后，如果你在开始时，执行了 iptables 命令，那也不要忘了删掉它：\n\n```scss\n$ iptables -D INPUT -p udp --sport 53 -m string --string googleusercontent --algo bm -j DROP\n```\n\n不过，删除后你肯定还有疑问，明明我们的案例跟 Google 没啥关系，为什么要根据 googleusercontent ，这个毫不相关的字符串来过滤包呢？\n\n实际上，如果换一个 DNS 服务器，就可以用 PTR 反查到 35.190.27.188 所对应的域名：\n\n```yaml\n $ nslookup -type=PTR 35.190.27.188 8.8.8.8\nServer:\t8.8.8.8\nAddress:\t8.8.8.8#53\nNon-authoritative answer:\n188.27.190.35.in-addr.arpa\tname = 188.27.190.35.bc.googleusercontent.com.\nAuthoritative answers can be found from:\n```\n\n你看，虽然查到了 PTR 记录，但结果并非 geekbang.org，而是 188.27.190.35.bc.googleusercontent.com。其实，这也是为什么，案例开始时将包含 googleusercontent 的丢弃后，ping 就慢了。因为 iptables ，实际上是把 PTR 响应给丢了，所以会导致 PTR 请求超时。\n\ntcpdump 可以说是网络性能分析最有效的利器。接下来，我再带你一起看看 tcpdump 的更多使用方法。\n\n## tcpdump\n\n我们知道，tcpdump 也是最常用的一个网络分析工具。它基于 [libpcap](https://www.tcpdump.org/) ，利用内核中的 AF_PACKET 套接字，抓取网络接口中传输的网络包；并提供了强大的过滤规则，帮你从大量的网络包中，挑出最想关注的信息。\n\ntcpdump 为你展示了每个网络包的详细细节，这就要求，在使用前，你必须要对网络协议有基本了解。而要了解网络协议的详细设计和实现细节， [RFC](https://www.rfc-editor.org/rfc-index.html) 当然是最权威的资料。\n\n不过，RFC 的内容，对初学者来说可能并不友好。如果你对网络协议还不太了解，推荐你去学《TCP/IP 详解》，特别是第一卷的 TCP/IP 协议族。这是每个程序员都要掌握的核心基础知识。\n\n再回到 tcpdump 工具本身，它的基本使用方法，还是比较简单的，也就是 **tcpdump [选项] [过滤表达式]**。当然，选项和表达式的外面都加了中括号，表明它们都是可选的。\n\n\u003e 提示：在 Linux 工具中，如果你在文档中看到，选项放在中括号里，就说明这是一个可选选项。这时候就要留意一下，这些选项是不是有默认值。\n\n查看 tcpdump 的 [手册](https://www.tcpdump.org/manpages/tcpdump.1.html) ，以及 pcap-filter 的[手册](https://www.tcpdump.org/manpages/pcap-filter.7.html)，你会发现，tcpdump 提供了大量的选项以及各式各样的过滤表达式。不过不要担心，只需要掌握一些常用选项和过滤表达式，就可以满足大部分场景的需要了。\n\n为了帮你更快上手 tcpdump 的使用，我在这里也帮你整理了一些最常见的用法，并且绘制成了表格，你可以参考使用。\n\n首先，来看一下常用的几个选项。在上面的 ping 案例中，我们用过 **-nn** 选项，表示不用对 IP 地址和端口号进行名称解析。其他常用选项，我用下面这张表格来解释。\n\n![img](859d3b5c0071335429620a3fcdde4fff.png)\n\n接下来，我们再来看常用的过滤表达式。刚刚用过的是 udp port 53 or host 35.190.27.188 ，表示抓取 DNS 协议的请求和响应包，以及源地址或目的地址为 35.190.27.188 的包。\n\n其他常用的过滤选项，我也整理成了下面这个表格。\n\n![img](4870a28c032bdd2a26561604ae2f7cb3.png)\n\n最后，再次强调 tcpdump 的输出格式，我在前面已经介绍了它的基本格式：\n\n```undefined\n时间戳 协议 源地址. 源端口 \u003e 目的地址. 目的端口 网络包详细信息\n```\n\n其中，网络包的详细信息取决于协议，不同协议展示的格式也不同。所以，更详细的使用方法，还是需要你去查询 tcpdump 的 [man](https://www.tcpdump.org/manpages/tcpdump.1.html) 手册（执行 man tcpdump 也可以得到）。\n\n不过，讲了这么多，你应该也发现了。tcpdump 虽然功能强大，可是输出格式却并不直观。特别是，当系统中网络包数比较多（比如 PPS 超过几千）的时候，你想从 tcpdump 抓取的网络包中分析问题，实在不容易。\n\n对比之下，Wireshark 则通过图形界面，以及一系列的汇总分析工具，提供了更友好的使用界面，让你可以用更快的速度，摆平网络性能问题。接下来，我们就详细来看看它。\n\n## Wireshark\n\nWireshark 也是最流行的一个网络分析工具，它最大的好处就是提供了跨平台的图形界面。跟 tcpdump 类似，Wireshark 也提供了强大的过滤规则表达式，同时，还内置了一系列的汇总分析工具。\n\n比如，拿刚刚的 ping 案例来说，你可以执行下面的命令，把抓取的网络包保存到 ping.pcap 文件中：\n\n```ruby\n$ tcpdump -nn udp port 53 or host 35.190.27.188 -w ping.pcap\n```\n\n接着，把它拷贝到你安装有 Wireshark 的机器中，比如你可以用 scp 把它拷贝到本地来：\n\n```shell\n$ scp host-ip/path/ping.pcap .\n```\n\n然后，再用 Wireshark 打开它。打开后，你就可以看到下面这个界面：\n\n![img](6b854703dcfcccf64c0a69adecf2f42c.png)\n\n从 Wireshark 的界面里，你可以发现，它不仅以更规整的格式，展示了各个网络包的头部信息；还用了不同颜色，展示 DNS 和 ICMP 这两种不同的协议。你也可以一眼看出，中间的两条 PTR 查询并没有响应包。\n\n接着，在网络包列表中选择某一个网络包后，在其下方的网络包详情中，你还可以看到，这个包在协议栈各层的详细信息。比如，以编号为 5 的 PTR 包为例：\n\n![img](59781a5dc7b1b9234643991365bfc925.png)\n\n你可以看到，IP 层（Internet Protocol）的源地址和目的地址、传输层的 UDP 协议（Uder Datagram Protocol）、应用层的 DNS 协议（Domain Name System）的概要信息。\n\n继续点击每层左边的箭头，就可以看到该层协议头的所有信息。比如点击 DNS 后，就可以看到 Transaction ID、Flags、Queries 等 DNS 协议各个字段的数值以及含义。\n\n当然，Wireshark 的功能远不止如此。接下来我再带你一起，看一个 HTTP 的例子，并理解 TCP 三次握手和四次挥手的工作原理。\n\n这个案例我们将要访问的是 http://example.com/ 。进入终端一，执行下面的命令，首先查出 example.com 的 IP。然后，执行 tcpdump 命令，过滤得到的 IP 地址，并将结果保存到 web.pcap 中。\n\n```ruby\n$ dig +short example.com\n93.184.216.34\n$ tcpdump -nn host 93.184.216.34 -w web.pcap\n```\n\n\u003e 实际上，你可以在 host 表达式中，直接使用域名，即 **tcpdump -nn host example.com -w web.pcap**。\n\n接下来，切换到终端二，执行下面的 curl 命令，访问 [http://example.com](http://example.com/)：\n\n```shell\n$ curl http://example.com\n```\n\n最后，再回到终端一，按下 Ctrl+C 停止 tcpdump，并把得到的 web.pcap 拷贝出来。\n\n使用 Wireshark 打开 web.pcap 后，你就可以在 Wireshark 中，看到如下的界面：\n\n![img](07bcdba5b563ebae36f5b5b453aacd9d.png)\n\n由于 HTTP 基于 TCP ，所以你最先看到的三个包，分别是 TCP 三次握手的包。接下来，中间的才是 HTTP 请求和响应包，而最后的三个包，则是 TCP 连接断开时的三次挥手包。\n\n从菜单栏中，点击 Statistics -\u003e Flow Graph，然后，在弹出的界面中的 Flow type 选择 TCP Flows，你可以更清晰的看到，整个过程中 TCP 流的执行过程：\n\n![img](4ec784752fdbc0cc5ead036a6419cbbb.png)\n\n这其实跟各种教程上讲到的，TCP 三次握手和四次挥手很类似，作为对比， 你通常看到的 TCP 三次握手和四次挥手的流程，基本是这样的：\n\n![img](5230fb678fcd3ca6b55d4644881811e8.png)\n\n(图片来自[酷壳](https://coolshell.cn/articles/11564.html))\n\n不过，对比这两张图，你会发现，这里抓到的包跟上面的四次挥手，并不完全一样，实际挥手过程只有三个包，而不是四个。\n\n其实，之所以有三个包，是因为服务器端收到客户端的 FIN 后，服务器端同时也要关闭连接，这样就可以把 ACK 和 FIN 合并到一起发送，节省了一个包，变成了“三次挥手”。\n\n而通常情况下，服务器端收到客户端的 FIN 后，很可能还没发送完数据，所以就会先回复客户端一个 ACK 包。稍等一会儿，完成所有数据包的发送后，才会发送 FIN 包。这也就是四次挥手了。\n\n抓包后， Wireshark 中就会显示下面这个界面（原始网络包来自 Wireshark TCP 4-times close 示例，你可以点击 [这里](https://wiki.wireshark.org/TCP 4-times close) 下载）：\n\n![img](0ecb6d11e5e7725107c0291c45aa7e99.png)\n\n当然，Wireshark 的使用方法绝不只有这些，更多的使用方法，同样可以参考 [官方文档](https://www.wireshark.org/docs/) 以及 [WIKI](https://wiki.wireshark.org/)。\n\n## 小结\n\n今天，我们一起学了 tcpdump 和 Wireshark 的使用方法，并通过几个案例，学会了如何运用这两个工具来分析网络的收发过程，并找出潜在的性能问题。\n\n当你发现针对相同的网络服务，使用 IP 地址快而换成域名却慢很多时，就要想到，有可能是 DNS 在捣鬼。DNS 的解析，不仅包括从域名解析出 IP 地址的 A 记录请求，还包括性能工具帮你，“聪明”地从 IP 地址反查域名的 PTR 请求。\n\n实际上，**根据 IP 地址反查域名、根据端口号反查协议名称，是很多网络工具默认的行为，而这往往会导致性能工具的工作缓慢**。所以，通常，网络性能工具都会提供一个选项（比如 -n 或者 -nn），来禁止名称解析。\n\n在工作中，当你碰到网络性能问题时，不要忘记 tcpdump 和 Wireshark 这两个大杀器。你可以用它们抓取实际传输的网络包，再排查是否有潜在的性能问题。\n\n## 思考\n\n最后，我想请你来聊一聊，你是如何使用 tcpdump 和 Wireshark 的。你用 tcpdump 或者 Wireshark 解决过哪些网络问题呢？你又是如何排查、分析并解决的呢？你可以结合今天学到的网络知识，总结自己的思路。\n\n欢迎在留言区和我讨论，也欢迎你把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。\n\n# Linux 性能优化专栏加餐（一）\n\n你好，我是倪朋飞。欢迎来到 Linux 性能优化专栏的加餐时间。\n\n之前，很多同学留言让我推荐一些性能优化以及 Linux 系统原理方面的书，今天我就和你分享一些我认为不错的书。\n\nLinux 系统原理和性能优化涉及的面很广，相关的书籍自然也很多。学习咱们专栏，你先要了解 Linux 系统的工作原理，基于此，再去分析、理解各类性能瓶颈，最终找出方法、优化性能。围绕这几个方面，我来推荐一些相应书籍。\n\n## Linux 基础入门书籍：《鸟哥的 Linux 私房菜》\n\n![img](8e3b114e11f6f5195e176290e4aa6eb4.png)\n\n咱们专栏的目标是优化 Linux 系统以及在 Linux 上运行的软件性能。那么，第一步当然是要熟悉 Linux 本身。所以，我推荐的第一本书，正是小有名气的 Linux 系统入门书——《鸟哥的 Linux 私房菜》。\n\n这本书以 CentOS 7 为例，介绍了 Linux 系统的基本使用和管理方法，主要内容包括系统安装、文件和目录操作、磁盘和文件系统管理、编辑器、Bash 以及 Linux 系统的管理维护等。这些内容都是 Linux 初学者需要掌握的基础知识，非常适合刚入门 Linux 系统的新手。\n\n当然，掌握这些基础知识，其实也是学习咱们专栏的基本门槛。比如，我在很多案例里提到的软件包的安装、Bash 命令的运行、grep 和 awk 等基本命令的使用、文档的查询方法等，这本书都有涉及。\n\n另外，这本书的大部分内容，还可以在其繁体中文[官方网站](http://linux.vbird.org/linux_basic/)上在线学习。\n\n## 计算机原理书籍：《深入理解计算机系统》\n\n![img](6b0cadb6858c3e00885e829d0910b207.png)\n\n掌握 Linux 基础后，接下来就该进一步理解计算机系统的工作原理。所以，我推荐的第二本书，正是计算机系统原理的经典黑皮书——《深入理解计算机系统》。\n\n这也是一本经典的计算机学科入门教材，它的英文版名称“Computer Systems: A Programmer’s Perspective”，其实更能体现本书的核心，即从开发者的角度来理解计算机系统。\n\n这本书介绍了计算机系统最基本的工作原理，内容比较广泛。它主要包括信息的计算机表示，程序的编译、链接及运行，处理器体系结构，虚拟内存，存储系统 I/O，网络以及并发等内容。\n\n书本身比较厚，内容也比较多，但作为一本优秀的入门书籍，这本书介绍的各个知识点虽然有点偏向于编程和系统底层，但并不会过于深入这些，对初学者来说非常合适。\n\n此外，这本书的[官方网站](http://csapp.cs.cmu.edu/)上还提供了丰富的资源，可以帮你进一步理解、深入书里的内容，还提供了多个实验操作，助你加深掌握。\n\n## Linux 编程书籍：《Linux 程序设计》和《UNIX 环境高级编程》\n\n![img](1fe3cc0a1d0772282be0047dbfd67fe7.png)\n\n![img](86ac9cfbba6a255c3592de13950be190.png)\n\n介绍完计算机系统工作原理的书籍，接下来，我要推荐的是编程相关的两本书，分别是《Linux 程序设计》和《UNIX 环境高级编程》。\n\n之所以要推荐编程书籍，是因为优化性能的过程中，理解应用程序的执行逻辑至关重要。而要做到这一点，编程基础就是刚需。\n\n我推荐的这两本书中，《Linux 程序设计》主要针对 Linux 系统中的应用程序开发，是一本入门书籍，内容包括 SHELL、标准库、数据库、多进程、进程间通信、套接字以及图像界面等。\n\n《UNIX 环境高级编程》则被誉为 UNIX 编程圣经，是深入 UNIX 环境（包括 Linux）编程的必读书籍。主要内容包括标准库、文件 I/O、进程控制、多进程和进程间通信、多线程以及高级 I/O 等，这些内容都是开发高性能、高可靠应用程序的必备基础。\n\n这两本书籍，可以让你更清楚 Linux 系统以及应用程序的执行过程，甚至在必要时帮你更好地理解应用程序乃至内核的源代码。\n\n## Linux 内核书籍：《深入 Linux 内核架构》\n\n![img](e1ed53283b51ed81a96b9c9d2e72d65e.png)\n\n为了方便你学习和运用，我们专栏内容都是从 Linux 系统的原理出发，借助系统内置或外部安装的各类工具，找出瓶颈所在。所以，理解 Linux 系统原理也是我们的重点，同时，了解内核架构，也可以帮助你分析清楚瓶颈为什么发生。\n\n所以，我推荐的第五本，就是关于 Linux 内核原理的一本书籍——《深入 Linux 内核架构》。这是一本大块头，涉及了 Linux 内核中的进程管理、内存管理、文件系统、磁盘、网络、设备驱动、时钟等大量知识。书中还引用了大量 Linux 内核的源码（内核版本为 2.6.24，虽然有些老，但不影响你理解原理），帮你透彻掌握相关知识点。\n\n如果你是第一次读这本书，不要因为厚厚的页码或者部分内容看不懂就放弃。换个时间重新来看，你会有不同的发现。\n\n## 性能优化书籍：《性能之巅：洞悉系统、企业与云计算》\n\n![img](5b8392e187c770b796c445ded4819655.png)\n\n最后一本，是我曾在 [Linux 性能优化答疑（二）]中提到过的《性能之巅：洞悉系统、企业与云计算》。\n\n这本书，堪称 Linux 性能优化最权威的一本书，而作者 Brendan Gregg ，也是很多我们熟悉的性能优化工具和方法的开创者。\n\n书里主要提供了 Linux 性能分析和调优的基本思路，并具体讲解，如何借助动态追踪等性能工具，分析并优化各种性能问题。同时，这本书也介绍了很多性能工具的使用方法，可以当作你性能优化过程的工具参考书。\n\n最后，我还想再说一句，读书不在多，而在精。\n\n今天我推荐的这些书，你可能或多或少都看过一部分，但这远远不够。要真正掌握它们的核心内容，不仅需要你理解书中讲解的内容，更需要你用大量实践来融汇贯通。\n\n有些书，你可能会觉得很难啃下来，还不如现在层出不穷的新技术时髦。但要注意，这些内容都是基本不会过时的硬知识，多花点儿时间坚持啃下来，相信你一定会有巨大的收获。\n","lastmodified":"2023-05-09T16:33:58.279366106Z","tags":[]},"/Mysql-%E5%8E%9F%E7%90%86":{"title":"Mysql 原理","content":"\n[[1 MySQL的数据目录]]  \n[[2 用户与权限管理]]  \n[[3 逻辑架构]]  \n[[6 索引的数据结构]]  \n[[9 性能分析工具的使用]]  \n[[11 数据库设计规范（范式）]]  \n[[13 事务基础知识]]  \n[[15 锁]]  \n[[16 多版本并发控制]]  \n[[17 其他数据库日志]]\n\n[[log]]\n","lastmodified":"2023-05-09T16:33:58.279366106Z","tags":["mysql"]},"/Mysql-%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5":{"title":"命令速查","content":"\n# 命令速查\n\n**重要！进行用户相关操作后记得刷新权限！！！`FLUSH PRIVILEGES;`**\n\n- 列出所有数据库\n\n  `show databases; `\n\n- 删除数据库\n\n  `drop database name;`\n\n- 新建数据库\n\n  `create database name;`\n\n- 查看用户基本资料\n\n  `SELECT DISTINCT CONCAT('User: ''',user,'''@''',host,''';') AS query FROM mysql.user;`\n\n- 新建用户\n\n  `create user movie_user@% identified by'123456';`\n\n  \u003e@后面的参数是指该用户可以登陆的ip地址，\"%\"是可以在任意ip登陆。\n\n\u003e用户名@IP地址 用户只能该IP下才能访问  \n\u003e用户名@192.168.1.% 用户只能在改IP段下才能访问(通配符%表示任意)  \n\u003e用户名@% 用户可以再任意IP下访问(默认IP地址为%)\n\n- 删除用户\n\n  `drop user '用户名'@'IP地址';`\n\n- 修改用户及密码\n\n  `rename user '用户名'@'IP地址' to '新用户名'@'IP地址';`\n\n  `set password for '用户名'@'IP地址' = Password('新密码');`\n\n- 查看权限\n\n  `show grants for '用户'@'IP地址';`\n\n- 赋予权限\n\n  `GRANT ALL PRIVILEGES ON mall.* TO 'test'@'%'`\n\n  ```shell\n  grant 权限 on 数据库.表 to '用户'@'IP地址';\n   \n  //all privileges  除grant外的所有权限\n  select          仅查权限\n  select,insert   查和插入权限\n  ...\n  usage                   无访问权限\n  alter                   使用alter table\n  alter routine           使用alter procedure和drop procedure\n  create                  使用create table\n  create routine          使用create procedure\n  create temporary tables 使用create temporary tables\n  create user             使用create user、drop user、rename user和revoke all privileges\n  create view             使用create view\n  delete                  使用delete\n  drop                    使用drop table\n  execute                 使用call和存储过程\n  file                    使用select into outfile 和 load data infile\n  grant option            使用grant 和 revoke\n  index                   使用index\n  insert                  使用insert\n  lock tables             使用lock table\n  process                 使用show full processlist\n  select                  使用select\n  show databases          使用show databases\n  show view               使用show view\n  update                  使用update\n  reload                  使用flush\n  shutdown                使用mysqladmin shutdown(关闭MySQL)\n  super                   使用change master、kill、logs、purge、master和set global。还允许mysqladmin????调试登陆\n  replication client      服务器位置的访问\n  replication slave       由复制从属使用\n  ================================================= \n  数据库名.*           数据库中的所有\n  数据库名.表             指定数据库中的某张表\n  数据库名.存储过程       指定数据库中的存储过程\n  *.*                    所有数据库\n  ```\n\n- 删除权限\n\n  `revoke 权限 on 数据库.表 from '用户'@'IP地址';`\n\n#\n","lastmodified":"2023-05-09T16:33:58.279366106Z","tags":[]},"/Nginx":{"title":"Nginx","content":"\n## Nginx 的产生\n\nNginx 同 Apache 一样都是一种 Web 服务器。基于 REST 架构风格，以统一资源描述符（Uniform Resources Identifier）URI 或者统一资源定位符（Uniform Resources Locator）URL 作为沟通依据，通过 HTTP 协议提供各种网络服务。\n\n然而，这些服务器在设计之初受到当时环境的局限，例如当时的用户规模，网络带宽，产品特点等局限并且各自的定位和发展都不尽相同。这也使得各个 Web 服务器有着各自鲜明的特点。\n\nApache 的发展时期很长，而且是毫无争议的世界第一大服务器。它有着很多优点：稳定、开源、跨平台等等。\n\n它出现的时间太长了，它兴起的年代，互联网产业远远比不上现在。所以它被设计为一个重量级的。它不支持高并发的服务器。在 Apache 上运行数以万计的并发访问，会导致服务器消耗大量内存。操作系统对其进行进程或线程间的切换也消耗了大量的 CPU 资源，导致 HTTP 请求的平均响应速度降低。这些都决定了 Apache 不可能成为高性能 Web 服务器，轻量级高并发服务器 Nginx 就应运而生了。\n\n俄罗斯的工程师 Igor Sysoev，他在为 Rambler Media 工作期间，使用 C 语言开发了 Nginx。\n\nNginx 作为 Web 服务器一直为 Rambler Media 提供出色而又稳定的服务。然后呢，Igor Sysoev 将 Nginx 代码开源，并且赋予自由软件许可证。\n\n由于以下这几点，所以，Nginx 火了：\n\n- Nginx 使用基于事件驱动架构，使得其可以支持数以百万级别的 TCP 连接。\n- 高度的模块化和自由软件许可证使得第三方模块层出不穷（这是个开源的时代啊）。\n- Nginx 是一个跨平台服务器，可以运行在 Linux、Windows、FreeBSD、Solaris、AIX、Mac OS 等操作系统上。\n- 这些优秀的设计带来的极大的稳定性。\n\n## Nginx定义\n\nNginx 是一款自由的、开源的、高性能的 HTTP 服务器和反向代理服务器；同时也是一个 IMAP、POP3、SMTP 代理服务器。\n\nNginx 可以作为一个 HTTP 服务器进行网站的发布处理，另外 Nginx 可以作为反向代理进行负载均衡的实现。\n\n## 代理\n\n说到代理，首先我们要明确一个概念，所谓代理就是一个代表、一个渠道；此时就涉及到两个角色，一个是被代理角色，一个是目标角色。\n\n被代理角色通过这个代理访问目标角色完成一些任务的过程称为代理操作过程；如同生活中的专卖店，客人到 adidas 专卖店买了一双鞋，这个专卖店就是代理，被代理角色就是 adidas 厂家，目标角色就是用户。\n\n### 正向代理\n\n说反向代理之前，我们先看看正向代理，正向代理也是大家最常接触到的代理模式，我们会从两个方面来说关于正向代理的处理模式，分别从软件方面和生活方面来解释一下什么叫正向代理。\n\n在如今的网络环境下，我们如果由于技术需要要去访问国外的某些网站，此时你会发现位于国外的某网站我们通过浏览器是没有办法访问的。\n\n此时大家可能都会用一个操作 FQ 进行访问，FQ 的方式主要是找到一个可以访问国外网站的代理服务器，我们将请求发送给代理服务器，代理服务器去访问国外的网站，然后将访问到的数据传递给我们！\n\n上述这样的代理模式称为正向代理，正向代理最大的特点是客户端非常明确要访问的服务器地址；服务器只清楚请求来自哪个代理服务器，而不清楚来自哪个具体的客户端；正向代理模式屏蔽或者隐藏了真实客户端信息。\n\n来看个示意图：\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rad-figure-bed/PicGo/blogs/nginx/20200514214242.jfif)\n\n客户端必须设置正向代理服务器，当然前提是要知道正向代理服务器的 IP 地址，还有代理程序的端口。\n\n**总结来说：**正向代理，\"它代理的是客户端\"，是一个位于客户端和原始服务器（Origin Server）之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标（原始服务器）。\n\n然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端必须要进行一些特别的设置才能使用正向代理。\n\n正向代理的用途：\n\n- 访问原来无法访问的资源，如 Google。\n- 可以做缓存，加速访问资源。\n- 对客户端访问授权，上网进行认证。\n- 代理可以记录用户访问记录（上网行为管理），对外隐藏用户信息。\n\n### 反向代理\n\n明白了什么是正向代理，我们继续看关于反向代理的处理方式，举例如我国的某宝网站，每天同时连接到网站的访问人数已经爆表，单个服务器远远不能满足人民日益增长的购买欲望了。\n\n此时就出现了一个大家耳熟能详的名词：分布式部署；也就是通过部署多台服务器来解决访问人数限制的问题。\n\n某宝网站中大部分功能也是直接使用 Nginx 进行反向代理实现的，并且通过封装 Nginx 和其他的组件之后起了个高大上的名字：Tengine。\n\n有兴趣的童鞋可以访问 Tengine 的官网查看具体的信息：\n\n```\nhttp://tengine.taobao.org/\n```\n\n那么反向代理具体是通过什么样的方式实现的分布式的集群操作呢，多个客户端给服务器发送的请求，Nginx 服务器接收到之后，按照一定的规则分发给了后端的业务处理服务器进行处理了。\n\n此时请求的来源也就是客户端是明确的，但是请求具体由哪台服务器处理的并不明确了，Nginx 扮演的就是一个反向代理角色。\n\n客户端是无感知代理的存在的，**反向代理对外都是透明的**，访问者并不知道自己访问的是一个代理。因为客户端不需要任何配置就可以访问。\n\n反向代理，\"它代理的是服务端\"，主要用于服务器集群分布式部署的情况下，反向代理隐藏了服务器的信息。\n\n反向代理的作用：\n\n- 保证内网的安全，通常将反向代理作为公网访问地址，Web 服务器是内网。\n- 负载均衡，通过反向代理服务器来优化网站的负载。\n\n## 项目场景\n\n通常情况下，我们在实际项目操作时，正向代理和反向代理很有可能会存在同一个应用场景中，正向代理代理客户端的请求去访问目标服务器，目标服务器是一个反向代理服务器，反向代理了多台真实的业务处理服务器。\n\n具体的拓扑图如下：\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rad-figure-bed/PicGo/blogs/nginx/20200514214507.jfif)\n\n截了一张图来说明正向代理和反向代理二者之间的区别，如下图：\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rad-figure-bed/PicGo/blogs/nginx/20200514214516.png)\n\n图解：\n\n- 在正向代理中，Proxy 和 Client 同属于一个 LAN（图中方框内），隐藏了客户端信息。\n- 在反向代理中，Proxy 和 Server 同属于一个 LAN（图中方框内），隐藏了服务端信息。\n\n实际上，Proxy 在两种代理中做的事情都是替服务器代为收发请求和响应，不过从结构上看正好左右互换了一下，所以把后出现的那种代理方式称为反向代理了。\n\n## 负载均衡\n\n我们已经明确了所谓代理服务器的概念，那么接下来，Nginx 扮演了反向代理服务器的角色，它是依据什么样的规则进行请求分发的呢？不用的项目应用场景，分发的规则是否可以控制呢？\n\n这里提到的客户端发送的、Nginx 反向代理服务器接收到的请求数量，就是我们说的负载量。\n\n请求数量按照一定的规则进行分发，到不同的服务器处理的规则，就是一种均衡规则。\n\n所以将服务器接收到的请求按照规则分发的过程，称为**负载均衡**。\n\n负载均衡在实际项目操作过程中，有硬件负载均衡和软件负载均衡两种，硬件负载均衡也称为硬负载，如 F5 负载均衡，相对造价昂贵成本较高。\n\n但是数据的稳定性安全性等等有非常好的保障，如中国移动中国联通这样的公司才会选择硬负载进行操作。\n\n更多的公司考虑到成本原因，会选择使用软件负载均衡，软件负载均衡是利用现有的技术结合主机硬件实现的一种消息队列分发机制。\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rad-figure-bed/PicGo/blogs/nginx/20200514214527.jfif)\n\nNginx 支持的负载均衡调度算法方式如下：\n\n**①weight 轮询（默认）：**接收到的请求按照顺序逐一分配到不同的后端服务器，即使在使用过程中，某一台后端服务器宕机，Nginx 会自动将该服务器剔除出队列，请求受理情况不会受到任何影响。\n\n这种方式下，可以给不同的后端服务器设置一个权重值（weight），用于调整不同的服务器上请求的分配率。\n\n权重数据越大，被分配到请求的几率越大；该权重值，主要是针对实际工作环境中不同的后端服务器硬件配置进行调整的。\n\n**②ip_hash：**每个请求按照发起客户端的 ip 的 hash 结果进行匹配，这样的算法下一个固定 ip 地址的客户端总会访问到同一个后端服务器，这也**在一定程度上解决了集群部署环境下 Session 共享的问题**。\n\n**③fair：**智能调整调度算法，动态的根据后端服务器的请求处理到响应的时间进行均衡分配。\n\n响应时间短处理效率高的服务器分配到请求的概率高，响应时间长处理效率低的服务器分配到的请求少，它是结合了前两者的优点的一种调度算法。\n\n但是需要注意的是 Nginx 默认不支持 fair 算法，如果要使用这种调度算法，请安装 upstream_fair 模块。\n\n**④url_hash：**按照访问的 URL 的 hash 结果分配请求，每个请求的 URL 会指向后端固定的某个服务器，可以在 Nginx 作为静态服务器的情况下提高缓存效率。\n\n同样要注意 Nginx 默认不支持这种调度算法，要使用的话需要安装 Nginx 的 hash 软件包。\n\n## Web 服务器对比\n\n几种常用 Web 服务器对比如下图：\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rad-figure-bed/PicGo/blogs/nginx/20200514214537.webp)\n\n## nginx 服务架构\n\n\u003e nginx 服务器的开发`Keep-Alive`遵循模块化设计思想\n\n### 模块化开发\n\n1. 单一职责原则，一个模块只负责一个功能\n2. 将程序分解，自顶向下，逐步求精\n3. 高内聚，低耦合\n\n### nginx 的模块化结构\n\n- 核心模块：nginx 最基本最核心的服务，如进程管理、权限控制、日志记录；\n- 标准 HTTP 模块：nginx 服务器的标准 HTTP 功能；\n- 可选 HTTP 模块：处理特殊的 HTTP 请求\n- 邮件服务模块：邮件服务\n- 第三方模块：作为扩展，完成特殊功能\n\n### nginx 的模块清单\n\n```yaml\n- 核心模块\n  - ngx_core\n  - ngx_errlog\n  - ngx_conf\n  - ngx_events\n  - ngx_event_core\n  - ngx_epll\n  - ngx_regex\n- 标准 HTTP 模块\n  - ngx_http\n  - ngx_http_core #配置端口，URI 分析，服务器相应错误处理，别名控制 (alias) 等\n  - ngx_http_log #自定义 access 日志\n  - ngx_http_upstream #定义一组服务器，可以接受来自 proxy, Fastcgi,Memcache 的重定向；主要用作负载均衡\n  - ngx_http_static\n  - ngx_http_autoindex #自动生成目录列表\n  - ngx_http_index #处理以`*`结尾的请求，如果没有找到 index 页，则看是否开启了`~`；如开启，则用之，否则用 autoindex\n  - ngx_http_auth_basic #基于 http 的身份认证 (auth_basic)\n  - ngx_http_access #基于 IP 地址的访问控制 (deny,allow)\n  - ngx_http_limit_conn #限制来自客户端的连接的响应和处理速率\n  - ngx_http_limit_req #限制来自客户端的请求的响应和处理速率\n  - ngx_http_geo\n  - ngx_http_map #创建任意的键值对变量\n  - ngx_http_split_clients\n  - ngx_http_referer #过滤 HTTP 头中 Referer 为空的对象\n  - ngx_http_rewrite #通过正则表达式重定向请求\n  - ngx_http_proxy\n  - ngx_http_fastcgi #支持 fastcgi\n  - ngx_http_uwsgi\n  - ngx_http_scgi\n  - ngx_http_memcached\n  - ngx_http_empty_gif #从内存创建一个 1×1 的透明 gif 图片，可以快速调用\n  - ngx_http_browser #解析 http 请求头部的 User-Agent 值\n  - ngx_http_charset #指定网页编码\n  - ngx_http_upstream_ip_hash\n  - ngx_http_upstream_least_conn\n  - ngx_http_upstream_keepalive\n  - ngx_http_write_filter\n  - ngx_http_header_filter\n  - ngx_http_chunked_filter\n  - ngx_http_range_header\n  - ngx_http_gzip_filter\n  - ngx_http_postpone_filter\n  - ngx_http_ssi_filter\n  - ngx_http_charset_filter\n  - ngx_http_userid_filter\n  - ngx_http_headers_filter #设置 http 响应头\n  - ngx_http_copy_filter\n  - ngx_http_range_body_filter\n  - ngx_http_not_modified_filter\n- 可选 HTTP 模块\n  - ngx_http_addition #在响应请求的页面开始或者结尾添加文本信息\n  - ngx_http_degradation #在低内存的情况下允许服务器返回 444 或者 204 错误\n  - ngx_http_perl\n  - ngx_http_flv #支持将 Flash 多媒体信息按照流文件传输，可以根据客户端指定的开始位置返回 Flash\n  - ngx_http_geoip #支持解析基于 GeoIP 数据库的客户端请求\n  - ngx_google_perftools\n  - ngx_http_gzip #gzip 压缩请求的响应\n  - ngx_http_gzip_static #搜索并使用预压缩的以.gz 为后缀的文件代替一般文件响应客户端请求\n  - ngx_http_image_filter #支持改变 png，jpeg，gif 图片的尺寸和旋转方向\n  - ngx_http_mp4 #支持.mp4,.m4v,.m4a 等多媒体信息按照流文件传输，常与 ngx_http_flv 一起使用\n  - ngx_http_random_index #当收到 / 结尾的请求时，在指定目录下随机选择一个文件作为 index\n  - ngx_http_secure_link #支持对请求链接的有效性检查\n  - ngx_http_ssl #支持 https\n  - ngx_http_stub_status\n  - ngx_http_sub_module #使用指定的字符串替换响应中的信息\n  - ngx_http_dav #支持 HTTP 和 WebDAV 协议中的 PUT/DELETE/MKCOL/COPY/MOVE 方法\n  - ngx_http_xslt #将 XML 响应信息使用 XSLT 进行转换\n- 邮件服务模块\n  - ngx_mail_core\n  - ngx_mail_pop3\n  - ngx_mail_imap\n  - ngx_mail_smtp\n  - ngx_mail_auth_http\n  - ngx_mail_proxy\n  - ngx_mail_ssl\n- 第三方模块\n  - echo-nginx-module #支持在 nginx 配置文件中使用 echo/sleep/time/exec 等类 Shell 命令\n  - memc-nginx-module\n  - rds-json-nginx-module #使 nginx 支持 json 数据的处理\n  - lua-nginx-module\n```\n\n### nginx 的 web 请求处理机制\n\n作为服务器软件，必须具备并行处理多个客户端的请求的能力， 工作方式主要以下 3 种：\n\n- 多进程 (Apache)\n  - 优点：设计和实现简单；子进程独立\n  - 缺点：生成一个子进程要内存复制，在资源和时间上造成额外开销\n- 多线程 (IIS)\n  - 优点：开销小\n  - 缺点：开发者自己要对内存进行管理；线程之间会相互影响\n- 异步方式 (nginx)\n\n经常说道异步非阻塞这个概念， 包含两层含义：\n\n通信模式： + 同步：发送方发送完请求后，等待并接受对方的回应后，再发送下个请求 + 异步：发送方发送完请求后，不必等待，直接发送下个请求\n\n## 配置文件实例\n\n```nginx\n#定义 nginx 运行的用户和用户组\nuser www www;\n\n#nginx 进程数，建议设置为等于 CPU 总核心数。\nworker_processes 8;\n\n#nginx 默认没有开启利用多核 CPU, 通过增加 worker_cpu_affinity 配置参数来充分利用多核 CPU 以下是 8 核的配置参数\nworker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000 10000000;\n\n#全局错误日志定义类型，[ debug | info | notice | warn | error | crit ]\nerror_log /var/log/nginx/error.log info;\n\n#进程文件\npid /var/run/nginx.pid;\n\n#一个 nginx 进程打开的最多文件描述符数目，理论值应该是最多打开文件数（系统的值 ulimit -n）与 nginx 进程数相除，但是 nginx 分配请求并不均匀，所以建议与 ulimit -n 的值保持一致。\nworker_rlimit_nofile 65535;\n\n#工作模式与连接数上限\nevents\n{\n    #参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll 模型是 Linux 2.6 以上版本内核中的高性能网络 I/O 模型，如果跑在 FreeBSD 上面，就用 kqueue 模型。\n    #epoll 是多路复用 IO(I/O Multiplexing) 中的一种方式，但是仅用于 linux2.6 以上内核，可以大大提高 nginx 的性能\n    use epoll;\n\n    ############################################################################\n    #单个后台 worker process 进程的最大并发链接数\n    #事件模块指令，定义 nginx 每个进程最大连接数，默认 1024。最大客户连接数由 worker_processes 和 worker_connections 决定\n    #即 max_client=worker_processes*worker_connections, 在作为反向代理时：max_client=worker_processes*worker_connections / 4\n    worker_connections 65535;\n    ############################################################################\n}\n\n#设定 http 服务器\nhttp {\n    include mime.types; #文件扩展名与文件类型映射表\n    default_type application/octet-stream; #默认文件类型\n    #charset utf-8; #默认编码\n\n    server_names_hash_bucket_size 128; #服务器名字的 hash 表大小\n    client_header_buffer_size 32k; #上传文件大小限制\n    large_client_header_buffers 4 64k; #设定请求缓\n    client_max_body_size 8m; #设定请求缓\n    sendfile on; #开启高效文件传输模式，sendfile 指令指定 nginx 是否调用 sendfile 函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘 IO 重负载应用，可设置为 off，以平衡磁盘与网络 I/O 处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成 off。\n    autoindex on; #开启目录列表访问，合适下载服务器，默认关闭。\n    tcp_nopush on; #防止网络阻塞\n    tcp_nodelay on; #防止网络阻塞\n\n    ##连接客户端超时时间各种参数设置##\n    keepalive_timeout  120;          #单位是秒，客户端连接时时间，超时之后服务器端自动关闭该连接 如果 nginx 守护进程在这个等待的时间里，一直没有收到浏览发过来 http 请求，则关闭这个 http 连接\n    client_header_timeout 10;        #客户端请求头的超时时间\n    client_body_timeout 10;          #客户端请求主体超时时间\n    reset_timedout_connection on;    #告诉 nginx 关闭不响应的客户端连接。这将会释放那个客户端所占有的内存空间\n    send_timeout 10;                 #客户端响应超时时间，在两次客户端读取操作之间。如果在这段时间内，客户端没有读取任何数据，nginx 就会关闭连接\n    ################################\n\n    #FastCGI 相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。\n    fastcgi_connect_timeout 300;\n    fastcgi_send_timeout 300;\n    fastcgi_read_timeout 300;\n    fastcgi_buffer_size 64k;\n    fastcgi_buffers 4 64k;\n    fastcgi_busy_buffers_size 128k;\n    fastcgi_temp_file_write_size 128k;\n\n    ###作为代理缓存服务器设置#######\n    ###先写到 temp 再移动到 cache\n    #proxy_cache_path /var/tmp/nginx/proxy_cache levels=1:2 keys_zone=cache_one:512m inactive=10m max_size=64m;\n    ###以上 proxy_temp 和 proxy_cache 需要在同一个分区中\n    ###levels=1:2 表示缓存级别，表示缓存目录的第一级目录是 1 个字符，第二级目录是 2 个字符 keys_zone=cache_one:128m 缓存空间起名为 cache_one 大小为 512m\n    ###max_size=64m 表示单个文件超过 128m 就不缓存了  inactive=10m 表示缓存的数据，10 分钟内没有被访问过就删除\n    #########end####################\n\n    #####对传输文件压缩###########\n    #gzip 模块设置\n    gzip on; #开启 gzip 压缩输出\n    gzip_min_length 1k; #最小压缩文件大小\n    gzip_buffers 4 16k; #压缩缓冲区\n    gzip_http_version 1.0; #压缩版本（默认 1.1，前端如果是 squid2.5 请使用 1.0）\n    gzip_comp_level 2; #压缩等级，gzip 压缩比，1 为最小，处理最快；9 为压缩比最大，处理最慢，传输速度最快，也最消耗 CPU；\n    gzip_types text/plain application/x-javascript text/css application/xml;\n    #压缩类型，默认就已经包含 text/html，所以下面就不用再写了，写上去也不会有问题，但是会有一个 warn。\n    gzip_vary on;\n    ##############################\n\n    #limit_zone crawler $binary_remote_addr 10m; #开启限制 IP 连接数的时候需要使用\n\n    upstream blog.ha97.com {\n        #upstream 的负载均衡，weight 是权重，可以根据机器配置定义权重。weigth 参数表示权值，权值越高被分配到的几率越大。\n        server 192.168.80.121:80 weight=3;\n        server 192.168.80.122:80 weight=2;\n        server 192.168.80.123:80 weight=3;\n    }\n\n    #虚拟主机的配置\n    server {\n        #监听端口\n        listen 80;\n\n        #############https##################\n        #listen 443 ssl;\n        #ssl_certificate /opt/https/xxxxxx.crt;\n        #ssl_certificate_key /opt/https/xxxxxx.key;\n        #ssl_protocols SSLv3 TLSv1;\n        #ssl_ciphers HIGH:!ADH:!EXPORT57:RC4+RSA:+MEDIUM;\n        #ssl_prefer_server_ciphers on;\n        #ssl_session_cache shared:SSL:2m;\n        #ssl_session_timeout 5m;\n        ####################################end\n\n        #域名可以有多个，用空格隔开\n        server_name www.ha97.com ha97.com;\n        index index.html index.htm index.php;\n        root /data/www/ha97;\n        location ~ .*.(php|php5)?$ {\n            fastcgi_pass 127.0.0.1:9000;\n            fastcgi_index index.php;\n            include fastcgi.conf;\n        }\n\n        #图片缓存时间设置\n        location ~ .*.(gif|jpg|jpeg|png|bmp|swf)$ {\n            expires 10d;\n        }\n\n        #JS 和 CSS 缓存时间设置\n        location ~ .*.(js|css)?$ {\n            expires 1h;\n        }\n\n        #日志格式设定\n        log_format access '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" $http_x_forwarded_for';\n\n        #定义本虚拟主机的访问日志\n        access_log /var/log/nginx/ha97access.log access;\n\n        #对 \"/\" 启用反向代理\n        location / {\n            proxy_pass http://127.0.0.1:88;\n            proxy_redirect off;\n            proxy_set_header X-Real-IP $remote_addr;\n            #后端的 Web 服务器可以通过 X-Forwarded-For 获取用户真实 IP\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            #以下是一些反向代理的配置，可选。\n            proxy_set_header Host $host;\n            client_max_body_size 10m; #允许客户端请求的最大单文件字节数\n            client_body_buffer_size 128k; #缓冲区代理缓冲用户端请求的最大字节数，\n\n            ##代理设置 以下设置是 nginx 和后端服务器之间通讯的设置##\n            proxy_connect_timeout 90; #nginx 跟后端服务器连接超时时间（代理连接超时）\n            proxy_send_timeout 90; #后端服务器数据回传时间（代理发送超时）\n            proxy_read_timeout 90; #连接成功后，后端服务器响应时间（代理接收超时）\n            proxy_buffering on;    #该指令开启从后端被代理服务器的响应内容缓冲 此参数开启后 proxy_buffers 和 proxy_busy_buffers_size 参数才会起作用\n            proxy_buffer_size 4k;  #设置代理服务器（nginx）保存用户头信息的缓冲区大小\n            proxy_buffers 4 32k;   #proxy_buffers 缓冲区，网页平均在 32k 以下的设置\n            proxy_busy_buffers_size 64k; #高负荷下缓冲大小（proxy_buffers*2）\n            proxy_max_temp_file_size 2048m; #默认 1024m, 该指令用于设置当网页内容大于 proxy_buffers 时，临时文件大小的最大值。如果文件大于这个值，它将从 upstream 服务器同步地传递请求，而不是缓冲到磁盘\n            proxy_temp_file_write_size 512k; 这是当被代理服务器的响应过大时 nginx 一次性写入临时文件的数据量。\n            proxy_temp_path  /var/tmp/nginx/proxy_temp;    ##定义缓冲存储目录，之前必须要先手动创建此目录\n            proxy_headers_hash_max_size 51200;\n            proxy_headers_hash_bucket_size 6400;\n            #######################################################\n        }\n\n        #设定查看 nginx 状态的地址\n        location /nginxStatus {\n            stub_status on;\n            access_log on;\n            auth_basic \"nginxStatus\";\n            auth_basic_user_file conf/htpasswd;\n            #htpasswd 文件的内容可以用 apache 提供的 htpasswd 工具来产生。\n        }\n\n        #本地动静分离反向代理配置\n        #所有 jsp 的页面均交由 tomcat 或 resin 处理\n        location ~ .(jsp|jspx|do)?$ {\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_pass http://127.0.0.1:8080;\n        }\n\n        #所有静态文件由 nginx 直接读取不经过 tomcat 或 resin\n        location ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt|pdf|xls|mp3|wma)$\n        { expires 15d; }\n\n        location ~ .*.(js|css)?$\n        { expires 1h; }\n    }\n}\n\n```\n\n## 配置文件指令说明\n\n### nginx.conf 文件的结构\n\n- Global: nginx 运行相关\n- events: 与用户的网络连接相关\n- http\n  - http Global: 代理，缓存，日志，以及第三方模块的配置\n  - server\n    - server Global: 虚拟主机相关\n    - location: 地址定向，数据缓存，应答控制，以及第三方模块的配置\n\n\u003e 所有的所有的所有的指令，都要以`标准 uri`结尾\n\n### nginx 运行相关的 Global 部分\n\n#### 配置运行 nginx 服务器用户\n\nuser nobody nobody;\n\n#### 配置允许生成的 worker process 数\n\nworker_processes auto; worker_processes 4;\n\n\u003e 这个数字，跟电脑 CPU 核数要保持一致\n\n```\n# grep ^proces /proc/cpuinfo\nprocessor       : 0\nprocessor       : 1\nprocessor       : 2\nprocessor       : 3\n# grep ^proces /proc/cpuinfo | wc -l\n4\n复制代码\n```\n\n#### 配置 nginx 进程 PID 存放路径\n\npid logs/nginx.pid;\n\n\u003e 这里面保存的就是一个数字，nginx master 进程的进程号\n\n#### 配置错误日志的存放路径\n\nerror_log logs/error.log; error_log logs/error.log error;\n\n#### 配置文件的引入\n\ninclude mime.types; include fastcgi_params; include ../../conf/*.conf;\n\n### 与用户的网络连接相关的 events\n\n#### 设置网络连接的序列化\n\naccept_mutex on;\n\n\u003e 对多个 nginx 进程接收连接进行序列化，防止多个进程对连接的争抢（惊群）\n\n#### 设置是否允许同时接收多个网络连接\n\nmulti_accept off;\n\n#### 事件驱动模型的选择\n\nuse select|poll|kqueue|epoll|rtsig|/dev/poll|eventport\n\n\u003e 这个重点，后面再看\n\n#### 配置最大连接数\n\nworker_connections 512;\n\n### http\n\n#### http Global 代理 - 缓存 - 日志 - 第三方模块配置\n\n#### 定义 MIME-Type\n\ninclude mime.types; default_type application/octet-stream;\n\n#### 自定义服务日志\n\naccess_log logs/access.log main; access_log off;\n\n#### 配置允许 sendfile 方式传输文件\n\nsendfile off;\n\nsendfile on; sendfile_max_chunk 128k;\n\n\u003e nginx 每个 worker process 每次调用 sendfile() 传输的数据量的最大值\n\nRefer:\n\n- [Linux kenel sendfile 如何提升性能](http://www.vpsee.com/2009/07/linux-sendfile-improve-performance/)\n- [nginx sendifle tcp_nopush tcp_nodelay 参数解释](http://blog.csdn.net/zmj_88888888/article/details/9169227)\n\n#### 配置连接超时时间\n\n\u003e 与用户建立连接后，nginx 可以保持这些连接一段时间，默认 75s 下面的 65s 可以被 Mozilla/Konqueror 识别，是发给用户端的头部信息`正则 uri`值\n\nkeepalive_timeout 75s 65s;\n\n#### 单连接请求数上限\n\n\u003e 和用户端建立连接后，用户通过此连接发送请求；这条指令用于设置请求的上限数\n\nkeepalive_requests 100;\n\n### server\n\n#### 配置网络监听\n\nlisten *:80 | *:8000; # 监听所有的 80 和 8000 端口\n\nlisten 192.168.1.10:8000; listen 192.168.1.10; listen 8000; # 等同于 listen *:8000; listen 192.168.1.10 default_server backlog=511; # 该 ip 的连接请求默认由此虚拟主机处理；最多允许 1024 个网络连接同时处于挂起状态\n\n#### 基于名称的虚拟主机配置\n\nserver_name myserver.com www.myserver.com;\n\nserver_name *.myserver.com www.myserver.* myserver2.*; # 使用通配符\n\n\u003e 不允许的情况： server_name www.ab*d.com; # `正则 uri`只允许出现在 www 和 com 的位置\n\nserver_name ~^www\\d+.myserver.com$; # 使用正则\n\n\u003e nginx 的配置中，可以用正则的地方，都以`正则 uri`开头\n\n\u003e from nginx~0.7.40 开始，server_name 中的正则支持 字符串捕获功能（capture）\n\nserver_name ~^www.(.+).com$; # 当请求通过 www.myserver.com 请求时， myserver 就被记录到`$1`中，在本 server 的上下文中就可以使用\n\n如果一个名称 被多个虚拟主机的 server_name 匹配成功，那这个请求到底交给谁处理呢？看优先级：\n\n1. 准确匹配到 server_name\n2. 通配符在开始时匹配到 server_name\n3. 通配符在结尾时匹配到 server_name\n4. 正则表达式匹配 server_name\n5. 先到先得\n\n#### 配置 https 证书\n\n**原理**\n\nhttps 是在 http 和 TCP 中间加上一层加密层\n\n\u003e - 浏览器向服务端发送消息时：本质上是浏览器（客户端）使用服务端的公钥来加密信息，服务端使用自己的私钥解密，\n\u003e - 浏览器从服务端获取消息是：服务端使用自己私钥加密，浏览器（客户端）使用服务端的公钥来解密信息\n\n在这个过程中，需要保证服务端给浏览器的公钥不是假冒的。证明服务端公钥信息的机构是 CA（数字认证中心）\n\n可以理解为：如果想证明一个人的身份是真的，就得证明这个人的身份证是真的\n\n**数字证书**\n\n```\n数字证书相当于物理世界中的身份证，\n在网络中传递信息的双方互相不能见面，利用数字证书可确认双方身份，而不是他人冒充的。\n这个数字证书由信任的第三方，即认证中心使用自己的私钥对 A 的公钥加密，加密后文件就是网络上的身份证了，即数字证书\n复制代码\n```\n\n大致可以理解为如下\n\n```\n1. 服务端将自己的公钥和其他信息（服务端数字证书），请求数字认证中心签名，数字认证中心使用自己的私钥在证书里加密（只有数字认证中心的公钥才能解开）\n2. 服务端将自己的证书（证书里面包括服务端的公钥）给浏览器\n3. 浏览器的“证书管理器”中有“受信任的根证书颁发机构”列表，客户端在接收到响应后，会在这个列表里查看是否存在解开该服务器数字证书的公钥。有两种错误情况：如果公钥在这个列表里，但是解码后的内容不匹配，说明证书被冒用；如果公钥不在这个列表里，说明这张证书不是受信任的机构所颁发，他的真实性无法确定\n4. 如果一切都没问题，浏览器就可以使用服务器的公钥对信息内容进行加密，然后与服务器交换信息（已加密）\n\n+--------------+           +------------------+\n|    服务端    |----------\u003e| 数字认证中心 (CA) |\n+------+-------+    1    X +------------------+\n       |                / /\n       |               / /\n       |              / /\n       |             / /\n       |2         3 / / 4\n       |           / /\n       |          / /\n       |         / /\n       X        / /\n+--------------+ /\n|    浏览器    |X\n+--------------+\n\n只要证书（证书里有服务端的公钥）是可信的，公钥就是可信的。\n复制代码\n```\n\n**证书格式**\n\nLinux 下的工具们通常使用 base64 编码的文本格式，相关常用后缀如下\n\n- 证书\n  - .crt\n  - .pem\n  - .cer(IIS 等一些平台下，则习惯用 cer 作为证书文件的扩展名，二进制证书）\n- 私钥：.key\n- 证书请求：.csr\n- 其他\n  - .keystore java 密钥库（包括证书和私钥）\n\n**制作证书**\n\n```\n1. 生成服务器端的私钥 (key 文件）\n$openssl genrsa  -out server.key 1024\n\n2. 生成服务器端证书签名请求文件 (csr 文件）;\n$ openssl req -new -key server.key -out server.csr\n\n...\nCountry Name:CN------------ 证书持有者所在国家\nState or Province Name:BJ-- 证书持有者所在州或省份（可省略不填）\nLocality Name:BJ----------- 证书持有者所在城市（可省略不填）\nOrganization Name:SC------- 证书持有者所属组织或公司\nOrganizational Unit Name:.- 证书持有者所属部门（可省略不填）\nCommon Name :ceshi.com----- 域名\nEmail Address:------------- 邮箱（可省略不填）\n\nA challenge password:------ 直接回车\nAn optional company name:-- 直接回车\n\n\n3. 生成证书文件 (crt 文件）\n$ openssl x509 -req -days 1000 -in server.csr -signkey server.key -out server.crt\n复制代码\n```\n\n以上生成 server.crt server.key 文件即是用于 HTTPS 配置的证书和 key\n\n如果想查看证书里面的内容，可以通过 $openssl x509 -in server.crt -text -noout 查看\n\n**配置 nginx**\n\n在 nginx 的 server 区域内添加如下\n\n```\nlisten 443 ssl;\nssl_certificate /opt/https/server.crt;\nssl_certificate_key /opt/https/server.key;\nssl_protocols SSLv3 TLSv1;\nssl_ciphers HIGH:!ADH:!EXPORT57:RC4+RSA:+MEDIUM;\nssl_prefer_server_ciphers on;\nssl_session_cache shared:SSL:2m;\nssl_session_timeout 5m;\n复制代码\n```\n\n#### 基于 IP 的虚拟主机配置\n\n\u003e 基于 IP 的虚拟主机，需要将网卡设置为同时能够监听多个 IP 地址\n\n```\nifconfig\n# 查看到本机 IP 地址为 192.168.1.30\nifconfig eth1:0 192.168.1.31 netmask 255.255.255.0 up\nifconfig eth1:1 192.168.1.32 netmask 255.255.255.0 up\nifconfig\n# 这时就看到 eth1 增加来 2 个别名， eth1:0 eth1:1\n\n# 如果需要机器重启后仍保持这两个虚拟的 IP\necho \"ifconfig eth1:0 192.168.1.31 netmask 255.255.255.0 up\" \u003e\u003e /etc/rc.local\necho \"ifconfig eth1:0 192.168.1.32 netmask 255.255.255.0 up\" \u003e\u003e /etc/rc.local\n复制代码\n```\n\n再来配置基于 IP 的虚拟主机\n\n```\nhttp {\n    ...\n    server {\n     listen 80;\n     server_name 192.168.1.31;\n     ...\n    }\n    server {\n     listen 80;\n     server_name 192.168.1.32;\n     ...\n    }\n}\n复制代码\n```\n\n#### 配置 location 块\n\n\u003e location 块的配置，应该是最常用的了\n\nlocation [ = | ~ | ~* | ^~ ] uri {…}\n\n这里内容分 2 块，匹配方式和 uri， 其中 uri 又分为 标准 uri 和正则 uri\n\n先不考虑 那 4 种匹配方式\n\n1. nginx 首先会再 server 块的多个 location 中搜索是否有`标准 uri`和请求字符串匹配， 如果有，记录匹配度最高的一个；\n2. 然后，再用 location 块中的`=`和请求字符串匹配， 当第一个`标准 uri`匹配成功，即停止搜索， 并使用该 location 块处理请求；\n3. 如果，所有的`^~`都匹配失败，就使用刚记录下的匹配度最高的一个`标准 uri`处理请求\n4. 如果都失败了，那就失败喽\n\n再看 4 种匹配方式：\n\n- `正则 uri`: 用于`~`前，要求请求字符串与其严格匹配，成功则立即处理\n- `正则 uri`: 用于`~*`前，并要求一旦匹配到，立即处理，不再去匹配其他的那些个`~*`\n- `正则 uri`: 用于`]( https://juejin.im/equation?tex=args%20%E6%98%AF%E4%BF%9D%E9%9A%9C%20nginx%20%E6%AD%A3%E5%88%99%E6%8D%95%E8%8E%B7%20get%20%E8%AF%B7%E6%B1%82%E6%97%B6%E4%B8%8D%E4%B8%A2%E5%A4%B1%EF%BC%8C%E5%A6%82%E6%9E%9C%E5%8F%AA%E6%98%AF%20post%20%E8%AF%B7%E6%B1%82%EF%BC%8C%60 )args`前，表示 uri 包含正则表达式，并区分大小写\n- `$1`: 用于{INLINE_CODE_BLOCK_PLACEHOLDER}前， 表示 uri 包含正则表达式， 不区分大小写\n\n\u003e ```\n\u003e ^~` 也是支持浏览器编码过的 URI 的匹配的哦， 如 `/html/%20/data` 可以成功匹配 `/html/ /data\n\u003e ```\n\n#### [root] 配置请求的根目录\n\nWeb 服务器收到请求后，首先要在服务端指定的目录中寻找请求资源\n\n```\nroot /var/www;\n复制代码\n```\n\n**root 后跟的指定目录是上级目录**\n\n该上级目录下要含有和 location 后指定名称的同名目录才行，末尾“/”加不加无所谓\n\n```\nlocation /c/ {\n      root /a/\n}\n复制代码\n```\n\n访问站点 http://location/c 访问的就是 /a/c 目录下的站点信息。\n\n#### [alias] 更改 location 的 URI\n\n除了使用 root 指明处理请求的根目录，还可以使用 alias 改变 location 收到的 URI 的请求路径\n\n```\nlocation ~ ^/data/(.+\\.(htm|html))$ {\n    alias /locatinotest1/other/$1;\n}\n复制代码\n```\n\n**alias 后跟的指定目录是准确的，并且末尾必须加“/”，否则找不到文件**\n\n```\nlocation /c/ {\n      alias /a/\n}\n复制代码\n```\n\n访问站点 http://location/c 访问的就是 /a/ 目录下的站点信息。\n\n【注】一般情况下，在 location / 中配置 root，在 location /other 中配置 alias 是一个好习惯。\n\n#### 设置网站的默认首页\n\nindex 指令主要有 2 个作用：\n\n- 对请求地址没有指明首页的，指定默认首页\n- 对一个请求，根据请求内容而设置不同的首页，如下：\n\n```\nlocation ~ ^/data/(.+)/web/$ {\n    index index.$1.html index.htm;\n}\n复制代码\n```\n\n#### 设置网站的错误页面\n\nerror_page 404 /404.html; error_page 403 /forbidden.html; error_page 404 =301 /404.html;\n\n```\nlocation /404.html {\n    root /myserver/errorpages/;\n}\n复制代码\n```\n\n#### 基于 IP 配置 nginx 的访问权限\n\n```\nlocation / {\n    deny 192.168.1.1;\n    allow 192.168.1.0/24;\n    allow 192.168.1.2/24;\n    deny all;\n}\n复制代码\n```\n\n\u003e 从 192.168.1.0 的用户时可以访问的，因为解析到 allow 那一行之后就停止解析了\n\n#### 基于密码配置 nginx 的访问权限\n\nauth_basic \"please login\"; auth_basic_user_file /etc/nginx/conf/pass_file;\n\n\u003e 这里的 file 必须使用绝对路径，使用相对路径无效\n\n```\n# /usr/local/apache2/bin/htpasswd -c -d pass_file user_name\n# 回车输入密码，-c 表示生成文件，-d 是以 crypt 加密。\n\nname1:password1\nname2:password2:comment\n复制代码\n```\n\n\u003e 经过 basic auth 认证之后没有过期时间，直到该页面关闭； 如果需要更多的控制，可以使用 HttpAuthDigestModule [wiki.nginx.org/HttpAuthDig…](http://wiki.nginx.org/HttpAuthDigestModule)\n\n## 命令\n\n```shell\nnginx -s reopen \t#重启Nginx\nnginx -s reload \t#重新加载Nginx配置文件，然后以优雅的方式重启Nginx\nnginx -s stop   \t#强制停止Nginx服务\nnginx -s quit   \t#优雅地停止Nginx服务（即处理完所有请求后再停止服务）\nnginx -h \t\t#打开帮助信息\nnginx -v \t\t#显示版本信息并退出\nnginx -V\t\t#显示版本和配置选项信息，然后退出\nnginx -t\t\t#检测配置文件是否有语法错误，然后退出\nnginx -T\t \t#检测配置文件是否有语法错误，转储并退出\nnginx -q \t  \t#在检测配置文件期间屏蔽非错误信息\nnginx -p prefix   \t#设置前缀路径(默认是:/usr/share/nginx/)\nnginx -c filename\t#设置配置文件(默认是:/etc/nginx/nginx.conf)\nnginx -g directives \t#设置配置文件外的全局指令\nkillall nginx\t\t#杀死所有nginx进程\n```\n\n## 应用\n\n### 架设简单文件服务器\n\n将 /data/public/ 目录下的文件通过 nginx 提供给外部访问\n\n```\n#mkdir /data/public/\n#chmod 777 /data/public/\n复制代码\nworker_processes 1;\nerror_log logs/error.log info;\nevents {\n    use epoll;\n}\nhttp {\n    server {\n        # 监听 8080 端口\n        listen 8080;\n        location /share/ {\n            # 打开自动列表功能，通常关闭\n            autoindex on;\n            # 将 /share/ 路径映射至 /data/public/，请保证 nginx 进程有权限访问 /data/public/\n            alias /data/public/;\n        }\n    }\n}\n复制代码\n```\n\n### nginx 正向代理\n\n- 正向代理指代理客户端访问服务器的一个中介服务器，代理的对象是客户端。正向代理就是代理服务器替客户端去访问目标服务器\n- 反向代理指代理后端服务器响应客户端请求的一个中介服务器，代理的对象是服务器。\n\n1. 配置\n\n代理服务器配置\n\nnginx.conf\n\n```\nserver{\n    resolver x.x.x.x;\n#       resolver 8.8.8.8;\n    listen 82;\n    location / {\n            proxy_pass http://$http_host$request_uri;\n    }\n    access_log  /data/httplogs/proxy-$host-aceess.log;\n}\n复制代码\n```\n\nlocation 保持原样即可，根据自己的配置更改 listen port 和 dnf 即 resolver 验证： 在需要访问外网的机器上执行以下操作之一即可：\n\n```\n1. export http_proxy=http://yourproxyaddress：proxyport（建议）\n2. vim ~/.bashrc\n    export http_proxy=http://yourproxyaddress：proxyport\n复制代码\n```\n\n2 不足 nginx 不支持 CONNECT 方法，不像我们平时用的 GET 或者 POST，可以选用 apache 或 squid 作为代替方案。\n\n### nginx 服务器基础配置实例\n\n```\nuser nginx nginx;\n\nworker_processes 3;\n\nerror_log logs/error.log;\npid myweb/nginx.pid;\n\nevents {\n    use epoll;\n    worker_connections 1024;\n}\n\nhttp {\n    include mime.types;\n    default_type applicatioin/octet-stream;\n\n    sendfile on;\n\n    keepalive_timeout 65;\n\n    log_format access.log '$remote_addr [$time_local] \"$request\" \"$http_user_agent\"';\n\n    server {\n        listen 8081;\n        server_name myServer1;\n\n        access_log myweb/server1/log/access.log;\n        error_page 404 /404.html;\n\n        location /server1/location1 {\n            root myweb;\n            index index.svr1-loc1.htm;\n        }\n\n        location /server1/location2 {\n            root myweb;\n            index index.svr1-loc2.htm;\n        }\n    }\n\n    server {\n        listen 8082;\n        server_name 192.168.0.254;\n\n        auth_basic \"please Login:\";\n        auth_basic_user_file /opt/X_nginx/nginx/myweb/user_passwd;\n\n        access_log myweb/server2/log/access.log;\n        error_page 404 /404.html;\n\n        location /server2/location1 {\n            root myweb;\n            index index.svr2-loc1.htm;\n        }\n\n        location /svr2/loc2 {\n            alias myweb/server2/location2/;\n            index index.svr2-loc2.htm;\n        }\n\n        location = /404.html {\n            root myweb/;\n            index 404.html;\n        }\n    }\n}\n复制代码\n#./sbin/nginx -c conf/nginx02.conf\nnginx: [warn] the \"user\" directive makes sense only if the master process runs with super-user privileges, ignored in /opt/X_nginx/nginx/conf/nginx02.conf:1\n.\n├── 404.html\n├── server1\n│   ├── location1\n│   │   └── index.svr1-loc1.htm\n│   ├── location2\n│   │   └── index.svr1-loc2.htm\n│   └── log\n│       └── access.log\n└── server2\n    ├── location1\n    │   └── index.svr2-loc1.htm\n    ├── location2\n    │   └── index.svr2-loc2.htm\n    └── log\n        └── access.log\n\n8 directories, 7 files\n复制代码\n```\n\n#### 测试 myServer1 的访问\n\n```\nhttp://myserver1:8081/server1/location1/\nthis is server1/location1/index.svr1-loc1.htm\n\nhttp://myserver1:8081/server1/location2/\nthis is server1/location1/index.svr1-loc2.htm\n复制代码\n```\n\n#### 测试 myServer2 的访问\n\n```\nhttp://192.168.0.254:8082/server2/location1/\nthis is server2/location1/index.svr2-loc1.htm\n\nhttp://192.168.0.254:8082/svr2/loc2/\nthis is server2/location1/index.svr2-loc2.htm\n\nhttp://192.168.0.254:8082/server2/location2/\n404 404 404 404\n复制代码\n```\n\n### 使用缓存\n\n创建缓存目录\n\n```\nmkdir  /tmp/nginx_proxy_cache2\nchmod 777 /tmp/nginx_proxy_cache2\n复制代码\n```\n\n修改配置文件\n\n```\n# http 区域下添加缓存区配置\nproxy_cache_path /tmp/nginx_proxy_cache2 levels=1 keys_zone=cache_one:512m inactive=60s max_size=1000m;\n\n# server 区域下添加缓存配置\n#缓存相应的文件（静态文件）\nlocation ~ \\.(gif|jpg|png|htm|html|css|js|flv|ico|swf)(.*) {\n     proxy_pass http://IP: 端口；#如果没有缓存则通过 proxy_pass 转向请求\n     proxy_redirect off;\n     proxy_set_header Host $host;\n     proxy_cache cache_one;\n     proxy_cache_valid 200 302 1h;            #对不同的 HTTP 状态码设置不同的缓存时间，h 小时，d 天数\n     proxy_cache_valid 301 1d;\n     proxy_cache_valid any 1m;\n     expires 30d;\n}\n复制代码\n```\n\n### 使用 location 反向代理到已有网站\n\n```\nlocation ~/bianque/(.*)$ {\n        proxy_pass http://127.0.0.1:8888/$1/?$args;\n    }\n```\n\n\u003e - 加内置变量 ![args 是保障 nginx 正则捕获 get 请求时不丢失，如果只是 post 请求，{INLINE_CODE_BLOCK_PLACEHOLDER}是非必须的\n\u003e - {INLINE_CODE_BLOCK_PLACEHOLDER} 取自正则表达式部分()里的内容\n\n### 其他\n\n#### ngx_http_sub_module 替换响应中内容\n\n- ngx_http_sub_module nginx 用来替换响应内容的一个模块（应用：有些程序中写死了端口，可以通过此工具将页面中的端口替换为其他端口）\n\n#### 配置 http 强制跳转 https\n\n在 nginx 配置文件中的 server 区域添加如下内容\n\n```\nif ($scheme = 'http') {\n    rewrite ^(.*)$ https://$host$uri;\n}\n```\n","lastmodified":"2023-05-09T16:33:58.279366106Z","tags":[]},"/Obsidian%E9%AB%98%E7%BA%A7%E6%8E%A2%E7%B4%A2":{"title":"Obsidian高级探索","content":"\n## 高级用法\n\n\u003e[!Note] 标注  \n\u003e[[标注]]\n\n[[Dataview]]\n\n[[YAML Front matter]]\n\n## 最重要的插件！\n\n看上 ob 最重要的当然是其和 vsc 媲美的插件系统，这里整理了一些我用的插件  \n\n| obsidian 插件 改成name而非id                       | 是否必装 | 原因                                                         | 待考察                                                         |\n| -------------------------------------------------- | -------- | ------------------------------------------------------------ | -------------------------------------------------------------- |\n| [[Word Splitting for Simplified Chinese in Edit Mode]] | y        | 和[[Omnisearch]]插件配合才能支持中文搜索                     |                                                                |\n| [[Advanced Slides]]                                | y        | 预览效果不错，也可以网页打开，基于 revealjs 已经相当成熟     |                                                                |\n| [[Advanced Tables]]                                | y        | 这个表格就是用它写的，贼方便                                 |                                                                |\n| [[Advanced URL]]                                   | y        | 别的软件link到obsidian，直达目的，[[万物互联]] 必备          |                                                                |\n| [[Bartender]]                                      |          | 编辑左侧和下方菜单显示和顺序，有用                           |                                                                |\n| [[Commander]]                                      | y        | 自定义ob界面的各种按钮                                       |                                                                |\n| [[DB Folder]]                                      |          |                                                              | 界面编辑用起来还是不舒服，没有notion的体验好                   |\n| [[Daily Notes Editor]]                             |          |                                                              | 批量处理每日日志，挺好用，但感觉要期待官方收编[[Calendar]]插件 |\n| [[Dataview]]                                       | y        | 待学习，有点麻烦，别人都说是神器                             |                                                                |\n| [[DigitalGarden]]                                  |          |                                                              | 替代官方发布的免费方案，后面有机会尝试                         |\n| [[ExcaliBrain]]                                    |          | 类 thebrain 的插件，给图谱引入父子和兄弟的关系               |                                                                |\n| [[Janitor]]                                        | y        | 目前最好的批量清理笔记插件                                   |                                                                |\n| [[LiveSync]]                                       |          | 通过自己的 server，进行多端同步                              |                                                                |\n| [[ProZen]]                                         | y        | 禅模式目前ob中最好的实现                                     |                                                                |\n| [[QuickAdd]]                                       |          |                                                              | 半自动化批量添加模板                                           |\n| [[Templater]]                                      |          | 快速插入模板，很重要                                         |                                                                |\n| [[Admonition\\|admonition]]                 |          | 引用美化，鲜活页面                                           | 美化quote样式，已经被内置[[Callout]] 取代                      |\n| [[auto-link-title]]                                | y        | 复制的网址自动获取其 title，爆赞                             |                                                                |\n| [[calendar]]                                       |          | 如果能坚持每日日记，必装                                     |                                                                |\n| [[Linter]]                                         | y        | 标准化、美化 markdown                                        |                                                                |\n| [[Supercharged Links]]                             | y        | 给链接加上样式，提高可识别性👍                               |                                                                |\n| [[Text expand]]                                    |          |                                                              |                                                                |\n| advanced-cursors                                   |          |                                                              | 目前不需要多光标操作                                           |\n| annotator                                          |          |                                                              | 插件太重，而且读书笔记做在这里太不轻便了                       |\n| chronology                                         |          | 以日历的形式显式文件编辑历史                                 |                                                                |\n| cmenu                                              |          | 编辑菜单很重要，而且支持定制                                 | [[cmenu学习笔记]]，貌似没多大用，用命令行就行了               |\n| code-block-copy                                    |          | 程序员必备：复制代码太方便了，以后应该会自带                 |                                                                |\n| creases                                            |          | 长文的局部折叠局部展开工具                                   |                                                                |\n| customizable-menu                                  |          | 自定义右键菜单                                               |                                                                |\n| customizable-page-header-buttons                   | y        | 可以给每个 page 加按钮，以及最右上角，有用，符合使用逻辑     |                                                                |\n| customizable-sidebar                               |          | 自定义侧边图标                                               |                                                                |\n| cycle-through-panes                                |          | 用快捷键快速在已打开的 pages 切换                            |                                                                |\n| dynamic-toc                                        |          | 动态目录，发布的时候很有用                                   |                                                                |\n| excalidraw-plugin                                  |          | 图中加入双链的场景很诱人，待持续探索                         | 没看到内置画图的必要性，可以用更专业工具                       |\n| find-unlinked-files                                |          | 批量整理悬空文件，有利于回顾                                 |                                                                |\n| focus-mode                                         | y        | 页面最大化或全局最大化都很有用                               |                                                                |\n| footnotes                                          |          | 便捷插入 footnote 有用                                       |                                                                |\n| hider                                              |          |                                                              | 就是美化用的，隐藏界面各个面板，但感觉没必要                   |\n| hover-editor                                       | y        | 预览的窗口也可以有更多操作                                   |                                                                |\n| icon-folder                                        |          | 给文件列表的文件夹和文件加 emoji，更有视觉美感和记忆感       |                                                                |\n| icons-plugin                                       |          | 很有用，各种网站 logo 都能快速搜索到                         |                                                                |\n| image-auto-upload-plugin                           | y        | 神器，markdown 一键粘贴图片                                  |                                                                |\n| link-favicon                                       |          | 给外链添加图标，增加可识别性，和 [[Supercharged Links]] 互补 |                                                                |\n| longform                                           |          | 将日记批量合并到月记里面，以及其他写小说场景                 |                                                                |\n| mrj-text-obsidian                                  | y        | 将搜索结果记录下来，从而实现 dataview 的补充效果             |                                                                |\n| note-refactor                                      |          |                                                              | [[note-refactor学习笔记]] 待真正场景使用                       |\n| notion-like-tables                                 |          | 通过最少侵入，引入好用表格                                   |                                                                |\n| obsidian-git                                       |          | 它自动刷 commit，挺好，目前自动的时候会卡                    | 不行，太卡了，自己自动 git 更好                                |\n| obsidian42-brat                                    | y        | 装未上架的插件方便                                           |                                                                |\n| outliner                                           |          | 装了它，高频使用的多级列表的编辑非常爽                       |                                                                |\n| pandoc                                             |          |                                                              | 目前还没用到导出功能                                           |\n| pane-relief                                        | y        | 给返回前进按钮加上历史记录                                   |                                                                |\n| periodic-notes                                     |          | 和 calendar 一样，能坚持必装                                 |                                                                |\n| quick-explorer                                     | y        | 在菜单栏显示当前文件所在文件夹路径，不知道是否有性能问题？|                                                                |\n| recent-files                                       | y        | 自带的 cmd +o 只能查看近 10 个，这个有 30 个而且有常驻按钮   |                                                                |\n| sliding-panes                                      | y        | 横向无限滑动场景有时候很有用                                 |                                                                |\n| spaced-repetition                                  | y        | 间隔复习，知识完美闭环，必装                                 | [[Spaced Repetition]]                                          |\n| tag-wrangler                                       | y        | 批量更改标签必用                                             |                                                                |\n| timelines                                          |          |                                                              | time 使用起来太麻烦了                                          |\n| tracker                                            |          |                                                              | 使用起来太麻烦了                                               |\n| various-complements                                | y        | 自动补全智能提示                                             |                                                                |\n| vault-statistics                                   |          | 快捷查看统计当前库的总数据                                   |                                                                |\n| weread-plugin                                      | y        | 将微信读书的笔记批量自动同步过来                             |                                                                |\n| zoom                                               |          |                                                              | 目前感觉不实用，而且有 bug 点击没反应有时                      |\n","lastmodified":"2023-05-09T16:33:58.279366106Z","tags":["obsidian"]},"/PKM":{"title":"PKM","content":"\n## personal knowledge management 我认为的 pkm\n\n一切以满足自我的需要为主的个人知识管理库\n\n- 满足将各方面所学科学的记录下来\n- 拥有 tag 和索引，能够快速查找\n- 拥有自动化数据统计，了解效率情况\n","lastmodified":"2023-05-09T16:33:58.279366106Z","tags":["花园"]},"/PM2":{"title":"PM2","content":"\n## 简介\n\n对于线上项目，如果直接通过 node app 来启动，如果报错了可能直接停止导致整个服务崩溃，一般监控 node 有几种方案。\n\n- supervisor: 一般用作开发环境的使用。\n- forever: 管理多个站点，一般每个站点的访问量不大的情况，不需要监控。\n- PM2: 一个进程管理工具，维护一个进程列表，可以用它来管理你的node进程，负责所有正在运行的进程，并查看node进程的状态，也支持性能监控，负载均衡等功能。\n\n## PM2 的主要特性\n\n- 内建负载均衡（使用 Node cluster 集群模块）\n- 后台运行\n- 0 秒停机重载，我理解大概意思是维护升级的时候不需要停机.\n- 具有 Ubuntu 和 CentOS 的启动脚本\n- 停止不稳定的进程（避免无限循环）\n- 控制台检测\n- 提供 HTTP API\n- 远程控制和实时的接口 API ( Nodejs 模块,允许和 PM2 进程管理器交互 )\n\n## 安装\n\n```shell\n// 全局安装pm2，依赖node和npm\nnpm install -g pm2\n```\n\n## 常用命令\n\n### PM2 start\n\n#### 启动一个node程序\n\n```shell\npm2 start start.js\n//Or start any other application easily:\n$ pm2 start bashscript.sh\n$ pm2 start python-app.py --watch\n$ pm2 start binary-file -- --port 1520\n```\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rad-figure-bed/PicGo/blogs/pm2/20200507163000)\n\n#### 启动进程并指定应用的程序名\n\n```shell\npm2 start app.js --name application1\n```\n\n#### 集群模式启动\n\n```shell\n// -i 表示 number-instances 实例数量\n// max 表示 PM2将自动检测可用CPU的数量 可以自己指定数量\npm2 start start.js -i max\n```\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rad-figure-bed/PicGo/blogs/pm2/20200507162950)\n\n#### 添加程监视\n\n```shell\n// 在文件改变的时候会重新启动程序\npm2 start app.js --name start --watch\n复制代码\n```\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rad-figure-bed/PicGo/blogs/pm2/20200507162941)\n\n#### 其他Options\n\n```shell\n# Specify an app name\n--name \u003capp_name\u003e\n\n# Watch and Restart app when files change\n--watch\n\n# Set memory threshold for app reload\n--max-memory-restart \u003c200MB\u003e\n\n# Specify log file\n--log \u003clog_path\u003e\n\n# Pass extra arguments to the script\n-- arg1 arg2 arg3\n\n# Delay between automatic restarts\n--restart-delay \u003cdelay in ms\u003e\n\n# Prefix logs with time\n--time\n\n# Do not auto restart app\n--no-autorestart\n\n# Specify cron for forced restart\n--cron \u003ccron_pattern\u003e\n\n# Attach to application log\n--no-daemon\n```\n\n### 列出所有进程\n\n```shell\n$ pm2 [list|ls|status]\n```\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rad-figure-bed/PicGo/blogs/pm2/20200507154916.png)\n\n### 重启、删除、停止、重新加载进程\n\n```shell\n$ pm2 restart app_name\n$ pm2 reload app_name\n$ pm2 stop app_name\n$ pm2 delete app_name\n```\n\n除了使用`app_name`,还可以：\n\n- `all` to act on all processes\n- `id` to act on a specific process id\n\n### 查看状态、日志、指标\n\n#### 日志\n\nTo display logs in realtime:\n\n```shell\n$ pm2 logs\n```\n\nTo dig in older logs:\n\n```shell\n$ pm2 logs --lines 200\n```\n\n#### 自适应监控面板\n\n```shell\n$ pm2 monit\n```\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rad-figure-bed/PicGo/blogs/pm2/20200507155125.png)\n\n还有个Web版\n\n```shell\n$ pm2 plus\n```\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rad-figure-bed/PicGo/blogs/pm2/20200507155220.png)\n\n### 查看某个进程具体情况\n\n```shell\npm2 describe app\n```\n\n![img](https://user-gold-cdn.xitu.io/2018/8/26/16574b5b3d899dd4?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)\n\n### 设置pm2开机自启\n\n开启启动设置，此处是CentOS系统，其他系统替换最后一个选项（可选项：ubuntu, centos, redhat, gentoo, systemd, darwin, amazon）\n\n```shell\npm2 startup centos \n```\n\n然后按照提示需要输入的命令进行输入\n\n最后保存设置\n\n```shell\npm2 save\n```\n\n### 官方推荐命令\n\n```shell\n# Fork mode\npm2 start app.js --name my-api # Name process\n\n# Cluster mode\npm2 start app.js -i 0        # Will start maximum processes with LB depending on available CPUs\npm2 start app.js -i max      # Same as above, but deprecated.\npm2 scale app +3             # Scales `app` up by 3 workers\npm2 scale app 2              # Scales `app` up or down to 2 workers total\n\n# Listing\n\npm2 list               # Display all processes status\npm2 jlist              # Print process list in raw JSON\npm2 prettylist         # Print process list in beautified JSON\n\npm2 describe 0         # Display all informations about a specific process\n\npm2 monit              # Monitor all processes\n\n# Logs\n\npm2 logs [--raw]       # Display all processes logs in streaming\npm2 flush              # Empty all log files\npm2 reloadLogs         # Reload all logs\n\n# Actions\n\npm2 stop all           # Stop all processes\npm2 restart all        # Restart all processes\n\npm2 reload all         # Will 0s downtime reload (for NETWORKED apps)\n\npm2 stop 0             # Stop specific process id\npm2 restart 0          # Restart specific process id\n\npm2 delete 0           # Will remove process from pm2 list\npm2 delete all         # Will remove all processes from pm2 list\n\n# Misc\n\npm2 reset \u003cprocess\u003e    # Reset meta data (restarted time...)\npm2 updatePM2          # Update in memory pm2\npm2 ping               # Ensure pm2 daemon has been launched\npm2 sendSignal SIGUSR2 my-app # Send system signal to script\npm2 start app.js --no-daemon\npm2 start app.js --no-vizion\npm2 start app.js --no-autorestart\n```\n\n## 管理多个应用\n\n您还可以创建一个名为生态系统文件的配置文件来管理多个应用程序。生成生态系统文件:\n\n```shell\n$ pm2 ecosystem\n```\n\n生成`ecosystem.config.js`文件：\n\n```javascript\nmodule.exports = {\n  apps : [{\n    name: \"app\",\n    script: \"./app.js\",\n    env: {\n      NODE_ENV: \"development\",\n    },\n    env_production: {\n      NODE_ENV: \"production\",\n    }\n  }, {\n     name: 'worker',\n     script: 'worker.js'\n  }]\n}\n```\n\nAnd start it easily:\n\n```shell\n$ pm2 start process.yml\n```\n\nRead more about application declaration [here](https://pm2.keymetrics.io/docs/usage/application-declaration/).\n\n## 通过pm2配置文件来自动部署项目\n\n[官网指南](https://pm2.keymetrics.io/docs/usage/deployment/)\n\n### 首先是配置服务器与Github的ssh:\n\n1. 在服务器中生成rsa公钥和私钥，当前是 **centos7** 下进行\n2. 前提服务器要安装git，没有安装的先安装git，已安装的跳过\n\n   ```\n   yum –y install git\n   ```\n\n3. 生成秘钥\n\n   ```\n   ssh-keygen -t rsa -C \"xxx@xxx.com\"\n   ```\n\n   在~/.ssh目录下有 id_rsa和 id_rsa.pub两个文件，其中id_rsa.pub文件里存放的即是公钥key。\n\n4. 登录到GitHub，点击右上方的头像，选择settings ，点击Add SSH key，把id_rsa.pub的内容复制到里面即可。\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rad-figure-bed/PicGo/blogs/pm2/20200507160716)\n\n### 本地项目PM2配置文件\n\n```shell\npm2 ecosystem\n```\n\n在项目跟目录下运行，会自动生成模板文件:\n\n```json\n{\n  // Applications part\n  \"apps\" : [{\n    \"name\"      : \"API\",\n    \"script\"    : \"app.js\",\n    \"env\": {\n      \"COMMON_VARIABLE\": \"true\"\n    },\n    // Environment variables injected when starting with --env production\n    // http://pm2.keymetrics.io/docs/usage/application-declaration/#switching-to-different-environments\n    \"env_production\" : {\n      \"NODE_ENV\": \"production\"\n    }\n  },{\n    \"name\"      : \"WEB\",\n    \"script\"    : \"web.js\"\n  }],\n  // Deployment part\n  // Here you describe each environment\n  \"deploy\" : {\n    \"production\" : {\n      // 服务器的用户名\n      \"user\" : \"node\",\n      // Multi host is possible, just by passing IPs/hostname as an array\n      \"host\" : [\"212.83.163.1\", \"212.83.163.2\", \"212.83.163.3\"],\n      // 要拉取的git分支\n      \"ref\"  : \"origin/master\",\n      // Git repository to clone\n      \"repo\" : \"git@github.com:repo.git\",\n      // 拉取到服务器某个目录下\n      \"path\" : \"/var/www/production\",\n      // Can be used to give options in the format used in the configura-\n      // tion file.  This is useful for specifying options for which there\n      // is no separate command-line flag, see 'man ssh'\n      // can be either a single string or an array of strings\n      \"ssh_options\": \"StrictHostKeyChecking=no\",\n      // To prepare the host by installing required software (eg: git)\n      // even before the setup process starts\n      // can be multiple commands separated by the character \";\"\n      // or path to a script on your local machine\n      \"pre-setup\" : \"apt-get install git\",\n      // Commands / path to a script on the host machine\n      // This will be executed on the host after cloning the repository\n      // eg: placing configurations in the shared dir etc\n      \"post-setup\": \"ls -la\",\n      // Commands to execute locally (on the same machine you deploy things)\n      // Can be multiple commands separated by the character \";\"\n      \"pre-deploy-local\" : \"echo 'This is a local executed command'\"\n      // Commands to be executed on the server after the repo has been cloned\n      \"post-deploy\" : \"npm install \u0026\u0026 pm2 startOrRestart ecosystem.json --env production\"\n      // Environment variables that must be injected in all applications on this env\n      \"env\"  : {\n        \"NODE_ENV\": \"production\"\n      }\n    },\n    \"staging\" : {\n      \"user\" : \"node\",\n      \"host\" : \"212.83.163.1\",\n      \"ref\"  : \"origin/master\",\n      \"repo\" : \"git@github.com:repo.git\",\n      \"path\" : \"/var/www/development\",\n      \"ssh_options\": [\"StrictHostKeyChecking=no\", \"PasswordAuthentication=no\"],\n      \"post-deploy\" : \"pm2 startOrRestart ecosystem.json --env dev\",\n      \"env\"  : {\n        \"NODE_ENV\": \"staging\"\n      }\n    }\n  }\n}\n```\n\n\u003e  关于 `post-deploy`\n\u003e\n\u003e you may have noticed the command `pm2 startOrRestart ecosystem.json --env production`. The `--env ` allows to inject different sets of environment variables.\n\u003e\n\u003e Read more [here](http://pm2.keymetrics.io/docs/usage/application-declaration/#switching-to-different-environments).\n\n按照自己要求修改好后，就可以部署啦：\n\n```shell\npm2 deploy \u003cconfiguration_file\u003e \u003cenvironment\u003e setup\n```\n\n如:(windows 记得使用 git bash 等unix命令行)\n\n```shell\npm2 deploy ecosystem.json production setup # 这个命令将会在远程服务器上创建文件\n```\n\n### pm2 deploy\n\n `pm2 deploy help`:\n\n```\npm2 deploy \u003cconfiguration_file\u003e \u003cenvironment\u003e \u003ccommand\u003e\n\n  Commands:\n    setup                run remote setup commands\n    update               update deploy to the latest release\n    revert [n]           revert to [n]th last deployment or 1\n    curr[ent]            output current release commit\n    prev[ious]           output previous release commit\n    exec|run \u003ccmd\u003e       execute the given \u003ccmd\u003e\n    list                 list previous deploy commits\n    [ref]                deploy to [ref], the \"ref\" setting, or latest tag\n```\n","lastmodified":"2023-05-09T16:33:58.279366106Z","tags":[]},"/Python":{"title":"Python目录","content":"\n[[强化学习课程-李宏毅]]\n\n[[特征工程]]\n\n[[GYM]]\n\n[[Python基础]]\n\n[[Pytorch]]\n\n[[Ray]]\n\n[[RL琐碎]]\n","lastmodified":"2023-05-09T16:33:58.279366106Z","tags":[]},"/Python%E5%9F%BA%E7%A1%80":{"title":"Python基础","content":"\n## 1 系统中的python\n\n1. 查看系统bin目录下所有的python\n\n   `ls /usr/bin/python*`\n\n## 2 源\n\n1. pip\n\n   配置文件位置：/Users/ericx/.config/pip/pip.conf\n\n   https://mirrors.tuna.tsinghua.edu.cn/help/pypi/\n\n   ```bash\n   # 设置源\n   pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n   \n   # 查看配置\n   pip config list\n   ```\n\n2. conda\n\n   配置文件位置：/Users/ericx/.condarc\n\n   https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/ 查看最新的即可\n\n   \u003e 恢复默认：`conda config --remove-key channels`\n\n# 3 创建环境\n\n### 1 virtualenv\n\n[virtutalenv](https://virtualenv.pypa.io/en/latest/reference/)\n\n`pip3 install virtualenv  `\n\n1. 创建虚拟环境目录  \n   `mkdir myproject cd myproject`\n\n2. 创建一个独立的Python运行环境: myenv  \n   `virtualenv --no-site-packages myenv `\n\n   \u003e 1. 参数：--no-site-packages （可省略） 其意义在于不复制已经安装到系统Python环境中的所有第三方包从而得到一个“纯净”的运行环境。\n   \u003e 2. 此时当前目录下会生成一个名为：“myenv” 的目录，该目录中存放刚生成的虚拟环境文件\n\n3. 激活虚拟运行环境:\n\n   ```bash\n   # Windows：\n   myenv\\Scripts\\activate.bat\n   # Linux/Mac:\n   source myenv/bin/activate\n   ```\n\n   \u003e **注：** 以上命令执行完后会发现命令提示符变了，有个(myenv)前缀，表示当前环境是一个名为“myenv”的Python环境，此时可以在该环境中按照我们熟悉的方式安装库、运行程序等。\n\n4. 安装各种第三方包，并运行Python命令:\n\n   ```bash\n   pip install jieba\n   python myapp.py\n   ```\n\n   \u003e **注：** 因为此时虚拟环境已经激活，所以可以直接使用命令`pip`、`python`而不是`pip3`和`python3`。\n\n5. 使用`deactivate`命令退出当前的myenv环境。\n\n### 2 venv\n\nPython 从3.3 版本开始，自带了一个虚拟环境 venv，在 [PEP-405](https://legacy.python.org/dev/peps/pep-0405/) 中可以看到它的详细介绍。它的很多操作都和 virtualenv 类似，但是两者运行机制不同。因为是从 3.3 版本开始自带的，这个工具也仅仅支持 python 3.3 和以后版本。所以，要在 python2 上使用虚拟环境，依然要利用 virtualenv 。\n\n\u003e 有关venv与virtualenv的区别，virtualenv官网: https://virtualenv.pypa.io/en/latest/reference/#compatibility-with-the-stdlib-venv-module 有详细介绍。\n\n1. 安装\n\n   ```bash\n   # Windows 中venv已经以标准库的形式存在，不用再单独安装\n   # Linux\n   sudo apt-get install python3-venv  # 如有不同版本的Python3,可指定具体版本venv：python3.5-venv\n   ```\n\n2. 在当前目录创建一个独立的Python运行环境: myenv\n\n   ```bash\n   # Windows\n   py -3 -m venv myenv  \n   # Linux \n   python3 -m venv myenv\n   ```\n\n3. **激活虚拟运行环境、安装第三方包、运行程序及退出虚拟环境等操作均与 virtualenv 相同，在此不再赘述。**\n\n### 3 pipenv\n\npipenv 是 Pipfile 主要倡导者、requests 作者 Kenneth Reitz 写的一个命令行工具，主要包含了Pipfile、pip、click、requests和virtualenv，能够有效管理Python多个环境，各种第三方包及模块。\n\n\u003e 1. pipenv集成了pip，virtualenv两者的功能，且完善了两者的一些缺陷。\n\u003e 2. 过去用virtualenv管理requirements.txt文件可能会有问题，Pipenv使用Pipfile和Pipfile.lock，后者存放将包的依赖关系，查看依赖关系是十分方便。\n\u003e 3. 各个地方使用了哈希校验，无论安装还是卸载包都十分安全，且会自动公开安全漏洞。\n\u003e 4. 通过加载.env文件简化开发工作流程。\n\u003e 5. 支持Python2 和 Python3，在各个平台的命令都是一样的。\n\n**Pipenv 所解决的问题:**\n\n1. requirements.txt 依赖管理的局限  \n   使用 requirements.txt 管理依赖的时候可能会出现 **不确定构建 (the build isn’t deterministic)** 问题，举个栗子:如果程序中要使用 TensorFlow 库，那么我们应该在requirements.txt里面写上:\n\n   ```bash\n   # requirements.txt:\n   TensorFlow \n   ...\n   ```\n\n   由于没有指定版本，因此通过指令 `pip install -r requirements.txt`安装依赖模块时，会默认安装最新版本的TensorFlow ，如果新版本向后兼容，这完全不用care，但是如果不能向后兼容就会出现：代码无法在该环境运行，即测试环境和生产环境的同一份requirement.txt，结果出来2份不同的环境。再考虑模块本身的依赖等问题，会发现越来越令人头大。而Pipenv使用的Pipfile和Pipfile.lock文件则可以比较好的解决这些问题。\n\n2. 多个项目依赖不同第三方库、包版本问题  \n如应用程序A需要特定模块的1.0版本但应用程序B需要2.0版本，当我在A和B程序间切换时，需要不断检测–卸载–安装模块。这意味着只安装其中一个版本可能无法满足每个应用程序的要求，因此需要创建虚拟环境来将A、B程序所需的第三方包分隔开来，此时常用的 virtualenv、venv均可以满足要求，Pipenv也同样集成了该功能。\n\n**使用**\n\n1. 安装：\n\n   ```bash\n   # pip 安装\n   pip3 install pipenv  # 全局安装，如果只想在当前用户模式下安装，可添加参数：--user\n   # 如需更新可使用\n   pip3 install --user --upgrade pipenv\n   ```\n\n   \u003e 注: 如果在使用当前用户模式下安装pipenv后在shell中提示不可用，则需要把用户库的目录“C:\\Users\\XXX\\AppData\\Roaming\\Python\\Python37\\Scripts”添加到你的PATH【系统环境路径】中，然后重启电脑，环境变量生效后即可使用。\n\n2. **Pipenv 常用命令**\n\n   ```bash\n   pipenv --two  # 使用当前系统中的Python2 创建环境\n   pipenv --three  # 使用当前系统中的Python3 创建环境\n   \n   pipenv --python 3  # 指定使用Python3创建环境\n   pipenv --python 3.6  # 指定使用Python3.6创建环境\n   pipenv --python 2.7.14  # 指定使用Python2.7.14创建环境\n   ```\n\n   \u003e 1. 创建环境时应使用系统中已经安装的、能够在环境变量中搜索到的Python 版本，否则会报错。\n   \u003e 2. 每次创建环境都会在当前目录下生成一个名为Pipfile文件，用来记录刚创建的环境信息，如果当前目录下之前存在该文件，会将其覆盖。\n   \u003e 3. 在使用指定版本创建环境的时候，版本号与参数 --python 之间有个空格。\n\n   ```bash\n   pipenv shell  # 激活虚拟环境\n   pipenv --where  # 显示目录信息\n   pipenv --venv  # 显示虚拟环境信息\n   pipenv --py  # 显示Python解释器信息\n   \n   pipenv install XXX  # 安装XXX模块并加入到Pipfile\n   pipenv install XXX==1.11  # 安装固定版本的XXX模块并加入到Pipfile\n   \n   pipenv graph  # 查看目前安装的库及其依赖\n   \n   pipenv check  # 检查安全漏洞\n   \n   pipenv update --outdated  # 查看所有需要更新的依赖项\n   pipenv update  # 更新所有包的依赖项\n   pipenv update \u003c包名\u003e  # 更新指定的包的依赖项\n   \n   pipenv uninstall XXX  # 卸载XXX模块并从Pipfile中移除\n   pipenv uninstall --all  # 卸载全部包并从Pipfile中移除\n   pipenv uninstall --all-dev  # 卸载全部开发包并从Pipfile中移除\n   \n   exit  # 退出当前虚拟环境\n   \n   pipenv --rm  # 删除虚拟环境\n   ```\n\n3. **requirements.txt 文件的兼容**\n\n   - pipenv可以像virtualenv一样用命令生成requirements.txt 文件\n\n     ```bash\n     pipenv lock -r \u003e requirements.txt  # 将Pipfile和Pipfile.lock文件里面的包导出为requirements.txt文件\n     pipenv lock -r --dev \u003e requirements.txt  # 将Pipfile和Pipfile.lock文件里面的开发包导出为requirements.txt文件\n     ```\n\n- pipenv 通过requirements.txt安装包\n\n  ```bash\n  pipenv install -r requirements.txt\n  pipenv install -r --dev requirements.txt  # 只安装开发包\n  ```\n\n4. **Python 文件的运行**  \n`pipenv run python xxx.py`或\n\n```bash\n# 进入激活环境\npipenv shell\n# 运行文件\npython xxx.py\n```\n\n### 4 conda\n\n```bash\n# 查看当前存在哪些虚拟环境\nconda env list \nconda info -e\n\n# 查看安装了哪些包\nconda list\n\n# 创建/删除虚拟环境\nconda create -n your_env_name python=3.8 # 注意加上版本\nconda remove -n your_env_name --all\n\n# 激活环境\nconda activate your_env_name\n\n安装的anaconda环境默认启动base环境，想要关闭，linux设置如下:\nconda config --set auto_activate_base false # 设置非自动启动\n```\n","lastmodified":"2023-05-09T16:33:58.279366106Z","tags":[]},"/Pytorch":{"title":"Pytorch","content":"\n![在这里插入图片描述](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/pytorch/20210315161856.png)\n\n### torch.dtype\n\n`torch.dtype`是表示`torch.Tensor`的数据类型的对象。`PyTorch`有八种不同的数据类型：\n\n| Data type                | dtype                         | Tensor types         |\n| ------------------------ | ----------------------------- | -------------------- |\n| 32-bit floating point    | torch.float32 or torch.float  | torch.*.FloatTensor  |\n| 64-bit floating point    | torch.float64 or torch.double | torch.*.DoubleTensor |\n| 16-bit floating point    | torch.float16 or torch.half   | torch.*.HalfTensor   |\n| 8-bit integer (unsigned) | torch.uint8                   | torch.*.ByteTensor   |\n| 8-bit integer (signed)   | torch.int8                    | torch.*.CharTensor   |\n| 16-bit integer (signed)  | torch.int16 or torch.short    | torch.*.ShortTensor  |\n| 32-bit integer (signed)  | torch.int32 or torch.int      | torch.*.IntTensor    |\n| 64-bit integer (signed)  | torch.int64 or torch.long     | torch.*.LongTensor   |\n","lastmodified":"2023-05-09T16:33:58.279366106Z","tags":[]},"/RL%E7%90%90%E7%A2%8E":{"title":"基础","content":"\n# 基础\n\n**episode**：从一个游戏开始到结束，叫做一个episode\n\n**loss**:可以作为负的reward\n\n**Monte-Carlo(MC)**:\n\n# 分类\n\n1. 是否理解环境？  \n   不理解环境：不尝试去理解环境，环境给什么就是什么 Model-free  \n   理解环境：为真实世界建模 Model-based  \n   Model-based 就是在model free的基础上多一个虚拟环境\n2. 基于Policy-Based与基于Value-Based  \n   基于概率：直接输出下一步要采取动作的概率，根据概率选取行动，可以支持连续动作 Policy Gradients  \n   基于价值（连续动作无能为力）：而基于价值的方法输出则是所有动作的价值, 根据最高价值来选着动作 Q-Learning Sarsa  \n   基于价值的谁价值高选谁，基于概率根据概率执行动作\n3. 更新  \n   回合更新：回合结束后更新行为准则 基础Policy Gradients Monte-carlo Learning  \n   单步更新：每一步都更新准则 Q-Learning Sarsa 升级版Policy Gradients\n4. 在线与否  \n   在线学习 边玩边学，sarsa、sarsa(lambda)  \n   离线学习 学完再玩，Q Learning、Deep Q Network\n\n![img](watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RheWRheWp1bXA=,size_16,color_FFFFFF,t_70.png)\n\n现在，如果我们知道MDP中的所有东西，那么我们可以不用在环境中做出动作便可直接求解，我们通常称在执行动作前作出的决策为规划(planning)，那么一些经典的规划算法能够直接求解MDP问题，包括值迭代和策略迭代等。\n\n那么，当agent不知道转移概率函数TT和奖励函数RR，它是如何找到一个好的策略的呢，当然会有很多方法：\n\n### Model-based RL\n\n一种方法就是Model-based方法，让agent学习一种模型，这种模型能够从它的观察角度描述环境是如何工作的，然后利用这个模型做出动作规划，具体来说，当agent处于s1s1状态，执行了a1a1动作，然后观察到了环境从s1s1转化到了s2s2以及收到的奖励rr, 那么这些信息能够用来提高它对T(s2|s1,a1)T(s2|s1,a1)和R(s1,a1)R(s1,a1)的估计的准确性，当agent学习的模型能够非常贴近于环境时，它就可以直接通过一些规划算法来找到最优策略，具体来说：当agent已知任何状态下执行任何动作获得的回报，即R(st,at)R(st,at)已知，而且下一个状态也能通过T(st+1|st,at)T(st+1|st,at)被计算，那么这个问题很容易就通过动态规划算法求解，尤其是当T(st+1|st,at)＝1T(st+1|st,at)＝1时，直接利用贪心算法，每次执行只需选择当前状态stst下回报函数取最大值的动作(maxaR(s,a|s=st)maxaR(s,a|s=st))即可，这种采取对环境进行建模的强化学习方法就是Model-based方法\n\n### Model free RL\n\n但是，事实证明，我们有时候并不需要对环境进行建模也能找到最优的策略，一种经典的例子就是Q-learning，Q-learning直接对未来的回报Q(s,a)Q(s,a)进行估计，Q(sk,ak)Q(sk,ak)表示对sksk状态下执行动作atat后获得的未来收益总和E(∑nt=kγkRk)E(∑t=knγkRk)的估计，若对这个Q值估计的越准确，那么我们就越能确定如何选择当前stst状态下的动作：选择让Q(st,at)Q(st,at)最大的atat即可，而Q值的更新目标由Bellman方程定义，更新的方式可以有TD（Temporal Difference）等，这种是基于值迭代的方法，类似的还有基于策略迭代的方法以及结合值迭代和策略迭代的actor-critic方法，基础的策略迭代方法一般回合制更新（Monte Carlo Update），这些方法由于没有去对环境进行建模，因此他们都是Model-free的方法\n\n所以，如果你想查看这个强化学习算法是model-based还是model-free的，你就问你自己这个问题：在agent执行它的动作之前，它是否能对下一步的状态和回报做出预测，如果可以，那么就是model-based方法，如果不能，即为model-free方法。\n\n### Policy-Based\u0026Value-Based\n\nPolicy-Based的方法直接输出下一步动作的概率，根据概率来选取动作。但不一定概率最高就会选择该动作，还是会从整体进行考虑。适用于非连续和连续的动作。常见的方法有**policy gradients**。\n\nValue-Based的方法输出的是动作的价值，选择价值最高的动作。适用于非连续的动作。常见的方法有**Q-learning**和Sarsa。\n\n\u003cimg src=\"../../pics/image-20211113200432205.png\" alt=\"image-20211113200432205\" style=\"zoom:50%;\" /\u003e\n\n更为厉害的方法是二者的结合：Actor-Critic，Actor根据概率做出动作，Critic根据动作给出价值，从而加速学习过程。\n\n\u003cimg src=\"../../pics/image-20211113200459813.png\" alt=\"image-20211113200459813\" style=\"zoom:20%;\" /\u003e\n\n### On-policy\u0026Off-policy\n\n更新值函数时是否**只使用**当前策略所产生的样本\n\n**Q-learning, Deterministic policy gradient是Off-police算法**,这是因为他们更新值函数时,不一定使用当前策略$\\pi_t$产生的样本. 可以回想DQN算法,其包含一个replay memory.这个经验池中存储的是很多历史样本(包含$\\pi_1 ,\\pi_2,…,\\pi_t$的样本 ),而更新Q函数时的target用的样本是从这些样本中采样而来,因此,其并不一定使用当前策略的样本.\n\nReinforce, trpo, sarsa都是On-policy,这是因为他们更新值函数时,只能使用当前策略产生的样本.具体的,reinforce的梯度更新公式中![image-20211113205001938](image-20211113205001938.png) ,这里的R就是整个episode的累积奖赏,它用到的样本必然只是来自于$\\pi_t$。\n\n# 算法\n\n### Q-learning\n\nQ-learning算法最主要的就是Q表格,里面存着每个状态的动作价值。然后用Q表格用来指导每一步的动作。并且每走一步,就更新一次Q表格,也就是说用下一个状态的Q值去更新当前状态的Q值。\n\n### DQN\n\nDeep Q Network(DQN)的本质其实是Q-learning算法,改进就是把Q表格换成了神经网络,向神经网络输入状态state,就能输出所有状态对应的动作action。\n\n### Policy Gradient\n\n在讲PG算法前,我们需要知道的是,在强化学习中,有两大类方法,一种基于值（Value-based）,一种基于策略（Policy-based）:\n\n![在这里插入图片描述](watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3picF8xMjEzOA==,size_16,color_FFFFFF,t_70.png)\n\nValue-based的算法的典型代表为Q-learning和SARSA,将Q函数优化到最优,再根据Q函数取最优策略;Policy-based的算法的典型代表为Policy Gradient,直接优化策略函数。\n\n\u003e 可以举一个例子区分这两种方法:如果用DQN玩剪刀石头布这种随机性很大的游戏,很可能训练到最后,一直输出同一个动作;但是用Policy Gradient的话,优化到最后就会发现三个动作的概率都是一样的。\n\n可以通过类比监督学习的方式来理解Policy Gradient。向神经网络输入状态state,输出的是每个动作的概率,然后选择概率最高的动作作为输出。训练时,要不断地优化概率,尽可能地使输出值的概率逼近1。\n\n### DPG\n\nDeterministic policy gradient(DPG)算法可以理解为PG+DQN,它是首次能处理确定性的连续动作空间问题的算法。要学习DPG算法,就要知道**Actor-Critic结构**,Actor的前生是Policy Gradient,可以在连续动作空间内选择合适的动作action;Critic的前生是DQN或者其他的以值为基础的算法，可以进行单步更新，效率更高。Actor基于概率分布选择行为,Critic基于Actor生成的行为评判得分,Actor再根据Critic的评分修改选行为的概率。DPG就是在Actor-Critic结构上做的改进,让Actor输出的action是确定值而不是概率分布。\n\n### DDPG\n\nDeep Deterministic Policy Gradient(DDPG)算法可以理解为DPG+DQN。因为Q网络的参数在频繁更新梯度的同时，又用于计算Q网络和策略网络的梯度,所以Q网络是不稳定的,所以为了稳定Q网络,DDPG分别给策略网络和Q网络都搭建了一个目标网络,专门用来稳定Q网络:\n\n![在这里插入图片描述](20200719091044431.png)\n\n### MADDPG\n\nMulti-Agent Deep Deterministic Policy Gradient\n\n![image-20211113191625880](image-20211113191625880.png)\n\n简单来看,MADDPG其实就是在DDPG的基础上,解决一个环境里存在多个智能体的问题。\n\n像Q-Learning或者policy gradient都不适用于多智能体环境。主要的问题是,在训练过程中,每个智能体的策略都在变化,因此从每个智能体的角度来看,环境变得十分不稳定,其他智能体的行动带来环境变化:\n\n- 对DQN算法来说,经验回放的方法变的不再适用,因为如果不知道其他智能体的状态,那么不同情况下自身的状态转移会不同。\n- 对PG算法来说,环境的不断变化导致了学习的方差进一步增大。\n\n**在单智能体强化学习中,智能体所在的环境是稳定不变的,但是在多智能体强化学习中,环境是复杂的、动态的**,因此给学习过程带来很大的困难。我理解的多智能体环境是一个环境下存在多个智能体,并且每个智能体都要互相学习,合作或者竞争。\n\n\u003e 比较有意思的环境是OpenAI的捉迷藏环境，主要讲的是两队开心的小朋友agents在玩捉迷藏游戏中经过训练逐渐学到的各种策略:\n\u003e\n\u003e ![在这里插入图片描述](watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3picF8xMjEzOA==,size_16,color_FFFFFF,t_70-20211113192651170.png)\n","lastmodified":"2023-05-09T16:33:58.279366106Z","tags":[]},"/Ray":{"title":"my","content":"\n# my\n\n```python\n'checkpoint_at_end' = {bool} True\n'checkpoint_freq' = {int} 4\n'max_failures' = {int} 1000\n'resume' = {bool} False\n'export_formats' = {list: 2} ['model', 'checkpoint']\n'stop' = {dict: 1} {'time_total_s': 14400}\n'config' = {dict: 8} {\n  'log_level': 'ERROR', \n  'num_workers': 1, \n  'num_gpus': 0, \n  'horizon': 1000, \n  'env': \u003cclass 'baselines.marl_benchmark.wrappers.rllib.frame_stack.FrameStack'\u003e,\n  'multiagent': {}\n  'env_config' = {dict: 5} {\n  \t'custom_config':{\n      'name': 'FrameStack', \n    \t'num_stack': 3, \n    \t'reward_adapter': \u003cfunction FrameStack.get_reward_adapter.\u003clocals\u003e.func at 0x7fab767d09e0\u003e, \n    \t'observation_adapter': \u003cfunction FrameStack.get_observation_adapter.\u003clocals\u003e.func at 0x7fab767d0b00\u003e, \n    \t'action_adapter': \u003cfunction ActionAdapter.discrete_action_adapter at 0x7fab7939ab90\u003e, \n    \t'info_adapter': None, \n    \t'observation_space': '',\n    \t'action_space': Discrete(4)\n    },\n    'seed':42,\n    'headless':False,\n    'scenarios':['/home/LiKuiHao/pycharm/s6/baselines/marl_benchmark/scenarios/two_ways/bid'],\n    'agent_specs':{'default_policy': \n                   AgentSpec(\n                     interface=AgentInterface(debug=False, event_configuration=EventConfiguration(not_moving_time=60, not_moving_distance=1), done_criteria=DoneCriteria(collision=True, off_road=True, off_route=True, on_shoulder=False, wrong_way=False, not_moving=False, agents_alive=None), max_episode_steps=1000, neighborhood_vehicles=NeighborhoodVehicles(radius=50), waypoints=Waypoints(lookahead=50), road_waypoints=False, drivable_area_grid_map=False, ogm=False, rgb=False, lidar=False, action=\u003cActionSpaceType.Lane: 1\u003e, vehicle_type='sedan', accelerometer=Accelerometer()), \n                     agent_builder=None, \n                     agent_params=None, \n                     observation_adapter=\u003cfunction AgentSpec.\u003clambda\u003e at 0x7fab786d8a70\u003e, \n                     action_adapter=\u003cfunction AgentSpec.\u003clambda\u003e at 0x7fab786d8c20\u003e, \n                     reward_adapter=\u003cfunction AgentSpec.\u003clambda\u003e at 0x7fab786d8b00\u003e, \n                     info_adapter=\u003cfunction AgentSpec.\u003clambda\u003e at 0x7fab786d8dd0\u003e)}\n  }\n\t'callbacks' = {type} \u003cclass 'baselines.my.MyCallback.SimpleCallbacks'\u003e\n}\n'run_or_experiment' = {type} \u003cclass 'ray.rllib.agents.trainer_template.DQN'\u003e\n'name' = {str} 'bid-4'\n'local_dir' = {str} '/home/LiKuiHao/pycharm/s6/baselines/marl_benchmark/log/results/myrun'\n'restore' = {NoneType} None\n```\n\n# Tune\n\n## tune.run(config)\n\n### config\n\n用于 Tune 变体生成的**特定于算法的配置**（例如 env、hyperparams）。默认为空字典。自定义搜索算法可能会忽略这一点。\n\n```python\nconfig = {\n    # === Debugging ===\n    # Whether to write episode stats and videos to the agent log dir\n    # 是否把每次迭代的状态和videos 写入智能体日志文件中\n    \"monitor\": False,\n    # Set the ray.rllib.* log level for the agent process and its workers.\n    # Should be one of DEBUG, INFO, WARN, or ERROR. The DEBUG level will also\n    # periodically print out summaries of relevant internal dataflow (this is\n    # also printed out once at startup at the INFO level).\n    # 设置ray.rllib.*代理（智能体）进程及其worker的日志级别。应该是调试（ DEBUG）、信息（INFO）、\n    # 警告（WARN）或错误（ERROR）之一。 调试级别还将定期打印出相关内部数据流的摘要(在INFO\n    # 级别启动时也会打印一次)。\n    \"log_level\": \"INFO\",\n    # Callbacks that will be run during various phases of training. These all\n    # take a single \"info\" dict as an argument. For episode callbacks, custom\n    # metrics can be attached to the episode by updating the episode object's\n    # custom metrics dict (see examples/custom_metrics_and_callbacks.py). You\n    # may also mutate the passed in batch data in your callback.\n    # 将在不同训练阶段运行的回调。这些都以一个“info”dict作为参数。对于迭代回调，可以通过更新迭代\n    # 对象的自定义度量dict将自定义度量附加到迭代中(参见示例/custom_metrics_and_callbacks.py)。\n    # 还可以在回调中修改传入的批处理数据。\n    \"callbacks\": {\n        \"on_episode_start\": None,     # arg: {\"env\": .., \"episode\": ...}\n        \"on_episode_step\": None,      # arg: {\"env\": .., \"episode\": ...}\n        \"on_episode_end\": None,       # arg: {\"env\": .., \"episode\": ...}\n        \"on_sample_end\": None,        # arg: {\"samples\": .., \"worker\": ...}\n        \"on_train_result\": None,      # arg: {\"trainer\": ..., \"result\": ...}\n        \"on_postprocess_traj\": None,  # arg: {\n                                      #   \"agent_id\": ..., \"episode\": ...,\n                                      #   \"pre_batch\": (before processing),\n                                      #   \"post_batch\": (after processing),\n                                      #   \"all_pre_batches\": (other agent ids),\n                                      # }\n    },\n    # Whether to attempt to continue training if a worker crashes.\n    # 是否忽略失败worker继续运行训练\n    \"ignore_worker_failures\": False,\n    # Execute TF loss functions in eager mode. This is currently experimental\n    # and only really works with the basic PG algorithm.\n    # 是否在紧急（eager）模式下执行TF的损失函数。\n    \"use_eager\": False,\n\n    # === Policy ===\n    # Arguments to pass to model. See models/catalog.py for a full list of the\n    # available model options.\n    # 传递给模型的参数。有关可用模型选项的完整列表，请参见models/catalog.py。\n    \"model\": MODEL_DEFAULTS,\n    # Arguments to pass to the policy optimizer. These vary by optimizer.\n    # 传递给策略优化器的参数。这些参数因优化器而异。\n    \"optimizer\": {},\n\n    # === Environment ===\n    # Discount factor of the MDP\n    # MDP的折扣系数\n    \"gamma\": 0.99,\n    # Number of steps after which the episode is forced to terminate. Defaults\n    # to `env.spec.max_episode_steps` (if present) for Gym envs.\n    # 事件被迫终止的步骤数。默认为“env.spec.max_episode_steps '(如果有的话)用于env.spec.max_episode_steps的envs。\n    \"horizon\": None,\n    # Calculate rewards but don't reset the environment when the horizon is\n    # hit. This allows value estimation and RNN state to span across logical\n    # episodes denoted by horizon. This only has an effect if horizon != inf.\n    # 计算奖励，但不要在horizon 被击中时重置环境。这使得值估计和RNN状态可以跨由horizon表示的逻辑事件。这只有在horizon != inf时才有效。\n    \"soft_horizon\": False,\n    # Arguments to pass to the env creator\n    # 传递给env创建者的参数\n    \"env_config\": {},\n    # Environment name can also be passed via config\n    # 环境名称，也可以通过配置传递\n    \"env\": None,\n    # Whether to clip rewards prior to experience postprocessing. Setting to\n    # None means clip for Atari only.\n    # 是否在实验后处理之前剪辑奖励。设置为None只表示剪辑Atari 。\n    \"clip_rewards\": None,\n    # Whether to np.clip() actions to the action space low/high range spec.\n    # 是否通过np.clip() 剪辑动作 到动作空间的低/高范围规范。\n    \"clip_actions\": True,\n    # Whether to use rllib or deepmind preprocessors by default\n    # 默认情况下是否使用rllib或deepmind预处理器\n    \"preprocessor_pref\": \"deepmind\",\n    # The default learning rate\n    # 学习率\n    \"lr\": 0.0001,\n\n    # === Evaluation ===\n    # Evaluate with every `evaluation_interval` training iterations.\n    # The evaluation stats will be reported under the \"evaluation\" metric key.\n    # Note that evaluation is currently not parallelized, and that for Ape-X\n    # metrics are already only reported for the lowest epsilon workers.\n    # 使用每个“evaluation_interval”训练迭代进行评估。评估统计数据将\n    # 在“评估”度量键下报告。注意，评估目前没有并行化，而且对于Ape-X指标，\n    # 只报告了最低的epsilon worker。\n    \"evaluation_interval\": None,\n    # Number of episodes to run per evaluation period.\n    # 每个评估期要运行的迭代数。\n    \"evaluation_num_episodes\": 10,\n    # Extra arguments to pass to evaluation workers.\n    # Typical usage is to pass extra args to evaluation env creator\n    # and to disable exploration by computing deterministic actions\n    # TODO(kismuz): implement determ. actions and include relevant keys hints\n    \"evaluation_config\": {},\n\n    # === Resources ===\n    # Number of actors used for parallelism\n    \"num_workers\": 2,\n    # Number of GPUs to allocate to the driver. Note that not all algorithms\n    # can take advantage of driver GPUs. This can be fraction (e.g., 0.3 GPUs).\n    \"num_gpus\": 0,\n    # Number of CPUs to allocate per worker.\n    \"num_cpus_per_worker\": 1,\n    # Number of GPUs to allocate per worker. This can be fractional.\n    \"num_gpus_per_worker\": 0,\n    # Any custom resources to allocate per worker.\n    \"custom_resources_per_worker\": {},\n    # Number of CPUs to allocate for the driver. Note: this only takes effect\n    # when running in Tune.\n    \"num_cpus_for_driver\": 1,\n\n    # === Execution ===\n    # Number of environments to evaluate vectorwise per worker.\n    \"num_envs_per_worker\": 1,\n    # Default sample batch size (unroll length). Batches of this size are\n    # collected from workers until train_batch_size is met. When using\n    # multiple envs per worker, this is multiplied by num_envs_per_worker.\n    \"sample_batch_size\": 200,\n    # Training batch size, if applicable. Should be \u003e= sample_batch_size.\n    # Samples batches will be concatenated together to this size for training.\n    \"train_batch_size\": 200,\n    # Whether to rollout \"complete_episodes\" or \"truncate_episodes\"\n    \"batch_mode\": \"truncate_episodes\",\n    # (Deprecated) Use a background thread for sampling (slightly off-policy)\n    \"sample_async\": False,\n    # Element-wise observation filter, either \"NoFilter\" or \"MeanStdFilter\"\n    \"observation_filter\": \"NoFilter\",\n    # Whether to synchronize the statistics of remote filters.\n    \"synchronize_filters\": True,\n    # Configure TF for single-process operation by default\n    \"tf_session_args\": {\n        # note: overriden by `local_tf_session_args`\n        \"intra_op_parallelism_threads\": 2,\n        \"inter_op_parallelism_threads\": 2,\n        \"gpu_options\": {\n            \"allow_growth\": True,\n        },\n        \"log_device_placement\": False,\n        \"device_count\": {\n            \"CPU\": 1\n        },\n        \"allow_soft_placement\": True,  # required by PPO multi-gpu\n    },\n    # Override the following tf session args on the local worker\n    \"local_tf_session_args\": {\n        # Allow a higher level of parallelism by default, but not unlimited\n        # since that can cause crashes with many concurrent drivers.\n        \"intra_op_parallelism_threads\": 8,\n        \"inter_op_parallelism_threads\": 8,\n    },\n    # Whether to LZ4 compress individual observations\n    \"compress_observations\": False,\n    # Drop metric batches from unresponsive workers after this many seconds\n    # 在经过这么多秒后，把反应迟钝的worker剔除\n    \"collect_metrics_timeout\": 180,\n    # Smooth metrics over this many episodes.\n    \"metrics_smoothing_episodes\": 100,\n    # If using num_envs_per_worker \u003e 1, whether to create those new envs in\n    # remote processes instead of in the same worker. This adds overheads, but\n    # can make sense if your envs can take much time to step / reset\n    # (e.g., for StarCraft). Use this cautiously; overheads are significant.\n    \"remote_worker_envs\": False,\n    # Timeout that remote workers are waiting when polling environments.\n    # 0 (continue when at least one env is ready) is a reasonable default,\n    # but optimal value could be obtained by measuring your environment\n    # step / reset and model inference perf.\n    \"remote_env_batch_wait_ms\": 0,\n    # Minimum time per iteration\n    \"min_iter_time_s\": 0,\n    # Minimum env steps to optimize for per train call. This value does\n    # not affect learning, only the length of iterations.\n    \"timesteps_per_iteration\": 0,\n\n    # === Offline Datasets ===\n    # Specify how to generate experiences:\n    #  - \"sampler\": generate experiences via online simulation (default)\n    #  - a local directory or file glob expression (e.g., \"/tmp/*.json\")\n    #  - a list of individual file paths/URIs (e.g., [\"/tmp/1.json\",\n    #    \"s3://bucket/2.json\"])\n    #  - a dict with string keys and sampling probabilities as values (e.g.,\n    #    {\"sampler\": 0.4, \"/tmp/*.json\": 0.4, \"s3://bucket/expert.json\": 0.2}).\n    #  - a function that returns a rllib.offline.InputReader\n    \"input\": \"sampler\",\n    # Specify how to evaluate the current policy. This only has an effect when\n    # reading offline experiences. Available options:\n    #  - \"wis\": the weighted step-wise importance sampling estimator.\n    #  - \"is\": the step-wise importance sampling estimator.\n    #  - \"simulation\": run the environment in the background, but use\n    #    this data for evaluation only and not for learning.\n    \"input_evaluation\": [\"is\", \"wis\"],\n    # Whether to run postprocess_trajectory() on the trajectory fragments from\n    # offline inputs. Note that postprocessing will be done using the *current*\n    # policy, not the *behaviour* policy, which is typically undesirable for\n    # on-policy algorithms.\n    \"postprocess_inputs\": False,\n    # If positive, input batches will be shuffled via a sliding window buffer\n    # of this number of batches. Use this if the input data is not in random\n    # enough order. Input is delayed until the shuffle buffer is filled.\n    \"shuffle_buffer_size\": 0,\n    # Specify where experiences should be saved:\n    #  - None: don't save any experiences\n    #  - \"logdir\" to save to the agent log dir\n    #  - a path/URI to save to a custom output directory (e.g., \"s3://bucket/\")\n    #  - a function that returns a rllib.offline.OutputWriter\n    \"output\": None,\n    # What sample batch columns to LZ4 compress in the output data.\n    \"output_compress_columns\": [\"obs\", \"new_obs\"],\n    # Max output file size before rolling over to a new file.\n    \"output_max_file_size\": 64 * 1024 * 1024,\n\n    # === Multiagent ===\n    \"multiagent\": {\n        # Map from policy ids to tuples of (policy_cls, obs_space,\n        # act_space, config). See rollout_worker.py for more info.\n        \"policies\": {},\n        # Function mapping agent ids to policy ids.\n        \"policy_mapping_fn\": None,\n        # Optional whitelist of policies to train, or None for all policies.\n        \"policies_to_train\": None,\n    },\n}\n\n```\n\n# Rllib\n\n### 算法超参数列表\n\n```python\nCOMMON_CONFIG = {\n    # === Debugging ===\n    # Whether to write episode stats and videos to the agent log dir\n    # 是否把每次迭代的状态和videos 写入智能体日志文件中\n    \"monitor\": False,\n    # Set the ray.rllib.* log level for the agent process and its workers.\n    # Should be one of DEBUG, INFO, WARN, or ERROR. The DEBUG level will also\n    # periodically print out summaries of relevant internal dataflow (this is\n    # also printed out once at startup at the INFO level).\n    # 设置ray.rllib.*代理（智能体）进程及其worker的日志级别。应该是调试（ DEBUG）、信息（INFO）、\n    # 警告（WARN）或错误（ERROR）之一。 调试级别还将定期打印出相关内部数据流的摘要(在INFO\n    # 级别启动时也会打印一次)。\n    \"log_level\": \"INFO\",\n    # Callbacks that will be run during various phases of training. These all\n    # take a single \"info\" dict as an argument. For episode callbacks, custom\n    # metrics can be attached to the episode by updating the episode object's\n    # custom metrics dict (see examples/custom_metrics_and_callbacks.py). You\n    # may also mutate the passed in batch data in your callback.\n    # 将在不同训练阶段运行的回调。这些都以一个“info”dict作为参数。对于迭代回调，可以通过更新迭代\n    # 对象的自定义度量dict将自定义度量附加到迭代中(参见示例/custom_metrics_and_callbacks.py)。\n    # 还可以在回调中修改传入的批处理数据。\n    \"callbacks\": {\n        \"on_episode_start\": None,     # arg: {\"env\": .., \"episode\": ...}\n        \"on_episode_step\": None,      # arg: {\"env\": .., \"episode\": ...}\n        \"on_episode_end\": None,       # arg: {\"env\": .., \"episode\": ...}\n        \"on_sample_end\": None,        # arg: {\"samples\": .., \"worker\": ...}\n        \"on_train_result\": None,      # arg: {\"trainer\": ..., \"result\": ...}\n        \"on_postprocess_traj\": None,  # arg: {\n                                      #   \"agent_id\": ..., \"episode\": ...,\n                                      #   \"pre_batch\": (before processing),\n                                      #   \"post_batch\": (after processing),\n                                      #   \"all_pre_batches\": (other agent ids),\n                                      # }\n    },\n    # Whether to attempt to continue training if a worker crashes.\n    # 是否忽略失败worker继续运行训练\n    \"ignore_worker_failures\": False,\n    # Execute TF loss functions in eager mode. This is currently experimental\n    # and only really works with the basic PG algorithm.\n    # 是否在紧急（eager）模式下执行TF的损失函数。\n    \"use_eager\": False,\n\n    # === Policy ===\n    # Arguments to pass to model. See models/catalog.py for a full list of the\n    # available model options.\n    # 传递给模型的参数。有关可用模型选项的完整列表，请参见models/catalog.py。\n    \"model\": MODEL_DEFAULTS,\n    # Arguments to pass to the policy optimizer. These vary by optimizer.\n    # 传递给策略优化器的参数。这些参数因优化器而异。\n    \"optimizer\": {},\n\n    # === Environment ===\n    # Discount factor of the MDP\n    # MDP的折扣系数\n    \"gamma\": 0.99,\n    # Number of steps after which the episode is forced to terminate. Defaults\n    # to `env.spec.max_episode_steps` (if present) for Gym envs.\n    # 事件被迫终止的步骤数。默认为“env.spec.max_episode_steps '(如果有的话)用于env.spec.max_episode_steps的envs。\n    \"horizon\": None,\n    # Calculate rewards but don't reset the environment when the horizon is\n    # hit. This allows value estimation and RNN state to span across logical\n    # episodes denoted by horizon. This only has an effect if horizon != inf.\n    # 计算奖励，但不要在horizon 被击中时重置环境。这使得值估计和RNN状态可以跨由horizon表示的逻辑事件。这只有在horizon != inf时才有效。\n    \"soft_horizon\": False,\n    # Arguments to pass to the env creator\n    # 传递给env创建者的参数\n    \"env_config\": {},\n    # Environment name can also be passed via config\n    # 环境名称，也可以通过配置传递\n    \"env\": None,\n    # Whether to clip rewards prior to experience postprocessing. Setting to\n    # None means clip for Atari only.\n    # 是否在实验后处理之前剪辑奖励。设置为None只表示剪辑Atari 。\n    \"clip_rewards\": None,\n    # Whether to np.clip() actions to the action space low/high range spec.\n    # 是否通过np.clip() 剪辑动作 到动作空间的低/高范围规范。\n    \"clip_actions\": True,\n    # Whether to use rllib or deepmind preprocessors by default\n    # 默认情况下是否使用rllib或deepmind预处理器\n    \"preprocessor_pref\": \"deepmind\",\n    # The default learning rate\n    # 学习率\n    \"lr\": 0.0001,\n\n    # === Evaluation ===\n    # Evaluate with every `evaluation_interval` training iterations.\n    # The evaluation stats will be reported under the \"evaluation\" metric key.\n    # Note that evaluation is currently not parallelized, and that for Ape-X\n    # metrics are already only reported for the lowest epsilon workers.\n    # 使用每个“evaluation_interval”训练迭代进行评估。评估统计数据将\n    # 在“评估”度量键下报告。注意，评估目前没有并行化，而且对于Ape-X指标，\n    # 只报告了最低的epsilon worker。\n    \"evaluation_interval\": None,\n    # Number of episodes to run per evaluation period.\n    # 每个评估期要运行的迭代数。\n    \"evaluation_num_episodes\": 10,\n    # Extra arguments to pass to evaluation workers.\n    # Typical usage is to pass extra args to evaluation env creator\n    # and to disable exploration by computing deterministic actions\n    # TODO(kismuz): implement determ. actions and include relevant keys hints\n    \"evaluation_config\": {},\n\n    # === Resources ===\n    # Number of actors used for parallelism\n    \"num_workers\": 2,\n    # Number of GPUs to allocate to the driver. Note that not all algorithms\n    # can take advantage of driver GPUs. This can be fraction (e.g., 0.3 GPUs).\n    \"num_gpus\": 0,\n    # Number of CPUs to allocate per worker.\n    \"num_cpus_per_worker\": 1,\n    # Number of GPUs to allocate per worker. This can be fractional.\n    \"num_gpus_per_worker\": 0,\n    # Any custom resources to allocate per worker.\n    \"custom_resources_per_worker\": {},\n    # Number of CPUs to allocate for the driver. Note: this only takes effect\n    # when running in Tune.\n    \"num_cpus_for_driver\": 1,\n\n    # === Execution ===\n    # Number of environments to evaluate vectorwise per worker.\n    \"num_envs_per_worker\": 1,\n    # Default sample batch size (unroll length). Batches of this size are\n    # collected from workers until train_batch_size is met. When using\n    # multiple envs per worker, this is multiplied by num_envs_per_worker.\n    \"sample_batch_size\": 200,\n    # Training batch size, if applicable. Should be \u003e= sample_batch_size.\n    # Samples batches will be concatenated together to this size for training.\n    \"train_batch_size\": 200,\n    # Whether to rollout \"complete_episodes\" or \"truncate_episodes\"\n    \"batch_mode\": \"truncate_episodes\",\n    # (Deprecated) Use a background thread for sampling (slightly off-policy)\n    \"sample_async\": False,\n    # Element-wise observation filter, either \"NoFilter\" or \"MeanStdFilter\"\n    \"observation_filter\": \"NoFilter\",\n    # Whether to synchronize the statistics of remote filters.\n    \"synchronize_filters\": True,\n    # Configure TF for single-process operation by default\n    \"tf_session_args\": {\n        # note: overriden by `local_tf_session_args`\n        \"intra_op_parallelism_threads\": 2,\n        \"inter_op_parallelism_threads\": 2,\n        \"gpu_options\": {\n            \"allow_growth\": True,\n        },\n        \"log_device_placement\": False,\n        \"device_count\": {\n            \"CPU\": 1\n        },\n        \"allow_soft_placement\": True,  # required by PPO multi-gpu\n    },\n    # Override the following tf session args on the local worker\n    \"local_tf_session_args\": {\n        # Allow a higher level of parallelism by default, but not unlimited\n        # since that can cause crashes with many concurrent drivers.\n        \"intra_op_parallelism_threads\": 8,\n        \"inter_op_parallelism_threads\": 8,\n    },\n    # Whether to LZ4 compress individual observations\n    \"compress_observations\": False,\n    # Drop metric batches from unresponsive workers after this many seconds\n    # 在经过这么多秒后，把反应迟钝的worker剔除\n    \"collect_metrics_timeout\": 180,\n    # Smooth metrics over this many episodes.\n    \"metrics_smoothing_episodes\": 100,\n    # If using num_envs_per_worker \u003e 1, whether to create those new envs in\n    # remote processes instead of in the same worker. This adds overheads, but\n    # can make sense if your envs can take much time to step / reset\n    # (e.g., for StarCraft). Use this cautiously; overheads are significant.\n    \"remote_worker_envs\": False,\n    # Timeout that remote workers are waiting when polling environments.\n    # 0 (continue when at least one env is ready) is a reasonable default,\n    # but optimal value could be obtained by measuring your environment\n    # step / reset and model inference perf.\n    \"remote_env_batch_wait_ms\": 0,\n    # Minimum time per iteration\n    \"min_iter_time_s\": 0,\n    # Minimum env steps to optimize for per train call. This value does\n    # not affect learning, only the length of iterations.\n    \"timesteps_per_iteration\": 0,\n\n    # === Offline Datasets ===\n    # Specify how to generate experiences:\n    #  - \"sampler\": generate experiences via online simulation (default)\n    #  - a local directory or file glob expression (e.g., \"/tmp/*.json\")\n    #  - a list of individual file paths/URIs (e.g., [\"/tmp/1.json\",\n    #    \"s3://bucket/2.json\"])\n    #  - a dict with string keys and sampling probabilities as values (e.g.,\n    #    {\"sampler\": 0.4, \"/tmp/*.json\": 0.4, \"s3://bucket/expert.json\": 0.2}).\n    #  - a function that returns a rllib.offline.InputReader\n    \"input\": \"sampler\",\n    # Specify how to evaluate the current policy. This only has an effect when\n    # reading offline experiences. Available options:\n    #  - \"wis\": the weighted step-wise importance sampling estimator.\n    #  - \"is\": the step-wise importance sampling estimator.\n    #  - \"simulation\": run the environment in the background, but use\n    #    this data for evaluation only and not for learning.\n    \"input_evaluation\": [\"is\", \"wis\"],\n    # Whether to run postprocess_trajectory() on the trajectory fragments from\n    # offline inputs. Note that postprocessing will be done using the *current*\n    # policy, not the *behaviour* policy, which is typically undesirable for\n    # on-policy algorithms.\n    \"postprocess_inputs\": False,\n    # If positive, input batches will be shuffled via a sliding window buffer\n    # of this number of batches. Use this if the input data is not in random\n    # enough order. Input is delayed until the shuffle buffer is filled.\n    \"shuffle_buffer_size\": 0,\n    # Specify where experiences should be saved:\n    #  - None: don't save any experiences\n    #  - \"logdir\" to save to the agent log dir\n    #  - a path/URI to save to a custom output directory (e.g., \"s3://bucket/\")\n    #  - a function that returns a rllib.offline.OutputWriter\n    \"output\": None,\n    # What sample batch columns to LZ4 compress in the output data.\n    \"output_compress_columns\": [\"obs\", \"new_obs\"],\n    # Max output file size before rolling over to a new file.\n    \"output_max_file_size\": 64 * 1024 * 1024,\n\n    # === Multiagent ===\n    \"multiagent\": {\n        # Map from policy ids to tuples of (policy_cls, obs_space,\n        # act_space, config). See rollout_worker.py for more info.\n        \"policies\": {},\n        # Function mapping agent ids to policy ids.\n        \"policy_mapping_fn\": None,\n        # Optional whitelist of policies to train, or None for all policies.\n        \"policies_to_train\": None,\n    },\n}\n```\n","lastmodified":"2023-05-09T16:33:58.283366204Z","tags":[]},"/Redis%E5%8E%9F%E7%90%86":{"title":"1.数据类型","content":"\n# 1.数据类型\n\n## 1 Redis键(key)\n\nkeys 查看当前库所有key    (匹配：keys 1)  \nexists key判断某个key是否存在  \ntype key 查看你的key是什么类型  \ndel key 删除指定的key数据  \nunlink key 根据value选择非阻塞删除\n\n\u003e仅将keys从keyspace元数据中删除，真正的删除会在后续异步操作。\n\nexpire key 10   10秒钟：为给定的key设置过期时间  \nttl key 查看还有多少秒过期，-1表示永不过期，-2表示已过期  \nselect 命令切换数据库  \ndbsize 查看当前数据库的key的数量  \nflushdb 清空当前库  \nflushall 通杀全部库\n\n## 2 字符串(String)\n\nString是Redis最基本的类型，你可以理解成与Memcached一模一样的类型，一个key对应一个value。  \nString类型是二进制安全的。意味着Redis的string可以包含任何数据。比如jpg图片或者序列化的对象。  \nString类型是Redis最基本的数据类型，一个Redis中字符串value最多可以是512M。\n\n### 命令\n\n![](Pasted%20image%2020221208223909.png)\n\n```bash\n*NX：当数据库中key不存在时，可以将key-value添加数据库\n*XX：当数据库中key存在时，可以将key-value添加数据库，与NX参数互斥\n*EX：key的超时秒数\n*PX：key的超时毫秒数，与EX互斥\n```\n\n- get   \\\u003ckey\\\u003e 查询对应键值\n- append  \\\u003ckey\\\u003e\\\u003cvalue\\\u003e 将给定的\\\u003cvalue\\\u003e追加到原值的末尾\n- strlen  \\\u003ckey\\\u003e获得值的长度\n- setnx  \\\u003ckey\\\u003e\\\u003cvalue\\\u003e 只有在 key 不存在时设置 key 的值\n- incr \\\u003ckey\\\u003e 将key中储存的数字值增1,只能对数字值操作，如果为空，新增值为1\n- decr  \\\u003ckey\\\u003e将 key 中储存的数字值减1,只能对数字值操作，如果为空，新增值为-1\n- incrby / decrby  \\\u003ckey\\\u003e\\\u003c步长\\\u003e将 key 中储存的数字值增减。自定义步长。\n- mset  \\\u003ckey1\\\u003e\\\u003cvalue1\\\u003e\\\u003ckey2\\\u003e\\\u003cvalue2\\\u003e… 同时设置一个或多个 key-value对 \n- mget  \\\u003ckey1\\\u003e\\\u003ckey2\\\u003e\\\u003ckey3\\\u003e… 同时获取一个或多个 value \n- msetnx \\\u003ckey1\\\u003e\\\u003cvalue1\\\u003e\\\u003ckey2\\\u003e\\\u003cvalue2\\\u003e … 同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在。\n\n\u003e所谓**原子**操作是指不会被线程调度机制打断的操作；  \n 这种操作一旦开始，就一直运行到结束，中间不会有任何 context switch （切换到另一个线程）。  \n（1）在单线程中，能够在单条指令中完成的操作都可以认为是\"原子操作\"，因为中断只能发生于指令之间。  \n（2）在多线程中，不能被其它进程（线程）打断的操作就叫原子操作。\n\n **Redis单命令的原子性主要得益于Redis的单线程。**\n\n- getrange  \\\u003ckey\\\u003e\\\u003c起始位置\\\u003e\\\u003c结束位置\\\u003e获得值的范围，类似java中的substring，前包，后包\n- setrange  \\\u003ckey\\\u003e\\\u003c起始位置\\\u003e\\\u003cvalue\\\u003e 用\\\u003cvalue\\\u003e覆写\\\u003ckey\\\u003e所储存的字符串值，从\u003c起始位置\u003e开始(**索引从0开始**)。\n- setex  \\\u003ckey\\\u003e\\\u003c过期时间\\\u003e\\\u003cvalue\\\u003e 设置键值的同时，设置过期时间，单位秒。\n- getset \\\u003ckey\\\u003e\\\u003cvalue\\\u003e 以新换旧，设置了新值同时获得旧值。\n\n### 数据结构\n\nString的数据结构为简单动态字符串(Simple Dynamic String,缩写SDS)。是可以修改的字符串，内部结构实现上类似于JavaArrayList，采用预分配冗余空间的方式来减少内存的频繁分配.  \n![](Pasted%20image%2020221208224851.png)  \n如图中所示，内部为当前字符串实际分配的空间capacity一般要高于实际字符串长度len。**当字符串长度小于1M时，扩容都是加倍现有的空间，如果超过1M，扩容时一次只会多扩1M的空间。需要注意的是字符串最大长度为512M。**\n\n## 3 列表List\n\n单键多值:Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。  \n它的底层实际是个双向链表，对两端的操作性能很高，通过索引下标的操作中间的节点性能会较差。  \n![](Pasted%20image%2020221208224951.png)\n\n### 命令\n\n- lpush/rpush  \\\u003ckey\u003e\\\u003cvalue1\u003e\\\u003cvalue2\u003e\\\u003cvalue3\u003e …. 从左边/右边插入一个或多个值。\n- lpop/rpop  \\\u003ckey\u003e从左边/右边吐出一个值。值在键在，值光键亡。\n- rpoplpush  \\\u003ckey1\u003e\\\u003ckey2\u003e从\\\u003ckey1\u003e列表右边吐出一个值，插到\\\u003ckey2\u003e列表左边。\n- lrange \\\u003ckey\u003e\\\u003cstart\u003e\\\u003cstop\u003e 按照索引下标获得元素(从左到右)\n- lrange mylist 0 -1   0左边第一个，-1右边第一个，（表示获取所有）\n- lindex \\\u003ckey\u003e\\\u003cindex\u003e按照索引下标获得元素(从左到右)\n- llen \\\u003ckey\u003e获得列表长度\n- linsert \\\u003ckey\u003e  before \\\u003cvalue\u003e\\\u003cnewvalue\u003e在\\\u003cvalue\u003e的后面插入\\\u003cnewvalue\u003e插入值\n- lrem \\\u003ckey\u003e\\\u003cn\u003e\\\u003cvalue\u003e从左边删除n个value(从左到右)\n- lset \\\u003ckey\u003e\\\u003cindex\u003e\\\u003cvalue\u003e将列表key下标为index的值替换成value\n\n### 数据结构\n\nList的数据结构为快速链表quickList。  \n首先在列表**元素较少的情况下会使用一块连续的内存存储**，这个结构是ziplist，也即是压缩列表。它将所有的元素紧挨着一起存储，分配的是一块连续的内存。  \n当数据量比较多的时候才会改成quicklist。因为普通的链表需要的附加指针空间太大，会比较浪费空间。比如这个列表里存的只是int类型的数据，结构上还需要两个额外的指针prev和next。  \n![](Pasted%20image%2020221208225524.png)  \nRedis将链表和ziplist结合起来组成了quicklist。也就是**将多个ziplist使用双向指针串起来使用**。这样既满足了快速的插入删除性能，又不会出现太大的空间冗余。\n\n## 4 Set\n\nRedis set对外提供的功能与list类似是一个列表的功能，特殊之处在于set是可以**自动排重**的，当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的。\n\nRedis的Set是string类型的无序集合。它底层其实是一个value为null的hash表，所以添加，删除，查找的**复杂度都是****O(1)**。\n\n一个算法，随着数据的增加，执行时间的长短，如果是O(1)，数据增加，查找数据的时间不变\n\n### 命令\n\n- sadd \\\u003ckey\u003e\\\u003cvalue1\u003e\\\u003cvalue2\u003e …..  \n将一个或多个 member 元素加入到集合 key 中，已经存在的 member 元素将被忽略\n- smembers \\\u003ckey\u003e取出该集合的所有值。\n- sismember \\\u003ckey\u003e\\\u003cvalue\u003e判断集合\\\u003ckey\u003e是否为含有该\\\u003cvalue\u003e值，有1，没有0\n- scard\\\u003ckey\u003e返回该集合的元素个数\n- srem \\\u003ckey\u003e\\\u003cvalue1\u003e\\\u003cvalue2\u003e …. 删除集合中的某个元素。\n- spop \\\u003ckey\u003e随机从该集合中吐出一个值。\n- srandmember \\\u003ckey\u003e\\\u003cn\u003e随机从该集合中取出n个值。不会从集合中删除\n- smove \u003csource\u003e\\\u003cdestination\u003evalue把集合中一个值从一个集合移动到另一个集合\n- sinter \\\u003ckey1\u003e\\\u003ckey2\u003e返回两个集合的交集元素。\n- sunion \\\u003ckey1\u003e\\\u003ckey2\u003e返回两个集合的并集元素。\n- sdiff \\\u003ckey1\u003e\\\u003ckey2\u003e返回两个集合的差集元素(key1中的，不包含 key2中的)\n\n### 数据结构\n\nSet数据结构是dict字典，字典是用哈希表实现的。\n\nJava中HashSet的内部实现使用的是HashMap，只不过所有的value都指向同一个对象。Redis的set结构也是一样，它的内部也使用hash结构，所有的value都指向同一个内部值。\n\n## 5 哈希(Hash)\n\nRedis hash 是一个键值对集合。\n\nRedis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。\n\n类似Java里面的Map\u003cString,Object\u003e  \n用户ID为查找的key，存储的value用户对象包含姓名，年龄，生日等信息，如果用普通的key/value结构来存储  \n主要有以下2种存储方式：  \n![](Pasted%20image%2020221209001600.png)  \n![](Pasted%20image%2020221209001607.png)  \n![](Pasted%20image%2020221209001913.png)  \n**通过 key(用户ID) + field(属性标签) 就可以操作对应属性数据了，既不需要重复存储数据，也不会带来序列化和并发修改控制的问题**\n\n### 命令\n\n- hset \\\u003ckey\u003e\\\u003cfield\u003e\\\u003cvalue\u003e给\\\u003ckey\u003e集合中的  \\\u003cfield\u003e键赋值\\\u003cvalue\u003e\n- hget \\\u003ckey1\u003e\\\u003cfield\u003e从\\\u003ckey1\u003e集合\\\u003cfield\u003e取出 value\n- hmset \\\u003ckey1\u003e\\\u003cfield1\u003e\\\u003cvalue1\u003e\\\u003cfield2\u003e\\\u003cvalue2\u003e… 批量设置hash的值\n- hexists\\\u003ckey1\u003e\\\u003cfield\u003e查看哈希表 key 中，给定域 field 是否存在。\n- hkeys \\\u003ckey\u003e列出该hash集合的所有field\n- hvals \\\u003ckey\u003e列出该hash集合的所有value\n- hincrby \\\u003ckey\u003e\\\u003cfield\u003e\\\u003cincrement\u003e为哈希表 key 中的域 field 的值加上增量 1   -1\n- hsetnx \\\u003ckey\u003e\\\u003cfield\u003e\\\u003cvalue\u003e将哈希表 key 中的域 field 的值设置为 value ，当且仅当域 field 不存在\n\n### 数据结构\n\nHash类型对应的数据结构是两种：ziplist（压缩列表），hashtable（哈希表）。当field-value长度较短且个数较少时，使用ziplist，否则使用hashtable。\n\n## 6 有序集合Zset(sorted set)\n\nRedis有序集合zset与普通集合set非常相似，是一个没有重复元素的字符串集合。\n\n不同之处是有序集合的每个成员都关联了一个**评分（score）,这个评分（score）被用来按照从最低分到最高分的方式排序集合中的成员。集合的成员是唯一的，但是评分可以是重复了。\n\n因为元素是有序的, 所以你也可以很快的根据评分（score）或者次序（position）来获取一个范围的元素。\n\n访问有序集合的中间元素也是非常快的,因此你能够使用有序集合作为一个没有重复成员的智能列表。\n\n### 命令\n\n- zadd  \\\u003ckey\u003e\\\u003cscore1\u003e\\\u003cvalue1\u003e\\\u003cscore2\u003e\\\u003cvalue2\u003e  \n将一个或多个 member 元素及其 score 值加入到有序集 key 当中。\n- zrange \\\u003ckey\u003e\\\u003cstart\u003e\\\u003cstop\u003e  [WITHSCORES]  \n返回有序集 key 中，下标在\\\u003cstart\u003e\\\u003cstop\u003e之间的元素  \n带WITHSCORES，可以让分数一起和值返回到结果集。\n- zrangebyscore key minmax [withscores] [limit offset count]  \n返回有序集 key 中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员。有序集成员按 score 值递增(从小到大)次序排列。\n- zrevrangebyscore key maxmin [withscores] \\[limit offset count]  \n同上，改为从大到小排列\n- zincrby \\\u003ckey\u003e\\\u003cincrement\u003e\\\u003cvalue\u003e      为元素的score加上增量\n- zrem  \\\u003ckey\u003e\\\u003cvalue\u003e删除该集合下，指定值的元素\n- zcount \\\u003ckey\u003e\\\u003cmin\u003e\\\u003cmax\u003e统计该集合，分数区间内的元素个数\n- zrank \\\u003ckey\u003e\\\u003cvalue\u003e返回该值在集合中的排名，从0开始  \n利用zset实现一个文章访问量的排行榜:  \n![](Pasted%20image%2020221209005108.png)\n\n### 数据结构\n\nSortedSet(zset)是Redis提供的一个非常特别的数据结构，一方面它等价于Java的数据结构Map\u003cString, Double\u003e，可以给每一个元素value赋予一个权重score，另一方面它又类似于TreeSet，内部的元素会按照权重score进行排序，可以得到每个元素的名次，还可以通过score的范围来获取元素的列表。  \nzset底层使用了两个数据结构  \n（1）hash，hash的作用就是关联元素value和权重score，保障元素value的唯一性，可以通过元素value找到相应的score值。  \n（2）跳跃表，跳跃表的目的在于给元素value排序，根据score的范围获取元素列表。  \n**跳跃表**  \n有序集合在生活中比较常见，例如根据成绩对学生排名，根据得分对玩家排名等。对于有序集合的底层实现，可以用数组、平衡树、链表等。数组不便元素的插入删除；平衡树或红黑树虽然效率高但结构复杂；链表查询需要遍历所有效率低。Redis采用的是跳跃表。跳跃表效率堪比红黑树，实现远比红黑树简单。  \n对比有序链表和跳跃表，从链表中查询出51：\n\n1. 有序链表  \n    ![](Pasted%20image%2020221209005250.png)  \n    要查找值为51的元素，需要从第一个元素开始依次查找、比较才能找到。共需要6次比较\n2. 跳跃表  \n![](Pasted%20image%2020221209005310.png)  \n从第2层开始，1节点比51节点小，向后比较  \n21节点比51节点小，继续向后比较，后面就是NULL了，所以从21节点向下到第1层  \n在第1层，41节点比51节点小，继续向后，61节点比51节点大，所以从41向下  \n在第0层，51节点为要查找的节点，节点被找到，共查找4次  \n从此可以看出跳跃表比有序链表效率要高\n\n## 7 Bitmaps\n\n现代计算机用二进制（位）作为信息的基础单位，1个字节等于8位，例如“abc”字符串是由3个字节组成，但实际在计算机存储时将其用二进制表示， “abc”分别对应的ASCII码分别是97、 98、 99， 对应的二进制分别是01100001、01100010和01100011，如下图  \n![](Pasted%20image%2020230103213511.png)  \n合理地使用操作位能够有效地提高内存使用率和开发效率。  \nRedis提供了Bitmaps这个“数据类型”可以实现对位的操作：\n\n1. Bitmaps本身不是一种数据类型，实际上它就是字符串（key-value） ， 但是它可以对字符串的位进行操作。\n2. Bitmaps单独提供了一套命令，所以在Redis中使用Bitmaps和使用字符串的方法不太相同。可以把Bitmaps想象成一个以位为单位的数组，数组的每个单元只能存储0和1，数组的下标在Bitmaps中叫做偏移量。  \n![](Pasted%20image%2020230103213602.png)\n\n### 命令\n\n`setbit\u003ckey\u003e\u003coffset\u003e\u003cvalue\u003e设置Bitmaps中某个偏移量的值（0或1）`  \n`getbit\u003ckey\u003e\u003coffset\u003e获取Bitmaps中某个偏移量的值`  \nbitcount 统计**字符串**被设置为1的bit数。一般情况下，给定的整个字符串都会被进行计数，通过指定额外的start或end参数，可以让计数只在特定的位上进行。start和end参数的设置，都可以使用负数值：比如-1 表示最后一个位，而 -2 表示倒数第二个位，start、end 是指bit组的字节的下标数，二者皆包含。  \n`bitcount\u003ckey\u003e[start end] 统计字符串从start字节到end字节比特值为1的数量`  \n`bitop  and(or/not/xor) \u003cdestkey\u003e [key…]`bitop是一个复合操作， 它可以做多个Bitmaps的and（交集） 、 or（并集） 、 not（非） 、 xor（异或） 操作并将结果保存在destkey中。\n\n### Bitmaps 与 set 对比\n\n假设网站有1亿用户，每天独立访问的用户有5千万， 如果每天用集合类型和Bitmaps分别存储活跃用户可以得到表  \n![](Pasted%20image%2020230103213743.png)  \n很明显，这种情况下使用Bitmaps能节省很多的内存空间，尤其是随着时间推移节省的内存还是非常可观的  \n![](Pasted%20image%2020230103213821.png)  \n但Bitmaps并不是万金油，假如该网站每天的独立访问用户很少，例如只有10万（大量的僵尸用户），那么两者的对比如下表所示，很显然，这时候使用Bitmaps就不太合适了，因为基本上大部分位都是0。  \n![](Pasted%20image%2020230103214011.png)\n\n## 8 HyperLogLog\n\n在工作当中，我们经常会遇到与统计相关的功能需求，比如统计网站P（PageView页面访问量）,可以使用Redis的incr、incrby轻松实现。  \n但像UV（UniqueVisitor，独立访客）、独立IP数、搜索记录数等需要去重和计数的问题如何解决？这种**求集合中不重复元素个数的问题称为基数问题**。\n\n解决基数问题有很多种方案：  \n（1）数据存储在MySQL表中，使用distinct count计算不重复个数  \n（2）使用Redis提供的hash、set、bitmaps等数据结构来处理  \n以上的方案结果精确，但随着数据不断增加，导致占用空间越来越大，对于非常大的数据集是不切实际的。  \n能否能够降低一定的精度来平衡存储空间？Redis推出了HyperLogLog  \nRedis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。  \n在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。  \n但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。  \n什么是基数?  \n比如数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。  \n`pfadd \u003ckey\u003e\u003c element\u003e [element …]   添加指定元素到 HyperLogLog 中`  \n`pfcount\u003ckey\u003e [key …] 计算HLL的近似基数，可以计算多个HLL，比如用HLL存储每天的UV，计算一周的UV可以使用7天的UV合并计算即可`  \n`pfmerge\u003cdestkey\u003e\u003csourcekey\u003e [sourcekey …]  将一个或多个HLL合并后的结果存储在另一个HLL中，比如每月活跃用户可以使用每天的活跃用户来合并计算可得`\n\n## 9 Geospatial\n\nRedis 3.2 中增加了对GEO类型的支持。GEO，Geographic，地理信息的缩写。该类型，就是元素的2维坐标，在地图上就是经纬度。redis基于该类型，提供了经纬度设置，查询，范围查询，距离查询，经纬度Hash等常见操作。  \n`geoadd\u003ckey\u003e\u003c longitude\u003e\u003clatitude\u003e\u003cmember\u003e [longitude latitude member…]   添加地理位置（经度，纬度，名称）`  \n两极无法直接添加，一般会下载城市数据，直接通过 Java 程序一次性导入。  \n有效的经度从 -180 度到 180 度。有效的纬度从 -85.05112878 度到 85.05112878 度。  \n当坐标位置超出指定范围时，该命令将会返回一个错误。已经添加的数据，是无法再次往里面添加的。  \n`geopos  \u003ckey\u003e\u003cmember\u003e [member…]  获得指定地区的坐标值`  \n`geodist\u003ckey\u003e\u003cmember1\u003e\u003cmember2\u003e  [m|km|ft|mi ]  获取两个位置之间的直线距离`  \n`georadius\u003ckey\u003e\u003c longitude\u003e\u003clatitude\u003eradius  m|km|ft|mi   以给定的经纬度为中心，找出某一半径内的元素`\n\n# 2.发布和订阅\n\n# 3.事务\n\nRedis事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。  \nRedis事务的主要作用就是串联多个命令防止别的命令插队。\n\n## Multi、Exec、discard\n\n从输入Multi命令开始，输入的命令都会依次进入命令队列中，但不会执行，直到输入Exec后，Redis会将之前的命令队列中的命令依次执行。\n\n组队的过程中可以通过discard来放弃组队。  \n![](Pasted%20image%2020230104010837.png)  \n![](Pasted%20image%2020230104010928.png)  \n![](Pasted%20image%2020230104011414.png)  \n**事务的错误处理**  \n组队中某个命令出现了报告错误，执行时整个的所有队列都会被取消\n","lastmodified":"2023-05-09T16:33:58.283366204Z","tags":[]},"/RegExp":{"title":"RegExp","content":"\n## 匹配字符规则\n\n| 预定义类                                                 | 量词符                             | 边界符                         |\n| -------------------------------------------------------- | ---------------------------------- | ------------------------------ |\n| **\\d** 匹配数字,相当于\\[0-9]                             | ***** 0个或者更多                  | **^** 一行的开头               |\n| **\\D** 匹配**非**数字,相当于\\[^0-9]                      | **+** 1个或更多，至少1个           | **$** 一行的结尾               |\n| **\\w** 匹配数字、字母、下划线,相当于\\[A-Za-z0-9_]        | **?** 0个或1个,一个Optional        | **\\b** 单词\"结界\"(word bounds) |\n| **\\W** 匹配**非**数字、字母、下划线,相当于\\[^A-Za-z0-9_] | **{min,max}** 出现次数在一个范围内 |                                |\n| **\\s** 匹配空格字符，如换行、制表，相当于\\[\\t\\r\\n\\v\\f]   | **{n，}** 重复n次或更多            |                                |\n| **\\S** 匹配**非**空格字符，相当于\\[^\\t\\r\\n\\v\\f]          | **{n}** 重复n次                    |                                |\n| **.** 匹配任何，任何的字符                               |                                    |                                |\n\n- **[]**\n\n  这个符号用来表示逻辑关系`或`，比如`[abc]`表示a或者b或c.`[-.]`表示符号`-`或者`.`号(**注意这里，在`[]`中的`.`号代表的就是这个符号，但是如果在其外面，表示个匹配所有。 所以如果不在`[]`之中，想要匹配'.'，就要通过转义符号`\\.`**)\n\n  **特殊情况**\n\n  1. -连接符是第一个字符时\n\n     比如`[-.]`的含义是连字符`-`或者点符`.`。 但是，如果当连字符不是第一个字符时，比如`[a-z]`，这就表示是从字母a到字符z。\n\n  2. []中的^\n\n     `^`在之前介绍中，是表示一行开头，但是在`[]`中，有着不同的含义。 `[ab]` 表示a或者b `[^ab]` 啥都行，只要不是a或b(anythings except a and b)，相当于取反\n\n- **()**\n\n  除了使用`[]`表示或逻辑,`()`也是可以的。用法是`(a|b)`表示a或者b。\n\n  **重要 分组捕获(capturing groups)**\n\n  对于电话号码212-555-1234，按照之前的做法`\\d{3}-\\d{3}-\\d{4}`,这种匹配的方式，是将整个电话号码作为一个组(group)匹配起来。 我们把`212-555-1234`这样的叫`Group0`。\n\n  这个时候，如果我们加了一个括号`\\d{3}-(\\d{3})-\\d{4}`，那么匹配到的`555`就叫`Group1`。 以此类推，如果有两个小括号`\\d{3}-(\\d{3})-(\\d{4})`那么分组就是下面的情况：\n\n  ```\n  212-555-1234   Group0\n  555            Group1\n  1234           Group2\n  ```\n\n  在js中，使用`/\\d{3}-(\\d{3})-\\d{4}/`则有：\n\n  ![image-20210405164147046](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/%E9%80%9A%E8%AF%86/%E6%AD%A3%E5%88%99/20210405164212.png)\n\n## 一些例子\n\n### 现在想找5个字母组成的单词：`\\b\\w{5}\\b`\n\n### 请匹配所有可能的电话号码：\n\n```\nThese are some phone numbers 915-134-3122. Also,\nyou can call me at 643.123.1333 and of course,\nI'm always reachable at (212)867-5509\n```\n\n第一步： `\\d{3}[-.]\\d{3}[-.]\\d{4}`\n\n第二步: 为了能够匹配括号，可以使用?来，因为这是一个option选择。所以最后就成了\n\n```javascript\n/\\(?\\d{3}[-.)]\\d{3}[-.]\\d{4}/\n```\n\n**这里还是要说明，在[]中，特殊字符不需要转义，可以直接使用，比如`[.()]`,但是在外面，是需要转义的`\\(` `\\.`等**\n\n### 匹配所有email\n\n```\ngaoyaqi411@126.com  \ndyumc@google.net \nsam@sjtu.edu\n```\n\n**思路**：\n\n首先要想我到底相匹配什么，这里我想匹配的是\n\n1. 任何一个以words开头的，一个或更多 `\\w+`\n2. 紧接着是一个`@`符号 `\\w+@`\n3. 接着有一个或者更多的words `\\w+@\\w+`\n4. 接着一个`.`标点 `\\w+@\\w+\\.`\n5. 接着一个`com` `net` 或 `edu` `\\w+@\\w+\\.(com|net|edu)`\n\n**还是提醒注意第四步的`\\.`转义符号**\n\n好了，这样几可以匹配以上的所有邮箱了。但是还有一个问题，因为邮箱用户名是可以有`.`的，比如`vincent.ko@126.com`\n\n其实仍然很简单，修复如下： `[\\w.]+@\\w+\\.(com|net|edu)`\n\n## 总结\n\n1. `/[-.(]/` 在符号中的连字符`-`放在第一位表示连字符本身，如果放在中间，表示\"从..到..\"，比如`[a-z]`表示a-z。\n2. `[.)]` 括号中的特殊符号不需要转义，就表示其本身\n3. `(a|b)`也可表示选择，但是它有更强大的功能….\n","lastmodified":"2023-05-09T16:33:58.283366204Z","tags":[]},"/SSH":{"title":"SSH 基本知识","content":"\nSSH 是 Linux 系统的登录工具，现在广泛用于服务器登录和各种加密通信。\n\n# SSH 基本知识\n\nSSH（Secure Shell 的缩写）是一种网络协议，用于加密两台计算机之间的通信，并且支持各种身份验证机制。\n\n实务中，它主要用于保证远程登录和远程通信的安全，任何网络服务都可以用这个协议来加密。\n\n## SSH 是什么\n\n历史上，网络主机之间的通信是不加密的，属于明文通信。这使得通信很不安全，一个典型的例子就是服务器登录。登录远程服务器的时候，需要将用户输入的密码传给服务器，如果这个过程是明文通信，就意味着传递过程中，线路经过的中间计算机都能看到密码，这是很可怕的。\n\nSSH 就是为了解决这个问题而诞生的，它能够加密计算机之间的通信，保证不被窃听或篡改。它还能对操作者进行认证（authentication）和授权（authorization）。明文的网络协议可以套用在它里面，从而实现加密。\n\n## 历史\n\n1995年，芬兰赫尔辛基工业大学的研究员 Tatu Ylönen 设计了 SSH 协议的第一个版本（现称为 SSH 1），同时写出了第一个实现（称为 SSH1）。\n\n当时，他所在的大学网络一直发生密码嗅探攻击，他不得不为服务器设计一个更安全的登录方式。写完以后，他就把这个工具公开了，允许其他人免费使用。\n\nSSH 可以替换 rlogin、TELNET、FTP 和 rsh 这些不安全的协议，所以大受欢迎，用户快速增长，1995年底已经发展到五十个国家的20,000个用户。SSH 1 协议也变成 IETF 的标准文档。\n\n1995年12月，由于客服需求越来越大，Tatu Ylönen 就成立了一家公司 SCS，专门销售和开发 SSH。这个软件的后续版本，逐渐从免费软件变成了专有的商业软件。\n\nSSH 1 协议存在一些安全漏洞，所以1996年又提出了 SSH 2 协议（或者称为 SSH 2.0）。这个协议与1.0版不兼容，在1997年进行了标准化，1998年推出了软件实现 SSH2。但是，官方的 SSH2 软件是一个专有软件，不能免费使用，而且 SSH1 的有些功能也没有提供。\n\n1999年，OpenBSD 的开发人员决定写一个 SSH 2 协议的开源实现，这就是 OpenSSH 项目。该项目最初是基于 SSH 1.2.12 版本，那是当时 SSH1 最后一个开源版本。但是，OpenSSH 很快就完全摆脱了原始的官方代码，在许多开发者的参与下，按照自己的路线发展。OpenSSH 随 OpenBSD 2.6 版本一起提供，以后又移植到其他操作系统，成为最流行的 SSH 实现。目前，Linux 的所有发行版几乎都自带 OpenSSH。\n\n现在，SSH-2 有多种实现，既有免费的，也有收费的。本书的内容主要是针对 OpenSSH。\n\n## SSH 架构\n\nSSH 的软件架构是服务器-客户端模式（Server - Client）。在这个架构中，SSH 软件分成两个部分：向服务器发出请求的部分，称为客户端（client），OpenSSH 的实现为 ssh；接收客户端发出的请求的部分，称为服务器（server），OpenSSH 的实现为 sshd。\n\n**本教程约定，大写的 SSH 表示协议，小写的 ssh 表示客户端软件。**\n\n另外，OpenSSH 还提供一些辅助工具软件（比如 ssh-keygen 、ssh-agent）和专门的客户端工具（比如 scp 和 sftp），这个教程也会予以介绍。\n\n# SSH 客户端\n\n## 简介\n\nOpenSSH 的客户端是二进制程序 ssh。它在 Linux/Unix 系统的位置是`/usr/local/bin/ssh`，Windows 系统的位置是`\\Program Files\\OpenSSH\\bin\\ssh.exe`。\n\nLinux 系统一般都自带 ssh，如果没有就需要安装。\n\n```bash\n# Ubuntu 和 Debian\n$ sudo apt install openssh-client\n\n# CentOS 和 Fedora\n$ sudo dnf install openssh-clients\n```\n\n安装以后，可以使用`-V`参数输出版本号，查看一下是否安装成功。\n\n```bash\n$ ssh -V\n```\n\n## 基本用法\n\nssh 最常见的用途就是登录服务器，这要求服务器安装并正在运行 SSH 服务器软件。\n\nssh 登录服务器的命令如下。\n\n```bash\n$ ssh hostname\n```\n\n上面命令中，`hostname`是主机名，它可以是域名，也可能是 IP 地址或局域网内部的主机名。不指定用户名的情况下，将使用客户端的当前用户名，作为远程服务器的登录用户名。如果要指定用户名，可以采用下面的语法。\n\n```bash\n$ ssh user@hostname\n```\n\n上面的命令中，用户名和主机名写在一起了，之间使用`@`分隔。\n\n用户名也可以使用`ssh`的`-l`参数指定，这样的话，用户名和主机名就不用写在一起了。\n\n```bash\n$ ssh -l username host\n```\n\nssh 默认连接服务器的22端口，`-p`参数可以指定其他端口。\n\n```bash\n$ ssh -p 8821 foo.com\n```\n\n上面命令连接服务器`foo.com`的8821端口。\n\n## 连接流程\n\nssh 连接远程服务器后，首先有一个验证过程，验证远程服务器是否为陌生地址。\n\n如果是第一次连接某一台服务器，命令行会显示一段文字，表示不认识这台机器，提醒用户确认是否需要连接。\n\n```bash\nThe authenticity of host 'foo.com (192.168.121.111)' can't be established.\nECDSA key fingerprint is SHA256:Vybt22mVXuNuB5unE++yowF7lgA/9/2bLSiO3qmYWBY.\nAre you sure you want to continue connecting (yes/no)?\n```\n\n上面这段文字告诉用户，`foo.com`这台服务器的指纹是陌生的，让用户选择是否要继续连接（输入 yes 或 no）。\n\n所谓“服务器指纹”，指的是 SSH 服务器公钥的哈希值。每台 SSH 服务器都有唯一一对密钥，用于跟客户端通信，其中公钥的哈希值就可以用来识别服务器。\n\n下面的命令可以查看某个公钥的指纹。\n\n```bash\n$ ssh-keygen -l -f /etc/ssh/ssh_host_ecdsa_key.pub\n256 da:24:43:0b:2e:c1:3f:a1:84:13:92:01:52:b4:84:ff   (ECDSA)\n```\n\n上面的例子中，`ssh-keygen -l -f`命令会输出公钥`/etc/ssh/ssh_host_ecdsa_key.pub`的指纹。\n\nssh 会将本机连接过的所有服务器公钥的指纹，都储存在本机的`~/.ssh/known_hosts`文件中。每次连接服务器时，通过该文件判断是否为陌生主机（陌生公钥）。\n\n在上面这段文字后面，输入`yes`，就可以将当前服务器的指纹也储存在本机`~/.ssh/known_hosts`文件中，并显示下面的提示。以后再连接的时候，就不会再出现警告了。\n\n```bash\nWarning: Permanently added 'foo.com (192.168.121.111)' (RSA) to the list of known hosts\n```\n\n然后，客户端就会跟服务器建立连接。接着，ssh 就会要求用户输入所要登录账户的密码。用户输入并验证密码正确以后，就能登录远程服务器的 Shell 了。\n\n## 服务器密钥变更\n\n服务器指纹可以防止有人恶意冒充远程主机。如果服务器的密钥发生变更（比如重装了 SSH 服务器），客户端再次连接时，就会发生公钥指纹不吻合的情况。这时，客户端就会中断连接，并显示一段警告信息。\n\n```bash\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that the RSA host key has just been changed.\nThe fingerprint for the RSA key sent by the remote host is\n77:a5:69:81:9b:eb:40:76:7b:13:04:a9:6c:f4:9c:5d.\nPlease contact your system administrator.\nAdd correct host key in /home/me/.ssh/known_hosts to get rid of this message.\nOffending key in /home/me/.ssh/known_hosts:36\n```\n\n上面这段文字的意思是，该主机的公钥指纹跟`~/.ssh/known_hosts`文件储存的不一样，必须处理以后才能连接。这时，你需要确认是什么原因，使得公钥指纹发生变更，到底是恶意劫持，还是管理员变更了 SSH 服务器公钥。\n\n如果新的公钥确认可以信任，需要继续执行连接，你可以执行下面的命令，将原来的公钥指纹从`~/.ssh/known_hosts`文件删除。\n\n```bash\n$ ssh-keygen -R hostname\n```\n\n上面命令中，`hostname`是发生公钥变更的主机名。\n\n除了使用上面的命令，你也可以手工修改`known_hosts`文件，将公钥指纹删除。\n\n删除了原来的公钥指纹以后，重新执行 ssh 命令连接远程服务器，将新的指纹加入`known_hosts`文件，就可以顺利连接了。\n\n## 执行远程命令\n\nSSH 登录成功后，用户就进入了远程主机的命令行环境，所看到的提示符，就是远程主机的提示符。这时，你就可以输入想要在远程主机执行的命令。\n\n另一种执行远程命令的方法，是将命令直接写在`ssh`命令的后面。\n\n```bash\n$ ssh username@hostname command\n```\n\n上面的命令会使得 SSH 在登录成功后，立刻在远程主机上执行命令`command`。\n\n下面是一个例子。\n\n```bash\n$ ssh foo@server.example.com cat /etc/hosts\n```\n\n上面的命令会在登录成功后，立即远程执行命令`cat /etc/hosts`。\n\n采用这种语法执行命令时，ssh 客户端不会提供互动式的 Shell 环境，而是直接远程命令的执行结果输出在命令行。但是，有些命令需要互动式的 Shell 环境，这时就要使用`-t`参数。\n\n```bash\n# 报错\n$ ssh remote.server.com emacs\nemacs: standard input is not a tty\n\n# 不报错\n$ ssh -t server.example.com emacs\n```\n\n上面代码中，`emacs`命令需要一个互动式 Shell，所以报错。只有加上`-t`参数，ssh 才会分配一个互动式 Shell。\n\n## 加密参数\n\nSSH 连接的握手阶段，客户端必须跟服务端约定加密参数集（cipher suite）。\n\n加密参数集包含了若干不同的加密参数，它们之间使用下划线连接在一起，下面是一个例子。\n\n```bash\nTLS_RSA_WITH_AES_128_CBC_SHA\n```\n\n它的含义如下。\n\n- TLS：协议\n- RSA：密钥交换算法\n- AES：加密算法\n- 128：加密强度\n- CBC：加密模式\n- SHA：数字签名的 Hash 函数\n\n下面是一个例子，客户端向服务器发出的握手信息。\n\n```http\nHandshake protocol: ClientHello\n    Version: TLS 1.2\n    Random\n        Client time: May 22, 2030 02:43:46 GMT\n        Random bytes: b76b0e61829557eb4c611adfd2d36eb232dc1332fe29802e321ee871\n    Session ID: (empty)\n    Cipher Suites\n        Suite: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256”\n        Suite: TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\n        Suite: TLS_RSA_WITH_AES_128_GCM_SHA256\n        Suite: TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\n        Suite: TLS_DHE_RSA_WITH_AES_128_CBC_SHA\n        Suite: TLS_RSA_WITH_AES_128_CBC_SHA\n        Suite: TLS_RSA_WITH_3DES_EDE_CBC_SHA\n        Suite: TLS_RSA_WITH_RC4_128_SHA\n    Compression methods\n        Method: null\n    Extensions\n        Extension: server_name\n            Hostname: www.feistyduck.com\n        Extension: renegotiation_info\n        Extension: elliptic_curves\n            Named curve: secp256r1\n            Named curve: secp384r1\n        Extension: signature_algorithms\n            Algorithm: sha1/rsa\n            Algorithm: sha256/rsa\n            Algorithm: sha1/ecdsa\n            Algorithm: sha256/ecdsa”\n```\n\n上面的握手信息（ClientHello）之中，`Cipher Suites`字段就是客户端列出可选的加密参数集，服务器在其中选择一个自己支持的参数集。\n\n服务器选择完毕之后，向客户端发出回应。\n\n```http\nHandshake protocol: ServerHello\n    Version: TLS 1.2\n    Random\n        Server time: Mar 10, 2059 02:35:57 GMT”\n        Random bytes: 8469b09b480c1978182ce1b59290487609f41132312ca22aacaf5012\n    Session ID: 4cae75c91cf5adf55f93c9fb5dd36d19903b1182029af3d527b7a42ef1c32c80\n    Cipher Suite: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\n    Compression method: null\n    Extensions\n        Extension: server_name\n        Extension: renegotiation_info”\n```\n\n上面的回应信息（ServerHello）中，`Cipher Suite`字段就是服务器最终选定的加密参数。\n\n## ssh 命令行配置项\n\nssh 命令有很多配置项，修改它的默认行为。\n\n**-c**\n\n`-c`参数指定加密算法。\n\n```bash\n$ ssh -c blowfish,3des server.example.com\n# 或者\n$ ssh -c blowfish -c 3des server.example.com\n```\n\n上面命令指定使用加密算法`blowfish`或`3des`。\n\n**-C**\n\n`-C`参数表示压缩数据传输。\n\n```bash\n$ ssh -C server.example.com\n```\n\n**-D**\n\n`-D`参数指定本机的 Socks 监听端口，该端口收到的请求，都将转发到远程的 SSH 主机，又称动态端口转发，详见《端口转发》一章。\n\n```bash\n$ ssh -D 1080 server\n```\n\n上面命令将本机 1080 端口收到的请求，都转发到服务器`server`。\n\n**-f**\n\n`-f`参数表示 SSH 连接在后台运行。\n\n**-F**\n\n`-F`参数指定配置文件。\n\n```bash\n$ ssh -F /usr/local/ssh/other_config\n```\n\n上面命令指定使用配置文件`other_config`。\n\n**-i**\n\n`-i`参数用于指定私钥，意为“identity_file”，默认值为`~/.ssh/id_dsa`。注意，对应的公钥必须存放到服务器，详见《密钥登录》一章。\n\n```bash\n$ ssh -i my-key server.example.com\n```\n\n**-l**\n\n`-l`参数指定远程登录的账户名。\n\n```bash\n$ ssh -l sally server.example.com\n# 等同于\n$ ssh sally@server.example.com\n```\n\n**-L**\n\n`-L`参数设置本地端口转发，详见《端口转发》一章。\n\n```bash\n$ ssh  -L 9999:targetServer:80 user@remoteserver\n```\n\n上面命令中，所有发向本地`9999`端口的请求，都会经过`remoteserver`发往 targetServer 的 80 端口，这就相当于直接连上了 targetServer 的 80 端口。\n\n**-m**\n\n`-m`参数指定校验数据完整性的算法（message authentication code，简称 MAC）。\n\n```bash\n$ ssh -m hmac-sha1,hmac-md5 server.example.com\n```\n\n上面命令指定数据校验算法为`hmac-sha1`或`hmac-md5`。\n\n**-o**\n\n`-o`参数用来指定一个配置命令。\n\n```bash\n$ ssh -o \"Keyword Value\"\n```\n\n举例来说，配置文件里面有如下内容。\n\n```bash\nUser sally\nPort 220\n```\n\n通过`-o`参数，可以把上面两个配置命令从命令行传入。\n\n```bash\n$ ssh -o \"User sally\" -o \"Port 220\" server.example.com\n```\n\n使用等号时，配置命令可以不用写在引号里面，但是等号前后不能有空格。\n\n```bash\n$ ssh -o User=sally -o Port=220 server.example.com\n```\n\n**-p**\n\n`-p`参数指定 SSH 客户端连接的服务器端口。\n\n```bash\n$ ssh -p 2035 server.example.com\n```\n\n上面命令连接服务器的2035端口。\n\n**-q**\n\n`-q`参数表示安静模式（quiet），不向用户输出任何警告信息。\n\n```bash\n$ ssh –q foo.com\nroot’s password:\n```\n\n上面命令使用`-q`参数，只输出要求用户输入密码的提示。\n\n**-R**\n\n`-R`参数指定远程端口转发，详见《端口转发》一章。\n\n```bash\n$ ssh -R 9999:targetServer:902 local\n```\n\n上面命令需在跳板服务器执行，指定本地计算机`local`监听自己的 9999 端口，所有发向这个端口的请求，都会转向 targetServer 的 902 端口。\n\n**-t**\n\n`-t`参数在 ssh 直接运行远端命令时，提供一个互动式 Shell。\n\n```bash\n$ ssh -t server.example.com emacs\n```\n\n**-v**\n\n`-v`参数显示详细信息。\n\n```bash\n$ ssh -v server.example.com\n```\n\n`-v`可以重复多次，表示信息的详细程度，比如`-vv`和`-vvv`。\n\n```bash\n$ ssh -vvv server.example.com\n# 或者\n$ ssh -v -v -v server.example.com\n```\n\n上面命令会输出最详细的连接信息。\n\n**-V**\n\n`-V`参数输出 ssh 客户端的版本。\n\n```bash\n$ ssh –V\nssh: SSH Secure Shell 3.2.3 (non-commercial version) on i686-pc-linux-gnu\n```\n\n上面命令输出本机 ssh 客户端版本是`SSH Secure Shell 3.2.3`。\n\n**-X**\n\n`-X`参数表示打开 X 窗口转发。\n\n```bash\n$ ssh -X server.example.com\n```\n\n**-1，-2**\n\n`-1`参数指定使用 SSH 1 协议。\n\n`-2`参数指定使用 SSH 2 协议。\n\n```ssh\n$ ssh -2 server.example.com\n```\n\n**-4，-6**\n\n`-4`指定使用 IPv4 协议，这是默认值。\n\n```bash\n$ ssh -4 server.example.com\n```\n\n`-6`指定使用 IPv6 协议。\n\n```bash\n$ ssh -6 server.example.com\n```\n\n## 客户端配置文件\n\n### 位置\n\nSSH 客户端的全局配置文件是`/etc/ssh/ssh_config`，用户个人的配置文件在`~/.ssh/config`，优先级高于全局配置文件。\n\n除了配置文件，`~/.ssh`目录还有一些用户个人的密钥文件和其他文件。下面是其中一些常见的文件。\n\n- `~/.ssh/id_ecdsa`：用户的 ECDSA 私钥。\n- `~/.ssh/id_ecdsa.pub`：用户的 ECDSA 公钥。\n- `~/.ssh/id_rsa`：用于 SSH 协议版本2 的 RSA 私钥。\n- `~/.ssh/id_rsa.pub`：用于SSH 协议版本2 的 RSA 公钥。\n- `~/.ssh/identity`：用于 SSH 协议版本1 的 RSA 私钥。\n- `~/.ssh/identity.pub`：用于 SSH 协议版本1 的 RSA 公钥。\n- `~/.ssh/known_hosts`：包含 SSH 服务器的公钥指纹。\n\n### 主机设置\n\n用户个人的配置文件`~/.ssh/config`，可以按照不同服务器，列出各自的连接参数，从而不必每一次登录都输入重复的参数。下面是一个例子。\n\n```bash\nHost *\n     Port 2222\n\nHost remoteserver\n     HostName remote.example.com\n     User neo\n     Port 2112\n```\n\n上面代码中，`Host *`表示对所有主机生效，后面的`Port 2222`表示所有主机的默认连接端口都是2222，这样就不用在登录时特别指定端口了。这里的缩进并不是必需的，只是为了视觉上，易于识别针对不同主机的设置。\n\n后面的`Host remoteserver`表示，下面的设置只对主机`remoteserver`生效。`remoteserver`只是一个别名，具体的主机由`HostName`命令指定，`User`和`Port`这两项分别表示用户名和端口。这里的`Port`会覆盖上面`Host *`部分的`Port`设置。\n\n以后，登录`remote.example.com`时，只要执行`ssh remoteserver`命令，就会自动套用 config 文件里面指定的参数。  \n单个主机的配置格式如下。\n\n```bash\n$ ssh remoteserver\n# 等同于\n$ ssh -p 2112 neo@remote.example.com\n```\n\n`Host`命令的值可以使用通配符，比如`Host *`表示对所有主机都有效的设置，`Host *.edu`表示只对一级域名为`.edu`的主机有效的设置。它们的设置都可以被单个主机的设置覆盖。\n\n例如，对于git的代理可以是：\n\n```bash\nHost github.com\n  Hostname github.com\n  User git\n  # 注意修改路径为你的路径\n  IdentityFile \"C:\\Users\\xxx\\.ssh\\github\"\n  # ProxyCommand connect -S 127.0.0.1:10809 -a none %h %p\n  ProxyCommand \"C:\\Program Files\\Git\\mingw64\\bin\\connect.exe\" -S 127.0.0.1:10808 -a none %h %p\n  TCPKeepAlive yes\n```\n\n### 配置命令的语法\n\nssh 客户端配置文件的每一行，就是一个配置命令。配置命令与对应的值之间，可以使用空格，也可以使用等号。\n\n```bash\nCompression yes\n# 等同于\nCompression = yes\n```\n\n`#`开头的行表示注释，会被忽略。空行等同于注释。\n\n### 主要配置命令\n\n下面是 ssh 客户端的一些主要配置命令，以及它们的范例值。\n\n- **ProxyCommand**\n\n  有点复杂//TODO\n\n- `AddressFamily inet`：表示只使用 IPv4 协议。如果设为`inet6`，表示只使用 IPv6 协议。\n- `BindAddress 192.168.10.235`：指定本机的 IP 地址（如果本机有多个 IP 地址）。\n- `CheckHostIP yes`：检查 SSH 服务器的 IP 地址是否跟公钥数据库吻合。\n- `Ciphers blowfish,3des`：指定加密算法。\n- `Compression yes`：是否压缩传输信号。\n- `ConnectionAttempts 10`：客户端进行连接时，最大的尝试次数。\n- `ConnectTimeout 60`：客户端进行连接时，服务器在指定秒数内没有回复，则中断连接尝试。\n- `DynamicForward 1080`：指定动态转发端口。\n- `GlobalKnownHostsFile /users/smith/.ssh/my_global_hosts_file`：指定全局的公钥数据库文件的位置。\n- `Host server.example.com`：**指定连接的域名或 IP 地址**，也可以是别名，支持通配符。`Host`命令后面的所有配置，都是针对该主机的，直到下一个`Host`命令为止。\n- `HostKeyAlgorithms ssh-dss,ssh-rsa`：指定密钥算法，优先级从高到低排列。\n- `HostName myserver.example.com`：在`Host`命令使用别名的情况下，`HostName`指定域名或 IP 地址。\n- `IdentityFile keyfile`：**指定私钥文件。**\n- `LocalForward 2001 localhost:143`：指定本地端口转发。\n- `LogLevel QUIET`：指定日志详细程度。如果设为`QUIET`，将不输出大部分的警告和提示。\n- `MACs hmac-sha1,hmac-md5`：指定数据校验算法。\n- `NumberOfPasswordPrompts 2`：密码登录时，用户输错密码的最大尝试次数。\n- `PasswordAuthentication no`：指定是否支持密码登录。不过，这里只是客户端禁止，真正的禁止需要在 SSH 服务器设置。\n- `Port 2035`：**指定客户端连接的 SSH 服务器端口。**\n- `PreferredAuthentications publickey,hostbased,password`：**指定各种登录方法的优先级。**\n- `Protocol 2`：支持的 SSH 协议版本，多个版本之间使用逗号分隔。\n- `PubKeyAuthentication yes`：是否支持密钥登录。这里只是客户端设置，还需要在 SSH 服务器进行相应设置。\n- `RemoteForward 2001 server:143`：指定远程端口转发。\n- `SendEnv COLOR`：SSH 客户端向服务器发送的环境变量名，多个环境变量之间使用空格分隔。环境变量的值从客户端当前环境中拷贝。\n- `ServerAliveCountMax 3`：如果没有收到服务器的回应，客户端连续发送多少次`keepalive`信号，才断开连接。该项默认值为3。\n- `ServerAliveInterval 300`：客户端建立连接后，如果在给定秒数内，没有收到服务器发来的消息，客户端向服务器发送`keepalive`消息。如果不希望客户端发送，这一项设为`0`。\n- `StrictHostKeyChecking yes`：`yes`表示严格检查，服务器公钥为未知或发生变化，则拒绝连接。`no`表示如果服务器公钥未知，则加入客户端公钥数据库，如果公钥发生变化，不改变客户端公钥数据库，输出一条警告，依然允许连接继续进行。`ask`（默认值）表示询问用户是否继续进行。\n- `TCPKeepAlive yes`：客户端是否定期向服务器发送`keepalive`信息。\n- `User userName`：指定远程登录的账户名。\n- `UserKnownHostsFile /users/smith/.ssh/my_local_hosts_file`：指定当前用户的`known_hosts`文件（服务器公钥指纹列表）的位置。\n- `VerifyHostKeyDNS yes`：是否通过检查 SSH 服务器的 DNS 记录，确认公钥指纹是否与`known_hosts`文件保存的一致。\n\n# SSH 密钥登录\n\nSSH 默认采用密码登录，这种方法有很多缺点，简单的密码不安全，复杂的密码不容易记忆，每次手动输入也很麻烦。密钥登录是更好的解决方案。\n\n## 密钥是什么\n\n密钥（key）是一个非常大的数字，通过加密算法得到。对称加密只需要一个密钥，非对称加密需要两个密钥成对使用，分为公钥（public key）和私钥（private key）。\n\nSSH 密钥登录采用的是**非对称加密**，每个用户通过自己的密钥登录。其中，私钥必须私密保存，不能泄漏；公钥则是公开的，可以对外发送。它们的关系是，公钥和私钥是一一对应的，每一个私钥都有且仅有一个对应的公钥，反之亦然。\n\n如果数据使用公钥加密，那么只有使用对应的私钥才能解密，其他密钥都不行；反过来，如果使用私钥加密（这个过程一般称为“签名”），也只有使用对应的公钥解密。\n\n## 密钥登录的过程\n\nSSH 密钥登录分为以下的步骤。\n\n预备步骤，客户端通过`ssh-keygen`生成自己的公钥和私钥。\n\n第一步，手动将客户端的公钥放入远程服务器的指定位置。\n\n第二步，客户端向服务器发起 SSH 登录的请求。\n\n第三步，服务器收到用户 SSH 登录的请求，发送一些随机数据给用户，要求用户证明自己的身份。\n\n第四步，客户端收到服务器发来的数据，使用私钥对数据进行签名，然后再发还给服务器。\n\n第五步，服务器收到客户端发来的加密签名后，使用对应的公钥解密，然后跟原始数据比较。如果一致，就允许用户登录。\n\n## `ssh-keygen`命令：生成密钥\n\n### 基本用法\n\n密钥登录时，首先需要生成公钥和私钥。OpenSSH 提供了一个工具程序`ssh-keygen`命令，用来生成密钥。\n\n直接输入`ssh-keygen`，程序会询问一系列问题，然后生成密钥。\n\n```bash\n$ ssh-keygen\n```\n\n通常做法是使用`-t`参数，指定密钥的加密算法。\n\n```bash\n$ ssh-keygen -t dsa\n```\n\n上面示例中，`-t`参数用来指定密钥的加密算法，一般会选择`dsa`算法或`rsa`算法。注意，这个参数没有默认值。\n\n输入上面的命令以后，`ssh-keygen`会要求用户回答一些问题。\n\n```bash\n$ ssh-keygen -t dsa\nGenerating public/private dsa key pair.\nEnter file in which to save the key (/home/username/.ssh/id_dsa):  press ENTER\nEnter passphrase (empty for no passphrase): ********\nEnter same passphrase again: ********\nYour identification has been saved in /home/username/.ssh/id_dsa.\nYour public key has been saved in /home/username/.ssh/id_dsa.pub.\nThe key fingerprint is:\n14:ba:06:98:a8:98:ad:27:b5:ce:55:85:ec:64:37:19 username@shell.isp.com\n```\n\n上面示例中，执行`ssh-keygen`命令以后，会出现第一个问题，询问密钥保存的文件名，默认是`~/.ssh/id_dsa`文件，这个是私钥的文件名，对应的公钥文件`~/.ssh/id_dsa.pub`是自动生成的。用户的密钥一般都放在主目录的`.ssh`目录里面。\n\n如果选择`rsa`算法，生成的密钥文件默认就会是`~/.ssh/id_rsa`（私钥）和`~/.ssh/id_rsa.pub`（公钥）。\n\n接着，就会是第二个问题，询问是否要为私钥文件设定密码保护（passphrase）。这样的话，即使入侵者拿到私钥，还是需要破解密码。如果为了方便，不想设定密码保护，可以直接按回车键，密码就会为空。后面还会让你再输入一次密码，两次输入必须一致。注意，这里“密码”的英文单词是 passphrase，这是为了避免与 Linux 账户的密码单词 password 混淆，表示这不是用户系统账户的密码。\n\n最后，就会生成私钥和公钥，屏幕上还会给出公钥的指纹，以及当前的用户名和主机名作为注释，用来识别密钥的来源。\n\n公钥文件和私钥文件都是文本文件，可以用文本编辑器看一下它们的内容。公钥文件的内容类似下面这样。\n\n```bash\nssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAIEAvpB4lUbAaEbh9u6HLig7amsfywD4fqSZq2ikACIUBn3GyRPfeF93l/\nweQh702ofXbDydZAKMcDvBJqRhUotQUwqV6HJxqoqPDlPGUUyo8RDIkLUIPRyq\nypZxmK9aCXokFiHoGCXfQ9imUP/w/jfqb9ByDtG97tUJF6nFMP5WzhM= username@shell.isp.com\n```\n\n上面示例中，末尾的`username@shell.isp.com`是公钥的注释，用来识别不同的公钥，表示这是哪台主机（`shell.isp.com`）的哪个用户（`username`）的公钥，不是必需项。\n\n注意，公钥只有一行。因为它太长了，所以上面分成三行显示。\n\n下面的命令可以列出用户所有的公钥。\n\n```bash\n$ ls -l ~/.ssh/id_*.pub\n```\n\n生成密钥以后，建议修改它们的权限，防止其他人读取。\n\n```bash\n$ chmod 600 ~/.ssh/id_rsa\n$ chmod 600 ~/.ssh/id_rsa.pub\n```\n\n### 配置项\n\n`ssh-keygen`的命令行配置项，主要有下面这些。\n\n**（1）`-b`**\n\n`-b`参数指定密钥的二进制位数。这个参数值越大，密钥就越不容易破解，但是加密解密的计算开销也会加大。\n\n一般来说，`-b`至少应该是`1024`，更安全一些可以设为`2048`或者更高。\n\n**（2）`-C`**\n\n`-C`参数可以为密钥文件指定新的注释，格式为`username@host`。\n\n下面命令生成一个4096位 RSA 加密算法的密钥对，并且给出了用户名和主机名。\n\n```bash\n$ ssh-keygen -t rsa -b 4096 -C \"your_email@domain.com\"\n```\n\n**（3）`-f`**\n\n`-f`参数指定生成的私钥文件。\n\n```bash\n$ ssh-keygen -t dsa -f mykey\n```\n\n上面命令会在当前目录生成私钥文件`mykey`和公钥文件`mykey.pub`。\n\n**（4）`-F`**\n\n`-F`参数检查某个主机名是否在`known_hosts`文件里面。\n\n```bash\n$ ssh-keygen -F example.com\n```\n\n**（5）`-N`**\n\n`-N`参数用于指定私钥的密码（passphrase）。\n\n```bash\n$ ssh-keygen -t dsa -N secretword\n```\n\n**（6）`-p`**\n\n`-p`参数用于重新指定私钥的密码（passphrase）。它与`-N`的不同之处在于，新密码不在命令中指定，而是执行后再输入。ssh 先要求输入旧密码，然后要求输入两遍新密码。\n\n**（7）`-R`**\n\n`-R`参数将指定的主机公钥指纹移出`known_hosts`文件。\n\n```bash\n$ ssh-keygen -R example.com\n```\n\n**（8）`-t`**\n\n`-t`参数用于指定生成密钥的加密算法，一般为`dsa`或`rsa`\n\n## 手动上传公钥到远程服务器\n\n生成密钥以后，公钥必须上传到服务器，才能使用公钥登录。\n\nOpenSSH 规定，用户公钥保存在服务器的`~/.ssh/authorized_keys`文件。你要以哪个用户的身份登录到服务器，密钥就必须保存在该用户主目录的`~/.ssh/authorized_keys`文件。只要把公钥添加到这个文件之中，就相当于公钥上传到服务器了。每个公钥占据一行。如果该文件不存在，可以手动创建。\n\n用户可以手动编辑该文件，把公钥粘贴进去，也可以在本机计算机上，执行下面的命令。\n\n```bash\n$ cat ~/.ssh/id_rsa.pub | ssh user@host \"mkdir -p ~/.ssh \u0026\u0026 cat \u003e\u003e ~/.ssh/authorized_keys\"\n```\n\n上面示例中，`user@host`要替换成你所要登录的用户名和主机名。\n\n注意，`authorized_keys`文件的权限要设为`644`，即只有文件所有者才能写。如果权限设置不对，SSH 服务器可能会拒绝读取该文件。\n\n```bash\n$ chmod 644 ~/.ssh/authorized_keys\n```\n\n只要公钥上传到服务器，下次登录时，OpenSSH 就会自动采用密钥登录，不再提示输入密码。\n\n```bash\n$ ssh -l username shell.isp.com\nEnter passphrase for key '/home/you/.ssh/id_dsa': ************\nLast login: Mon Mar 24 02:17:27 2014 from ex.ample.com\nshell.isp.com\u003e\n```\n\n上面例子中，SSH 客户端使用私钥之前，会要求用户输入密码（passphrase），用来解开私钥。\n\n## ssh-copy-id 命令：自动上传公钥\n\nOpenSSH 自带一个`ssh-copy-id`命令，可以自动将公钥拷贝到远程服务器的`~/.ssh/authorized_keys`文件。如果`~/.ssh/authorized_keys`文件不存在，`ssh-copy-id`命令会自动创建该文件。\n\n用户在本地计算机执行下面的命令，就可以把本地的公钥拷贝到服务器。\n\n```bash\n$ ssh-copy-id -i key_file user@host\n```\n\n上面命令中，`-i`参数用来指定公钥文件，`user`是所要登录的账户名，`host`是服务器地址。如果省略用户名，默认为当前的本机用户名。执行完该命令，公钥就会拷贝到服务器。\n\n注意，公钥文件可以不指定路径和`.pub`后缀名，`ssh-copy-id`会自动在`~/.ssh`目录里面寻找。\n\n```bash\n$ ssh-copy-id -i 1215157189 root@121.5.157.189\n```\n\n上面命令中，公钥文件会自动匹配到`~/.ssh/id_rsa.pub`。\n\n`ssh-copy-id`会采用密码登录，系统会提示输入远程服务器的密码。\n\n**注意**，`ssh-copy-id`是直接将公钥添加到`authorized_keys`文件的末尾。如果`authorized_keys`文件的末尾不是一个换行符，会导致新的公钥添加到前一个公钥的末尾，两个公钥连在一起，使得它们都无法生效。所以，如果`authorized_keys`文件已经存在，使用`ssh-copy-id`命令之前，务必保证`authorized_keys`文件的末尾是换行符（假设该文件已经存在）。\n\n## ssh-agent 命令，ssh-add 命令\n\n### 基本用法\n\n私钥设置了密码以后，每次使用都必须输入密码，有时让人感觉非常麻烦。比如，连续使用`scp`命令远程拷贝文件时，每次都要求输入密码。\n\n`ssh-agent`命令就是为了解决这个问题而设计的，它让用户在整个 Bash 对话（session）之中，只在第一次使用 SSH 命令时输入密码，然后将私钥保存在内存中，后面都不需要再输入私钥的密码了。\n\n第一步，使用下面的命令新建一次命令行对话。\n\n```bash\n$ ssh-agent bash\n```\n\n上面命令中，如果你使用的命令行环境不是 Bash，可以用其他的 Shell 命令代替。比如`zsh`和`fish`。\n\n如果想在当前对话启用`ssh-agent`，可以使用下面的命令。\n\n```bash\n$ eval `ssh-agent`\n```\n\n上面命令中，`ssh-agent`会先自动在后台运行，并将需要设置的环境变量输出在屏幕上，类似下面这样。\n\n```bash\n$ ssh-agent\nSSH_AUTH_SOCK=/tmp/ssh-barrett/ssh-22841-agent; export SSH_AUTH_SOCK;\nSSH_AGENT_PID=22842; export SSH_AGENT_PID;\necho Agent pid 22842;\n```\n\n`eval`命令的作用，就是运行上面的`ssh-agent`命令的输出，设置环境变量。\n\n第二步，在新建的 Shell 对话里面，使用`ssh-add`命令添加默认的私钥（比如`~/.ssh/id_rsa`，或`~/.ssh/id_dsa`，或`~/.ssh/id_ecdsa`，或`~/.ssh/id_ed25519`）。\n\n```bash\n$ ssh-add\nEnter passphrase for /home/you/.ssh/id_dsa: ********\nIdentity added: /home/you/.ssh/id_dsa (/home/you/.ssh/id_dsa)\n```\n\n上面例子中，添加私钥时，会要求输入密码。以后，在这个对话里面再使用密钥时，就不需要输入私钥的密码了，因为私钥已经加载到内存里面了。\n\n如果添加的不是默认私钥，`ssh-add`命令需要显式指定私钥文件。\n\n```bash\n$ ssh-add my-other-key-file\n```\n\n上面的命令中，`my-other-key-file`就是用户指定的私钥文件。\n\n第三步，使用 ssh 命令正常登录远程服务器。\n\n```bash\n$ ssh remoteHost\n```\n\n上面命令中，`remoteHost`是远程服务器的地址，ssh 使用的是默认的私钥。这时如果私钥设有密码，ssh 将不再询问密码，而是直接取出内容里面的私钥。\n\n如果要使用其他私钥登录服务器，需要使用 ssh 命令的`-i`参数指定私钥文件。\n\n```bash\n$ ssh –i OpenSSHPrivateKey remoteHost\n```\n\n最后，如果要退出`ssh-agent`，可以直接退出子 Shell（按下 Ctrl + d），也可以使用下面的命令。\n\n```bash\n$ ssh-agent -k\n```\n\n### `ssh-add`命令\n\n`ssh-add`命令用来将私钥加入`ssh-agent`，它有如下的参数。\n\n**（1）`-d`**\n\n`-d`参数从内存中删除指定的私钥。\n\n```bash\n$ ssh-add -d name-of-key-file\n```\n\n**（2）`-D`**\n\n`-D`参数从内存中删除所有已经添加的私钥。\n\n```bash\n$ ssh-add -D\n```\n\n**（3）`-l`**\n\n`-l`参数列出所有已经添加的私钥。\n\n```bash\n$ ssh-add -l\n```\n\n## 关闭密码登录\n\n为了安全性，启用密钥登录之后，最好关闭服务器的密码登录。\n\n对于 OpenSSH，具体方法就是打开服务器 sshd 的配置文件`/etc/ssh/sshd_config`，将`PasswordAuthentication`这一项设为`no`。\n\n```bash\nPasswordAuthentication no\n```\n\n修改配置文件以后，不要忘了重新启动 sshd，否则不会生效。\n\n```bash\n1. 查看sshd服务是否启动了.\nsystemctl status sshd.service\n\n2. 如果没有启动,则需要启动该服务:\nsystemctl start sshd.service\n\n3. 如果需要重启sshd服务可使得\nsystemctl restart sshd.service\n\n4. 设置为开机启动可使用:\nsystemctl enable sshd.service\n```\n\n# SSH 服务器\n\n## 简介\n\nSSH 的架构是服务器/客户端模式，两端运行的软件是不一样的。OpenSSH 的客户端软件是 ssh，服务器软件是 sshd。本章介绍 sshd 的各种知识。\n\n如果没有安装 sshd，可以用下面的命令安装。\n\n```bash\n# Debian\n$ sudo aptitude install openssh-server\n\n# Red Hat\n$ sudo yum install openssh-server\n```\n\n一般来说，sshd 安装后会跟着系统一起启动。如果当前 sshd 没有启动，可以用下面的命令启动。\n\n```bash\n$ sshd\n```\n\n上面的命令运行后，如果提示“sshd re-exec requires execution with an absolute path”，就需要使用绝对路径来启动。这是为了防止有人出于各种目的，放置同名软件在`$PATH`变量指向的目录中，代替真正的 sshd。\n\n```bash\n# Centos、Ubuntu、OS X\n$ /usr/sbin/sshd\n```\n\n上面的命令运行以后，sshd 自动进入后台，所以命令后面不需要加上`\u0026`。\n\n除了直接运行可执行文件，也可以通过 Systemd 启动 sshd。\n\n```bash\n# 启动\n$ sudo systemctl start sshd.service\n\n# 停止\n$ sudo systemctl stop sshd.service\n\n# 重启\n$ sudo systemctl restart sshd.service\n```\n\n下面的命令让 sshd 在计算机下次启动时自动运行。\n\n```bash\n$ sudo systemctl enable sshd.service\n```\n\n## sshd 配置文件\n\nsshd 的配置文件在`/etc/ssh`目录，主配置文件是`sshd_config`，此外还有一些安装时生成的密钥。\n\n- `/etc/ssh/sshd_config`：配置文件\n- `/etc/ssh/ssh_host_ecdsa_key`：ECDSA 私钥。\n- `/etc/ssh/ssh_host_ecdsa_key.pub`：ECDSA 公钥。\n- `/etc/ssh/ssh_host_key`：用于 SSH 1 协议版本的 RSA 私钥。\n- `/etc/ssh/ssh_host_key.pub`：用于 SSH 1 协议版本的 RSA 公钥。\n- `/etc/ssh/ssh_host_rsa_key`：用于 SSH 2 协议版本的 RSA 私钥。\n- `/etc/ssh/ssh_host_rsa_key.pub`：用于 SSH 2 协议版本的 RSA 公钥。\n- `/etc/pam.d/sshd`：PAM 配置文件。\n\n注意，如果重装 sshd，上面这些密钥都会重新生成，导致客户端重新连接 ssh 服务器时，会跳出警告，拒绝连接。为了避免这种情况，可以在重装 sshd 时，先备份`/etc/ssh`目录，重装后再恢复这个目录。\n\n配置文件`sshd_config`的格式是，每个命令占据一行。每行都是配置项和对应的值，配置项的大小写不敏感，与值之间使用空格分隔。\n\n```bash\nPort 2034\n```\n\n上面的配置命令指定，配置项`Port`的值是`2034`。`Port`写成`port`也可。\n\n配置文件还有另一种格式，就是配置项与值之间有一个等号，等号前后的空格可选。\n\n```bash\nPort = 2034\n```\n\n配置文件里面，`#`开头的行表示注释。\n\n```bash\n# 这是一行注释\n```\n\n注意，注释只能放在一行的开头，不能放在一行的结尾。\n\n```bash\nPort 2034 # 此处不允许注释\n```\n\n上面的写法是错误的。\n\n另外，空行等同于注释。\n\nsshd 启动时会自动读取默认的配置文件。如果希望使用其他的配置文件，可以用 sshd 命令的`-f`参数指定。\n\n```bash\n$ sshd -f /usr/local/ssh/my_config\n```\n\n上面的命令指定 sshd 使用另一个配置文件`my_config`。\n\n修改配置文件以后，可以用 sshd 命令的`-t`（test）检查有没有语法错误。\n\n```bash\n$ sshd -t\n```\n\n配置文件修改以后，并不会自动生效，必须重新启动 sshd。\n\n```bash\n$ sudo systemctl restart sshd.service\n```\n\n## sshd 密钥\n\nsshd 有自己的一对或多对密钥。它使用密钥向客户端证明自己的身份。所有密钥都是公钥和私钥成对出现，公钥的文件名一般是私钥文件名加上后缀`.pub`。\n\nDSA 格式的密钥文件默认为`/etc/ssh/ssh_host_dsa_key`（公钥为`ssh_host_dsa_key.pub`），RSA 格式的密钥为`/etc/ssh/ssh_host_rsa_key`（公钥为`ssh_host_rsa_key.pub`）。如果需要支持 SSH 1 协议，则必须有密钥`/etc/ssh/ssh_host_key`。\n\n如果密钥不是默认文件，那么可以通过配置文件`sshd_config`的`HostKey`配置项指定。默认密钥的`HostKey`设置如下。\n\n```bash\n# HostKey for protocol version 1\n# HostKey /etc/ssh/ssh_host_key\n\n# HostKeys for protocol version 2\n# HostKey /etc/ssh/ssh_host_rsa_key\n# HostKey /etc/ssh/ssh_host_dsa_ke\n```\n\n上面命令前面的`#`表示这些行都是注释，因为这是默认值，有没有这几行都一样。\n\n如果要修改密钥，就要去掉行首的`#`，指定其他密钥。\n\n```bash\nHostKey /usr/local/ssh/my_dsa_key\nHostKey /usr/local/ssh/my_rsa_key\nHostKey /usr/local/ssh/my_old_ssh1_key\n```\n\n## sshd 配置项\n\n以下是`/etc/ssh/sshd_config`文件里面的配置项。\n\n**AcceptEnv**\n\n`AcceptEnv`指定允许接受客户端通过`SendEnv`命令发来的哪些环境变量，即允许客户端设置服务器的环境变量清单，变量名之间使用空格分隔（`AcceptEnv PATH TERM`）。\n\n**AllowGroups**\n\n`AllowGroups`指定允许登录的用户组（`AllowGroups groupName`，多个组之间用空格分隔。如果不使用该项，则允许所有用户组登录。\n\n**AllowUsers**\n\n`AllowUsers`指定允许登录的用户，用户名之间使用空格分隔（`AllowUsers user1 user2`），也可以使用多行`AllowUsers`命令指定，用户名支持使用通配符。如果不使用该项，则允许所有用户登录。该项也可以使用`用户名@域名`的格式（比如`AllowUsers jones@example.com`）。\n\n**AllowTcpForwarding**\n\n`AllowTcpForwarding`指定是否允许端口转发，默认值为`yes`（`AllowTcpForwarding yes`），`local`表示只允许本地端口转发，`remote`表示只允许远程端口转发。\n\n**AuthorizedKeysFile**\n\n`AuthorizedKeysFile`指定储存用户公钥的目录，默认是用户主目录的`ssh/authorized_keys`目录（`AuthorizedKeysFile .ssh/authorized_keys`）。\n\n**Banner**\n\n`Banner`指定用户登录后，sshd 向其展示的信息文件（`Banner /usr/local/etc/warning.txt`），默认不展示任何内容。\n\n**ChallengeResponseAuthentication**\n\n`ChallengeResponseAuthentication`指定是否使用“键盘交互”身份验证方案，默认值为`yes`（`ChallengeResponseAuthentication yes`）。\n\n从理论上讲，“键盘交互”身份验证方案可以向用户询问多重问题，但是实践中，通常仅询问用户密码。如果要完全禁用基于密码的身份验证，请将`PasswordAuthentication`和`ChallengeResponseAuthentication`都设置为`no`。\n\n**Ciphers**\n\n`Ciphers`指定 sshd 可以接受的加密算法（`Ciphers 3des-cbc`），多个算法之间使用逗号分隔。\n\n**ClientAliveCountMax**\n\n`ClientAliveCountMax`指定建立连接后，客户端失去响应时，服务器尝试连接的次数（`ClientAliveCountMax 8`）。\n\n**ClientAliveInterval**\n\n`ClientAliveInterval`指定允许客户端发呆的时间，单位为秒（`ClientAliveInterval 180`）。如果这段时间里面，客户端没有发送任何信号，SSH 连接将关闭。\n\n**Compression**\n\n`Compression`指定客户端与服务器之间的数据传输是否压缩。默认值为`yes`（`Compression yes`）\n\n**DenyGroups**\n\n`DenyGroups`指定不允许登录的用户组（`DenyGroups groupName`）。\n\n**DenyUsers**\n\n`DenyUsers`指定不允许登录的用户（`DenyUsers user1`），用户名之间使用空格分隔，也可以使用多行`DenyUsers`命令指定。\n\n**FascistLogging**\n\nSSH 1 版本专用，指定日志输出全部 Debug 信息（`FascistLogging yes`）。\n\n**HostKey**\n\n`HostKey`指定 sshd 服务器的密钥，详见前文。\n\n**KeyRegenerationInterval**\n\n`KeyRegenerationInterval`指定 SSH 1 版本的密钥重新生成时间间隔，单位为秒，默认是3600秒（`KeyRegenerationInterval 3600`）。\n\n**ListenAddress**\n\n`ListenAddress`指定 sshd 监听的本机 IP 地址，即 sshd 启用的 IP 地址，默认是 0.0.0.0（`ListenAddress 0.0.0.0`）表示在本机所有网络接口启用。可以改成只在某个网络接口启用（比如`ListenAddress 192.168.10.23`），也可以指定某个域名启用（比如`ListenAddress server.example.com`）。\n\n如果要监听多个指定的 IP 地址，可以使用多行`ListenAddress`命令。\n\n```bash\nListenAddress 172.16.1.1\nListenAddress 192.168.0.1\n```\n\n**LoginGraceTime**\n\n`LoginGraceTime`指定允许客户端登录时发呆的最长时间，比如用户迟迟不输入密码，连接就会自动断开，单位为秒（`LoginGraceTime 60`）。如果设为`0`，就表示没有限制。\n\n**LogLevel**\n\n`LogLevel`指定日志的详细程度，可能的值依次为`QUIET`、`FATAL`、`ERROR`、`INFO`、`VERBOSE`、`DEBUG`、`DEBUG1`、`DEBUG2`、`DEBUG3`，默认为`INFO`（`LogLevel INFO`）。\n\n**MACs**\n\n`MACs`指定sshd 可以接受的数据校验算法（`MACs hmac-sha1`），多个算法之间使用逗号分隔。\n\n**MaxAuthTries**\n\n`MaxAuthTries`指定允许 SSH 登录的最大尝试次数（`MaxAuthTries 3`），如果密码输入错误达到指定次数，SSH 连接将关闭。\n\n**MaxStartups**\n\n`MaxStartups`指定允许同时并发的 SSH 连接数量（MaxStartups）。如果设为`0`，就表示没有限制。\n\n这个属性也可以设为`A:B:C`的形式，比如`MaxStartups 10:50:20`，表示如果达到10个并发连接，后面的连接将有50%的概率被拒绝；如果达到20个并发连接，则后面的连接将100%被拒绝。\n\n**PasswordAuthentication**\n\n`PasswordAuthentication`指定是否允许密码登录，默认值为`yes`（`PasswordAuthentication yes`），建议改成`no`（禁止密码登录，只允许密钥登录）。\n\n**PermitEmptyPasswords**\n\n`PermitEmptyPasswords`指定是否允许空密码登录，即用户的密码是否可以为空，默认为`yes`（`PermitEmptyPasswords yes`），建议改成`no`（禁止无密码登录）。\n\n**PermitRootLogin**\n\n`PermitRootLogin`指定是否允许根用户登录，默认为`yes`（`PermitRootLogin yes`），建议改成`no`（禁止根用户登录）。\n\n还有一种写法是写成`prohibit-password`，表示 root 用户不能用密码登录，但是可以用密钥登录。\n\n```bash\nPermitRootLogin prohibit-password\n```\n\n**PermitUserEnvironment**\n\n`PermitUserEnvironment`指定是否允许 sshd 加载客户端的`~/.ssh/environment`文件和`~/.ssh/authorized_keys`文件里面的`environment= options`环境变量设置。默认值为`no`（`PermitUserEnvironment no`）。\n\n**Port**\n\n`Port`指定 sshd 监听的端口，即客户端连接的端口，默认是22（`Port 22`）。出于安全考虑，可以改掉这个端口（比如`Port 8822`）。\n\n配置文件可以使用多个`Port`命令，同时监听多个端口。\n\n```bash\nPort 22\nPort 80\nPort 443\nPort 8080\n```\n\n上面的示例表示同时监听4个端口。\n\n**PrintMotd**\n\n`PrintMotd`指定用户登录后，是否向其展示系统的 motd（Message of the day）的信息文件`/etc/motd`。该文件用于通知所有用户一些重要事项，比如系统维护时间、安全问题等等。默认值为`yes`（`PrintMotd yes`），由于 Shell 一般会展示这个信息文件，所以这里可以改为`no`。\n\n**PrintLastLog**\n\n`PrintLastLog`指定是否打印上一次用户登录时间，默认值为`yes`（`PrintLastLog yes`）。\n\n**Protocol**\n\n`Protocol`指定 sshd 使用的协议。`Protocol 1`表示使用 SSH 1 协议，建议改成`Protocol 2`（使用 SSH 2 协议）。`Protocol 2,1`表示同时支持两个版本的协议。\n\n**PubKeyAuthentication**\n\n`PubKeyAuthentication`指定是否允许公钥登录，默认值为`yes`（`PubKeyAuthentication yes`）。\n\n**QuietMode**\n\nSSH 1 版本专用，指定日志只输出致命的错误信息（`QuietMode yes`）。\n\n**RSAAuthentication**\n\n`RSAAuthentication`指定允许 RSA 认证，默认值为`yes`（`RSAAuthentication yes`）。\n\n**ServerKeyBits**\n\n`ServerKeyBits`指定 SSH 1 版本的密钥重新生成时的位数，默认是768（`ServerKeyBits 768`）。\n\n**StrictModes**\n\n`StrictModes`指定 sshd 是否检查用户的一些重要文件和目录的权限。默认为`yes`（`StrictModes yes`），即对于用户的 SSH 配置文件、密钥文件和所在目录，SSH 要求拥有者必须是根用户或用户本人，用户组和其他人的写权限必须关闭。\n\n**SyslogFacility**\n\n`SyslogFacility`指定 Syslog 如何处理 sshd 的日志，默认是 Auth（`SyslogFacility AUTH`）。\n\n**TCPKeepAlive**\n\n`TCPKeepAlive`指定打开 sshd 跟客户端 TCP 连接的 keepalive 参数（`TCPKeepAlive yes`）。\n\n**UseDNS**\n\n`UseDNS`指定用户 SSH 登录一个域名时，服务器是否使用 DNS，确认该域名对应的 IP 地址包含本机（`UseDNS yes`）。打开该选项意义不大，而且如果 DNS 更新不及时，还有可能误判，建议关闭。\n\n**UseLogin**\n\n`UseLogin`指定用户认证内部是否使用`/usr/bin/login`替代 SSH 工具，默认为`no`（`UseLogin no`）。\n\n**UserPrivilegeSeparation**\n\n`UserPrivilegeSeparation`指定用户认证通过以后，使用另一个子线程处理用户权限相关的操作，这样有利于提高安全性。默认值为`yes`（`UsePrivilegeSeparation yes`）。\n\n**VerboseMode**\n\nSSH 2 版本专用，指定日志输出详细的 Debug 信息（`VerboseMode yes`）。\n\n**X11Forwarding**\n\n`X11Forwarding`指定是否打开 X window 的转发，默认值为 no（`X11Forwarding no`）。\n\n修改配置文件以后，可以使用下面的命令验证，配置文件是否有语法错误。\n\n```bash\n$ sshd -t\n```\n\n新的配置文件生效，必须重启 sshd。\n\n```bash\n$ sudo systemctl restart sshd\n```\n\n## sshd 的命令行配置项\n\nsshd 命令有一些配置项。这些配置项在调用时指定，可以覆盖配置文件的设置。\n\n（1）`-d`\n\n`-d`参数用于显示 debug 信息。\n\n```bash\n$ sshd -d\n```\n\n（2）`-D`\n\n`-D`参数指定 sshd 不作为后台守护进程运行。\n\n```bash\n$ sshd -D\n```\n\n（3）`-e`\n\n`-e`参数将 sshd 写入系统日志 syslog 的内容导向标准错误（standard error）。\n\n（4）`-f`\n\n`-f`参数指定配置文件的位置。\n\n（5）`-h`\n\n`-h`参数用于指定密钥。\n\n```bash\n$ sshd -h /usr/local/ssh/my_rsa_key\n```\n\n（6）`-o`\n\n`-o`参数指定配置文件的一个配置项和对应的值。\n\n```bash\n$ sshd -o \"Port 2034\"\n```\n\n配置项和对应值之间，可以使用等号。\n\n```bash\n$ sshd -o \"Port = 2034\"\n```\n\n如果省略等号前后的空格，也可以不使用引号。\n\n```bash\n$ sshd -o Port=2034\n```\n\n`-o`参数可以多个一起使用，用来指定多个配置关键字。\n\n（7）`-p`\n\n`-p`参数指定 sshd 的服务端口。\n\n```bash\n$ sshd -p 2034\n```\n\n上面命令指定 sshd 在`2034`端口启动。\n\n`-p`参数可以指定多个端口。\n\n```bash\n$ sshd -p 2222 -p 3333\n```\n\n（8）`-t`\n\n`-t`参数检查配置文件的语法是否正确。\n\n# SSH 端口转发\n\n## 简介\n\nSSH 除了登录服务器，还有一大用途，就是作为加密通信的中介，充当两台服务器之间的通信加密跳板，使得原本不加密的通信变成加密通信。这个功能称为端口转发（port forwarding），又称 SSH 隧道（tunnel）。\n\n端口转发有两个主要作用：\n\n（1）将不加密的数据放在 SSH 安全连接里面传输，使得原本不安全的网络服务增加了安全性，比如通过端口转发访问 Telnet、FTP 等明文服务，数据传输就都会加密。\n\n（2）作为数据通信的加密跳板，绕过网络防火墙。\n\n端口转发有三种使用方法：动态转发，本地转发，远程转发。下面逐一介绍。\n\n## 动态转发\n\n动态转发指的是，本机与 SSH 服务器之间创建了一个加密连接，然后本机内部针对某个端口的通信，都通过这个加密连接转发。它的一个使用场景就是，访问所有外部网站，都通过 SSH 转发。\n\n动态转发需要把本地端口绑定到 SSH 服务器。至于 SSH 服务器要去访问哪一个网站，完全是动态的，取决于原始通信，所以叫做动态转发。\n\n```bash\n$ ssh -D local-port tunnel-host -N\n```\n\n上面命令中，`-D`表示动态转发，`local-port`是本地端口，`tunnel-host`是 SSH 服务器，`-N`表示这个 SSH 连接只进行端口转发，不登录远程 Shell，不能执行远程命令，只能充当隧道。\n\n举例来说，如果本地端口是`2121`，那么动态转发的命令就是下面这样。\n\n```bash\n$ ssh -D 2121 tunnel-host -N\n```\n\n注意，这种转发采用了 SOCKS5 协议。访问外部网站时，需要把 HTTP 请求转成 SOCKS5 协议，才能把本地端口的请求转发出去。\n\n下面是 SSH 隧道建立后的一个使用实例。\n\n```bash\n$ curl -x socks5://localhost:2121 http://www.example.com\n```\n\n上面命令中，curl 的`-x`参数指定代理服务器，即通过 SOCKS5 协议的本地`2121`端口，访问`http://www.example.com`。\n\n如果经常使用动态转发，可以将设置写入 SSH 客户端的用户个人配置文件（`~/.ssh/config`）。\n\n```bash\nDynamicForward tunnel-host:local-port\n```\n\n## 本地转发\n\n本地转发（local forwarding）指的是，SSH 服务器作为中介的跳板机，建立本地计算机与特定目标网站之间的加密连接。本地转发是在本地计算机的 SSH 客户端建立的转发规则。\n\n它会指定一个本地端口（local-port），所有发向那个端口的请求，都会转发到 SSH 跳板机（tunnel-host），然后 SSH 跳板机作为中介，将收到的请求发到目标服务器（target-host）的目标端口（target-port）。\n\n```html\n$ ssh -L local-port:target-host:target-port tunnel-host\n```\n\n上面命令中，`-L`参数表示本地转发，`local-port`是本地端口，`target-host`是你想要访问的目标服务器，`target-port`是目标服务器的端口，`tunnel-host`是 SSH 跳板机。\n\n举例来说，现在有一台 SSH 跳板机`tunnel-host`，我们想要通过这台机器，在本地`2121`端口与目标网站`www.example.com`的80端口之间建立 SSH 隧道，就可以写成下面这样。\n\n```bash\n$ ssh -L 2121:www.example.com:80 tunnel-host -N\n```\n\n然后，访问本机的`2121`端口，就是访问`www.example.com`的80端口。\n\n```bash\n$ curl http://localhost:2121\n```\n\n注意，本地端口转发采用 HTTP 协议，不用转成 SOCKS5 协议。\n\n另一个例子是加密访问邮件获取协议 POP3。\n\n```bash\n$ ssh -L 1100:mail.example.com:110 mail.example.com\n```\n\n上面命令将本机的1100端口，绑定邮件服务器`mail.example.com`的110端口（POP3 协议的默认端口）。端口转发建立以后，POP3 邮件客户端只需要访问本机的1100端口，请求就会通过 SSH 跳板机（这里是`mail.example.com`），自动转发到`mail.example.com`的110端口。\n\n上面这种情况有一个前提条件，就是`mail.example.com`必须运行 SSH 服务器。否则，就必须通过另一台 SSH 服务器中介，执行的命令要改成下面这样。\n\n```bash\n$ ssh -L 1100:mail.example.com:110 other.example.com\n```\n\n上面命令中，本机的1100端口还是绑定`mail.example.com`的110端口，但是由于`mail.example.com`没有运行 SSH 服务器，所以必须通过`other.example.com`中介。本机的 POP3 请求通过1100端口，先发给`other.example.com`的22端口（sshd 默认端口），再由后者转给`mail.example.com`，得到数据以后再原路返回。\n\n注意，采用上面的中介方式，只有本机到`other.example.com`的这一段是加密的，`other.example.com`到`mail.example.com`的这一段并不加密。\n\n这个命令最好加上`-N`参数，表示不在 SSH 跳板机执行远程命令，让 SSH 只充当隧道。另外还有一个`-f`参数表示 SSH 连接在后台运行。\n\n如果经常使用本地转发，可以将设置写入 SSH 客户端的用户个人配置文件（`~/.ssh/config`）。\n\n```bash\nHost test.example.com\nLocalForward client-IP:client-port server-IP:server-port\n```\n\n## 远程转发\n\n远程端口指的是在远程 SSH 服务器建立的转发规则。\n\n这种场景比较特殊，主要针对内网的情况。本地计算机在外网，SSH 跳板机和目标服务器都在内网，而且本地计算机无法访问内网之中的 SSH 跳板机，但是 SSH 跳板机可以访问本机计算机。\n\n由于本机无法访问内网 SSH 跳板机，就无法从外网发起 SSH 隧道，建立端口转发。必须反过来，从 SSH 跳板机发起隧道，建立端口转发，这时就形成了远程端口转发。\n\n```bash\n$ ssh -R local-port:target-host:target-port -N local\n```\n\n上面的命令，首先需要注意，不是在本机执行的，而是在 SSH 跳板机执行的，从跳板机去连接本地计算机。`-R`参数表示远程端口转发，`local-port`是本地计算机的端口，`target-host`和`target-port`是目标服务器及其端口，`local`是本地计算机。\n\n显然，远程端口转发要求本地计算机也安装了 SSH 服务器，这样才能接受 SSH 跳板机的远程登录。\n\n比如，跳板机执行下面的命令，绑定本地计算机的`2121`端口，去访问`www.example.com:80`。\n\n```bash\n$ ssh -R 2121:www.example.com:80 local -N\n```\n\n执行上面的命令以后，跳板机到本地计算机的隧道已经建立了。然后，就可以从本机访问目标服务器了，即在本机执行下面的命令。\n\n```bash\n$ curl http://localhost:2121\n```\n\n执行上面的命令以后，命令就会输出服务器`www.example.com`的80端口返回的内容。\n\n如果经常执行远程端口转发，可以将设置写入 SSH 客户端的用户个人配置文件（`~/.ssh/config`）。\n\n```bash\nHost test.example.com\nRemoteForward local-IP:local-port target-ip:target-port\n```\n\n## 实例\n\n下面看两个端口转发的实例。\n\n### 简易 VPN\n\nVPN 用来在外网与内网之间建立一条加密通道。内网的服务器不能从外网直接访问，必须通过一个跳板机，如果本机可以访问跳板机，就可以使用 SSH 本地转发，简单实现一个 VPN。\n\n```bash\n$ ssh -L 2080:corp-server:80 -L 2443:corp-server:443 tunnel-host -N\n```\n\n上面命令通过 SSH 跳板机，将本机的`2080`端口绑定内网服务器的`80`端口，本机的`2443`端口绑定内网服务器的`443`端口。\n\n### 两级跳板\n\n端口转发可以有多级，比如新建两个 SSH 隧道，第一个隧道转发给第二个隧道，第二个隧道才能访问目标服务器。\n\n首先，在本机新建第一级隧道。\n\n```bash\n$ ssh -L 7999:localhost:2999 tunnel1-host\n```\n\n上面命令在本地`7999`端口与`tunnel1-host`之间建立一条隧道，隧道的出口是`tunnel1-host`的`localhost:2999`，也就是`tunnel1-host`收到本机的请求以后，转发给自己的`2999`端口。\n\n然后，在第一台跳板机（`tunnel1-host`）执行下面的命令，新建第二级隧道。\n\n```bash\n$ ssh -L 2999:target-host:7999 tunnel2-host -N\n```\n\n上面命令将第一台跳板机`tunnel1-host`的`2999`端口，通过第二台跳板机`tunnel2-host`，连接到目标服务器`target-host`的`7999`端口。\n\n最终效果就是，访问本机的`7999`端口，就会转发到`target-host`的`7999`端口。\n\n## 参考链接\n\n- [An Illustrated Guide to SSH Tunnels](https://solitum.net/posts/an-illustrated-guide-to-ssh-tunnels/), Scott Wiersdorf\n\n# SSH 证书登录\n\nSSH 是服务器登录工具，一般情况下都采用密码登录或密钥登录。\n\n但是，SSH 还有第三种登录方法，那就是证书登录。某些情况下，它是更合理、更安全的登录方法，本文就介绍这种登录方法。\n\n## 非证书登录的缺点\n\n密码登录和密钥登录，都有各自的缺点。\n\n密码登录需要输入服务器密码，这非常麻烦，也不安全，存在被暴力破解的风险。\n\n密钥登录需要服务器保存用户的公钥，也需要用户保存服务器公钥的指纹。这对于多用户、多服务器的大型机构很不方便，如果有员工离职，需要将他的公钥从每台服务器删除。\n\n## 证书登录是什么？\n\n证书登录就是为了解决上面的缺点而设计的。它引入了一个证书颁发机构（Certificate Authority，简称 CA），对信任的服务器颁发服务器证书，对信任的用户颁发用户证书。\n\n登录时，用户和服务器不需要提前知道彼此的公钥，只需要交换各自的证书，验证是否可信即可。\n\n证书登录的主要优点有两个：（1）用户和服务器不用交换公钥，这更容易管理，也具有更好的可扩展性。（2）证书可以设置到期时间，而公钥没有到期时间。针对不同的情况，可以设置有效期很短的证书，进一步提高安全性。\n\n## 证书登录的流程\n\nSSH 证书登录之前，如果还没有证书，需要生成证书。具体方法是：（1）用户和服务器都将自己的公钥，发给 CA；（2）CA 使用服务器公钥，生成服务器证书，发给服务器；（3）CA 使用用户的公钥，生成用户证书，发给用户。\n\n有了证书以后，用户就可以登录服务器了。整个过程都是 SSH 自动处理，用户无感知。\n\n第一步，用户登录服务器时，SSH 自动将用户证书发给服务器。\n\n第二步，服务器检查用户证书是否有效，以及是否由可信的 CA 颁发。证实以后，就可以信任用户。\n\n第三步，SSH 自动将服务器证书发给用户。\n\n第四步，用户检查服务器证书是否有效，以及是否由信任的 CA 颁发。证实以后，就可以信任服务器。\n\n第五步，双方建立连接，服务器允许用户登录。\n\n## 生成 CA 的密钥\n\n证书登录的前提是，必须有一个 CA，而 CA 本质上就是一对密钥，跟其他密钥没有不同，CA 就用这对密钥去签发证书。\n\n虽然 CA 可以用同一对密钥签发用户证书和服务器证书，但是出于安全性和灵活性，最好用不同的密钥分别签发。所以，CA 至少需要两对密钥，一对是签发用户证书的密钥，假设叫做`user_ca`，另一对是签发服务器证书的密钥，假设叫做`host_ca`。\n\n使用下面的命令，生成`user_ca`。\n\n```bash\n# 生成 CA 签发用户证书的密钥\n$ ssh-keygen -t rsa -b 4096 -f ~/.ssh/user_ca -C user_ca\n```\n\n上面的命令会在`~/.ssh`目录生成一对密钥：`user_ca`（私钥）和`user_ca.pub`（公钥）。\n\n这个命令的各个参数含义如下。\n\n- `-t rsa`：指定密钥算法 RSA。\n- `-b 4096`：指定密钥的位数是4096位。安全性要求不高的场合，这个值可以小一点，但是不应小于1024。\n- `-f ~/.ssh/user_ca`：指定生成密钥的位置和文件名。\n- `-C user_ca`：指定密钥的识别字符串，相当于注释，可以随意设置。\n\n使用下面的命令，生成`host_ca`。\n\n```bash\n# 生成 CA 签发服务器证书的密钥\n$ ssh-keygen -t rsa -b 4096 -f host_ca -C host_ca\n```\n\n上面的命令会在`~/.ssh`目录生成一对密钥：`host_ca`（私钥）和`host_ca.pub`（公钥）。\n\n现在，`~/.ssh`目录应该至少有四把密钥。\n\n- `~/.ssh/user_ca`\n- `~/.ssh/user_ca.pub`\n- `~/.ssh/host_ca`\n- `~/.ssh/host_ca.pub`\n\n## CA 签发服务器证书\n\n有了 CA 以后，就可以签发服务器证书了。\n\n签发证书，除了 CA 的密钥以外，还需要服务器的公钥。一般来说，SSH 服务器（通常是`sshd`）安装时，已经生成密钥`/etc/ssh/ssh_host_rsa_key`了。如果没有的话，可以用下面的命令生成。\n\n```bash\n$ sudo ssh-keygen -f /etc/ssh/ssh_host_rsa_key -b 4096 -t rsa\n```\n\n上面命令会在`/etc/ssh`目录，生成`ssh_host_rsa_key`（私钥）和`ssh_host_rsa_key.pub`（公钥）。然后，需要把服务器公钥`ssh_host_rsa_key.pub`，复制或上传到 CA 所在的服务器。\n\n上传以后，CA 就可以使用密钥`host_ca`为服务器的公钥`ssh_host_rsa_key.pub`签发服务器证书。\n\n```bash\n$ ssh-keygen -s host_ca -I host.example.com -h -n host.example.com -V +52w ssh_host_rsa_key.pub\n```\n\n上面的命令会生成服务器证书`ssh_host_rsa_key-cert.pub`（服务器公钥名字加后缀`-cert`）。这个命令各个参数的含义如下。\n\n- `-s`：指定 CA 签发证书的密钥。\n- `-I`：身份字符串，可以随便设置，相当于注释，方便区分证书，将来可以使用这个字符串撤销证书。\n- `-h`：指定该证书是服务器证书，而不是用户证书。\n- `-n host.example.com`：指定服务器的域名，表示证书仅对该域名有效。如果有多个域名，则使用逗号分隔。用户登录该域名服务器时，SSH 通过证书的这个值，分辨应该使用哪张证书发给用户，用来证明服务器的可信性。\n- `-V +52w`：指定证书的有效期，这里为52周（一年）。默认情况下，证书是永远有效的。建议使用该参数指定有效期，并且有效期最好短一点，最长不超过52周。\n- `ssh_host_rsa_key.pub`：服务器公钥。\n\n生成证书以后，可以使用下面的命令，查看证书的细节。\n\n```bash\n$ ssh-keygen -L -f ssh_host_rsa_key-cert.pub\n```\n\n最后，为证书设置权限。\n\n```bash\n$ chmod 600 ssh_host_rsa_key-cert.pub\n```\n\n## CA 签发用户证书\n\n下面，再用 CA 签发用户证书。这时需要用户的公钥，如果没有的话，客户端可以用下面的命令生成一对密钥。\n\n```bash\n$ ssh-keygen -f ~/.ssh/user_key -b 4096 -t rsa\n```\n\n上面命令会在`~/.ssh`目录，生成`user_key`（私钥）和`user_key.pub`（公钥）。\n\n然后，将用户公钥`user_key.pub`，上传或复制到 CA 服务器。接下来，就可以使用 CA 的密钥`user_ca`为用户公钥`user_key.pub`签发用户证书。\n\n```bash\n$ ssh-keygen -s user_ca -I user@example.com -n user -V +1d user_key.pub\n```\n\n上面的命令会生成用户证书`user_key-cert.pub`（用户公钥名字加后缀`-cert`）。这个命令各个参数的含义如下。\n\n- `-s`：指定 CA 签发证书的密钥\n- `-I`：身份字符串，可以随便设置，相当于注释，方便区分证书，将来可以使用这个字符串撤销证书。\n- `-n user`：指定用户名，表示证书仅对该用户名有效。如果有多个用户名，使用逗号分隔。用户以该用户名登录服务器时，SSH 通过这个值，分辨应该使用哪张证书，证明自己的身份，发给服务器。\n- `-V +1d`：指定证书的有效期，这里为1天，强制用户每天都申请一次证书，提高安全性。默认情况下，证书是永远有效的。\n- `user_key.pub`：用户公钥。\n\n生成证书以后，可以使用下面的命令，查看证书的细节。\n\n```bash\n$ ssh-keygen -L -f user_key-cert.pub\n```\n\n最后，为证书设置权限。\n\n```bash\n$ chmod 600 user_key-cert.pub\n```\n\n## 服务器安装证书\n\nCA 生成服务器证书`ssh_host_rsa_key-cert.pub`以后，需要将该证书发回服务器，可以使用下面的`scp`命令，将证书拷贝过去。\n\n```bash\n$ scp ~/.ssh/ssh_host_rsa_key-cert.pub root@host.example.com:/etc/ssh/\n```\n\n然后，将下面一行添加到服务器配置文件`/etc/ssh/sshd_config`。\n\n```bash\nHostCertificate /etc/ssh/ssh_host_rsa_key-cert.pub\n```\n\n上面的代码告诉 sshd，服务器证书是哪一个文件。\n\n重新启动 sshd。\n\n```bash\n$ sudo systemctl restart sshd.service\n# 或者\n$ sudo service sshd restart\n```\n\n## 服务器安装 CA 公钥\n\n为了让服务器信任用户证书，必须将 CA 签发用户证书的公钥`user_ca.pub`，拷贝到服务器。\n\n```bash\n$ scp ~/.ssh/user_ca.pub root@host.example.com:/etc/ssh/\n```\n\n上面的命令，将 CA 签发用户证书的公钥`user_ca.pub`，拷贝到 SSH 服务器的`/etc/ssh`目录。\n\n然后，将下面一行添加到服务器配置文件`/etc/ssh/sshd_config`。\n\n```bash\nTrustedUserCAKeys /etc/ssh/user_ca.pub\n```\n\n上面的做法是将`user_ca.pub`加到`/etc/ssh/sshd_config`，这会产生全局效果，即服务器的所有账户都会信任`user_ca`签发的所有用户证书。\n\n另一种做法是将`user_ca.pub`加到服务器某个账户的`~/.ssh/authorized_keys`文件，只让该账户信任`user_ca`签发的用户证书。具体方法是打开`~/.ssh/authorized_keys`，追加一行，开头是`@cert-authority principals=\"…\"`，然后后面加上`user_ca.pub`的内容，大概是下面这个样子。\n\n```bash\n@cert-authority principals=\"user\" ssh-rsa AAAAB3Nz...XNRM1EX2gQ==\n```\n\n上面代码中，`principals=\"user\"`指定用户登录的服务器账户名，一般就是`authorized_keys`文件所在的账户。\n\n重新启动 sshd。\n\n```bash\n$ sudo systemctl restart sshd.service\n# 或者\n$ sudo service sshd restart\n```\n\n至此，SSH 服务器已配置为信任`user_ca`签发的证书。\n\n## 客户端安装证书\n\n客户端安装用户证书很简单，就是从 CA 将用户证书`user_key-cert.pub`复制到客户端，与用户的密钥`user_key`保存在同一个目录即可。\n\n## 客户端安装 CA 公钥\n\n为了让客户端信任服务器证书，必须将 CA 签发服务器证书的公钥`host_ca.pub`，加到客户端的`/etc/ssh/ssh_known_hosts`文件（全局级别）或者`~/.ssh/known_hosts`文件（用户级别）。\n\n具体做法是打开`ssh_known_hosts`或`known_hosts`文件，追加一行，开头为`@cert-authority *.example.com`，然后将`host_ca.pub`文件的内容（即公钥）粘贴在后面，大概是下面这个样子。\n\n```bash\n@cert-authority *.example.com ssh-rsa AAAAB3Nz...XNRM1EX2gQ==\n```\n\n上面代码中，`*.example.com`是域名的模式匹配，表示只要服务器符合该模式的域名，且签发服务器证书的 CA 匹配后面给出的公钥，就都可以信任。如果没有域名限制，这里可以写成`*`。如果有多个域名模式，可以使用逗号分隔；如果服务器没有域名，可以用主机名（比如`host1,host2,host3`）或者 IP 地址（比如`11.12.13.14,21.22.23.24`）。\n\n然后，就可以使用证书，登录远程服务器了。\n\n```bash\n$ ssh -i ~/.ssh/user_key user@host.example.com\n```\n\n上面命令的`-i`参数用来指定用户的密钥。如果证书与密钥在同一个目录，则连接服务器时将自动使用该证书。\n\n## 废除证书\n\n废除证书的操作，分成用户证书的废除和服务器证书的废除两种。\n\n服务器证书的废除，用户需要在`known_hosts`文件里面，修改或删除对应的`@cert-authority`命令的那一行。\n\n用户证书的废除，需要在服务器新建一个`/etc/ssh/revoked_keys`文件，然后在配置文件`sshd_config`添加一行，内容如下。\n\n```bash\nRevokedKeys /etc/ssh/revoked_keys\n```\n\n`revoked_keys`文件保存不再信任的用户公钥，由下面的命令生成。\n\n```bash\n$ ssh-keygen -kf /etc/ssh/revoked_keys -z 1 ~/.ssh/user1_key.pub\n```\n\n上面命令中，`-z`参数用来指定用户公钥保存在`revoked_keys`文件的哪一行，这个例子是保存在第1行。\n\n如果以后需要废除其他的用户公钥，可以用下面的命令保存在第2行。\n\n```bash\n$ ssh-keygen -ukf /etc/ssh/revoked_keys -z 2 ~/.ssh/user2_key.pub\n```\n\n# scp 命令\n\n`scp`是 SSH 提供的一个客户端程序，用来在两台主机之间加密传送文件（即复制文件）。\n\n## 简介\n\n`scp`是 secure copy 的缩写，相当于`cp`命令 + SSH。它的底层是 SSH 协议，默认端口是22，相当于先使用`ssh`命令登录远程主机，然后再执行拷贝操作。\n\n`scp`主要用于以下三种复制操作。\n\n- 本地复制到远程。\n- 远程复制到本地。\n- 两个远程系统之间的复制。\n\n使用`scp`传输数据时，文件和密码都是加密的，不会泄漏敏感信息。\n\n## 基本语法\n\n`scp`的语法类似`cp`的语法。\n\n```bash\n$ scp source destination\n```\n\n上面命令中，`source`是文件当前的位置，`destination`是文件所要复制到的位置。它们都可以包含用户名和主机名。\n\n```bash\n$ scp user@host:foo.txt bar.txt\n```\n\n上面命令将远程主机（`user@host`）用户主目录下的`foo.txt`，复制为本机当前目录的`bar.txt`。可以看到，主机与文件之间要使用冒号（`:`）分隔。\n\n`scp`会先用 SSH 登录到远程主机，然后在加密连接之中复制文件。客户端发起连接后，会提示用户输入密码，这部分是跟 SSH 的用法一致的。\n\n用户名和主机名都是可以省略的。用户名的默认值是本机的当前用户名，主机名默认为当前主机。注意，`scp`会使用 SSH 客户端的配置文件`.ssh/config`，如果配置文件里面定义了主机的别名，这里也可以使用别名连接。\n\n`scp`支持一次复制多个文件。\n\n```bash\n$ scp source1 source2 destination\n```\n\n上面命令会将`source1`和`source2`两个文件，复制到`destination`。\n\n注意，如果所要复制的文件，在目标位置已经存在同名文件，`scp`会在没有警告的情况下覆盖同名文件。\n\n## 用法示例\n\n**（1）本地文件复制到远程**\n\n复制本机文件到远程系统的用法如下。\n\n```bash\n# 语法\n$ scp SourceFile user@host:directory/TargetFile\n\n# 示例\n$ scp file.txt remote_username@10.10.0.2:/remote/directory\n```\n\n下面是复制整个目录的例子。\n\n```bash\n# 将本机的 documents 目录拷贝到远程主机，\n# 会在远程主机创建 documents 目录\n$ scp -r documents username@server_ip:/path_to_remote_directory\n\n# 将本机整个目录拷贝到远程目录下\n$ scp -r localmachine/path_to_the_directory username@server_ip:/path_to_remote_directory/\n\n# 将本机目录下的所有内容拷贝到远程目录下\n$ scp -r localmachine/path_to_the_directory/* username@server_ip:/path_to_remote_directory/\n```\n\n**（2）远程文件复制到本地**\n\n从远程主机复制文件到本地的用法如下。\n\n```bash\n# 语法\n$ scp user@host:directory/SourceFile TargetFile\n\n# 示例\n$ scp remote_username@10.10.0.2:/remote/file.txt /local/directory\n```\n\n下面是复制整个目录的例子。\n\n```bash\n# 拷贝一个远程目录到本机目录下\n$ scp -r username@server_ip:/path_to_remote_directory local-machine/path_to_the_directory/\n\n# 拷贝远程目录下的所有内容，到本机目录下\n$ scp -r username@server_ip:/path_to_remote_directory/* local-machine/path_to_the_directory/\n$ scp -r user@host:directory/SourceFolder TargetFolder\n```\n\n**（3）两个远程系统之间的复制**\n\n本机发出指令，从远程主机 A 拷贝到远程主机 B 的用法如下。\n\n```bash\n# 语法\n$ scp user@host1:directory/SourceFile user@host2:directory/SourceFile\n\n# 示例\n$ scp user1@host1.com:/files/file.txt user2@host2.com:/files\n```\n\n系统将提示你输入两个远程帐户的密码。数据将直接从一个远程主机传输到另一个远程主机。\n\n## 配置项\n\n**（1）`-c`**\n\n`-c`参数用来指定文件拷贝数据传输的加密算法。\n\n```bash\n$ scp -c blowfish some_file your_username@remotehost.edu:~\n```\n\n上面代码指定加密算法为`blowfish`。\n\n**（2）`-C`**\n\n`-C`参数表示是否在传输时压缩文件。\n\n```bash\n$ scp -c blowfish -C local_file your_username@remotehost.edu:~\n```\n\n**（3）`-F`**\n\n`-F`参数用来指定 ssh_config 文件，供 ssh 使用。\n\n```bash\n$ scp -F /home/pungki/proxy_ssh_config Label.pdf root@172.20.10.8:/root\n```\n\n**（4）`-i`**\n\n`-i`参数用来指定密钥。\n\n```bash\n$ scp -vCq -i private_key.pem ~/test.txt root@192.168.1.3:/some/path/test.txt\n```\n\n**（5）`-l`**\n\n`-l`参数用来限制传输数据的带宽速率，单位是 Kbit/sec。对于多人分享的带宽，这个参数可以留出一部分带宽供其他人使用。\n\n```bash\n$ scp -l 80 yourusername@yourserver:/home/yourusername/* .\n```\n\n上面代码中，`scp`命令占用的带宽限制为每秒 80K 比特位，即每秒 10K 字节。\n\n**（6）`-p`**\n\n`-p`参数用来保留修改时间（modification time）、访问时间（access time）、文件状态（mode）等原始文件的信息。\n\n```bash\n$ scp -p ~/test.txt root@192.168.1.3:/some/path/test.txt\n```\n\n**（7）`-P`**\n\n`-P`参数用来指定远程主机的 SSH 端口。如果远程主机使用默认端口22，可以不用指定，否则需要用`-P`参数在命令中指定。\n\n```bash\n$ scp -P 2222 user@host:directory/SourceFile TargetFile\n```\n\n**（8）`-q`**\n\n`-q`参数用来关闭显示拷贝的进度条。\n\n```bash\n$ scp -q Label.pdf mrarianto@202.x.x.x:.\n```\n\n**（9）`-r`**\n\n`-r`参数表示是否以递归方式复制目录。\n\n**（10）`-v`**\n\n`-v`参数用来显示详细的输出。\n\n```bash\n$ scp -v ~/test.txt root@192.168.1.3:/root/help2356.txt\n```\n\n# rsync 命令\n\n## 简介\n\nrsync 是一个常用的 Linux 应用程序，用于文件同步。\n\n它可以在本地计算机与远程计算机之间，或者两个本地目录之间同步文件（但不支持两台远程计算机之间的同步）。它也可以当作文件复制工具，替代`cp`和`mv`命令。\n\n它名称里面的`r`指的是 remote，rsync 其实就是“远程同步”（remote sync）的意思。与其他文件传输工具（如 FTP 或 scp）不同，rsync 的最大特点是会检查发送方和接收方已有的文件，仅传输有变动的部分（默认规则是文件大小或修改时间有变动）。\n\n虽然 rsync 不是 SSH 工具集的一部分，但因为也涉及到远程操作，所以放在这里一起介绍。\n\n## 安装\n\n如果本机或者远程计算机没有安装 rsync，可以用下面的命令安装。\n\n```bash\n# Debian\n$ sudo apt-get install rsync\n\n# Red Hat\n$ sudo yum install rsync\n\n# Arch Linux\n$ sudo pacman -S rsync\n```\n\n注意，传输的双方都必须安装 rsync。\n\n## 基本用法\n\nrsync 可以用于本地计算机的两个目录之间的同步。下面就用本地同步举例，顺便讲解 rsync 几个主要参数的用法。\n\n### `-r`参数\n\n本机使用 rsync 命令时，可以作为`cp`和`mv`命令的替代方法，将源目录拷贝到目标目录。\n\n```bash\n$ rsync -r source destination\n```\n\n上面命令中，`-r`表示递归，即包含子目录。注意，`-r`是必须的，否则 rsync 运行不会成功。`source`目录表示源目录，`destination`表示目标目录。上面命令执行以后，目标目录下就会出现`destination/source`这个子目录。\n\n如果有多个文件或目录需要同步，可以写成下面这样。\n\n```bash\n$ rsync -r source1 source2 destination\n```\n\n上面命令中，`source1`、`source2`都会被同步到`destination`目录。\n\n### `-a`参数\n\n`-a`参数可以替代`-r`，除了可以递归同步以外，还可以同步元信息（比如修改时间、权限等）。由于 rsync 默认使用文件大小和修改时间决定文件是否需要更新，所以`-a`比`-r`更有用。下面的用法才是常见的写法。\n\n```bash\n$ rsync -a source destination\n```\n\n目标目录`destination`如果不存在，rsync 会自动创建。执行上面的命令后，源目录`source`被完整地复制到了目标目录`destination`下面，即形成了`destination/source`的目录结构。\n\n如果只想同步源目录`source`里面的内容到目标目录`destination`，则需要在源目录后面加上斜杠。\n\n```bash\n$ rsync -a source/ destination\n```\n\n上面命令执行后，`source`目录里面的内容，就都被复制到了`destination`目录里面，并不会在`destination`下面创建一个`source`子目录。\n\n### `-n`参数\n\n如果不确定 rsync 执行后会产生什么结果，可以先用`-n`或`--dry-run`参数模拟执行的结果。\n\n```bash\n$ rsync -anv source/ destination\n```\n\n上面命令中，`-n`参数模拟命令执行的结果，并不真的执行命令。`-v`参数则是将结果输出到终端，这样就可以看到哪些内容会被同步。\n\n### `--delete`参数\n\n默认情况下，rsync 只确保源目录的所有内容（明确排除的文件除外）都复制到目标目录。它不会使两个目录保持相同，并且不会删除文件。如果要使得目标目录成为源目录的镜像副本，则必须使用`--delete`参数，这将删除只存在于目标目录、不存在于源目录的文件。\n\n```bash\n$ rsync -av --delete source/ destination\n```\n\n上面命令中，`--delete`参数会使得`destination`成为`source`的一个镜像。\n\n## 排除文件\n\n### `--exclude`参数\n\n有时，我们希望同步时排除某些文件或目录，这时可以用`--exclude`参数指定排除模式。\n\n```bash\n$ rsync -av --exclude='*.txt' source/ destination\n# 或者\n$ rsync -av --exclude '*.txt' source/ destination\n```\n\n上面命令排除了所有 TXT 文件。\n\n注意，rsync 会同步以“点”开头的隐藏文件，如果要排除隐藏文件，可以这样写`--exclude=\".*\"`。\n\n如果要排除某个目录里面的所有文件，但不希望排除目录本身，可以写成下面这样。\n\n```bash\n$ rsync -av --exclude 'dir1/*' source/ destination\n```\n\n多个排除模式，可以用多个`--exclude`参数。\n\n```bash\n$ rsync -av --exclude 'file1.txt' --exclude 'dir1/*' source/ destination\n```\n\n多个排除模式也可以利用 Bash 的大扩号的扩展功能，只用一个`--exclude`参数。\n\n```bash\n$ rsync -av --exclude={'file1.txt','dir1/*'} source/ destination\n```\n\n如果排除模式很多，可以将它们写入一个文件，每个模式一行，然后用`--exclude-from`参数指定这个文件。\n\n```bash\n$ rsync -av --exclude-from='exclude-file.txt' source/ destination\n```\n\n### `--include`参数\n\n`--include`参数用来指定必须同步的文件模式，往往与`--exclude`结合使用。\n\n```bash\n$ rsync -av --include=\"*.txt\" --exclude='*' source/ destination\n```\n\n上面命令指定同步时，排除所有文件，但是会包括 TXT 文件。\n\n## 远程同步\n\n### SSH 协议\n\nrsync 除了支持本地两个目录之间的同步，也支持远程同步。它可以将本地内容，同步到远程服务器。\n\n```bash\n$ rsync -av source/ username@remote_host:destination\n```\n\n也可以将远程内容同步到本地。\n\n```bash\n$ rsync -av username@remote_host:source/ destination\n```\n\nrsync 默认使用 SSH 进行远程登录和数据传输。\n\n由于早期 rsync 不使用 SSH 协议，需要用`-e`参数指定协议，后来才改的。所以，下面`-e ssh`可以省略。\n\n```bash\n$ rsync -av -e ssh source/ user@remote_host:/destination\n```\n\n但是，如果 ssh 命令有附加的参数，则必须使用`-e`参数指定所要执行的 SSH 命令。\n\n```bash\n$ rsync -av -e 'ssh -p 2234' source/ user@remote_host:/destination\n```\n\n上面命令中，`-e`参数指定 SSH 使用2234端口。\n\n### rsync 协议\n\n除了使用 SSH，如果另一台服务器安装并运行了 rsync 守护程序，则也可以用`rsync://`协议（默认端口873）进行传输。具体写法是服务器与目标目录之间使用双冒号分隔`::`。\n\n```bash\n$ rsync -av source/ 192.168.122.32::module/destination\n```\n\n注意，上面地址中的`module`并不是实际路径名，而是 rsync 守护程序指定的一个资源名，由管理员分配。\n\n如果想知道 rsync 守护程序分配的所有 module 列表，可以执行下面命令。\n\n```bash\n$ rsync rsync://192.168.122.32\n```\n\nrsync 协议除了使用双冒号，也可以直接用`rsync://`协议指定地址。\n\n```bash\n$ rsync -av source/ rsync://192.168.122.32/module/destination\n```\n\n## 增量备份\n\nrsync 的最大特点就是它可以完成增量备份，也就是默认只复制有变动的文件。\n\n除了源目录与目标目录直接比较，rsync 还支持使用基准目录，即将源目录与基准目录之间变动的部分，同步到目标目录。\n\n具体做法是，第一次同步是全量备份，所有文件在基准目录里面同步一份。以后每一次同步都是增量备份，只同步源目录与基准目录之间有变动的部分，将这部分保存在一个新的目标目录。这个新的目标目录之中，也是包含所有文件，但实际上，只有那些变动过的文件是存在于该目录，其他没有变动的文件都是指向基准目录文件的硬链接。\n\n`--link-dest`参数用来指定同步时的基准目录。\n\n```bash\n$ rsync -a --delete --link-dest /compare/path /source/path /target/path\n```\n\n上面命令中，`--link-dest`参数指定基准目录`/compare/path`，然后源目录`/source/path`跟基准目录进行比较，找出变动的文件，将它们拷贝到目标目录`/target/path`。那些没变动的文件则会生成硬链接。这个命令的第一次备份时是全量备份，后面就都是增量备份了。\n\n下面是一个脚本示例，备份用户的主目录。\n\n```bash\n#!/bin/bash\n\n# A script to perform incremental backups using rsync\n\nset -o errexit\nset -o nounset\nset -o pipefail\n\nreadonly SOURCE_DIR=\"${HOME}\"\nreadonly BACKUP_DIR=\"/mnt/data/backups\"\nreadonly DATETIME=\"$(date '+%Y-%m-%d_%H:%M:%S')\"\nreadonly BACKUP_PATH=\"${BACKUP_DIR}/${DATETIME}\"\nreadonly LATEST_LINK=\"${BACKUP_DIR}/latest\"\n\nmkdir -p \"${BACKUP_DIR}\"\n\nrsync -av --delete \\\n  \"${SOURCE_DIR}/\" \\\n  --link-dest \"${LATEST_LINK}\" \\\n  --exclude=\".cache\" \\\n  \"${BACKUP_PATH}\"\n\nrm -rf \"${LATEST_LINK}\"\nln -s \"${BACKUP_PATH}\" \"${LATEST_LINK}\"\n```\n\n上面脚本中，每一次同步都会生成一个新目录`${BACKUP_DIR}/${DATETIME}`，并将软链接`${BACKUP_DIR}/latest`指向这个目录。下一次备份时，就将`${BACKUP_DIR}/latest`作为基准目录，生成新的备份目录。最后，再将软链接`${BACKUP_DIR}/latest`指向新的备份目录。\n\n## 配置项\n\n`-a`、`--archive`参数表示存档模式，保存所有的元数据，比如修改时间（modification time）、权限、所有者等，并且软链接也会同步过去。\n\n`--append`参数指定文件接着上次中断的地方，继续传输。\n\n`--append-verify`参数跟`--append`参数类似，但会对传输完成后的文件进行一次校验。如果校验失败，将重新发送整个文件。\n\n`-b`、`--backup`参数指定在删除或更新目标目录已经存在的文件时，将该文件更名后进行备份，默认行为是删除。更名规则是添加由`--suffix`参数指定的文件后缀名，默认是`~`。\n\n`--backup-dir`参数指定文件备份时存放的目录，比如`--backup-dir=/path/to/backups`。\n\n`--bwlimit`参数指定带宽限制，默认单位是 KB/s，比如`--bwlimit=100`。\n\n`-c`、`--checksum`参数改变`rsync`的校验方式。默认情况下，rsync 只检查文件的大小和最后修改日期是否发生变化，如果发生变化，就重新传输；使用这个参数以后，则通过判断文件内容的校验和，决定是否重新传输。\n\n`--delete`参数删除只存在于目标目录、不存在于源目标的文件，即保证目标目录是源目标的镜像。\n\n`-e`参数指定使用 SSH 协议传输数据。\n\n`--exclude`参数指定排除不进行同步的文件，比如`--exclude=\"*.iso\"`。\n\n`--exclude-from`参数指定一个本地文件，里面是需要排除的文件模式，每个模式一行。\n\n`--existing`、`--ignore-non-existing`参数表示不同步目标目录中不存在的文件和目录。\n\n`-h`参数表示以人类可读的格式输出。\n\n`-h`、`--help`参数返回帮助信息。\n\n`-i`参数表示输出源目录与目标目录之间文件差异的详细情况。\n\n`--ignore-existing`参数表示只要该文件在目标目录中已经存在，就跳过去，不再同步这些文件。\n\n`--include`参数指定同步时要包括的文件，一般与`--exclude`结合使用。\n\n`--link-dest`参数指定增量备份的基准目录。\n\n`-m`参数指定不同步空目录。\n\n`--max-size`参数设置传输的最大文件的大小限制，比如不超过200KB（`--max-size='200k'`）。\n\n`--min-size`参数设置传输的最小文件的大小限制，比如不小于10KB（`--min-size=10k`）。\n\n`-n`参数或`--dry-run`参数模拟将要执行的操作，而并不真的执行。配合`-v`参数使用，可以看到哪些内容会被同步过去。\n\n`-P`参数是`--progress`和`--partial`这两个参数的结合。\n\n`--partial`参数允许恢复中断的传输。不使用该参数时，`rsync`会删除传输到一半被打断的文件；使用该参数后，传输到一半的文件也会同步到目标目录，下次同步时再恢复中断的传输。一般需要与`--append`或`--append-verify`配合使用。\n\n`--partial-dir`参数指定将传输到一半的文件保存到一个临时目录，比如`--partial-dir=.rsync-partial`。一般需要与`--append`或`--append-verify`配合使用。\n\n`--progress`参数表示显示进展。\n\n`-r`参数表示递归，即包含子目录。\n\n`--remove-source-files`参数表示传输成功后，删除发送方的文件。\n\n`--size-only`参数表示只同步大小有变化的文件，不考虑文件修改时间的差异。\n\n`--suffix`参数指定文件名备份时，对文件名添加的后缀，默认是`~`。\n\n`-u`、`--update`参数表示同步时跳过目标目录中修改时间更新的文件，即不同步这些有更新的时间戳的文件。\n\n`-v`参数表示输出细节。`-vv`表示输出更详细的信息，`-vvv`表示输出最详细的信息。\n\n`--version`参数返回 rsync 的版本。\n\n`-z`参数指定同步时压缩数据。\n\n## 参考链接\n\n- [How To Use Rsync to Sync Local and Remote Directories on a VPS](https://www.digitalocean.com/community/tutorials/how-to-use-rsync-to-sync-local-and-remote-directories-on-a-vps), Justin Ellingwood\n- [Mirror Your Web Site With rsync](https://www.howtoforge.com/mirroring_with_rsync), Falko Timme\n- [Examples on how to use Rsync](https://linuxconfig.org/examples-on-how-to-use-rsync-for-local-and-remote-data-backups-and-synchonization), Egidio Docile\n- [How to create incremental backups using rsync on Linux](https://linuxconfig.org/how-to-create-incremental-backups-using-rsync-on-linux), Egidio Docile\n\n# sftp 命令\n\n`sftp`是 SSH 提供的一个客户端应用程序，主要用来安全地访问 FTP。因为 FTP 是不加密协议，很不安全，`sftp`就相当于将 FTP 放入了 SSH。\n\n下面的命令连接 FTP 主机。\n\n```bash\n$ sftp username@hostname\n```\n\n执行上面的命令，会要求输入 FTP 的密码。密码验证成功以后，就会出现 FTP 的提示符`sftp\u003e `，下面是一个例子。\n\n```bash\n$ sftp USER@penguin.example.com\nUSER@penguin.example.com's password:\nConnected to penguin.example.com.\nsftp\u003e\n```\n\nFTP 的提示符下面，就可以输入各种 FTP 命令了，这部分完全跟传统的 FTP 用法完全一样。\n\n- `ls [directory]`：列出一个远程目录的内容。如果没有指定目标目录，则默认列出当前目录。\n- `cd directory`：从当前目录改到指定目录。\n- `mkdir directory`：创建一个远程目录。\n- `rmdir path`：删除一个远程目录。\n- `put localfile [remotefile]`：本地文件传输到远程主机。\n- `get remotefile [localfile]`：远程文件传输到本地。\n- `help`：显示帮助信息。\n- `bye`：退出 sftp。\n- `quit`：退出 sftp。\n- `exit`：退出 sftp。\n","lastmodified":"2023-05-09T16:33:58.287366302Z","tags":[]},"/Vue2%E5%9F%BA%E7%A1%80":{"title":"Vue","content":"\n之前学习很有问题，只看视频不记笔记，直接把机构的笔记复制来也不整理，痛定思痛，特整理如下。\n\n## Vue 是什么？\n\n- **Vue (读音 /vjuː/，类似于 **view**) 是一套用于构建用户界面的渐进式框架**\n- Vue.js 是目前最火的一个前端框架，React是最流行的一个前端框架（React除了开发网站，还可以开发手机App， Vue语法也是可以用于进行手机App开发的，需要借助于Weex）\n- Vue.js 是前端的**主流框架之一**，和Angular.js、React.js 一起，并成为前端三大主流框架！\n- Vue.js 是一套构建用户界面的框架，**只关注视图层**，它不仅易于上手，还便于与第三方库或既有项目整合。（Vue有配套的第三方类库，可以整合起来做大型项目的开发）\n- 前端的主要工作？主要负责MVC中的V这一层；主要工作就是和界面打交道，来制作前端页面效果；\n\n## 框架和库的区别\n\n - 框架：是一套完整的解决方案；对项目的侵入性较大，项目如果需要更换框架，则需要重新架构整个项目。\n  - node 中的 express；\n - 库（插件）：提供某一个小功能，对项目的侵入性较小，如果某个库无法完成某些需求，可以很容易切换到其它库实现需求。\n  - 1. 从Jquery 切换到 Zepto\n  - 2. 从 EJS 切换到 art-template\n\n## Node（后端）中的 MVC 与 前端中的 MVVM 之间的区别\n\n - MVC 是后端的分层开发概念；\n - MVVM是前端视图层的概念，主要关注于 视图层分离，也就是说：MVVM把前端的视图层，分为了 三部分 Model, View , VM ViewModel\n - 为什么有了MVC还要有MVVM\n\n## Vue.js 基本代码 和 MVVM 之间的对应关系\n\n![01.MVC和MVVM的关系图解](01.MVC和MVVM的关系图解-3997941.png)\n\n## 使用Vue将helloworld 渲染到页面上\n\n## 指令\n\n- 本质就是自定义属性\n- Vue中指定都是以 v- 开头\n\n### v-cloak\n\n防止页面加载时出现闪烁问题\n\nv-cloak指令的用法\n\n1. 提供样式\n\n*[v-cloak]{*\n\ndisplay: none;\n\n}\n\n2. 在插值表达式所在的标签中添加v-cloak指令\n\n *背后的原理：先通过样式隐藏内容，然后在内存中进行值的替换，替换好之后再显示最终的结果*\n\n```html\n \u003cstyle type=\"text/css\"\u003e\n  /* \n    1、通过属性选择器 选择到 带有属性 v-cloak的标签  让他隐藏\n */\n  [v-cloak]{\n    /* 元素隐藏    */\n    display: none;\n  }\n  \u003c/style\u003e\n\u003cbody\u003e\n  \u003cdiv id=\"app\"\u003e\n    \u003c!-- 2、 让带有插值语法的添加 v-cloak 属性 \n         在 数据渲染完场之后，v-cloak 属性会被自动去除，\n         v-cloak一旦移除也就是没有这个属性了  属性选择器就选择不到该标签\n\t\t\t\t也就是对应的标签会变为可见\n    --\u003e\n    \u003cdiv v-cloak\u003e{{msg}}\u003c/div\u003e\n  \u003c/div\u003e\n  \u003cscript type=\"text/javascript\" src=\"js/vue.js\"\u003e\u003c/script\u003e\n  \u003cscript type=\"text/javascript\"\u003e\n    var vm = new Vue({\n      //  el指定元素 id 是 app 的元素  \n      el: '#app',\n      //  data  里面存储的是数据\n      data: {\n        msg: 'Hello Vue'\n      }\n    });\n\u003c/script\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n```\n\n### v-text\n\n- v-text指令用于将数据填充到标签中，作用于插值表达式类似，但是没有闪动问题\n- 如果数据中有HTML标签会将html标签一并输出\n- 注意：此处为单向绑定，数据对象上的值改变，插值会发生变化；但是当插值发生变化并不会影响数据对象的值。\n\n```html\n\u003cdiv id=\"app\"\u003e\n    \u003c!--  \n\t\t注意:在指令中不要写插值语法  直接写对应的变量名称 \n        在 v-text 中 赋值的时候不要在写 插值语法\n\t\t一般属性中不加 {{}}  直接写 对应 的数据名 \n\t--\u003e\n    \u003cp v-text=\"msg\"\u003e\u003c/p\u003e\n    \u003cp\u003e\n        \u003c!-- Vue  中只有在标签的 内容中 才用插值语法 --\u003e\n        {{msg}}\n    \u003c/p\u003e\n\u003c/div\u003e\n\n\u003cscript\u003e\n    new Vue({\n        el: '#app',\n        data: {\n            msg: 'Hello Vue.js'\n        }\n    });\n\n\u003c/script\u003e\n```\n\n### v-html\n\n- 用法和v-text 相似 但是他可以将HTML片段填充到标签中\n- 可能有安全问题, 一般只在可信任内容上使用 `v-html`，**永不**用在用户提交的内容上\n- 它与v-text区别在于v-text输出的是纯文本，浏览器不会对其再进行html解析，但v-html会将其当html标签解析后输出。\n\n  ```html\n  \u003cdiv id=\"app\"\u003e\n  　　\u003cp v-html=\"html\"\u003e\u003c/p\u003e \u003c!-- 输出：html标签在渲染的时候被解析 --\u003e\n      \n      \u003cp\u003e{{message}}\u003c/p\u003e \u003c!-- 输出：\u003cspan\u003e通过双括号绑定\u003c/span\u003e --\u003e\n      \n  　　\u003cp v-text=\"text\"\u003e\u003c/p\u003e \u003c!-- 输出：\u003cspan\u003ehtml标签在渲染的时候被源码输出\u003c/span\u003e --\u003e\n  \u003c/div\u003e\n  \u003cscript\u003e\n  　　let app = new Vue({\n  　　el: \"#app\",\n  　　data: {\n  　　　　message: \"\u003cspan\u003e通过双括号绑定\u003c/span\u003e\",\n  　　　　html: \"\u003cspan\u003ehtml标签在渲染的时候被解析\u003c/span\u003e\",\n  　　　　text: \"\u003cspan\u003ehtml标签在渲染的时候被源码输出\u003c/span\u003e\",\n  　　}\n   });\n  \u003c/script\u003e\n  ```\n\n### v-pre\n\n- 显示原始信息跳过编译过程\n- 跳过这个元素和它的子元素的编译过程。\n- **一些静态的内容不需要编译加这个指令可以加快渲染**\n\n```html\n    \u003cspan v-pre\u003e{{ this will not be compiled }}\u003c/span\u003e    \n\t\u003c!--  显示的是{{ this will not be compiled }}  --\u003e\n\t\u003cspan v-pre\u003e{{msg}}\u003c/span\u003e  \n     \u003c!--   即使data里面定义了msg这里仍然是显示的{{msg}}  --\u003e\n\u003cscript\u003e\n    new Vue({\n        el: '#app',\n        data: {\n            msg: 'Hello Vue.js'\n        }\n    });\n\u003c/script\u003e\n```\n\n### text html pre比较\n\n![image-20200312153450168](image-20200312153450168.png)\n\nv-text指令用于将数据填充到标签中，作用于插值表达式类似，但是没有闪动问题  \nv-html指令用于将HTML片段填充到标签中，但是可能有安全问题  \nv-pre用于显示原始信息\n\n### **v-once**\n\n- 执行一次性的插值【当数据改变时，插值处的内容不会继续更新】\n\n```html\n  \u003c!-- 即使data里面定义了msg 后期我们修改了 仍然显示的是第一次data里面存储的数据即 Hello Vue.js  --\u003e\n     \u003cspan v-once\u003e{{ msg}}\u003c/span\u003e    \n\u003cscript\u003e\n    new Vue({\n        el: '#app',\n        data: {\n            msg: 'Hello Vue.js'\n        }\n    });\n\u003c/script\u003e\n```\n\n### 双向数据绑定\n\n- 当数据发生变化的时候，视图也就发生变化\n- 当视图发生变化的时候，数据也会跟着同步变化\n\n#### v-model\n\n- **v-model**是一个指令，限制在 `\u003cinput\u003e、\u003cselect\u003e、\u003ctextarea\u003e、components`中使用\n\n```html\n \u003cdiv id=\"app\"\u003e\n      \u003cdiv\u003e{{msg}}\u003c/div\u003e\n      \u003cdiv\u003e\n          当输入框中内容改变的时候，  页面上的msg  会自动更新\n        \u003cinput type=\"text\" v-model='msg'\u003e\n      \u003c/div\u003e\n  \u003c/div\u003e\n```\n\n### v-on\n\n- 用来绑定事件的\n- 形式如：v-on:click 缩写为 @click;\n\n\u003cimg src=\"17-21 Vue.js项目实战开发/17-19vue基础/day01/4-笔记/images/@click.png\" width=\"90%\"\u003e\n\n### v-on事件函数中传入参数\n\n```html\n\u003cbody\u003e\n\u003cdiv id=\"app\"\u003e\n    \u003cdiv\u003e{{num}}\u003c/div\u003e\n    \u003cdiv\u003e\n        \u003c!-- 如果事件直接绑定函数名称，那么默认会传递事件对象作为事件函数的第一个参数 --\u003e\n        \u003cbutton v-on:click='handle1'\u003e点击1\u003c/button\u003e\n        \u003c!-- 2、如果事件绑定函数调用，那么事件对象必须作为最后一个参数显示传递，\n             并且事件对象的名称必须是$event \n        --\u003e\n        \u003cbutton v-on:click='handle2(123, 456, $event)'\u003e点击2\u003c/button\u003e\n    \u003c/div\u003e\n\u003c/div\u003e\n\u003cscript type=\"text/javascript\" src=\"js/vue.js\"\u003e\u003c/script\u003e\n\u003cscript type=\"text/javascript\"\u003e\n    var vm = new Vue({\n        el: '#app',\n        data: {\n            num: 0\n        },\n        methods: {\n            handle1: function(event) {\n                console.log(event.target.innerHTML)\n            },\n            handle2: function(p, p1, event) {\n                console.log(p, p1)\n                console.log(event.target.innerHTML)\n                this.num++;\n            }\n        }\n    });\n\u003c/script\u003e\n```\n\n### v-bind\n\n- v-bind 指令被用来响应地更新HTML属性\n- v-bind:href 可以缩写为 :href;\n\n```html\n\u003c!-- 绑定一个属性 --\u003e\n\u003cimg v-bind:src=\"imageSrc\"\u003e\n\n\u003c!-- 缩写 --\u003e\n\u003cimg :src=\"imageSrc\"\u003e\n```\n\n#### 绑定对象\n\n- 我们可以给`v-bind:class` 一个对象，以动态地切换class。\n- 注意：`v-bind:class`指令可以与普通的class特性共存\n\n```html\n1、 v-bind 中支持绑定一个对象 \n\t如果绑定的是一个对象 则 键为 对应的类名  值 为对应data中的数据 \n\u003c!-- \n\tHTML最终渲染为 \u003cul class=\"box textColor textSize\"\u003e\u003c/ul\u003e\n\t注意：\n\t\ttextColor，textSize  对应的渲染到页面上的CSS类名\t\n\t\tisColor，isSize  对应vue data中的数据  如果为true 则对应的类名 渲染到页面上 \n\t\t当 isColor 和 isSize 变化时，class列表将相应的更新，\n\t\t例如，将isSize改成false，\n\t\tclass列表将变为 \u003cul class=\"box textColor\"\u003e\u003c/ul\u003e\n--\u003e\n\n\u003cul class=\"box\" v-bind:class=\"{textColor:isColor, textSize:isSize}\"\u003e\n    \u003cli\u003e学习Vue\u003c/li\u003e\n    \u003cli\u003e学习Node\u003c/li\u003e\n    \u003cli\u003e学习React\u003c/li\u003e\n\u003c/ul\u003e\n  \u003cdiv v-bind:style=\"{color:activeColor,fontSize:activeSize}\"\u003e对象语法\u003c/div\u003e\n\n\u003csript\u003e\nvar vm= new Vue({\n    el:'.box',\n    data:{\n        isColor:true,\n        isSize:true，\n    \t\tactiveColor:\"red\",\n        activeSize:\"25px\",\n    }\n})\n\u003c/sript\u003e\n\u003cstyle\u003e\n\n    .box{\n        border:1px dashed #f0f;\n    }\n    .textColor{\n        color:#f00;\n        background-color:#eef;\n    }\n    .textSize{\n        font-size:30px;\n        font-weight:bold;\n    }\n\u003c/style\u003e\n```\n\n#### 绑定class\n\n```html\n2、  v-bind 中支持绑定一个数组    数组中classA和 classB 对应为data中的数据\n\n这里的classA  对用data 中的  classA\n这里的classB  对用data 中的  classB\n\u003cul class=\"box\" :class=\"[classA, classB]\"\u003e\n    \u003cli\u003e学习Vue\u003c/li\u003e\n    \u003cli\u003e学习Node\u003c/li\u003e\n    \u003cli\u003e学习React\u003c/li\u003e\n\u003c/ul\u003e\n\u003cscript\u003e\nvar vm= new Vue({\n    el:'.box',\n    data:{\n        classA:‘textColor‘,\n        classB:‘textSize‘\n    }\n})\n\u003c/script\u003e\n\u003cstyle\u003e\n    .box{\n        border:1px dashed #f0f;\n    }\n    .textColor{\n        color:#f00;\n        background-color:#eef;\n    }\n    .textSize{\n        font-size:30px;\n        font-weight:bold;\n    }\n\u003c/style\u003e\n```\n\n#### 绑定对象和绑定数组 的区别\n\n- 绑定对象的时候 对象的属性 即要渲染的类名 对象的属性值对应的是 data 中的数据\n- 绑定数组的时候数组里面存的是data 中的数据\n\n#### 绑定style\n\n```html\n \u003cdiv v-bind:style=\"styleObject\"\u003e绑定样式对象\u003c/div\u003e'\n \n\u003c!-- CSS 属性名可以用驼峰式 (camelCase) 或短横线分隔 (kebab-case，记得用单引号括起来)    --\u003e\n \u003cdiv v-bind:style=\"{ color: activeColor, fontSize: fontSize,background:'red' }\"\u003e内联样式\u003c/div\u003e\n\n\u003c!--组语法可以将多个样式对象应用到同一个元素 --\u003e\n\u003cdiv v-bind:style=\"[styleObj1, styleObj2]\"\u003e\u003c/div\u003e\n\n\u003cscript\u003e\n\tnew Vue({\n      el: '#app',\n      data: {\n        styleObject: {\n          color: 'green',\n          fontSize: '30px',\n          background:'red'\n        }，\n        activeColor: 'green',\n   \t\tfontSize: \"30px\"\n      },\n      styleObj1: {\n             color: 'red'\n       },\n       styleObj2: {\n            fontSize: '30px'\n       }\n\n\u003c/script\u003e\n```\n\n### v-if判断\n\n#### v-if 使用场景\n\n- 1- 多个元素 通过条件判断展示或者隐藏某个元素。或者多个元素\n- 2- 进行两个视图之间的切换\n\n```html\n\u003cdiv id=\"app\"\u003e\n        \u003c!--  判断是否加载，如果为真，就加载，否则不加载--\u003e\n        \u003cspan v-if=\"flag\"\u003e\n           如果flag为true则显示,false不显示!\n        \u003c/span\u003e\n\u003c/div\u003e\n\n\u003cscript\u003e\n    var vm = new Vue({\n        el:\"#app\",\n        data:{\n            flag:true\n        }\n    })\n\u003c/script\u003e\n\n----------------------------------------------------------\n\n    \u003cdiv v-if=\"type === 'A'\"\u003e\n       A\n    \u003c/div\u003e\n  \u003c!-- v-else-if紧跟在v-if或v-else-if之后   表示v-if条件不成立时执行--\u003e\n    \u003cdiv v-else-if=\"type === 'B'\"\u003e\n       B\n    \u003c/div\u003e\n    \u003cdiv v-else-if=\"type === 'C'\"\u003e\n       C\n    \u003c/div\u003e\n  \u003c!-- v-else紧跟在v-if或v-else-if之后--\u003e\n    \u003cdiv v-else\u003e\n       Not A/B/C\n    \u003c/div\u003e\n\n\u003cscript\u003e\n    new Vue({\n      el: '#app',\n      data: {\n        type: 'C'\n      }\n    })\n\u003c/script\u003e\n```\n\n#### v-show 和 v-if的区别\n\n- v-show本质就是标签display设置为none，控制隐藏\n  - v-show只编译一次，后面其实就是控制css，而v-if不停的销毁和创建，故v-show性能更好一点。\n- v-if是动态的向DOM树内添加或者删除DOM元素\n  - v-if切换有一个局部编译/卸载的过程，切换过程中合适地销毁和重建内部的事件监听和子组件\n\n### v-for循环结构\n\n用于循环的数组里面的值可以是对象，也可以是普通元素  \n\n```html\n\u003cul id=\"example-1\"\u003e\n   \u003c!-- 循环结构-遍历数组  \n\titem 是我们自己定义的一个名字  代表数组里面的每一项  \n\titems对应的是 data中的数组--\u003e\n  \u003cli v-for=\"item in items\"\u003e\n    {{ item.message }}\n  \u003c/li\u003e \n\n\u003c/ul\u003e\n\u003cscript\u003e\n new Vue({\n  el: '#example-1',\n  data: {\n    items: [\n      { message: 'Foo' },\n      { message: 'Bar' }\n    ]，\n   \n  }\n})\n\u003c/script\u003e\n```\n\n- **不推荐**同时使用 `v-if` 和 `v-for`\n- 当 `v-if` 与 `v-for` 一起使用时，`v-for` 具有比 `v-if` 更高的优先级。\n\n```html\n   \u003c!--  循环结构-遍历对象\n\t\tv 代表   对象的value\n\t\tk  代表对象的 键 \n\t\ti  代表索引\t\n\t---\u003e \n     \u003cdiv v-if='v==13' v-for='(v,k,i) in obj'\u003e{{v + '---' + k + '---' + i}}\u003c/div\u003e\n\n\u003cscript\u003e\n new Vue({\n  el: '#example-1',\n  data: {\n    items: [\n      { message: 'Foo' },\n      { message: 'Bar' }\n    ]，\n    obj: {\n        uname: 'zhangsan',\n        age: 13,\n        gender: 'female'\n    }\n  }\n})\n\u003c/script\u003e\n```\n\n- key 的作用\n  - **key来给每个节点做一个唯一标识**\n  - **key的作用主要是为了高效的更新虚拟DOM**\n\n```html\n\u003cul\u003e\n  \u003cli v-for=\"item in items\" :key=\"item.id\"\u003e...\u003c/li\u003e\n\u003c/ul\u003e\n\n```\n\n### 案例选项卡\n\n#### 1、 HTML 结构\n\n```html\n`\n    \u003cdiv id=\"app\"\u003e\n        \u003cdiv class=\"tab\"\u003e\n            \u003c!--  tab栏  --\u003e\n            \u003cul\u003e\n                \u003cli class=\"active\"\u003eapple\u003c/li\u003e\n                \u003cli class=\"\"\u003eorange\u003c/li\u003e\n                \u003cli class=\"\"\u003elemon\u003c/li\u003e\n            \u003c/ul\u003e\n              \u003c!--  对应显示的图片 --\u003e\n            \u003cdiv class=\"current\"\u003e\u003cimg src=\"img/apple.png\"\u003e\u003c/div\u003e\n            \u003cdiv class=\"\"\u003e\u003cimg src=\"img/orange.png\"\u003e\u003c/div\u003e\n            \u003cdiv class=\"\"\u003e\u003cimg src=\"img/lemon.png\"\u003e\u003c/div\u003e\n        \u003c/div\u003e\n    \u003c/div\u003e\n\n\n`\n```\n\n#### 2、 提供的数据\n\n```js\n         list: [{\n                    id: 1,\n                    title: \"'apple',\"\n                    path: 'img/apple.png'\n                }, {\n                    id: 2,\n                    title: \"'orange',\"\n                    path: 'img/orange.png'\n                }, {\n                    id: 3,\n                    title: \"'lemon',\"\n                    path: 'img/lemon.png'\n                }]\n```\n\n#### 3、 把数据渲染到页面\n\n- 把tab栏 中的数替换到页面上\n  - 把 data 中 title 利用 v-for 循环渲染到页面上\n  - 把 data 中 path利用 v-for 循环渲染到页面上\n\n  ```html\n      \u003cdiv id=\"app\"\u003e\n          \u003cdiv class=\"tab\"\u003e  \n              \u003cul\u003e\n                    \u003c!--  \n                      1、绑定key的作用 提高Vue的性能 \n                      2、 key 需要是唯一的标识 所以需要使用id， 也可以使用index ，\n  \t\t\t\t\t\tindex 也是唯一的 \n                      3、 item 是 数组中对应的每一项  \n                      4、 index 是 每一项的 索引\n                  --\u003e\n                     \u003cli :key='item.id' v-for='(item,index) in list'\u003e{{item.title}}\u003c/li\u003e\n                \u003c/ul\u003e\n                \u003cdiv  :key='item.id' v-for='(item, index) in list'\u003e\n                      \u003c!-- :  是 v-bind 的简写   绑定属性使用 v-bind --\u003e\n                      \u003cimg :src=\"item.path\"\u003e\n                \u003c/div\u003e\n          \u003c/div\u003e\n      \u003c/div\u003e\n  \u003cscript\u003e\n      new  Vue({\n          //  指定 操作元素 是 id 为app 的 \n          el: '#app',\n              data: {\n                  list: [{\n                      id: 1,\n                      title: \"'apple',\"\n                      path: 'img/apple.png'\n                  }, {\n                      id: 2,\n                      title: \"'orange',\"\n                      path: 'img/orange.png'\n                  }, {\n                      id: 3,\n                      title: \"'lemon',\"\n                      path: 'img/lemon.png'\n                  }]\n              }\n      })\n  \n  \u003c/script\u003e\n  ```\n\n#### 4、 给每一个tab栏添加事件,并让选中的高亮\n\n- 4.1 、让默认的第一项tab栏高亮\n  - tab栏高亮 通过添加类名active 来实现 （CSS active 的样式已经提前写好）\n    - 在data 中定义一个 默认的 索引 currentIndex 为  0\n    - 给第一个li 添加 active 的类名  \n      - 通过动态绑定class 来实现 第一个li 的索引为 0 和 currentIndex 的值刚好相等\n      - currentIndex === index 如果相等 则添加类名 active 否则 添加 空类名\n- 4.2 、让默认的第一项tab栏对应的div 显示\n  - 实现思路 和 第一个 tab 实现思路一样 只不过 这里控制第一个div 显示的类名是 current\n\n  ```html\n    \u003cul\u003e\n  \t   \u003c!-- 动态绑定class   有 active   类名高亮  无 active   不高亮--\u003e\n         \u003cli  :class='currentIndex==index?\"active\":\"\"'\n             :key='item.id' v-for='(item,index) in list'\n             \u003e{{item.title}}\u003c/li\u003e\n    \u003c/ul\u003e\n  \t\u003c!-- 动态绑定class   有 current  类名显示  无 current  隐藏--\u003e\n    \u003cdiv :class='currentIndex==index?\"current\":\"\"' \n         \n         :key='item.id' v-for='(item, index) in list'\u003e\n          \u003c!-- :  是 v-bind 的简写   绑定属性使用 v-bind --\u003e\n          \u003cimg :src=\"item.path\"\u003e\n    \u003c/div\u003e\n  \n  \u003cscript\u003e\n      new  Vue({\n          el: '#app',\n              data: {\n                  currentIndex: 0, // 选项卡当前的索引  默认为 0  \n                  list: [{\n                      id: 1,\n                      title: \"'apple',\"\n                      path: 'img/apple.png'\n                  }, {\n                      id: 2,\n                      title: \"'orange',\"\n                      path: 'img/orange.png'\n                  }, {\n                      id: 3,\n                      title: \"'lemon',\"\n                      path: 'img/lemon.png'\n                  }]\n              }\n      })\n  \n  \u003c/script\u003e\n  ```\n\n- 4.3 、点击每一个tab栏 当前的高亮 其他的取消高亮\n  - 给每一个li添加点击事件\n  - 让当前的索引 index 和  当前 currentIndex 的 值 进项比较\n  - 如果相等 则当前li 添加active 类名 当前的 li 高亮 当前对应索引的 div 添加 current 当前div 显示 其他隐藏\n\n    ```html\n        \u003cdiv id=\"app\"\u003e\n            \u003cdiv class=\"tab\"\u003e\n                \u003cul\u003e\n                    \u003c!--  通过v-on 添加点击事件   需要把当前li 的索引传过去 \n    \t\t\t\t--\u003e\n                    \u003cli v-on:click='change(index)'\t\t           \t\t\t\n                        :class='currentIndex==index?\"active\":\"\"'                   \n                        :key='item.id' \n                        v-for='(item,index) in list'\u003e{{item.title}}\u003c/li\u003e\n                \u003c/ul\u003e\n                \u003cdiv :class='currentIndex==index?\"current\":\"\"' \n                     :key='item.id' v-for='(item, index) in list'\u003e\n                    \u003cimg :src=\"item.path\"\u003e\n                \u003c/div\u003e\n            \u003c/div\u003e\n        \u003c/div\u003e\n    \n    \u003cscript\u003e\n        new  Vue({\n            el: '#app',\n                data: {\n                    currentIndex: 0, // 选项卡当前的索引  默认为 0  \n                    list: [{\n                        id: 1,\n                        title: \"'apple',\"\n                        path: 'img/apple.png'\n                    }, {\n                        id: 2,\n                        title: \"'orange',\"\n                        path: 'img/orange.png'\n                    }, {\n                        id: 3,\n                        title: \"'lemon',\"\n                        path: 'img/lemon.png'\n                    }]\n                },\n                methods: {\n                    change: function(index) {\n                        // 通过传入过来的索引来让当前的  currentIndex  和点击的index 值 相等 \n                        //  从而实现 控制类名    \n                        this.currentIndex = index;\n                    }\n                }\n        \n        })\n    \n    \u003c/script\u003e\n    ```\n\n## Vue选项\n\n### 表单基本操作\n\n- 获取单选框中的值\n  - 通过v-model\n\n  ```html\n   \t\u003c!-- \n  \t\t1、 两个单选框需要同时通过v-model 双向绑定 一个值 \n          2、 每一个单选框必须要有value属性  且value 值不能一样 \n  \t\t3、 当某一个单选框选中的时候 v-model  会将当前的 value值 改变 data 中的 数据\n  \t\tgender 的值就是选中的值，我们只需要实时监控他的值就可以了\n  \t--\u003e\n     \u003cinput type=\"radio\" id=\"male\" value=\"1\" v-model='gender'\u003e\n     \u003clabel for=\"male\"\u003e男\u003c/label\u003e\n  \n     \u003cinput type=\"radio\" id=\"female\" value=\"2\" v-model='gender'\u003e\n     \u003clabel for=\"female\"\u003e女\u003c/label\u003e\n  \n  \u003cscript\u003e\n      new Vue({\n           data: {\n               // 默认会让当前的 value 值为 2 的单选框选中\n                  gender: 2,  \n              },\n      })\n  \n  \u003c/script\u003e\n  ```\n\n- 获取复选框中的值\n  - 通过v-model\n  - 和获取单选框中的值一样\n  - 复选框 `checkbox` 这种的组合时 data 中的 hobby 我们要定义成数组 否则无法实现多选\n\n  ```html\n  \t\u003c!-- \n  \t\t1、 复选框需要同时通过v-model 双向绑定 一个值 \n          2、 每一个复选框必须要有value属性  且value 值不能一样 \n  \t\t3、 当某一个单选框选中的时候 v-model  会将当前的 value值 改变 data 中的 数据\n  \n  \t\thobby 的值就是选中的值，我们只需要实时监控他的值就可以了\n  \t--\u003e\n  \n  \u003cdiv\u003e\n     \u003cspan\u003e爱好：\u003c/span\u003e\n     \u003cinput type=\"checkbox\" id=\"ball\" value=\"1\" v-model='hobby'\u003e\n     \u003clabel for=\"ball\"\u003e篮球\u003c/label\u003e\n     \u003cinput type=\"checkbox\" id=\"sing\" value=\"2\" v-model='hobby'\u003e\n     \u003clabel for=\"sing\"\u003e唱歌\u003c/label\u003e\n     \u003cinput type=\"checkbox\" id=\"code\" value=\"3\" v-model='hobby'\u003e\n     \u003clabel for=\"code\"\u003e写代码\u003c/label\u003e\n   \u003c/div\u003e\n  \u003cscript\u003e\n      new Vue({\n           data: {\n                  // 默认会让当前的 value 值为 2 和 3 的复选框选中\n                  hobby: ['2', '3'],\n              },\n      })\n  \u003c/script\u003e\n  ```\n\n- #### 获取下拉框和文本框中的值\n  - 通过v-model\n\n  ```html\n     \u003cdiv\u003e\n        \u003cspan\u003e职业：\u003c/span\u003e\n         \u003c!--\n  \t\t\t1、 需要给select  通过v-model 双向绑定 一个值 \n              2、 每一个option  必须要有value属性  且value 值不能一样 \n  \t\t    3、 当某一个option选中的时候 v-model  会将当前的 value值 改变 data 中的 数据\n  \t\t     occupation 的值就是选中的值，我们只需要实时监控他的值就可以了\n  \t\t--\u003e\n         \u003c!-- multiple  多选 --\u003e\n        \u003cselect v-model='occupation' multiple\u003e\n            \u003coption value=\"0\"\u003e请选择职业...\u003c/option\u003e\n            \u003coption value=\"1\"\u003e教师\u003c/option\u003e\n            \u003coption value=\"2\"\u003e软件工程师\u003c/option\u003e\n            \u003coption value=\"3\"\u003e律师\u003c/option\u003e\n        \u003c/select\u003e\n           \u003c!-- textarea 是 一个双标签   不需要绑定value 属性的  --\u003e\n          \u003ctextarea v-model='desc'\u003e\u003c/textarea\u003e\n    \u003c/div\u003e\n  \u003cscript\u003e\n      new Vue({\n           data: {\n                  // 默认会让当前的 value 值为 2 和 3 的下拉框选中\n                   occupation: ['2', '3'],\n               \t desc: 'nihao'\n              },\n      })\n  \u003c/script\u003e\n  ```\n\n### 表单修饰符\n\n- .number 转换为数值\n\n  ![image-20200329173658594](image-20200329173658594.png)\n\n  - 如果你先输入数字，那它就会限制你输入的只能是数字。\n  - 如果你先输入字符串，那它就相当于没有加.number\n- .trim 自动过滤用户输入的首尾空白字符\n  - 只能去掉首尾的 不能去除中间的空格\n- .lazy 将input事件切换成change事件\n  - .lazy 修饰符延迟了同步更新属性值的时机。即将原本绑定在 input 事件的同步逻辑转变为绑定在 change 事件上,在失去焦点 或者 按下回车键时才更新。\n\n  ```html\n  \u003c!-- 自动将用户的输入值转为数值类型 --\u003e\n  \u003cinput v-model.number=\"age\" type=\"number\"\u003e\n  \n  \u003c!--自动过滤用户输入的首尾空白字符   --\u003e\n  \u003cinput v-model.trim=\"msg\"\u003e\n  \n  \u003c!-- 在“change”时而非“input”时更新 --\u003e\n  \u003cinput v-model.lazy=\"msg\" \u003e\n  ```\n\n### v-bind修饰符\n\n- **.sync**(2.3.0+ 新增)\n\n在有些情况下，我们可能需要对一个 prop 进行“双向绑定”。不幸的是，真正的双向绑定会带来维护上的问题，因为子组件可以修改父组件，且在父组件和子组件都没有明显的改动来源。我们通常的做法是\n\n```html\n//父亲组件\n\u003ccomp :myMessage=\"bar\" @update:myMessage=\"func\"\u003e\u003c/comp\u003e\n//js\nfunc(e){\n this.bar = e;\n}\n//子组件js\nfunc2(){\n  this.$emit('update:myMessage',params);\n}\n```\n\n现在这个.sync修饰符就是简化了上面的步骤\n\n```html\n//父组件\n\u003ccomp :myMessage.sync=\"bar\"\u003e\u003c/comp\u003e \n//子组件\nthis.$emit('update:myMessage',params);\n```\n\n这样确实会方便很多，但是也有很多需要**注意**的点\n\n1. 使用sync的时候，子组件传递的事件名必须为update:value，其中value必须与子组件中props中声明的名称完全一致(如上例中的myMessage，不能使用my-message)\n2. 注意带有 .sync 修饰符的 v-bind 不能和表达式一起使用 (例如 v-bind:title.sync=”doc.title + ‘!’” 是无效的)。取而代之的是，你只能提供你想要绑定的属性名，类似 v-model。\n3. 将 v-bind.sync 用在一个字面量的对象上，例如 v-bind.sync=”{ title: \"doc.title }”，是无法正常工作的，因为在解析一个像这样的复杂表达式的时候，有很多边缘情况需要考虑。\"\n\n- **.prop**\n\n要学习这个修饰符，我们首先要搞懂两个东西的区别。\n\n```\nProperty：节点对象在内存中存储的属性，可以访问和设置。\nAttribute：节点对象的其中一个属性( property )，值是一个对象。\n可以通过点访问法 document.getElementById('xx').attributes 或者 document.getElementById('xx').getAttributes('xx') 读取，通过 document.getElementById('xx').setAttribute('xx',value) 新增和修改。\n在标签里定义的所有属性包括 HTML 属性和自定义属性都会在 attributes 对象里以键值对的方式存在。\n```\n\n其实attribute和property两个单词，翻译出来都是属性，但是《javascript高级程序设计》将它们翻译为特性和属性，以示区分\n\n```\n//这里的id,value,style都属于property\n//index属于attribute\n//id、title等既是属性，也是特性。修改属性，其对应的特性会发生改变；修改特性，属性也会改变\n\u003cinput id=\"uid\" title=\"title1\" value=\"1\" :index=\"index\"\u003e\n//input.index === undefined\n//input.attributes.index === this.index\n```\n\n从上面我们可以看到如果直接使用v-bind绑定，则默认会绑定到dom节点的attribute。  \n为了\n\n- 通过自定义属性存储变量，避免暴露数据\n- 防止污染 HTML 结构\n\n我们可以使用这个修饰符，如下\n\n```\n\u003cinput id=\"uid\" title=\"title1\" value=\"1\" :index.prop=\"index\"\u003e\n//input.index === this.index\n//input.attributes.index === undefined\n```\n\n- **.camel**\n\n由于HTML 特性是不区分大小写的。\n\n```\n\u003csvg :viewBox=\"viewBox\"\u003e\u003c/svg\u003e\n```\n\n实际上会渲染为\n\n```\n\u003csvg viewbox=\"viewBox\"\u003e\u003c/svg\u003e\n```\n\n这将导致渲染失败，因为 SVG 标签只认 viewBox，却不知道 viewbox 是什么。  \n如果我们使用.camel修饰符，那它就会被渲染为驼峰名。  \n另，如果你使用字符串模版，则没有这些限制。\n\n```javascript\nnew Vue({\n  template: '\u003csvg :viewBox=\"viewBox\"\u003e\u003c/svg\u003e'\n})\n```\n\n### 事件修饰符/v-on\n\n在事件处理程序中调用 `event.preventDefault()` 或 `event.stopPropagation()` 是非常常见的需求。\n\nVue 不推荐我们操作DOM，为了解决这个问题，Vue.js 为 `v-on` 提供了**事件修饰符**\n\n- 修饰符是由点开头的指令后缀来表示的\n- .stop 阻止冒泡\n\n  一键阻止事件冒泡，简直方便得不行。相当于调用了event.stopPropagation()方法。\n\n- .prevent 用于阻止事件的默认行为，例如，当点击提交按钮时阻止对表单的提交。相当于调用了event.preventDefault()方法。\n- .capture 从上面我们知道了事件的冒泡，其实完整的事件机制是：捕获阶段--目标阶段--冒泡阶段。  \n  默认的呢，是事件触发是从目标开始往上冒泡。  \n  当我们加了这个.capture以后呢，我们就反过来了，事件触发从包含这个元素的顶层开始往下触发。\n\n  ```html\n     \u003cdiv @click.capture=\"shout(1)\"\u003e\n        obj1\n        \u003cdiv @click.capture=\"shout(2)\"\u003e\n          obj2\n          \u003cdiv @click=\"shout(3)\"\u003e\n            obj3\n            \u003cdiv @click=\"shout(4)\"\u003e\n              obj4\n            \u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n      // 1 2 4 3 \n  ```\n\n  从上面这个例子我们点击obj4的时候，就可以清楚地看出区别，obj1，obj2在捕获阶段就触发了事件，因此是先1后2，后面的obj3，obj4是默认的冒泡阶段触发，因此是先4然后冒泡到3~\n\n- .self 只当事件是从事件绑定的元素本身触发时才触发回调。像下面所示，刚刚我们从.stop时候知道子元素会冒泡到父元素导致触发父元素的点击事件，当我们加了这个.self以后，我们点击button不会触发父元素的点击事件shout，只有当点击到父元素的时候（蓝色背景）才会shout~从这个self的英文翻译过来就是‘自己，本身’可以看出这个修饰符的用法\n- .once 事件只触发一次\n- **.passive** 当我们在监听元素滚动事件的时候，会一直触发onscroll事件，在pc端是没啥问题的，但是在移动端，会让我们的网页变卡，因此我们使用这个修饰符的时候，相当于给onscroll事件整了一个.lazy修饰符\n\n```html\n\u003c!-- 滚动事件的默认行为 (即滚动行为) 将会立即触发 --\u003e\n\u003c!-- 而不会等待 `onScroll` 完成  --\u003e\n\u003c!-- 这其中包含 `event.preventDefault()` 的情况 --\u003e\n\u003cdiv v-on:scroll.passive=\"onScroll\"\u003e...\u003c/div\u003e\n```\n\n- **.native** 我们经常会写很多的小组件，有些小组件可能会绑定一些事件，但是，像下面这样绑定事件是不会触发的\n\n```html\n\u003cMy-component @click=\"shout(3)\"\u003e\u003c/My-component\u003e\n```\n\n必须使用.native来修饰这个click事件（即\u003cMy-component @click.native=\"shout(3)\"\u003e\\\u003c/My-component\u003e），可以理解为该修饰符的作用就是把一个vue组件转化为一个普通的HTML标签，  \n注意：**使用.native修饰符来操作普通HTML标签是会令事件失效的**\n\n**注意：**修饰符可以同时使用多个,但是可能会因为顺序而有所不同。  \n用 v-on:click.prevent.self 会阻止所有的点击，而 v-on:click.self.prevent 只会阻止对元素自身的点击。  \n也就是**从左往右判断~**\n\n```html\n\u003c!-- 阻止单击事件继续传播 --\u003e\n\u003ca v-on:click.stop=\"doThis\"\u003e\u003c/a\u003e\n\n\u003c!-- 提交事件不再重载页面 --\u003e\n\u003cform v-on:submit.prevent=\"onSubmit\"\u003e\u003c/form\u003e\n\n\u003c!-- 修饰符可以串联，即阻止冒泡也阻止默认事件 --\u003e\n\u003ca v-on:click.stop.prevent=\"doThat\"\u003e\u003c/a\u003e\n\n\u003c!-- 只当在 event.target 是当前元素自身时触发处理函数 --\u003e\n\u003c!-- 即事件不是从内部元素触发的 --\u003e\n\u003cdiv v-on:click.self=\"doThat\"\u003e...\u003c/div\u003e\n```\n\n使用修饰符时，顺序很重要；相应的代码会以同样的顺序产生。因此，用 v-on:click.prevent.self 会阻止所有的点击，而 v-on:click.self.prevent 只会阻止对元素自身的点击。\n\n### 鼠标按钮修饰符\n\n刚刚我们讲到这个click事件，我们一般是会用左键触发，有时候我们需要更改右键菜单啥的，就需要用到右键点击或者中间键点击，这个时候就要用到鼠标按钮修饰符\n\n- .left 左键点击\n- .right 右键点击\n- .middle 中键点击\n\n```html\n\u003cbutton @click.right=\"shout(1)\"\u003eok\u003c/button\u003e\n```\n\n### 按键修饰符\n\n在做项目中有时会用到键盘事件，在监听键盘事件时，我们经常需要检查详细的按键。Vue 允许为 `v-on` 在监听键盘事件时添加按键修饰符。\n\n```html\n\u003c!-- 只有在 `keyCode` 是 13 时调用 `vm.submit()` --\u003e\n\u003cinput v-on:keyup.13=\"submit\"\u003e\n\n\u003c!-- -当点击enter 时调用 `vm.submit()` --\u003e\n\u003cinput v-on:keyup.enter=\"submit\"\u003e\n\n\u003c!--当点击enter或者space时  时调用 `vm.alertMe()`   --\u003e\n\u003cinput type=\"text\" v-on:keyup.enter.space=\"alertMe\" \u003e\n\n\u003c!--\n常用的按键修饰符\n.enter =\u003e    enter键\n.tab =\u003e tab键\n.delete (捕获“删除”和“退格”按键) =\u003e  删除键\n.esc =\u003e 取消键\n.space =\u003e  空格键\n.up =\u003e  上\n.down =\u003e  下\n.left =\u003e  左\n.right =\u003e  右\n\n//系统修饰键\n.ctrl\n.alt\n.meta\n.shift\n--\u003e\n\u003cscript\u003e\n\tvar vm = new Vue({\n        el:\"#app\",\n        methods: {\n              submit:function(){},\n              alertMe:function(){},\n        }\n    })\n\n\u003c/script\u003e\n// 可以通过全局 config.keyCodes 对象自定义按键修饰符别名：\n// 可以使用 `v-on:keyup.f1`\nVue.config.keyCodes.f1 = 112\n```\n\n我们从上面看到，键分成了普通常用的键和系统修饰键，区别是什么呢？  \n当我们写如下代码的时候,我们会发现如果**仅仅**使用系统修饰键是无法触发keyup事件的。\n\n```html\n\u003cinput type=\"text\" @keyup.ctrl=\"shout(4)\"\u003e\n```\n\n那该如何呢？我们需要将系统修饰键和其他键码链接起来使用，比如\n\n```html\n\u003cinput type=\"text\" @keyup.ctrl.67=\"shout(4)\"\u003e\n```\n\n这样当我们同时按下ctrl+c时，就会触发keyup事件。  \n另，如果是鼠标事件，那就可以单独使用系统修饰符。\n\n```html\n      \u003cbutton @mouseover.ctrl=\"shout(1)\"\u003eok\u003c/button\u003e\n      \u003cbutton @mousedown.ctrl=\"shout(1)\"\u003eok\u003c/button\u003e\n      \u003cbutton @click.ctrl.67=\"shout(1)\"\u003eok\u003c/button\u003e\n```\n\n大概是什么意思呢，就是你不能**单手指使用系统修饰键的修饰符**（最少两个手指，可以多个）。你可以一个手指按住系统修饰键一个手指按住另外一个键来实现键盘事件。也可以用一个手指按住系统修饰键，另一只手按住鼠标来实现鼠标事件。\n\n- **.exact** (2.5新增)\n\n我们上面说了这个系统修饰键，当我们像这样\u003cbutton type=\"text\" @click.ctrl=\"shout(4)\"\u003e\u003c/button\u003e绑定了click键按下的事件，惊奇的是，我们同时按下几个系统修饰键，比如ctrl shift点击，也能触发，可能有些场景我们**只需要或者只能**按一个系统修饰键来触发（像制作一些快捷键的时候），而当我们按下ctrl和其他键的时候则无法触发。那就这样写。  \n注意：这个**只是限制系统修饰键**的，像下面这样书写以后你还是可以按下ctrl + c，ctrl+v或者ctrl+普通键 来触发，但是不能按下ctrl + shift +普通键来触发。\n\n```html\n\u003cbutton type=\"text\" @click.ctrl.exact=\"shout(4)\"\u003eok\u003c/button\u003e\n```\n\n然后下面这个你可以同时按下enter+普通键来触发，但是不能按下系统修饰键+enter来触发。相信你已经能听懂了8~\n\n```html\n\u003cinput type=\"text\" @keydown.enter.exact=\"shout('我被触发了')\"\u003e\n```\n\n### 自定义按键修饰符别名\n\n在Vue中可以通过`config.keyCodes`自定义按键修饰符别名\n\n```html\n\u003cdiv id=\"app\"\u003e\n    \u003c!-- 预先定义了keycode 116（即F5）的别名为f5，因此在文字输入框中按下F5，会触发prompt方法 --\u003e\n    \u003cinput type=\"text\" v-on:keydown.f5=\"prompt()\"\u003e\n\u003c/div\u003e\n\u003cscript\u003e\n    Vue.config.keyCodes.f5 = 116;\n    let app = new Vue({\n        el: '#app',\n        methods: {\n            prompt: function() {\n                alert('我是 F5！');\n            }\n        }\n    });\n\u003c/script\u003e\n```\n\n### 自定义指令\n\n- 内置指令不能满足我们特殊的需求\n- Vue允许我们自定义指令\n\n#### Vue.directive 注册全局指令\n\n使用自定义的指令，只需在对用的元素中，加上'v-'的前缀形成类似于内部指令'v-if'，'v-text'的形式。\n\n```html\n\u003cinput type=\"text\" v-focus\u003e\n\u003cscript\u003e\n// 注意点： \n//   1、 在自定义指令中  如果以驼峰命名的方式定义 如  Vue.directive('focusA',function(){}) \n//   2、 在HTML中使用的时候 只能通过 v-focus-a 来使用 \n    \n// 注册一个全局自定义指令 v-focus\nVue.directive('focus', {\n  \t// 当绑定元素插入到 DOM 中。 其中 el为dom元素\n  \tinserted: function (el) {\n    \t\t// 聚焦元素\n    \t\tel.focus();\n \t}\n});\nnew Vue({\n　　el:'#app'\n});\n\u003c/script\u003e\n```\n\n#### Vue.directive 注册全局指令 带参数\n\n```html\n  \u003cinput type=\"text\" v-color='msg'\u003e\n \u003cscript type=\"text/javascript\"\u003e\n    /*\n      自定义指令-带参数\n      bind - 只调用一次，在指令第一次绑定到元素上时候调用\n    */\n    Vue.directive('color', {\n      // bind声明周期, 只调用一次，指令第一次绑定到元素时调用。在这里可以进行一次性的初始化设置\n      // el 为当前自定义指令的DOM元素  \n      // binding 为自定义的函数形参   通过自定义属性传递过来的值 存在 binding.value 里面\n      bind: function(el, binding){\n        // 根据指令的参数设置背景色\n        // console.log(binding.value.color)\n        el.style.backgroundColor = binding.value.color;\n      }\n    });\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        msg: {\n          color: 'blue'\n        }\n      }\n    });\n  \u003c/script\u003e\n```\n\n#### 自定义指令局部指令\n\n- 局部指令，需要定义在`directives` 的选项 用法和全局用法一样\n- 局部指令只能在当前组件里面使用\n- 当全局指令和局部指令同名时以局部指令为准\n\n```html\n\u003cinput type=\"text\" v-color='msg'\u003e\n \u003cinput type=\"text\" v-focus\u003e\n \u003cscript type=\"text/javascript\"\u003e\n    /*\n      自定义指令-局部指令\n    */\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        msg: {\n          color: 'red'\n        }\n      },\n   \t  //局部指令，需要定义在  directives 的选项\n      directives: {\n        color: {\n          bind: function(el, binding){\n            el.style.backgroundColor = binding.value.color;\n          }\n        },\n        focus: {\n          inserted: function(el) {\n            el.focus();\n          }\n        }\n      }\n    });\n  \u003c/script\u003e\n```\n\n### 计算属性 computed\n\n- 模板中放入太多的逻辑会让模板过重且难以维护 使用计算属性可以让模板更加的简洁\n- **计算属性是基于它们的响应式依赖进行缓存的**\n- computed比较适合对多个变量或者对象进行处理后返回一个结果值，也就是数多个变量中的某一个值发生了变化则我们监控的这个值也就会发生变化\n\n```html\n \u003cdiv id=\"app\"\u003e\n     \u003c!--  \n        当多次调用 reverseString  的时候 \n        只要里面的 num 值不改变 他会把第一次计算的结果直接返回\n\t\t直到data 中的num值改变 计算属性才会重新发生计算\n     --\u003e\n    \u003cdiv\u003e{{reverseString}}\u003c/div\u003e\n    \u003cdiv\u003e{{reverseString}}\u003c/div\u003e\n     \u003c!-- 调用methods中的方法的时候  他每次会重新调用 --\u003e\n    \u003cdiv\u003e{{reverseMessage()}}\u003c/div\u003e\n    \u003cdiv\u003e{{reverseMessage()}}\u003c/div\u003e\n  \u003c/div\u003e\n  \u003cscript type=\"text/javascript\"\u003e\n    /*\n      计算属性与方法的区别:计算属性是基于依赖进行缓存的，而方法不缓存\n    */\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        msg: 'Nihao',\n        num: 100\n      },\n      methods: {\n        reverseMessage: function(){\n          console.log('methods')\n          return this.msg.split('').reverse().join('');\n        }\n      },\n      //computed  属性 定义 和 data 已经 methods 平级 \n      computed: {\n        //  reverseString   这个是我们自己定义的名字 \n        reverseString: function(){\n          console.log('computed')\n          var total = 0;\n          //  当data 中的 num 的值改变的时候  reverseString  会自动发生计算  \n          for(var i=0;i\u003c=this.num;i++){\n            total += i;\n          }\n          // 这里一定要有return 否则 调用 reverseString 的 时候无法拿到结果    \n          return total;\n        }\n      }\n    });\n  \u003c/script\u003e\n```\n\n你可能已经注意到我们可以通过在表达式中调用方法来达到同样的效果：\n\n```javascript\n\u003cp\u003eReversed message: \"{{ reversedMessage() }}\"\u003c/p\u003e\n// 在组件中\nmethods: {\n  reversedMessage: function () {\n    return this.message.split('').reverse().join('')\n  }\n}\n```\n\n我们可以将同一函数定义为一个方法而不是一个计算属性。两种方式的最终结果确实是完全相同的。然而，不同的是**计算属性是基于它们的响应式依赖进行缓存的**。只在相关响应式依赖发生改变时它们才会重新求值。这就意味着只要 `message` 还没有发生改变，多次访问 `reversedMessage` 计算属性会立即返回之前的计算结果，而不必再次执行函数。\n\n### 侦听器 watch\n\n- 使用watch来响应数据的变化\n- 一般用于异步或者开销较大的操作\n- watch 中的属性 一定是data 中 已经存在的数据\n- **当需要监听一个对象的改变时，普通的watch方法无法监听到对象内部属性的改变，只有data中的数据才能够监听到变化，此时就需要deep属性对对象进行深度监听**\n\n```html\n \u003cdiv id=\"app\"\u003e\n        \u003cdiv\u003e\n            \u003cspan\u003e名：\u003c/span\u003e\n            \u003cspan\u003e\n        \u003cinput type=\"text\" v-model='firstName'\u003e\n      \u003c/span\u003e\n        \u003c/div\u003e\n        \u003cdiv\u003e\n            \u003cspan\u003e姓：\u003c/span\u003e\n            \u003cspan\u003e\n        \u003cinput type=\"text\" v-model='lastName'\u003e\n      \u003c/span\u003e\n        \u003c/div\u003e\n        \u003cdiv\u003e{{fullName}}\u003c/div\u003e\n    \u003c/div\u003e\n\n  \u003cscript type=\"text/javascript\"\u003e\n        /*\n              侦听器\n            */\n        var vm = new Vue({\n            el: '#app',\n            data: {\n                firstName: 'Jim',\n                lastName: 'Green',\n                // fullName: 'Jim Green'\n            },\n             //watch  属性 定义 和 data 已经 methods 平级 \n            watch: {\n                //   注意：  这里firstName  对应着data 中的 firstName \n                //   当 firstName 值 改变的时候  会自动触发 watch\n                firstName: function(val) {\n                    this.fullName = val + ' ' + this.lastName;\n                },\n                //   注意：  这里 lastName 对应着data 中的 lastName \n                lastName: function(val) {\n                    this.fullName = this.firstName + ' ' + val;\n                }\n            }\n        });\n    \u003c/script\u003e\n```\n\n虽然计算属性在大多数情况下更合适，但有时也需要一个自定义的侦听器。这就是为什么 Vue 通过 `watch` 选项提供了一个更通用的方法，来响应数据的变化。当需要在数据变化时执行异步或开销较大的操作时，这个方式是最有用的。\n\n例如：\n\n```html\n\u003cdiv id=\"watch-example\"\u003e\n  \u003cp\u003e\n    Ask a yes/no question:\n    \u003cinput v-model=\"question\"\u003e\n  \u003c/p\u003e\n  \u003cp\u003e{{ answer }}\u003c/p\u003e\n\u003c/div\u003e\n\u003c!-- 因为 AJAX 库和通用工具的生态已经相当丰富，Vue 核心代码没有重复 --\u003e\n\u003c!-- 提供这些功能以保持精简。这也可以让你自由选择自己更熟悉的工具。 --\u003e\n\u003cscript src=\"https://cdn.jsdelivr.net/npm/axios@0.12.0/dist/axios.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"https://cdn.jsdelivr.net/npm/lodash@4.13.1/lodash.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\nvar watchExampleVM = new Vue({\n  el: '#watch-example',\n  data: {\n    question: '',\n    answer: 'I cannot give you an answer until you ask a question!'\n  },\n  watch: {\n    // 如果 `question` 发生改变，这个函数就会运行\n    question: function (newQuestion, oldQuestion) {\n      this.answer = 'Waiting for you to stop typing...'\n      this.debouncedGetAnswer()\n    }\n  },\n  created: function () {\n    // `_.debounce` 是一个通过 Lodash 限制操作频率的函数。\n    // 在这个例子中，我们希望限制访问 yesno.wtf/api 的频率\n    // AJAX 请求直到用户输入完毕才会发出。想要了解更多关于\n    // `_.debounce` 函数 (及其近亲 `_.throttle`) 的知识，\n    // 请参考：https://lodash.com/docs#debounce\n    this.debouncedGetAnswer = _.debounce(this.getAnswer, 500)\n  },\n  methods: {\n    getAnswer: function () {\n      if (this.question.indexOf('?') === -1) {\n        this.answer = 'Questions usually contain a question mark. ;-)'\n        return\n      }\n      this.answer = 'Thinking...'\n      var vm = this\n      axios.get('https://yesno.wtf/api')\n        .then(function (response) {\n          vm.answer = _.capitalize(response.data.answer)\n        })\n        .catch(function (error) {\n          vm.answer = 'Error! Could not reach the API. ' + error\n        })\n    }\n  }\n})\n\u003c/script\u003e\n```\n\n结果：\n\nAsk a yes/no question:\n\nI cannot give you an answer until you ask a question!\n\n在这个示例中，使用 `watch` 选项允许我们执行异步操作 (访问一个 API)，限制我们执行该操作的频率，并在我们得到最终结果前，设置中间状态。这些都是计算属性无法做到的。\n\n除了 `watch` 选项之外，您还可以使用命令式的 [vm.$watch API](https://cn.vuejs.org/v2/api/#vm-watch)。\n\n### 计算属性 vs 侦听属性\n\nVue 提供了一种更通用的方式来观察和响应 Vue 实例上的数据变动：**侦听属性**。当你有一些数据需要随着其它数据变动而变动时，你很容易滥用 `watch`——特别是如果你之前使用过 AngularJS。然而，通常更好的做法是使用计算属性而不是命令式的 `watch` 回调。细想一下这个例子：\n\n```javascript\n\u003cdiv id=\"demo\"\u003e{{ fullName }}\u003c/div\u003e\nvar vm = new Vue({\n  el: '#demo',\n  data: {\n    firstName: 'Foo',\n    lastName: 'Bar',\n    fullName: 'Foo Bar'\n  },\n  watch: {\n    firstName: function (val) {\n      this.fullName = val + ' ' + this.lastName\n    },\n    lastName: function (val) {\n      this.fullName = this.firstName + ' ' + val\n    }\n  }\n})\n```\n\n上面代码是命令式且重复的。将它与计算属性的版本进行比较：\n\n```javascript\nvar vm = new Vue({\n  el: '#demo',\n  data: {\n    firstName: 'Foo',\n    lastName: 'Bar'\n  },\n  computed: {\n    fullName: function () {\n      return this.firstName + ' ' + this.lastName\n    }\n  }\n})\n```\n\n好得多了，不是吗？\n\n### 过滤器\n\n- Vue.js允许自定义过滤器，可被用于一些常见的文本格式化。\n- 过滤器可以用在两个地方：**双花括号插值和v-bind表达式。**\n- 过滤器应该被添加在JavaScript表达式的尾部，由“管道”符号指示\n- 支持级联操作\n- 过滤器不改变真正的`data`，而只是改变渲染的结果，并返回过滤后的版本\n- 全局注册时是filter，没有s的。而局部过滤器是filters，是有s的\n\n```html\n  \u003cdiv id=\"app\"\u003e\n    \u003cinput type=\"text\" v-model='msg'\u003e\n      \u003c!-- upper 被定义为接收单个参数的过滤器函数，表达式  msg  的值将作为参数传入到函数中 --\u003e\n    \u003cdiv\u003e{{msg | upper}}\u003c/div\u003e\n    \u003c!--  \n      支持级联操作\n      upper  被定义为接收单个参数的过滤器函数，表达式msg 的值将作为参数传入到函数中。\n\t  然后继续调用同样被定义为接收单个参数的过滤器 lower ，将upper 的结果传递到lower中\n \t--\u003e\n    \u003cdiv\u003e{{msg | upper | lower}}\u003c/div\u003e\n    \u003cdiv :abc='msg | upper'\u003e测试数据\u003c/div\u003e\n  \u003c/div\u003e\n\n\u003cscript type=\"text/javascript\"\u003e\n   //  lower  为全局过滤器     \n   Vue.filter('lower', function(val) {\n      return val.charAt(0).toLowerCase() + val.slice(1);\n    });\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        msg: ''\n      },\n       //filters  属性 定义 和 data 已经 methods 平级 \n       //  定义filters 中的过滤器为局部过滤器 \n      filters: {\n        //   upper  自定义的过滤器名字 \n        //    upper 被定义为接收单个参数的过滤器函数，表达式  msg  的值将作为参数传入到函数中\n        upper: function(val) {\n         //  过滤器中一定要有返回值 这样外界使用过滤器的时候才能拿到结果\n          return val.charAt(0).toUpperCase() + val.slice(1);\n        }\n      }\n    });\n  \u003c/script\u003e\n```\n\n#### 过滤器中传递参数\n\n```html\n    \u003cdiv id=\"box\"\u003e\n        \u003c!--\n\t\t\tfilterA 被定义为接收三个参数的过滤器函数。\n  \t\t\t其中 message 的值作为第一个参数，\n\t\t\t普通字符串 'arg1' 作为第二个参数，表达式 arg2 的值作为第三个参数。\n\t\t--\u003e\n        {{ message | filterA('arg1', 'arg2') }}\n    \u003c/div\u003e\n    \u003cscript\u003e\n        // 在过滤器中 第一个参数 对应的是  管道符前面的数据   n  此时对应 message\n        // 第2个参数  a 对应 实参  arg1 字符串\n        // 第3个参数  b 对应 实参  arg2 字符串\n        Vue.filter('filterA',function(n,a,b){\n            if(n\u003c10){\n                return n+a;\n            }else{\n                return n+b;\n            }\n        });\n        \n        new Vue({\n            el:\"#box\",\n            data:{\n                message: \"哈哈哈\"\n            }\n        })\n    \u003c/script\u003e\n```\n\n### 生命周期\n\n- 事物从出生到死亡的过程\n- Vue实例从创建 到销毁的过程 ，这些过程中会伴随着一些函数的自调用。我们称这些函数为钩子函数\n\n不要在选项属性或回调上使用[箭头函数](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Functions/Arrow_functions)，比如 `created: () =\u003e console.log(this.a)` 或 `vm.$watch('a', newValue =\u003e this.myMethod())`。因为箭头函数并没有 `this`，`this` 会作为变量一直向上级词法作用域查找，直至找到为止，经常导致 `Uncaught TypeError: Cannot read property of undefined` 或 `Uncaught TypeError: this.myMethod is not a function` 之类的错误。\n\n####常用的 钩子函数\n\n| beforeCreate  | 在实例初始化之后，数据观测和事件配置之前被调用 此时data 和 methods 以及页面的DOM结构都没有初始化   什么都做不了 |\n| ------------- | ------------------------------------------------------------ |\n| created       | 在实例创建完成后被立即调用此时data 和 methods已经可以使用  但是页面还没有渲染出来 |\n| beforeMount   | 在挂载开始之前被调用   此时页面上还看不到真实数据 只是一个模板页面而已 |\n| mounted       | el被新创建的vm.$el替换，并挂载到实例上去之后调用该钩子。  数据已经真实渲染到页面上  在这个钩子函数里面我们可以使用一些第三方的插件 |\n| beforeUpdate  | 数据更新时调用，发生在虚拟DOM打补丁之前。   页面上数据还是旧的 |\n| updated       | 由于数据更改导致的虚拟DOM重新渲染和打补丁，在这之后会调用该钩子。 页面上数据已经替换成最新的 |\n| beforeDestroy | 实例销毁之前调用                                             |\n| destroyed     | 实例销毁后调用                                               |\n\n\u003cimg src=\"Vue.assets/lifecycle-20200312161816738.png\" alt=\"Vue 实例生命周期\" style=\"zoom:50%;\" /\u003e\n\n### 数组变异方法\n\n- 在 Vue 中，直接修改对象属性的值无法触发响应式。当你直接修改了对象属性的值，你会发现，只有数据改了，但是页面内容并没有改变\n- 变异数组方法即保持数组方法原有功能不变的前提下对其进行功能拓展\n\n| `push()`    | 往数组最后面添加一个元素，成功返回当前数组的长度             |\n| ----------- | ------------------------------------------------------------ |\n| `pop()`     | 删除数组的最后一个元素，成功返回删除元素的值                 |\n| `shift()`   | 删除数组的第一个元素，成功返回删除元素的值                   |\n| `unshift()` | 往数组最前面添加一个元素，成功返回当前数组的长度             |\n| `splice()`  | 有三个参数，第一个是想要删除的元素的下标（必选），第二个是想要删除的个数（必选），第三个是删除 后想要在原位置替换的值 |\n| `sort()`    | sort()  使数组按照字符编码默认从小到大排序,成功返回排序后的数组 |\n| `reverse()` | reverse()  将数组倒序，成功返回倒序后的数组                  |\n\n### 替换数组\n\n不会改变原始数组，但总是返回一个新数组\n\n| filter | filter() 方法创建一个新的数组，新数组中的元素是通过检查指定数组中符合条件的所有元素。 |\n| ------ | ------------------------------------------------------------ |\n| concat | concat() 方法用于连接两个或多个数组。该方法不会改变现有的数组 |\n| slice  | slice() 方法可从已有的数组中返回选定的元素。该方法并不会修改数组，而是返回一个子数组 |\n\n### 动态数组响应式数据\n\n- `Vue.set(a,b,c)` 让 触发视图重新更新一遍，数据动态起来\n- a是要更改的数据 、 b是数据的第几项、 c是更改后的数据\n\n### 图书列表案例\n\n- 静态列表效果\n- 基于数据实现模板效果\n- 处理每行的操作按钮\n\n#### 1、 提供的静态数据\n\n- 数据存放在vue 中 data 属性中\n\n```js\n var vm = new Vue({\n      el: '#app',\n      data: {\n        books: [{\n          id: 1,\n          name: '三国演义',\n          date: ''\n        },{\n          id: 2,\n          name: '水浒传',\n          date: ''\n        },{\n          id: 3,\n          name: '红楼梦',\n          date: ''\n        },{\n          id: 4,\n          name: '西游记',\n          date: ''\n        }]\n      }\n    }); var vm = new Vue({\n      el: '#app',\n      data: {\n        books: [{\n          id: 1,\n          name: '三国演义',\n          date: ''\n        },{\n          id: 2,\n          name: '水浒传',\n          date: ''\n        },{\n          id: 3,\n          name: '红楼梦',\n          date: ''\n        },{\n          id: 4,\n          name: '西游记',\n          date: ''\n        }]\n      }\n    });\n```\n\n#### 2、 把提供好的数据渲染到页面上\n\n- 利用 v-for循环 遍历 books 将每一项数据渲染到对应的数据中\n\n```html\n \u003ctbody\u003e\n    \u003ctr :key='item.id' v-for='item in books'\u003e\n       \u003c!-- 对应的id 渲染到页面上 --\u003e\n       \u003ctd\u003e{{item.id}}\u003c/td\u003e\n        \u003c!-- 对应的name 渲染到页面上 --\u003e\n       \u003ctd\u003e{{item.name}}\u003c/td\u003e\n       \u003ctd\u003e{{item.date}}\u003c/td\u003e\n       \u003ctd\u003e\n         \u003c!-- 阻止 a 标签的默认跳转 --\u003e\n         \u003ca href=\"\" @click.prevent\u003e修改\u003c/a\u003e\n         \u003cspan\u003e|\u003c/span\u003e\n       \t  \u003ca href=\"\" @click.prevent\u003e删除\u003c/a\u003e\n       \u003c/td\u003e\n     \u003c/tr\u003e\n\u003c/tbody\u003e\n```\n\n#### 3、 添加图书\n\n- 通过双向绑定获取到输入框中的输入内容\n- 给按钮添加点击事件\n- 把输入框中的数据存储到 data 中的 books 里面\n\n```html\n\u003cdiv\u003e\n  \u003ch1\u003e图书管理\u003c/h1\u003e\n  \u003cdiv class=\"book\"\u003e\n       \u003cdiv\u003e\n         \u003clabel for=\"id\"\u003e\n           编号：\n         \u003c/label\u003e\n          \u003c!-- 3.1、通过双向绑定获取到输入框中的输入的 id  --\u003e\n         \u003cinput type=\"text\" id=\"id\" v-model='id'\u003e\n         \u003clabel for=\"name\"\u003e\n           名称：\n         \u003c/label\u003e\n           \u003c!-- 3.2、通过双向绑定获取到输入框中的输入的 name  --\u003e\n         \u003cinput type=\"text\" id=\"name\" v-model='name'\u003e\n            \u003c!-- 3.3、给按钮添加点击事件  --\u003e\n         \u003cbutton @click='handle'\u003e提交\u003c/button\u003e\n       \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n  \u003cscript type=\"text/javascript\"\u003e\n    /*\n      图书管理-添加图书\n    */\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        id: '',\n        name: '',\n        books: [{\n          id: 1,\n          name: '三国演义',\n          date: ''\n        },{\n          id: 2,\n          name: '水浒传',\n          date: ''\n        },{\n          id: 3,\n          name: '红楼梦',\n          date: ''\n        },{\n          id: 4,\n          name: '西游记',\n          date: ''\n        }]\n      },\n      methods: {\n        handle: function(){\n          // 3.4 定义一个新的对象book 存储 获取到输入框中 书 的id和名字 \n          var book = {};\n          book.id = this.id;\n          book.name = this.name;\n          book.date = '';\n         // 3.5 把book  通过数组的变异方法 push 放到    books 里面\n          this.books.push(book);\n          //3.6 清空输入框\n          this.id = '';\n          this.name = '';\n        }\n      }\n    });\n  \u003c/script\u003e\n```\n\n#### 4 修改图书-上\n\n- 点击修改按钮的时候 获取到要修改的书籍名单\n  - 4.1 给修改按钮添加点击事件， 需要把当前的图书的id 传递过去 这样才知道需要修改的是哪一本书籍\n- 把需要修改的书籍名单填充到表单里面\n  - 4.2 根据传递过来的id 查出books 中 对应书籍的详细信息\n  - 4.3 把获取到的信息填充到表单\n\n```html\n \u003cdiv id=\"app\"\u003e\n    \u003cdiv class=\"grid\"\u003e\n      \u003cdiv\u003e\n        \u003ch1\u003e图书管理\u003c/h1\u003e\n        \u003cdiv class=\"book\"\u003e\n          \u003cdiv\u003e\n            \u003clabel for=\"id\"\u003e\n              编号：\n            \u003c/label\u003e\n            \u003cinput type=\"text\" id=\"id\" v-model='id' :disabled=\"flag\"\u003e\n            \u003clabel for=\"name\"\u003e\n              名称：\n            \u003c/label\u003e\n            \u003cinput type=\"text\" id=\"name\" v-model='name'\u003e\n            \u003cbutton @click='handle'\u003e提交\u003c/button\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n      \u003ctable\u003e\n        \u003cthead\u003e\n          \u003ctr\u003e\n            \u003cth\u003e编号\u003c/th\u003e\n            \u003cth\u003e名称\u003c/th\u003e\n            \u003cth\u003e时间\u003c/th\u003e\n            \u003cth\u003e操作\u003c/th\u003e\n          \u003c/tr\u003e\n        \u003c/thead\u003e\n        \u003ctbody\u003e\n          \u003ctr :key='item.id' v-for='item in books'\u003e\n            \u003ctd\u003e{{item.id}}\u003c/td\u003e\n            \u003ctd\u003e{{item.name}}\u003c/td\u003e\n            \u003ctd\u003e{{item.date}}\u003c/td\u003e\n            \u003ctd\u003e\n              \u003c!--- \n\t\t\t\t4.1  给修改按钮添加点击事件，  需要把当前的图书的id 传递过去 \n\t\t\t\t这样才知道需要修改的是哪一本书籍\n  \t\t\t\t---\u003e  \n              \u003ca href=\"\" @click.prevent='toEdit(item.id)'\u003e修改\u003c/a\u003e\n              \u003cspan\u003e|\u003c/span\u003e\n              \u003ca href=\"\" @click.prevent\u003e删除\u003c/a\u003e\n            \u003c/td\u003e\n          \u003c/tr\u003e\n        \u003c/tbody\u003e\n      \u003c/table\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n \u003cscript type=\"text/javascript\"\u003e\n    /*\n      图书管理-添加图书\n    */\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        flag: false,\n        id: '',\n        name: '',\n        books: [{\n          id: 1,\n          name: '三国演义',\n          date: ''\n        },{\n          id: 2,\n          name: '水浒传',\n          date: ''\n        },{\n          id: 3,\n          name: '红楼梦',\n          date: ''\n        },{\n          id: 4,\n          name: '西游记',\n          date: ''\n        }]\n      },\n      methods: {\n        handle: function(){\n          // 3.4 定义一个新的对象book 存储 获取到输入框中 书 的id和名字 \n          var book = {};\n          book.id = this.id;\n          book.name = this.name;\n          book.date = '';\n         // 3.5 把book  通过数组的变异方法 push 放到    books 里面\n          this.books.push(book);\n          //3.6 清空输入框\n          this.id = '';\n          this.name = '';\n        },\n        toEdit: function(id){\n          console.log(id)\n          //4.2  根据传递过来的id 查出books 中 对应书籍的详细信息\n          var book = this.books.filter(function(item){\n            return item.id == id;\n          });\n          console.log(book)\n          //4.3 把获取到的信息填充到表单\n          // this.id   和  this.name 通过双向绑定 绑定到了表单中  一旦数据改变视图自动更新\n          this.id = book[0].id;\n          this.name = book[0].name;\n        }\n      }\n    });\n  \u003c/script\u003e\n```\n\n#### 5 修改图书-下\n\n- 5.1 定义一个标识符， 主要是控制 编辑状态下当前编辑书籍的id 不能被修改 即 处于编辑状态下 当前控制书籍编号的输入框禁用  \n- 5.2 通过属性绑定给书籍编号的 绑定 disabled 的属性 flag 为 true 即为禁用\n- 5.3 flag 默认值为false 处于编辑状态 要把 flag 改为true 即当前表单为禁用\n- 5.4 复用添加方法 用户点击提交的时候依然执行 handle 中的逻辑如果 flag为true 即 表单处于不可输入状态 此时执行的用户编辑数据数据\n\n```html\n\u003cdiv id=\"app\"\u003e\n    \u003cdiv class=\"grid\"\u003e\n      \u003cdiv\u003e\n        \u003ch1\u003e图书管理\u003c/h1\u003e\n        \u003cdiv class=\"book\"\u003e\n          \u003cdiv\u003e\n            \u003clabel for=\"id\"\u003e\n              编号：\n            \u003c/label\u003e\n              \u003c!-- 5.2 通过属性绑定 绑定 disabled 的属性  flag 为 true 即为禁用      --\u003e\n            \u003cinput type=\"text\" id=\"id\" v-model='id' :disabled=\"flag\"\u003e\n            \u003clabel for=\"name\"\u003e\n              名称：\n            \u003c/label\u003e\n            \u003cinput type=\"text\" id=\"name\" v-model='name'\u003e\n            \u003cbutton @click='handle'\u003e提交\u003c/button\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n      \u003ctable\u003e\n        \u003cthead\u003e\n          \u003ctr\u003e\n            \u003cth\u003e编号\u003c/th\u003e\n            \u003cth\u003e名称\u003c/th\u003e\n            \u003cth\u003e时间\u003c/th\u003e\n            \u003cth\u003e操作\u003c/th\u003e\n          \u003c/tr\u003e\n        \u003c/thead\u003e\n        \u003ctbody\u003e\n          \u003ctr :key='item.id' v-for='item in books'\u003e\n            \u003ctd\u003e{{item.id}}\u003c/td\u003e\n            \u003ctd\u003e{{item.name}}\u003c/td\u003e\n            \u003ctd\u003e{{item.date}}\u003c/td\u003e\n            \u003ctd\u003e\n              \u003ca href=\"\" @click.prevent='toEdit(item.id)'\u003e修改\u003c/a\u003e\n              \u003cspan\u003e|\u003c/span\u003e\n              \u003ca href=\"\" @click.prevent\u003e删除\u003c/a\u003e\n            \u003c/td\u003e\n          \u003c/tr\u003e\n        \u003c/tbody\u003e\n      \u003c/table\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e   \n\u003cscript type=\"text/javascript\"\u003e\n        /*图书管理-添加图书*/\n        var vm = new Vue({\n            el: '#app',\n            data: {\n                // 5.1  定义一个标识符， 主要是控制 编辑状态下当前编辑书籍的id 不能被修改 \n                // 即 处于编辑状态下 当前控制书籍编号的输入框禁用 \n                flag: false,\n                id: '',\n                name: '',\n              \n            },\n            methods: {\n                handle: function() {\n                   /*\n                     5.4  复用添加方法   用户点击提交的时候依然执行 handle 中的逻辑\n                 \t\t 如果 flag为true 即 表单处于不可输入状态 此时执行的用户编辑数据数据\t\n                   */ \n                    if (this.flag) {\n                        // 编辑图书\n                        // 5.5  根据当前的ID去更新数组中对应的数据  \n                        this.books.some((item) =\u003e {\n                            if (item.id == this.id) {\n                                // 箭头函数中 this 指向父级作用域的this \n                                item.name = this.name;\n                                // 完成更新操作之后，需要终止循环\n                                return true;\n                            }\n                        });\n                        // 5.6 编辑完数据后表单要处以可以输入的状态\n                        this.flag = false;\n                    //  5.7  如果 flag为false  表单处于输入状态 此时执行的用户添加数据    \n                    } else { \n                        var book = {};\n                        book.id = this.id;\n                        book.name = this.name;\n                        book.date = '';\n                        this.books.push(book);\n                        // 清空表单\n                        this.id = '';\n                        this.name = '';\n                    }\n                    // 清空表单\n                    this.id = '';\n                    this.name = '';\n                },\n                toEdit: function(id) {\n                     /*\n                     5.3  flag 默认值为false   处于编辑状态 要把 flag 改为true 即当前表单为禁\t\t\t\t\t  用 \n                     */ \n                    this.flag = true;\n                    console.log(id)\n                    var book = this.books.filter(function(item) {\n                        return item.id == id;\n                    });\n                    console.log(book)\n                    this.id = book[0].id;\n                    this.name = book[0].name;\n                }\n            }\n        });\n    \u003c/script\u003e\n```\n\n#### 6 删除图书\n\n- 6.1 给删除按钮添加事件 把当前需要删除的书籍id 传递过来\n- 6.2 根据id从数组中查找元素的索引\n- 6.3 根据索引删除数组元素\n\n```html\n  \u003ctbody\u003e\n          \u003ctr :key='item.id' v-for='item in books'\u003e\n            \u003ctd\u003e{{item.id}}\u003c/td\u003e\n            \u003ctd\u003e{{item.name}}\u003c/td\u003e\n            \u003ctd\u003e{{item.date}}\u003c/td\u003e\n            \u003ctd\u003e\n              \u003ca href=\"\" @click.prevent='toEdit(item.id)'\u003e修改\u003c/a\u003e\n              \u003cspan\u003e|\u003c/span\u003e\n               \u003c!--  6.1 给删除按钮添加事件 把当前需要删除的书籍id 传递过来  --\u003e \n              \u003ca href=\"\" @click.prevent='deleteBook(item.id)'\u003e删除\u003c/a\u003e\n            \u003c/td\u003e\n          \u003c/tr\u003e\n\u003c/tbody\u003e\n  \u003cscript type=\"text/javascript\"\u003e\n    /*\n      图书管理-添加图书\n    */\n    var vm = new Vue({\n      methods: {\n        deleteBook: function(id){\n          // 删除图书\n          #// 6.2 根据id从数组中查找元素的索引\n          // var index = this.books.findIndex(function(item){\n          //   return item.id == id;\n          // });\n          #// 6.3 根据索引删除数组元素\n          // this.books.splice(index, 1);\n          // -------------------------\n         #// 方法二：通过filter方法进行删除\n\t\t\n          # 6.4  根据filter 方法 过滤出来id 不是要删除书籍的id \n          # 因为 filter 是替换数组不会修改原始数据 所以需要 把 不是要删除书籍的id  赋值给 books \n          this.books = this.books.filter(function(item){\n            return item.id != id;\n          });\n        }\n      }\n    });\n  \u003c/script\u003e\n```\n\n### 常用特性应用场景\n\n#### 1 过滤器\n\n- Vue.filter 定义一个全局过滤器\n\n```html\n \u003ctr :key='item.id' v-for='item in books'\u003e\n            \u003ctd\u003e{{item.id}}\u003c/td\u003e\n            \u003ctd\u003e{{item.name}}\u003c/td\u003e\n     \t\t\u003c!-- 1.3  调用过滤器 --\u003e\n            \u003ctd\u003e{{item.date | format('yyyy-MM-dd hh:mm:ss')}}\u003c/td\u003e\n            \u003ctd\u003e\n              \u003ca href=\"\" @click.prevent='toEdit(item.id)'\u003e修改\u003c/a\u003e\n              \u003cspan\u003e|\u003c/span\u003e\n              \u003ca href=\"\" @click.prevent='deleteBook(item.id)'\u003e删除\u003c/a\u003e\n            \u003c/td\u003e\n\u003c/tr\u003e\n\n\u003cscript\u003e\n    \t#1.1  Vue.filter  定义一个全局过滤器\n\t    Vue.filter('format', function(value, arg) {\n              function dateFormat(date, format) {\n                if (typeof date === \"string\") {\n                  var mts = date.match(/(\\/Date\\((\\d+)\\)\\/)/);\n                  if (mts \u0026\u0026 mts.length \u003e= 3) {\n                    date = parseInt(mts[2]);\n                  }\n                }\n                date = new Date(date);\n                if (!date || date.toUTCString() == \"Invalid Date\") {\n                  return \"\";\n                }\n                var map = {\n                  \"M\": date.getMonth() + 1, //月份 \n                  \"d\": date.getDate(), //日 \n                  \"h\": date.getHours(), //小时 \n                  \"m\": date.getMinutes(), //分 \n                  \"s\": date.getSeconds(), //秒 \n                  \"q\": Math.floor((date.getMonth() + 3) / 3), //季度 \n                  \"S\": date.getMilliseconds() //毫秒 \n                };\n                format = format.replace(/([yMdhmsqS])+/g, function(all, t) {\n                  var v = map[t];\n                  if (v !== undefined) {\n                    if (all.length \u003e 1) {\n                      v = '0' + v;\n                      v = v.substr(v.length - 2);\n                    }\n                    return v;\n                  } else if (t === 'y') {\n                    return (date.getFullYear() + '').substr(4 - all.length);\n                  }\n                  return all;\n                });\n                return format;\n              }\n              return dateFormat(value, arg);\n            })\n#1.2  提供的数据 包含一个时间戳   为毫秒数\n   [{\n          id: 1,\n          name: '三国演义',\n          date: 2525609975000\n        },{\n          id: 2,\n          name: '水浒传',\n          date: 2525609975000\n        },{\n          id: 3,\n          name: '红楼梦',\n          date: 2525609975000\n        },{\n          id: 4,\n          name: '西游记',\n          date: 2525609975000\n        }];\n\u003c/script\u003e\n```\n\n#### 2 自定义指令\n\n- 让表单自动获取焦点\n- 通过Vue.directive 自定义指定\n\n```html\n\u003c!-- 2.2  通过v-自定义属性名 调用自定义指令 --\u003e\n\u003cinput type=\"text\" id=\"id\" v-model='id' :disabled=\"flag\" v-focus\u003e\n\n\u003cscript\u003e\n    # 2.1   通过Vue.directive 自定义指定\n\tVue.directive('focus', {\n      inserted: function (el) {\n        el.focus();\n      }\n    });\n\n\u003c/script\u003e\n```\n\n#### 3 计算属性\n\n- 通过计算属性计算图书的总数\n  - 图书的总数就是计算数组的长度\n\n```html\n \u003cdiv class=\"total\"\u003e\n        \u003cspan\u003e图书总数：\u003c/span\u003e\n     \t\u003c!-- 3.2 在页面上 展示出来 --\u003e\n        \u003cspan\u003e{{total}}\u003c/span\u003e\n\u003c/div\u003e\n\n  \u003cscript type=\"text/javascript\"\u003e\n    /*\n      计算属性与方法的区别:计算属性是基于依赖进行缓存的，而方法不缓存\n    */\n    var vm = new Vue({\n      data: {\n        flag: false,\n        submitFlag: false,\n        id: '',\n        name: '',\n        books: []\n      },\n      computed: {\n        total: function(){\n          // 3.1  计算图书的总数\n          return this.books.length;\n        }\n      },\n    });\n  \u003c/script\u003e\n\n```\n\n## 组件\n\n- 组件 (Component) 是 Vue.js 最强大的功能之一\n- 组件可以扩展 HTML 元素，封装可重用的代\n\n### 组件注册\n\n#### 全局注册\n\n- `Vue.component('组件名称', { })` 第1个参数是标签名称，第2个参数是一个选项对象\n- **全局组件**注册后，任何**vue实例**都可以用\n\n##### 组件基础用\n\n```html\n\u003cdiv id=\"example\"\u003e\n  \u003c!-- 2、 组件使用 组件名称 是以HTML标签的形式使用  --\u003e  \n  \u003cmy-component\u003e\u003c/my-component\u003e\n\u003c/div\u003e\n\u003cscript\u003e\n    //   注册组件 \n    // 1、 my-component 就是组件中自定义的标签名\n\tVue.component('my-component', {\n      template: '\u003cdiv\u003eA custom component!\u003c/div\u003e'\n    })\n\n    // 创建根实例\n    new Vue({\n      el: '#example'\n    })\n\u003c/script\u003e\n```\n\n##### 组件注意事项\n\n- 组件参数的data值必须是函数同时这个函数要求返回一个对象\n- 组件模板必须是单个根元素\n- 组件模板的内容可以是模板字符串\n\n```html\n \u003cdiv id=\"app\"\u003e\n     \u003c!-- \n\t\t4、  组件可以重复使用多次 \n\t      因为data中返回的是一个对象所以每个组件中的数据是私有的\n\t\t  即每个实例可以维护一份被返回对象的独立的拷贝   \n\t--\u003e \n    \u003cbutton-counter\u003e\u003c/button-counter\u003e\n    \u003cbutton-counter\u003e\u003c/button-counter\u003e\n    \u003cbutton-counter\u003e\u003c/button-counter\u003e\n      \u003c!-- 8、必须使用短横线的方式使用组件 --\u003e\n     \u003chello-world\u003e\u003c/hello-world\u003e\n\u003c/div\u003e\n\n\u003cscript type=\"text/javascript\"\u003e\n\t//5  如果使用驼峰式命名组件，那么在使用组件的时候，只能在字符串模板中用驼峰的方式使用组件，\n    // 7、但是在普通的标签模板中，必须使用短横线的方式使用组件\n     Vue.component('HelloWorld', {\n      data: function(){\n        return {\n          msg: 'HelloWorld'\n        }\n      },\n      template: '\u003cdiv\u003e{{msg}}\u003c/div\u003e'\n    });\n     \n    Vue.component('button-counter', {\n      // 1、组件参数的data值必须是函数 \n      // 同时这个函数要求返回一个对象  \n      data: function(){\n        return {\n          count: 0\n        }\n      },\n      //  2、组件模板必须是单个根元素\n      //  3、组件模板的内容可以是模板字符串  \n      template: `\n        \u003cdiv\u003e\n          \u003cbutton @click=\"handle\"\u003e点击了{{count}}次\u003c/button\u003e\n          \u003cbutton\u003e测试123\u003c/button\u003e\n\t\t\t#  6 在字符串模板中可以使用驼峰的方式使用组件\t\n\t\t   \u003cHelloWorld\u003e\u003c/HelloWorld\u003e\n        \u003c/div\u003e\n      `,\n      methods: {\n        handle: function(){\n          this.count += 2;\n        }\n      }\n    })\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        \n      }\n    });\n  \u003c/script\u003e\n```\n\n#### 局部注册\n\n- 只能在当前注册它的vue实例中使用\n\n```html\n\u003cdiv id=\"app\"\u003e\n      \u003chello-jerry\u003e\u003c/hello-jerry\u003e\n\u003c/div\u003e\n\n\u003cscript\u003e\n    // 定义组件的模板\n    var HelloJerry = {\n      data: function(){\n        return {\n          msg: 'HelloJerry'\n        }\n      },\n      template: '\u003cdiv\u003e{{msg}}\u003c/div\u003e'\n    };\n    var vm = new Vue({\n      el: '#app',\n      data: {\n      },\n      components: {\n        'hello-jerry': HelloJerry\n      }\n    });\n \u003c/script\u003e\n```\n\n### Vue组件之间传值\n\n#### 父组件向子组件传值\n\n- 父组件发送的形式是以属性的形式绑定值到子组件身上。\n- 然后子组件用属性props接收\n- 在props中使用驼峰形式，模板中需要使用短横线的形式字符串形式的模板中没有这个限制\n\n```html\n  \u003cdiv id=\"app\"\u003e\n    \u003cdiv\u003e{{pmsg}}\u003c/div\u003e\n     \u003c!--1、menu-item  在 APP中嵌套着 故 menu-item   为  子组件      --\u003e\n     \u003c!-- 给子组件传入一个静态的值 --\u003e\n    \u003cmenu-item title='来自父组件的值'\u003e\u003c/menu-item\u003e\n    \u003c!-- 2、 需要动态的数据的时候 需要属性绑定的形式设置 此时 ptitle  来自父组件data 中的数据 . \n\t\t  传的值可以是数字、对象、数组等等\n\t--\u003e\n    \u003cmenu-item :title='ptitle' content='hello'\u003e\u003c/menu-item\u003e\n  \u003c/div\u003e\n\n  \u003cscript type=\"text/javascript\"\u003e\n    Vue.component('menu-item', {\n      // 3、 子组件用属性props接收父组件传递过来的数据  \n      props: ['title', 'content'],\n      data: function() {\n        return {\n          msg: '子组件本身的数据'\n        }\n      },\n      template: '\u003cdiv\u003e{{msg + \"----\" + title + \"-----\" + content}}\u003c/div\u003e'\n    });\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        pmsg: '父组件中内容',\n        ptitle: \"'动态绑定属性'\"\n      }\n    });\n  \u003c/script\u003e\n```\n\n#### 子组件向父组件传值\n\n- 子组件用`$emit()`触发事件\n- `$emit()` 第一个参数为 自定义的事件,第二个参数为需要传递的数据\n- 父组件用v-on 监听子组件的事件\n\n```html\n \u003cdiv id=\"app\"\u003e\n    \u003cdiv :style='{fontSize: fontSize + \"px\"}'\u003e{{pmsg}}\u003c/div\u003e\n     \u003c!-- 2 父组件用v-on 监听子组件的事件\n\t\t这里 enlarge-text  是从 $emit 中的第一个参数对应   handle 为对应的事件处理函数\t\n\t--\u003e\t\n    \u003cmenu-item :parr='parr' @enlarge-text='handle($event)'\u003e\u003c/menu-item\u003e\n   \u003c!-- 注意handle($event)显式指定event，要么是handle不加括号 --\u003e\n  \u003c/div\u003e\n  \u003cscript type=\"text/javascript\" src=\"js/vue.js\"\u003e\u003c/script\u003e\n  \u003cscript type=\"text/javascript\"\u003e\n    /*\n      子组件向父组件传值-携带参数\n    */\n    \n    Vue.component('menu-item', {\n      props: ['parr'],\n      template: `\n        \u003cdiv\u003e\n          \u003cul\u003e\n            \u003cli :key='index' v-for='(item,index) in parr'\u003e{{item}}\u003c/li\u003e\n          \u003c/ul\u003e\n\t\t\t###  1、子组件用$emit()触发事件\n\t\t\t### 第一个参数为 自定义的事件名称   第二个参数为需要传递的数据  \n          \u003cbutton @click='$emit(\"enlarge-text\", 5)'\u003e扩大父组件中字体大小\u003c/button\u003e\n          \u003cbutton @click='$emit(\"enlarge-text\", 10)'\u003e扩大父组件中字体大小\u003c/button\u003e\n        \u003c/div\u003e\n      `\n    });\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        pmsg: '父组件中内容',\n        parr: ['apple','orange','banana'],\n        fontSize: 10\n      },\n      methods: {\n        handle: function(val){\n          // 扩大字体大小\n          this.fontSize += val;\n        }\n      }\n    });\n  \u003c/script\u003e\n\n```\n\n#### 兄弟之间的传递\n\n- 兄弟之间传递数据需要借助于事件中心，通过事件中心传递数据\n  - 提供事件中心 var hub = new Vue()\n- 传递数据方，通过一个事件触发hub.$emit(方法名，传递的数据)\n- 接收数据方，通过mounted(){} 钩子中 触发hub.$on()方法名\n- 销毁事件 通过hub.$off()方法名销毁之后无法进行传递数据\n\n```html\n \u003cdiv id=\"app\"\u003e\n    \u003cdiv\u003e父组件\u003c/div\u003e\n    \u003cdiv\u003e\n      \u003cbutton @click='handle'\u003e销毁事件\u003c/button\u003e\n    \u003c/div\u003e\n    \u003ctest-tom\u003e\u003c/test-tom\u003e\n    \u003ctest-jerry\u003e\u003c/test-jerry\u003e\n  \u003c/div\u003e\n  \u003cscript type=\"text/javascript\" src=\"js/vue.js\"\u003e\u003c/script\u003e\n  \u003cscript type=\"text/javascript\"\u003e\n    /*\n      兄弟组件之间数据传递\n    */\n    //1、 提供事件中心\n    var hub = new Vue();\n\n    Vue.component('test-tom', {\n      data: function(){\n        return {\n          num: 0\n        }\n      },\n      template: `\n        \u003cdiv\u003e\n          \u003cdiv\u003eTOM:{{num}}\u003c/div\u003e\n          \u003cdiv\u003e\n            \u003cbutton @click='handle'\u003e点击\u003c/button\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      `,\n      methods: {\n        handle: function(){\n          //2、传递数据方，通过一个事件触发hub.$emit(方法名，传递的数据)   触发兄弟组件的事件\n          hub.$emit('jerry-event', 2);\n        }\n      },\n      mounted: function() {\n       // 3、接收数据方，通过mounted(){} 钩子中  触发hub.$on(方法名\n        hub.$on('tom-event', (val) =\u003e {\n          this.num += val;\n        });\n      }\n    });\n    Vue.component('test-jerry', {\n      data: function(){\n        return {\n          num: 0\n        }\n      },\n      template: `\n        \u003cdiv\u003e\n          \u003cdiv\u003eJERRY:{{num}}\u003c/div\u003e\n          \u003cdiv\u003e\n            \u003cbutton @click='handle'\u003e点击\u003c/button\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      `,\n      methods: {\n        handle: function(){\n          //2、传递数据方，通过一个事件触发hub.$emit(方法名，传递的数据)   触发兄弟组件的事件\n          hub.$emit('tom-event', 1);\n        }\n      },\n      mounted: function() {\n        // 3、接收数据方，通过mounted(){} 钩子中  触发hub.$on()方法名\n        hub.$on('jerry-event', (val) =\u003e {\n          this.num += val;\n        });\n      }\n    });\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        \n      },\n      methods: {\n        handle: function(){\n          //4、销毁事件 通过hub.$off()方法名销毁之后无法进行传递数据  \n          hub.$off('tom-event');\n          hub.$off('jerry-event');\n        }\n      }\n    });\n  \u003c/script\u003e\n\n```\n\n### 组件插槽\n\n组件的最大特性就是复用性，而用好插槽能大大提高组件的可复用能力。\n\n#### 匿名插槽\n\n```html\n  \u003cdiv id=\"app\"\u003e\n    \u003c!-- 这里的所有组件标签中嵌套的内容会替换掉slot  如果不传值则使用 slot 中的默认值  --\u003e  \n    \u003calert-box\u003e有bug发生\u003c/alert-box\u003e\n    \u003calert-box\u003e有一个警告\u003c/alert-box\u003e\n    \u003calert-box\u003e\u003c/alert-box\u003e\n  \u003c/div\u003e\n\n  \u003cscript type=\"text/javascript\"\u003e\n    /*\n      组件插槽：父组件向子组件传递内容\n    */\n    Vue.component('alert-box', {\n      template: `\n        \u003cdiv\u003e\n          \u003cstrong\u003eERROR:\u003c/strong\u003e\n\t\t# 当组件渲染的时候，这个 \u003cslot\u003e 元素将会被替换为“组件标签中嵌套的内容”。\n\t\t# 插槽内可以包含任何模板代码，包括 HTML\n          \u003cslot\u003e默认内容\u003c/slot\u003e\n        \u003c/div\u003e\n      `\n    });\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        \n      }\n    });\n  \u003c/script\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n\n```\n\n![image-20200312164746313](image-20200312164746313.png)\n\n#### 具名插槽\n\n- 具有名字的插槽\n- 使用 \u003cslot\u003e 中的 \"name\" 属性绑定元素\n\n```HTML\n  \u003cdiv id=\"app\"\u003e\n    \u003cbase-layout\u003e\n       \u003c!-- 2、 通过slot属性来指定, 这个slot的值必须和下面slot组件得name值对应上\n\t\t\t\t如果没有匹配到 则放到匿名的插槽中   --\u003e \n      \u003cp slot='header'\u003e标题信息\u003c/p\u003e\n      \u003cp\u003e主要内容1\u003c/p\u003e\n      \u003cp\u003e主要内容2\u003c/p\u003e\n      \u003cp slot='footer'\u003e底部信息信息\u003c/p\u003e\n    \u003c/base-layout\u003e\n\n    \u003cbase-layout\u003e\n      \u003c!-- 注意点：template临时的包裹标签最终不会渲染到页面上     --\u003e  \n      \u003ctemplate slot='header'\u003e\n        \u003cp\u003e标题信息1\u003c/p\u003e\n        \u003cp\u003e标题信息2\u003c/p\u003e\n      \u003c/template\u003e\n      \u003cp\u003e主要内容1\u003c/p\u003e\n      \u003cp\u003e主要内容2\u003c/p\u003e\n      \u003ctemplate slot='footer'\u003e\n        \u003cp\u003e底部信息信息1\u003c/p\u003e\n        \u003cp\u003e底部信息信息2\u003c/p\u003e\n      \u003c/template\u003e\n    \u003c/base-layout\u003e\n  \u003c/div\u003e\n  \u003cscript type=\"text/javascript\" src=\"js/vue.js\"\u003e\u003c/script\u003e\n  \u003cscript type=\"text/javascript\"\u003e\n    /*\n      具名插槽\n    */\n    Vue.component('base-layout', {\n      template: `\n        \u003cdiv\u003e\n          \u003cheader\u003e\n\t\t\t###\t1、 使用 \u003cslot\u003e 中的 \"name\" 属性绑定元素 指定当前插槽的名字\n            \u003cslot name='header'\u003e\u003c/slot\u003e\n          \u003c/header\u003e\n          \u003cmain\u003e\n            \u003cslot\u003e\u003c/slot\u003e\n          \u003c/main\u003e\n          \u003cfooter\u003e\n\t\t\t###  注意点： \n\t\t\t###  具名插槽的渲染顺序，完全取决于模板，而不是取决于父组件中元素的顺序\n            \u003cslot name='footer'\u003e\u003c/slot\u003e\n          \u003c/footer\u003e\n        \u003c/div\u003e\n      `\n    });\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        \n      }\n    });\n  \u003c/script\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n```\n\n\u003cimg src=\"Vue.assets/image-20200312165238018.png\" alt=\"image-20200312165238018\" style=\"zoom:50%;\" /\u003e\n\n关于上面的template，最终渲染少一个根标签。\n\n#### 作用域插槽\n\n- 父组件对子组件加工处理\n- 既可以复用子组件的slot，又可以使slot内容不一致\n\n```html\n  \u003cdiv id=\"app\"\u003e\n    \u003c!-- \n\t\t1、当我们希望li 的样式由外部使用组件的地方定义，因为可能有多种地方要使用该组件，\n\t\t但样式希望不一样 这个时候我们需要使用作用域插槽 \n\t--\u003e  \n    \u003cfruit-list :list='list'\u003e\n       \u003c!-- 2、 父组件中使用了\u003ctemplate\u003e元素,而且包含scope=\"slotProps\",\n\t\t\tslotProps在这里只是临时变量   \n\t\t---\u003e \t\n      \u003ctemplate slot-scope='slotProps'\u003e\n        \u003cstrong v-if='slotProps.info.id==3' class=\"current\"\u003e\n            {{slotProps.info.name}}\t\t         \n         \u003c/strong\u003e\n        \u003cspan v-else\u003e{{slotProps.info.name}}\u003c/span\u003e\n      \u003c/template\u003e\n    \u003c/fruit-list\u003e\n  \u003c/div\u003e\n  \u003cscript type=\"text/javascript\" src=\"js/vue.js\"\u003e\u003c/script\u003e\n  \u003cscript type=\"text/javascript\"\u003e\n    /*\n      作用域插槽\n    */\n    Vue.component('fruit-list', {\n      props: ['list'],\n      template: `\n        \u003cdiv\u003e\n          \u003cli :key='item.id' v-for='item in list'\u003e\n\t\t\t###  3、 在子组件模板中,\u003cslot\u003e元素上有一个类似props传递数据给组件的写法msg=\"xxx\",\n\t\t\t###   插槽可以提供一个默认内容，如果如果父组件没有为这个插槽提供了内容，会显示默认的内容。\n\t\t\t\t\t如果父组件为这个插槽提供了内容，则默认的内容会被替换掉\n            \u003cslot :info='item'\u003e{{item.name}}\u003c/slot\u003e\n          \u003c/li\u003e\n        \u003c/div\u003e\n      `\n    });\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        list: [{\n          id: 1,\n          name: 'apple'\n        },{\n          id: 2,\n          name: 'orange'\n        },{\n          id: 3,\n          name: 'banana'\n        }]\n      }\n    });\n  \u003c/script\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n\n```\n\n### 购物车案例\n\n#### 1.实现组件化布局\n\n- 把静态页面转换成组件化模式\n- 把组件渲染到页面上\n\n````html\n \u003cdiv id=\"app\"\u003e\n    \u003cdiv class=\"container\"\u003e\n      \u003c!-- 2、把组件渲染到页面上 --\u003e \n      \u003cmy-cart\u003e\u003c/my-cart\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003cscript type=\"text/javascript\" src=\"js/vue.js\"\u003e\u003c/script\u003e\n  \u003cscript type=\"text/javascript\"\u003e\n    # 1、 把静态页面转换成组件化模式\n    # 1.1  标题组件 \n    var CartTitle = {\n      template: `\n        \u003cdiv class=\"title\"\u003e我的商品\u003c/div\u003e\n      `\n    }\n    # 1.2  商品列表组件 \n    var CartList = {\n      #  注意点 ：  组件模板必须是单个根元素  \n      template: `\n        \u003cdiv\u003e\n          \u003cdiv class=\"item\"\u003e\n            \u003cimg src=\"img/a.jpg\"/\u003e\n            \u003cdiv class=\"name\"\u003e\u003c/div\u003e\n            \u003cdiv class=\"change\"\u003e\n              \u003ca href=\"\"\u003e－\u003c/a\u003e\n              \u003cinput type=\"text\" class=\"num\" /\u003e\n              \u003ca href=\"\"\u003e＋\u003c/a\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"del\"\u003e×\u003c/div\u003e\n          \u003c/div\u003e\n          \u003cdiv class=\"item\"\u003e\n            \u003cimg src=\"img/b.jpg\"/\u003e\n            \u003cdiv class=\"name\"\u003e\u003c/div\u003e\n            \u003cdiv class=\"change\"\u003e\n              \u003ca href=\"\"\u003e－\u003c/a\u003e\n              \u003cinput type=\"text\" class=\"num\" /\u003e\n              \u003ca href=\"\"\u003e＋\u003c/a\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"del\"\u003e×\u003c/div\u003e\n          \u003c/div\u003e\n          \u003cdiv class=\"item\"\u003e\n            \u003cimg src=\"img/c.jpg\"/\u003e\n            \u003cdiv class=\"name\"\u003e\u003c/div\u003e\n            \u003cdiv class=\"change\"\u003e\n              \u003ca href=\"\"\u003e－\u003c/a\u003e\n              \u003cinput type=\"text\" class=\"num\" /\u003e\n              \u003ca href=\"\"\u003e＋\u003c/a\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"del\"\u003e×\u003c/div\u003e\n          \u003c/div\u003e\n          \u003cdiv class=\"item\"\u003e\n            \u003cimg src=\"img/d.jpg\"/\u003e\n            \u003cdiv class=\"name\"\u003e\u003c/div\u003e\n            \u003cdiv class=\"change\"\u003e\n              \u003ca href=\"\"\u003e－\u003c/a\u003e\n              \u003cinput type=\"text\" class=\"num\" /\u003e\n              \u003ca href=\"\"\u003e＋\u003c/a\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"del\"\u003e×\u003c/div\u003e\n          \u003c/div\u003e\n          \u003cdiv class=\"item\"\u003e\n            \u003cimg src=\"img/e.jpg\"/\u003e\n            \u003cdiv class=\"name\"\u003e\u003c/div\u003e\n            \u003cdiv class=\"change\"\u003e\n              \u003ca href=\"\"\u003e－\u003c/a\u003e\n              \u003cinput type=\"text\" class=\"num\" /\u003e\n              \u003ca href=\"\"\u003e＋\u003c/a\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"del\"\u003e×\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      `\n    }\n    # 1.3  商品结算组件 \n    var CartTotal = {\n      template: `\n        \u003cdiv class=\"total\"\u003e\n          \u003cspan\u003e总价：123\u003c/span\u003e\n          \u003cbutton\u003e结算\u003c/button\u003e\n        \u003c/div\u003e\n      `\n    }\n    ## 1.4  定义一个全局组件 my-cart\n    Vue.component('my-cart',{\n      ##  1.6 引入子组件  \n      template: `\n        \u003cdiv class='cart'\u003e\n          \u003ccart-title\u003e\u003c/cart-title\u003e\n          \u003ccart-list\u003e\u003c/cart-list\u003e\n          \u003ccart-total\u003e\u003c/cart-total\u003e\n        \u003c/div\u003e\n      `,\n      # 1.5  注册子组件   \n      components: {\n        'cart-title': CartTitle,\n        'cart-list': CartList,\n        'cart-total': CartTotal\n      }\n    });\n    var vm = new Vue({\n      el: '#app',\n      data: {\n\n      }\n    });\n\u003c/script\u003e\n````\n\n#### 2、实现 标题和结算功能组件\n\n- 标题组件实现动态渲染\n  - 从父组件把标题数据传递过来 即 父向子组件传值\n  - 把传递过来的数据渲染到页面上  \n- 结算功能组件\n  - 从父组件把商品列表list 数据传递过来 即 父向子组件传值\n  - 把传递过来的数据计算最终价格渲染到页面上  \n\n```html\n \u003cdiv id=\"app\"\u003e\n    \u003cdiv class=\"container\"\u003e\n      \u003cmy-cart\u003e\u003c/my-cart\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003cscript type=\"text/javascript\" src=\"js/vue.js\"\u003e\u003c/script\u003e\n  \u003cscript type=\"text/javascript\"\u003e\n     # 2.2  标题组件     子组件通过props形式接收父组件传递过来的uname数据\n    var CartTitle = {\n      props: ['uname'],\n      template: `\n        \u003cdiv class=\"title\"\u003e{{uname}}的商品\u003c/div\u003e\n      `\n    }\n\t# 2.3  商品结算组件  子组件通过props形式接收父组件传递过来的list数据   \n    var CartTotal = {\n      props: ['list'],\n      template: `\n        \u003cdiv class=\"total\"\u003e\n          \u003cspan\u003e总价：{{total}}\u003c/span\u003e\n          \u003cbutton\u003e结算\u003c/button\u003e\n        \u003c/div\u003e\n      `,\n      computed: {\n        # 2.4    计算商品的总价  并渲染到页面上 \n        total: function() {\n          var t = 0;\n          this.list.forEach(item =\u003e {\n            t += item.price * item.num;\n          });\n          return t;\n        }\n      }\n    }\n    Vue.component('my-cart',{\n      data: function() {\n        return {\n          uname: '张三',\n          list: [{\n            id: 1,\n            name: 'TCL彩电',\n            price: 1000,\n            num: 1,\n            img: 'img/a.jpg'\n          },{\n            id: 2,\n            name: '机顶盒',\n            price: 1000,\n            num: 1,\n            img: 'img/b.jpg'\n          },{\n            id: 3,\n            name: '海尔冰箱',\n            price: 1000,\n            num: 1,\n            img: 'img/c.jpg'\n          },{\n            id: 4,\n            name: '小米手机',\n            price: 1000,\n            num: 1,\n            img: 'img/d.jpg'\n          },{\n            id: 5,\n            name: 'PPTV电视',\n            price: 1000,\n            num: 2,\n            img: 'img/e.jpg'\n          }]\n        }\n      },\n      #  2.1  父组件向子组件以属性传递的形式 传递数据\n      #   向 标题组件传递 uname 属性   向 商品结算组件传递 list  属性  \n      template: `\n        \u003cdiv class='cart'\u003e\n          \u003ccart-title :uname='uname'\u003e\u003c/cart-title\u003e\n          \u003ccart-list\u003e\u003c/cart-list\u003e\n          \u003ccart-total :list='list'\u003e\u003c/cart-total\u003e\n        \u003c/div\u003e\n      `,\n      components: {\n        'cart-title': CartTitle,\n        'cart-list': CartList,\n        'cart-total': CartTotal\n      }\n    });\n    var vm = new Vue({\n      el: '#app',\n      data: {\n\n      }\n    });\n\u003c/script\u003e\n```\n\n#### 3.实现列表组件删除功能\n\n- 从父组件把商品列表list 数据传递过来 即 父向子组件传值\n- 把传递过来的数据渲染到页面上\n- 点击删除按钮的时候删除对应的数据\n  - 给按钮添加点击事件把需要删除的id传递过来  \n    - 子组件中不推荐操作父组件的数据有可能多个子组件使用父组件的数据 我们需要把数据传递给父组件让父组件操作数据\n    - 父组件删除对应的数据\n\n```html\n \u003cdiv id=\"app\"\u003e\n    \u003cdiv class=\"container\"\u003e\n      \u003cmy-cart\u003e\u003c/my-cart\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003cscript type=\"text/javascript\" src=\"js/vue.js\"\u003e\u003c/script\u003e\n  \u003cscript type=\"text/javascript\"\u003e\n    \n    var CartTitle = {\n      props: ['uname'],\n      template: `\n        \u003cdiv class=\"title\"\u003e{{uname}}的商品\u003c/div\u003e\n      `\n    }\n    #  3.2 把列表数据动态渲染到页面上  \n    var CartList = {\n      props: ['list'],\n      template: `\n        \u003cdiv\u003e\n          \u003cdiv :key='item.id' v-for='item in list' class=\"item\"\u003e\n            \u003cimg :src=\"item.img\"/\u003e\n            \u003cdiv class=\"name\"\u003e{{item.name}}\u003c/div\u003e\n            \u003cdiv class=\"change\"\u003e\n              \u003ca href=\"\"\u003e－\u003c/a\u003e\n              \u003cinput type=\"text\" class=\"num\" /\u003e\n              \u003ca href=\"\"\u003e＋\u003c/a\u003e\n            \u003c/div\u003e\n\t\t\t# 3.3  给按钮添加点击事件把需要删除的id传递过来\n            \u003cdiv class=\"del\" @click='del(item.id)'\u003e×\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      `,\n      methods: {\n        del: function(id){\n           # 3.4 子组件中不推荐操作父组件的数据有可能多个子组件使用父组件的数据 \n          # \t  我们需要把数据传递给父组件 让父组件操作数据 \n          this.$emit('cart-del', id);\n        }\n      }\n    }\n    var CartTotal = {\n      props: ['list'],\n      template: `\n        \u003cdiv class=\"total\"\u003e\n          \u003cspan\u003e总价：{{total}}\u003c/span\u003e\n          \u003cbutton\u003e结算\u003c/button\u003e\n        \u003c/div\u003e\n      `,\n      computed: {\n        total: function() {\n          // 计算商品的总价\n          var t = 0;\n          this.list.forEach(item =\u003e {\n            t += item.price * item.num;\n          });\n          return t;\n        }\n      }\n    }\n    Vue.component('my-cart',{\n      data: function() {\n        return {\n          uname: '张三',\n          list: [{\n            id: 1,\n            name: 'TCL彩电',\n            price: 1000,\n            num: 1,\n            img: 'img/a.jpg'\n          },{\n            id: 2,\n            name: '机顶盒',\n            price: 1000,\n            num: 1,\n            img: 'img/b.jpg'\n          },{\n            id: 3,\n            name: '海尔冰箱',\n            price: 1000,\n            num: 1,\n            img: 'img/c.jpg'\n          },{\n            id: 4,\n            name: '小米手机',\n            price: 1000,\n            num: 1,\n            img: 'img/d.jpg'\n          },{\n            id: 5,\n            name: 'PPTV电视',\n            price: 1000,\n            num: 2,\n            img: 'img/e.jpg'\n          }]\n        }\n      },\n      # 3.1 从父组件把商品列表list 数据传递过来 即 父向子组件传值  \n      template: `\n        \u003cdiv class='cart'\u003e\n          \u003ccart-title :uname='uname'\u003e\u003c/cart-title\u003e\n\t\t  #  3.5  父组件通过事件绑定 接收子组件传递过来的数据 \n          \u003ccart-list :list='list' @cart-del='delCart($event)'\u003e\u003c/cart-list\u003e\n          \u003ccart-total :list='list'\u003e\u003c/cart-total\u003e\n        \u003c/div\u003e\n      `,\n      components: {\n        'cart-title': CartTitle,\n        'cart-list': CartList,\n        'cart-total': CartTotal\n      },\n      methods: {\n        # 3.6    根据id删除list中对应的数据        \n        delCart: function(id) {\n          // 1、找到id所对应数据的索引\n          var index = this.list.findIndex(item=\u003e{\n            return item.id == id;\n          });\n          // 2、根据索引删除对应数据\n          this.list.splice(index, 1);\n        }\n      }\n    });\n    var vm = new Vue({\n      el: '#app',\n      data: {\n\n      }\n    });\n\n  \u003c/script\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n```\n\n#### 4.实现组件更新数据功能 上\n\n- 将输入框中的默认数据动态渲染出来\n- 输入框失去焦点的时候 更改商品的数量\n- 子组件中不推荐操作数据 把这些数据传递给父组件 让父组件处理这些数据\n- 父组件中接收子组件传递过来的数据并处理\n\n```html\n \u003cdiv id=\"app\"\u003e\n    \u003cdiv class=\"container\"\u003e\n      \u003cmy-cart\u003e\u003c/my-cart\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003cscript type=\"text/javascript\" src=\"js/vue.js\"\u003e\u003c/script\u003e\n  \u003cscript type=\"text/javascript\"\u003e\n    \n    var CartTitle = {\n      props: ['uname'],\n      template: `\n        \u003cdiv class=\"title\"\u003e{{uname}}的商品\u003c/div\u003e\n      `\n    }\n    var CartList = {\n      props: ['list'],\n      template: `\n        \u003cdiv\u003e\n          \u003cdiv :key='item.id' v-for='item in list' class=\"item\"\u003e\n            \u003cimg :src=\"item.img\"/\u003e\n            \u003cdiv class=\"name\"\u003e{{item.name}}\u003c/div\u003e\n            \u003cdiv class=\"change\"\u003e\n              \u003ca href=\"\"\u003e－\u003c/a\u003e\n\t\t\t\t# 1. 将输入框中的默认数据动态渲染出来\n\t\t\t\t# 2. 输入框失去焦点的时候 更改商品的数量  需要将当前商品的id 传递过来\n              \u003cinput type=\"text\" class=\"num\" :value='item.num' @blur='changeNum(item.id, $event)'/\u003e\n              \u003ca href=\"\"\u003e＋\u003c/a\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"del\" @click='del(item.id)'\u003e×\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      `,\n      methods: {\n        changeNum: function(id, event){\n          # 3 子组件中不推荐操作数据  因为别的组件可能也引用了这些数据\n          #  把这些数据传递给父组件 让父组件处理这些数据\n          this.$emit('change-num', {\n            id: id,\n            num: event.target.value\n          });\n        },\n        del: function(id){\n          // 把id传递给父组件\n          this.$emit('cart-del', id);\n        }\n      }\n    }\n    var CartTotal = {\n      props: ['list'],\n      template: `\n        \u003cdiv class=\"total\"\u003e\n          \u003cspan\u003e总价：{{total}}\u003c/span\u003e\n          \u003cbutton\u003e结算\u003c/button\u003e\n        \u003c/div\u003e\n      `,\n      computed: {\n        total: function() {\n          // 计算商品的总价\n          var t = 0;\n          this.list.forEach(item =\u003e {\n            t += item.price * item.num;\n          });\n          return t;\n        }\n      }\n    }\n    Vue.component('my-cart',{\n      data: function() {\n        return {\n          uname: '张三',\n          list: [{\n            id: 1,\n            name: 'TCL彩电',\n            price: 1000,\n            num: 1,\n            img: 'img/a.jpg'\n          }]\n      },\n      template: `\n        \u003cdiv class='cart'\u003e\n          \u003ccart-title :uname='uname'\u003e\u003c/cart-title\u003e\n\t\t\t# 4  父组件中接收子组件传递过来的数据 \n          \u003ccart-list :list='list' @change-num='changeNum($event)' @cart-del='delCart($event)'\u003e\u003c/cart-list\u003e\n          \u003ccart-total :list='list'\u003e\u003c/cart-total\u003e\n        \u003c/div\u003e\n      `,\n      components: {\n        'cart-title': CartTitle,\n        'cart-list': CartList,\n        'cart-total': CartTotal\n      },\n      methods: {\n        changeNum: function(val) {\n          //4.1 根据子组件传递过来的数据，跟新list中对应的数据\n          this.list.some(item=\u003e{\n            if(item.id == val.id) {\n              item.num = val.num;\n              // 终止遍历\n              return true;\n            }\n          });\n        },\n        delCart: function(id) {\n          // 根据id删除list中对应的数据\n          // 1、找到id所对应数据的索引\n          var index = this.list.findIndex(item=\u003e{\n            return item.id == id;\n          });\n          // 2、根据索引删除对应数据\n          this.list.splice(index, 1);\n        }\n      }\n    });\n    var vm = new Vue({\n      el: '#app',\n      data: {\n\n      }\n    });\n\u003c/script\u003e\n```\n\n#### 5.实现组件更新数据功能 下\n\n- 子组件通过一个标识符来标记对用的用户点击 + - 或者输入框输入的内容\n- 父组件拿到标识符更新对应的组件\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n  \u003cmeta charset=\"UTF-8\"\u003e\n  \u003ctitle\u003eDocument\u003c/title\u003e\n  \u003cstyle type=\"text/css\"\u003e\n    .container {\n    }\n    .container .cart {\n      width: 300px;\n      margin: auto;\n    }\n    .container .title {\n      background-color: lightblue;\n      height: 40px;\n      line-height: 40px;\n      text-align: center;\n      /*color: #fff;*/  \n    }\n    .container .total {\n      background-color: #FFCE46;\n      height: 50px;\n      line-height: 50px;\n      text-align: right;\n    }\n    .container .total button {\n      margin: 0 10px;\n      background-color: #DC4C40;\n      height: 35px;\n      width: 80px;\n      border: 0;\n    }\n    .container .total span {\n      color: red;\n      font-weight: bold;\n    }\n    .container .item {\n      height: 55px;\n      line-height: 55px;\n      position: relative;\n      border-top: 1px solid #ADD8E6;\n    }\n    .container .item img {\n      width: 45px;\n      height: 45px;\n      margin: 5px;\n    }\n    .container .item .name {\n      position: absolute;\n      width: 90px;\n      top: 0;left: 55px;\n      font-size: 16px;\n    }\n\n    .container .item .change {\n      width: 100px;\n      position: absolute;\n      top: 0;\n      right: 50px;\n    }\n    .container .item .change a {\n      font-size: 20px;\n      width: 30px;\n      text-decoration:none;\n      background-color: lightgray;\n      vertical-align: middle;\n    }\n    .container .item .change .num {\n      width: 40px;\n      height: 25px;\n    }\n    .container .item .del {\n      position: absolute;\n      top: 0;\n      right: 0px;\n      width: 40px;\n      text-align: center;\n      font-size: 40px;\n      cursor: pointer;\n      color: red;\n    }\n    .container .item .del:hover {\n      background-color: orange;\n    }\n  \u003c/style\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n  \u003cdiv id=\"app\"\u003e\n    \u003cdiv class=\"container\"\u003e\n      \u003cmy-cart\u003e\u003c/my-cart\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003cscript type=\"text/javascript\" src=\"js/vue.js\"\u003e\u003c/script\u003e\n  \u003cscript type=\"text/javascript\"\u003e\n    \n    var CartTitle = {\n      props: ['uname'],\n      template: `\n        \u003cdiv class=\"title\"\u003e{{uname}}的商品\u003c/div\u003e\n      `\n    }\n    var CartList = {\n      props: ['list'],\n      template: `\n        \u003cdiv\u003e\n          \u003cdiv :key='item.id' v-for='item in list' class=\"item\"\u003e\n            \u003cimg :src=\"item.img\"/\u003e\n            \u003cdiv class=\"name\"\u003e{{item.name}}\u003c/div\u003e\n            \u003cdiv class=\"change\"\u003e\n\t\t\t  # 1.  + - 按钮绑定事件 \n              \u003ca href=\"\" @click.prevent='sub(item.id)'\u003e－\u003c/a\u003e\n              \u003cinput type=\"text\" class=\"num\" :value='item.num' @blur='changeNum(item.id, $event)'/\u003e\n              \u003ca href=\"\" @click.prevent='add(item.id)'\u003e＋\u003c/a\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"del\" @click='del(item.id)'\u003e×\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      `,\n      methods: {\n        changeNum: function(id, event){\n          this.$emit('change-num', {\n            id: id,\n            type: 'change',\n            num: event.target.value\n          });\n        },\n        sub: function(id){\n          # 2 数量的增加和减少通过父组件来计算   每次都是加1 和 减1 不需要传递数量   父组件需要一个类型来判断 是 加一 还是减1  以及是输入框输入的数据  我们通过type 标识符来标记 不同的操作   \n          this.$emit('change-num', {\n            id: id,\n            type: 'sub'\n          });\n        },\n        add: function(id){\n         # 2 数量的增加和减少通过父组件来计算   每次都是加1 和 减1 不需要传递数量   父组件需要一个类型来判断 是 加一 还是减1  以及是输入框输入的数据  我们通过type 标识符来标记 不同的操作\n          this.$emit('change-num', {\n            id: id,\n            type: 'add'\n          });\n        },\n        del: function(id){\n          // 把id传递给父组件\n          this.$emit('cart-del', id);\n        }\n      }\n    }\n    var CartTotal = {\n      props: ['list'],\n      template: `\n        \u003cdiv class=\"total\"\u003e\n          \u003cspan\u003e总价：{{total}}\u003c/span\u003e\n          \u003cbutton\u003e结算\u003c/button\u003e\n        \u003c/div\u003e\n      `,\n      computed: {\n        total: function() {\n          // 计算商品的总价\n          var t = 0;\n          this.list.forEach(item =\u003e {\n            t += item.price * item.num;\n          });\n          return t;\n        }\n      }\n    }\n    Vue.component('my-cart',{\n      data: function() {\n        return {\n          uname: '张三',\n          list: [{\n            id: 1,\n            name: 'TCL彩电',\n            price: 1000,\n            num: 1,\n            img: 'img/a.jpg'\n          },{\n            id: 2,\n            name: '机顶盒',\n            price: 1000,\n            num: 1,\n            img: 'img/b.jpg'\n          },{\n            id: 3,\n            name: '海尔冰箱',\n            price: 1000,\n            num: 1,\n            img: 'img/c.jpg'\n          },{\n            id: 4,\n            name: '小米手机',\n            price: 1000,\n            num: 1,\n            img: 'img/d.jpg'\n          },{\n            id: 5,\n            name: 'PPTV电视',\n            price: 1000,\n            num: 2,\n            img: 'img/e.jpg'\n          }]\n        }\n      },\n      template: `\n        \u003cdiv class='cart'\u003e\n          \u003ccart-title :uname='uname'\u003e\u003c/cart-title\u003e\t\n\t\t# 3 父组件通过事件监听   接收子组件的数据  \n          \u003ccart-list :list='list' @change-num='changeNum($event)' @cart-del='delCart($event)'\u003e\u003c/cart-list\u003e\n          \u003ccart-total :list='list'\u003e\u003c/cart-total\u003e\n        \u003c/div\u003e\n      `,\n      components: {\n        'cart-title': CartTitle,\n        'cart-list': CartList,\n        'cart-total': CartTotal\n      },\n      methods: {\n        changeNum: function(val) {\n          #4 分为三种情况：输入框变更、加号变更、减号变更\n          if(val.type=='change') {\n            // 根据子组件传递过来的数据，跟新list中对应的数据\n            this.list.some(item=\u003e{\n              if(item.id == val.id) {\n                item.num = val.num;\n                // 终止遍历\n                return true;\n              }\n            });\n          }else if(val.type=='sub'){\n            // 减一操作\n            this.list.some(item=\u003e{\n              if(item.id == val.id) {\n                item.num -= 1;\n                // 终止遍历\n                return true;\n              }\n            });\n          }else if(val.type=='add'){\n            // 加一操作\n            this.list.some(item=\u003e{\n              if(item.id == val.id) {\n                item.num += 1;\n                // 终止遍历\n                return true;\n              }\n            });\n          }\n        }\n      }\n    });\n    var vm = new Vue({\n      el: '#app',\n      data: {\n\n      }\n    });\n\n  \u003c/script\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n```\n\n## 接口调用方式\n\n- 原生ajax\n- 基于jQuery的ajax\n- fetch\n- axios\n\n### 异步\n\n- JavaScript的执行环境是「单线程」\n- 所谓单线程，是指JS引擎中负责解释和执行JavaScript代码的线程只有一个，也就是一次只能完成一项任务，这个任务执行完后才能执行下一个，它会「阻塞」其他任务。这个任务可称为主线程\n- 异步模式可以一起执行**多个任务**\n- JS中常见的异步调用\n  - 定时任何\n  - ajax\n  - 事件函数\n\n### promise\n\n- 主要解决异步深层嵌套的问题\n- promise 提供了简洁的API 使得异步操作更加容易\n\n```html\n\u003cscript type=\"text/javascript\"\u003e\n    /*\n     1. Promise基本使用\n           我们使用new来构建一个Promise  Promise的构造函数接收一个参数，是函数，并且传入两个参数：\t\t   resolve，reject， 分别表示异步操作执行成功后的回调函数和异步操作执行失败后的回调函数\n    */\n    var p = new Promise(function(resolve, reject){\n      //2. 这里用于实现异步任务  setTimeout\n      setTimeout(function(){\n        var flag = false;\n        if(flag) {\n          //3. 正常情况\n          resolve('hello');\n        }else{\n          //4. 异常情况\n          reject('出错了');\n        }\n      }, 100);\n    });\n    //  5 Promise实例生成以后，可以用then方法指定resolved状态和reject状态的回调函数 \n    //  在then方法中，你也可以直接return数据而不是Promise对象，在后面的then中就可以接收到数据了  \n    p.then(function(data){\n      console.log(data)\n    },function(info){\n      console.log(info)\n    });\n  \u003c/script\u003e\n```\n\n### 基于Promise发送Ajax请求\n\n```html\n\u003cscript type=\"text/javascript\"\u003e\n    /*\n      基于Promise发送Ajax请求\n    */\n    function queryData(url) {\n     #   1.1 创建一个Promise实例\n      var p = new Promise(function(resolve, reject){\n        var xhr = new XMLHttpRequest();\n        xhr.onreadystatechange = function(){\n          if(xhr.readyState != 4) return;\n          if(xhr.readyState == 4 \u0026\u0026 xhr.status == 200) {\n            # 1.2 处理正常的情况\n            resolve(xhr.responseText);\n          }else{\n            # 1.3 处理异常情况\n            reject('服务器错误');\n          }\n        };\n        xhr.open('get', url);\n        xhr.send(null);\n      });\n      return p;\n    }\n\t  # 注意：  这里需要开启一个服务 \n    # 在then方法中，你也可以直接return数据而不是Promise对象，在后面的then中就可以接收到数据了\n    queryData('http://localhost:3000/data')\n      .then(function(data){\n        console.log(data)\n        #  1.4 想要继续链式编程下去 需要 return  \n        return queryData('http://localhost:3000/data1');\n      })\n      .then(function(data){\n        console.log(data);\n        return queryData('http://localhost:3000/data2');\n      })\n      .then(function(data){\n        console.log(data)\n      });\n\u003c/script\u003e\n```\n\n### Promise 基本API\n\n#### 实例方法\n\n##### `.then()`\n\n- 得到异步任务正确的结果\n\n##### `.catch()`\n\n- 获取异常信息\n\n##### `.finally()`\n\n- 成功与否都会执行（不是正式标准）\n\n```html\n\u003cscript type=\"text/javascript\"\u003e\n    /*\n      Promise常用API-实例方法\n    */\n    // console.dir(Promise);\n    function foo() {\n      return new Promise(function(resolve, reject){\n        setTimeout(function(){\n          // resolve(123);\n          reject('error');\n        }, 100);\n      })\n    }\n    // foo()\n    //   .then(function(data){\n    //     console.log(data)\n    //   })\n    //   .catch(function(data){\n    //     console.log(data)\n    //   })\n    //   .finally(function(){\n    //     console.log('finished')\n    //   });\n\n    // --------------------------\n    // 两种写法是等效的\n    foo()\n      .then(function(data){\n        # 得到异步任务正确的结果\n        console.log(data)\n      },function(data){\n        # 获取异常信息\n        console.log(data)\n      })\n      # 成功与否都会执行（不是正式标准） \n      .finally(function(){\n        console.log('finished')\n      });\n  \u003c/script\u003e\n```\n\n#### 静态方法\n\n##### `.all()`\n\n- `Promise.all`方法接受一个数组作参数，数组中的对象（p1、p2、p3）均为promise实例（如果不是一个promise，该项会被用`Promise.resolve`转换为一个promise)。它的状态由这三个promise实例决定\n\n##### `.race()`\n\n- `Promise.race`方法同样接受一个数组作参数。当p1, p2, p3中有一个实例的状态发生改变（变为`fulfilled`或`rejected`），p的状态就跟着改变。并把第一个改变状态的promise的返回值，传给p的回调函数\n\n```html\n\u003cscript type=\"text/javascript\"\u003e\n    /*\n      Promise常用API-对象方法\n    */\n    // console.dir(Promise)\n    function queryData(url) {\n      return new Promise(function(resolve, reject){\n        var xhr = new XMLHttpRequest();\n        xhr.onreadystatechange = function(){\n          if(xhr.readyState != 4) return;\n          if(xhr.readyState == 4 \u0026\u0026 xhr.status == 200) {\n            // 处理正常的情况\n            resolve(xhr.responseText);\n          }else{\n            // 处理异常情况\n            reject('服务器错误');\n          }\n        };\n        xhr.open('get', url);\n        xhr.send(null);\n      });\n    }\n\n    var p1 = queryData('http://localhost:3000/a1');\n    var p2 = queryData('http://localhost:3000/a2');\n    var p3 = queryData('http://localhost:3000/a3');\n     Promise.all([p1,p2,p3]).then(function(result){\n       //   all 中的参数  [p1,p2,p3]   和 返回的结果一 一对应[\"HELLO TOM\", \"HELLO JERRY\", \"HELLO SPIKE\"]\n       console.log(result) //[\"HELLO TOM\", \"HELLO JERRY\", \"HELLO SPIKE\"]\n     })\n    Promise.race([p1,p2,p3]).then(function(result){\n      // 由于p1执行较快，Promise的then()将获得结果'P1'。p2,p3仍在继续执行，但执行结果将被丢弃。\n      console.log(result) // \"HELLO TOM\"\n    })\n  \u003c/script\u003e\n```\n\n### fetch\n\n- Fetch API是新的ajax解决方案 Fetch会返回Promise\n- **fetch不是ajax的进一步封装，而是原生js，没有使用XMLHttpRequest对象**。\n- fetch(url, options).then(）\n\n```html\n  \u003cscript type=\"text/javascript\"\u003e\n    /*\n      Fetch API 基本用法\n      \tfetch(url).then()\n     \t第一个参数请求的路径   Fetch会返回Promise   所以我们可以使用then 拿到请求成功的结果 \n    */\n    fetch('http://localhost:3000/fdata').then(function(data){\n      // text()方法属于fetchAPI的一部分，它返回一个Promise实例对象，用于获取后台返回的数据\n      return data.text();\n    }).then(function(data){\n      //   在这个then里面我们能拿到最终的数据  \n      console.log(data);\n    })\n  \u003c/script\u003e\n```\n\n#### fetch API 中的 HTTP 请求\n\n- fetch(url, options).then()\n- HTTP协议，它给我们提供了很多的方法，如POST，GET，DELETE，UPDATE，PATCH和PUT\n  - 默认的是 GET 请求\n  - 需要在 options 对象中 指定对应的 method method:请求使用的方法\n  - post 和 普通 请求的时候 需要在options 中 设置 请求头 headers 和  body\n\n```html\n   \u003cscript type=\"text/javascript\"\u003e\n        /*\n              Fetch API 调用接口传递参数\n        */\n       #1.1 GET参数传递 - 传统URL  通过url  ？ 的形式传参 \n        fetch('http://localhost:3000/books?id=123', {\n            \t# get 请求可以省略不写 默认的是GET \n                method: 'get'\n            })\n            .then(function(data) {\n            \t# 它返回一个Promise实例对象，用于获取后台返回的数据\n                return data.text();\n            }).then(function(data) {\n            \t# 在这个then里面我们能拿到最终的数据  \n                console.log(data)\n            });\n\n      #1.2  GET参数传递  restful形式的URL  通过/ 的形式传递参数  即  id = 456 和id后台的配置有关 \n        fetch('http://localhost:3000/books/456', {\n            \t# get 请求可以省略不写 默认的是GET \n                method: 'get'\n            })\n            .then(function(data) {\n                return data.text();\n            }).then(function(data) {\n                console.log(data)\n            });\n\n       #2.1  DELETE请求方式参数传递      删除id  是  id=789\n        fetch('http://localhost:3000/books/789', {\n                method: 'delete'\n            })\n            .then(function(data) {\n                return data.text();\n            }).then(function(data) {\n                console.log(data)\n            });\n\n       #3 POST请求传参\n        fetch('http://localhost:3000/books', {\n                method: 'post',\n            \t# 3.1  传递数据 \n                body: 'uname=lisi\u0026pwd=123',\n            \t#  3.2  设置请求头 \n                headers: {\n                    'Content-Type': 'application/x-www-form-urlencoded'\n                }\n            })\n            .then(function(data) {\n                return data.text();\n            }).then(function(data) {\n                console.log(data)\n            });\n\n       # POST请求传参\n        fetch('http://localhost:3000/books', {\n                method: 'post',\n                body: JSON.stringify({\n                    uname: '张三',\n                    pwd: '456'\n                }),\n                headers: {\n                    'Content-Type': 'application/json'\n                }\n            })\n            .then(function(data) {\n                return data.text();\n            }).then(function(data) {\n                console.log(data)\n            });\n\n        # PUT请求传参     修改id 是 123 的 \n        fetch('http://localhost:3000/books/123', {\n                method: 'put',\n                body: JSON.stringify({\n                    uname: '张三',\n                    pwd: '789'\n                }),\n                headers: {\n                    'Content-Type': 'application/json'\n                }\n            })\n            .then(function(data) {\n                return data.text();\n            }).then(function(data) {\n                console.log(data)\n            });\n    \u003c/script\u003e\n```\n\n#### fetchAPI 中 响应格式\n\n- 用fetch来获取数据，如果响应正常返回，我们首先看到的是一个response对象，其中包括返回的一堆原始字节，这些字节需要在收到后，需要我们通过调用方法将其转换为相应格式的数据，比如`JSON`，`BLOB`或者`TEXT`等等\n\n```js\n    /*\n      Fetch响应结果的数据格式\n    */\n    fetch('http://localhost:3000/json').then(function(data){\n      // return data.json();   //  将获取到的数据使用 json 转换对象\n      return data.text(); //  //  将获取到的数据 转换成字符串 \n    }).then(function(data){\n      // console.log(data.uname)\n      // console.log(typeof data)\n      var obj = JSON.parse(data);\n      console.log(obj.uname,obj.age,obj.gender)\n    })\n\n```\n\n### axios\n\n- 基于promise用于浏览器和node.js的http客户端\n- 支持浏览器和node.js\n- 支持promise\n- 能拦截请求和响应\n- 自动转换JSON数据\n- 能转换请求和响应数据\n\n#### axios基础用法\n\n- get和 delete请求传递参数\n  - 通过传统的url 以 ? 的形式传递参数\n  - restful 形式传递参数\n  - 通过params形式传递参数\n- post 和 put 请求传递参数\n  - 通过选项传递参数\n  - 通过 URLSearchParams 传递参数\n\n```js\n# 1. 发送get 请求 \n\taxios.get('http://localhost:3000/adata').then(function(ret){ \n      #  拿到 ret 是一个对象      所有的对象都存在 ret 的data 属性里面\n      // 注意data属性是固定的用法，用于获取后台的实际数据\n      // console.log(ret.data)\n      console.log(ret)\n    })\n# 2.  get 请求传递参数\n    # 2.1  通过传统的url  以 ? 的形式传递参数\n\taxios.get('http://localhost:3000/axios?id=123').then(function(ret){\n      console.log(ret.data)\n    })\n    # 2.2  restful 形式传递参数 \n    axios.get('http://localhost:3000/axios/123').then(function(ret){\n      console.log(ret.data)\n    })\n\t  # 2.3  通过params  形式传递参数 \n    axios.get('http://localhost:3000/axios', {\n      params: {\n        id: 789\n      }\n    }).then(function(ret){\n      console.log(ret.data)\n    })\n#3 axios delete 请求传参     传参的形式和 get 请求一样\n    axios.delete('http://localhost:3000/axios', {\n      params: {\n        id: 111\n      }\n    }).then(function(ret){\n      console.log(ret.data)\n    })\n\n# 4  axios 的 post 请求\n    # 4.1  通过选项传递参数\n    axios.post('http://localhost:3000/axios', {\n      uname: 'lisi',\n      pwd: 123\n    }).then(function(ret){\n      console.log(ret.data)\n    })\n# 4.2  通过 URLSearchParams  传递参数 \n    var params = new URLSearchParams();\n    params.append('uname', 'zhangsan');\n    params.append('pwd', '111');\n    axios.post('http://localhost:3000/axios', params).then(function(ret){\n      console.log(ret.data)\n    })\n\n#5  axios put 请求传参   和 post 请求一样 \n    axios.put('http://localhost:3000/axios/123', {\n      uname: 'lisi',\n      pwd: 123\n    }).then(function(ret){\n      console.log(ret.data)\n    })\n```\n\n#### axios 全局配置\n\n```js\n#  配置公共的请求头 \naxios.defaults.baseURL = 'https://api.example.com';\n#  配置 超时时间\naxios.defaults.timeout = 2500;\n#  配置公共的请求头\naxios.defaults.headers.common['Authorization'] = AUTH_TOKEN;\n# 配置公共的 post 的 Content-Type\naxios.defaults.headers.post['Content-Type'] = 'application/x-www-form-urlencoded';\n```\n\n#### axios 拦截器\n\n- 请求拦截器\n  - 请求拦截器的作用是在请求发送前进行一些操作\n    - 例如在每个请求体里加上token，统一做了处理如果以后要改也非常容易\n- 响应拦截器\n  - 响应拦截器的作用是在接收到响应后进行一些操作\n    - 例如在服务器返回登录状态失效，需要重新登录的时候，跳转到登录页\n\n```js\n# 1. 请求拦截器 \naxios.interceptors.request.use(function(config) {\n     console.log(config.url)\n     # 1.1  任何请求都会经过这一步   在发送请求之前做些什么   \n     config.headers.mytoken = 'nihao';\n     # 1.2  这里一定要return   否则配置不成功  \n     return config;\n   }, function(err){\n      #1.3 对请求错误做点什么    \n     console.log(err)\n   })\n#2. 响应拦截器 \n   axios.interceptors.response.use(function(res) {\n     #2.1  在接收响应做些什么  \n     var data = res.data;\n     return data;\n   }, function(err){\n     #2.2 对响应错误做点什么  \n     console.log(err)\n   })\n```\n\n### async 和 await\n\n- `async`作为一个关键字放到函数前面\n  - 任何一个`async`函数都会隐式返回一个`promise`\n- `await`关键字只能在使用`async`定义的函数中使用\n  - await后面可以直接跟一个 Promise实例对象\n  - await函数不能单独使用\n- **async/await 让异步代码看起来、表现起来更像同步代码**\n\n```js\n# 1.  async 基础用法\n    # 1.1 async作为一个关键字放到函数前面\n\tasync function queryData() {\n      # 1.2 await关键字只能在使用async定义的函数中使用 await后面可以直接跟一个 Promise实例对象\n      var ret = await new Promise(function(resolve, reject){\n        setTimeout(function(){\n          resolve('nihao')\n        },1000);\n      })\n      // console.log(ret.data)\n      return ret;\n    }\n\t# 1.3 任何一个async函数都会隐式返回一个promise   我们可以使用then 进行链式编程\n    queryData().then(function(data){\n      console.log(data)\n    })\n\n\t#2.  async函数处理多个异步函数\n    axios.defaults.baseURL = 'http://localhost:3000';\n    async function queryData() {\n      # 2.1  添加await之后 当前的await 返回结果之后才会执行后面的代码   \n      var info = await axios.get('async1');\n      #2.2  让异步代码看起来、表现起来更像同步代码\n      var ret = await axios.get('async2?info=' + info.data);\n      return ret.data;\n    }\n    queryData().then(function(data){\n      console.log(data)\n    })\n```\n\n### 图书列表案例\n\n#### 1. 基于接口案例-获取图书列表\n\n- 导入axios 用来发送ajax\n- 把获取到的数据渲染到页面上\n\n```html\n  \u003cdiv id=\"app\"\u003e\n        \u003cdiv class=\"grid\"\u003e\n            \u003ctable\u003e\n                \u003cthead\u003e\n                    \u003ctr\u003e\n                        \u003cth\u003e编号\u003c/th\u003e\n                        \u003cth\u003e名称\u003c/th\u003e\n                        \u003cth\u003e时间\u003c/th\u003e\n                        \u003cth\u003e操作\u003c/th\u003e\n                    \u003c/tr\u003e\n                \u003c/thead\u003e\n                \u003ctbody\u003e\n                    \u003c!-- 5.  把books  中的数据渲染到页面上   --\u003e\n                    \u003ctr :key='item.id' v-for='item in books'\u003e\n                        \u003ctd\u003e{{item.id}}\u003c/td\u003e\n                        \u003ctd\u003e{{item.name}}\u003c/td\u003e\n                        \u003ctd\u003e{{item.date }}\u003c/td\u003e\n                        \u003ctd\u003e\n                            \u003ca href=\"\"\u003e修改\u003c/a\u003e\n                            \u003cspan\u003e|\u003c/span\u003e\n                            \u003ca href=\"\"\u003e删除\u003c/a\u003e\n                        \u003c/td\u003e\n                    \u003c/tr\u003e\n                \u003c/tbody\u003e\n            \u003c/table\u003e\n        \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cscript type=\"text/javascript\" src=\"js/vue.js\"\u003e\u003c/script\u003e\n\t1.  导入axios   \n    \u003cscript type=\"text/javascript\" src=\"js/axios.js\"\u003e\u003c/script\u003e\n    \u003cscript type=\"text/javascript\"\u003e\n        /*\n             图书管理-添加图书\n         */\n        # 2   配置公共的url地址  简化后面的调用方式\n        axios.defaults.baseURL = 'http://localhost:3000/';\n        axios.interceptors.response.use(function(res) {\n            return res.data;\n        }, function(error) {\n            console.log(error)\n        });\n\n        var vm = new Vue({\n            el: '#app',\n            data: {\n                flag: false,\n                submitFlag: false,\n                id: '',\n                name: '',\n                books: []\n            },\n            methods: {\n                # 3 定义一个方法 用来发送 ajax \n                # 3.1  使用 async  来 让异步的代码  以同步的形式书写 \n                queryData: async function() {\n                    // 调用后台接口获取图书列表数据\n                    // var ret = await axios.get('books');\n                    // this.books = ret.data;\n\t\t\t\t\t# 3.2  发送ajax请求  把拿到的数据放在books 里面   \n                    this.books = await axios.get('books');\n                }\n            },\n\n            mounted: function() {\n\t\t\t\t#  4 mounted  里面 DOM已经加载完毕  在这里调用函数  \n                this.queryData();\n            }\n        });\n    \u003c/script\u003e\n```\n\n#### 2 添加图书\n\n- 获取用户输入的数据 发送到后台\n- 渲染最新的数据到页面上\n\n```js\n methods: {\n    handle: async function(){\n          if(this.flag) {\n            // 编辑图书\n            // 就是根据当前的ID去更新数组中对应的数据\n            this.books.some((item) =\u003e {\n              if(item.id == this.id) {\n                item.name = this.name;\n                // 完成更新操作之后，需要终止循环\n                return true;\n              }\n            });\n            this.flag = false;\n          }else{\n            # 1.1  在前面封装好的 handle 方法中  发送ajax请求  \n            # 1.2  使用async  和 await 简化操作 需要在 function 前面添加 async   \n            var ret = await axios.post('books', {\n              name: this.name\n            })\n            # 1.3  根据后台返回的状态码判断是否加载数据 \n            if(ret.status == 200) {\n             # 1.4  调用 queryData 这个方法  渲染最新的数据 \n              this.queryData();\n            }\n          }\n          // 清空表单\n          this.id = '';\n          this.name = '';\n        },        \n }         \n```\n\n#### 3 验证图书名称是否存在\n\n- 添加图书之前发送请求验证图示是否已经存在\n- 如果不存在 往后台里面添加图书名称\n  - 图书存在与否只需要修改submitFlag的值即可\n\n```js\n watch: {\n        name: async function(val) {\n          // 验证图书名称是否已经存在\n          // var flag = this.books.some(function(item){\n          //   return item.name == val;\n          // });\n          var ret = await axios.get('/books/book/' + this.name);\n          if(ret.status == 1) {\n            // 图书名称存在\n            this.submitFlag = true;\n          }else{\n            // 图书名称不存在\n            this.submitFlag = false;\n          }\n        }\n},\n```\n\n#### 4. 编辑图书\n\n- 根据当前书的id 查询需要编辑的书籍\n- 需要根据状态位判断是添加还是编辑\n\n```js\n methods: {\n        handle: async function(){\n          if(this.flag) {\n            #4.3 编辑图书   把用户输入的信息提交到后台\n            var ret = await axios.put('books/' + this.id, {\n              name: this.name\n            });\n            if(ret.status == 200){\n              #4.4  完成添加后 重新加载列表数据\n              this.queryData();\n            }\n            this.flag = false;\n          }else{\n            // 添加图书\n            var ret = await axios.post('books', {\n              name: this.name\n            })\n            if(ret.status == 200) {\n              // 重新加载列表数据\n              this.queryData();\n            }\n          }\n          // 清空表单\n          this.id = '';\n          this.name = '';\n        },\n        toEdit: async function(id){\n          #4.1  flag状态位用于区分编辑和添加操作\n          this.flag = true;\n          #4.2  根据id查询出对应的图书信息  页面中可以加载出来最新的信息\n          # 调用接口发送ajax 请求  \n          var ret = await axios.get('books/' + id);\n          this.id = ret.id;\n          this.name = ret.name;\n        },\n```\n\n#### 5 删除图书\n\n- 把需要删除的id书籍 通过参数的形式传递到后台\n\n```js\n   deleteBook: async function(id){\n          // 删除图书\n          var ret = await axios.delete('books/' + id);\n          if(ret.status == 200) {\n            // 重新加载列表数据\n            this.queryData();\n          }\n   }\n```\n\n## 路由\n\n### 1.路由的概念\n\n路由的本质就是一种对应关系，比如说我们在url地址中输入我们要访问的url地址之后，浏览器要去请求这个url地址对应的资源。  \n那么url地址和真实的资源之间就有一种对应的关系，就是路由。\n\n路由分为前端路由和后端路由  \n1).后端路由是由服务器端进行实现，并完成资源的分发  \n2).前端路由是依靠hash值(锚链接)的变化进行实现\n\n后端路由性能相对前端路由来说较低，所以，我们接下来主要学习的是前端路由  \n前端路由的基本概念：根据不同的事件来显示不同的页面内容，即事件与事件处理函数之间的对应关系  \n前端路由主要做的事情就是监听事件并分发执行事件处理函数\n\n### 2.前端路由的初体验\n\n前端路由是基于hash值的变化进行实现的（比如点击页面中的菜单或者按钮改变URL的hash值，根据hash值的变化来控制组件的切换）  \n核心实现依靠一个事件，即监听hash值变化的事件。\n\n```javascript\nwindow.onhashchange = function(){\n    //location.hash可以获取到最新的hash值\n    location.hash\n}\n```\n\n前端路由实现tab栏切换：\n\n```html\n\u003c!DOCTYPE html\u003e\n    \u003chtml lang=\"en\"\u003e\n    \u003chead\u003e\n        \u003cmeta charset=\"UTF-8\" /\u003e\n        \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /\u003e\n        \u003cmeta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\" /\u003e\n        \u003ctitle\u003eDocument\u003c/title\u003e\n        \u003c!-- 导入 vue 文件 --\u003e\n        \u003cscript src=\"./lib/vue_2.5.22.js\"\u003e\u003c/script\u003e\n    \u003c/head\u003e\n    \u003cbody\u003e\n        \u003c!-- 被 vue 实例控制的 div 区域 --\u003e\n        \u003cdiv id=\"app\"\u003e\n        \u003c!-- 切换组件的超链接 --\u003e\n        \u003ca href=\"#/zhuye\"\u003e主页\u003c/a\u003e \n        \u003ca href=\"#/keji\"\u003e科技\u003c/a\u003e \n        \u003ca href=\"#/caijing\"\u003e财经\u003c/a\u003e\n        \u003ca href=\"#/yule\"\u003e娱乐\u003c/a\u003e\n\n        \u003c!-- 根据 :is 属性指定的组件名称，把对应的组件渲染到 component 标签所在的位置 --\u003e\n        \u003c!-- 可以把 component 标签当做是【组件的占位符】 --\u003e\n        \u003ccomponent :is=\"comName\"\u003e\u003c/component\u003e\n        \u003c/div\u003e\n\n        \u003cscript\u003e\n        // #region 定义需要被切换的 4 个组件\n        // 主页组件\n        const zhuye = {\n            template: '\u003ch1\u003e主页信息\u003c/h1\u003e'\n        }\n\n        // 科技组件\n        const keji = {\n            template: '\u003ch1\u003e科技信息\u003c/h1\u003e'\n        }\n\n        // 财经组件\n        const caijing = {\n            template: '\u003ch1\u003e财经信息\u003c/h1\u003e'\n        }\n\n        // 娱乐组件\n        const yule = {\n            template: '\u003ch1\u003e娱乐信息\u003c/h1\u003e'\n        }\n        // #endregion\n\n        // #region vue 实例对象\n        const vm = new Vue({\n            el: '#app',\n            data: {\n            comName: 'zhuye'\n            },\n            // 注册私有组件\n            components: {\n            zhuye,\n            keji,\n            caijing,\n            yule\n            }\n        })\n        // #endregion\n\n        // 监听 window 的 onhashchange 事件，根据获取到的最新的 hash 值，切换要显示的组件的名称\n        window.onhashchange = function() {\n            // 通过 location.hash 获取到最新的 hash 值\n            console.log(location.hash);\n            switch(location.hash.slice(1)){\n            case '/zhuye':\n                vm.comName = 'zhuye'\n            break\n            case '/keji':\n                vm.comName = 'keji'\n            break\n            case '/caijing':\n                vm.comName = 'caijing'\n            break\n            case '/yule':\n                vm.comName = 'yule'\n            break\n            }\n        }\n        \u003c/script\u003e\n    \u003c/body\u003e\n    \u003c/html\u003e\n```\n\n案例效果图:(缺失)\n\n点击每个超链接之后，会进行相应的内容切换，如下：\n\n![](01前端路由效果图.png)\n\n核心思路：  \n在页面中有一个vue实例对象，vue实例对象中有四个组件，分别是tab栏切换需要显示的组件内容  \n在页面中有四个超链接，如下：\n\n```\n\u003ca href=\"#/zhuye\"\u003e主页\u003c/a\u003e \n\u003ca href=\"#/keji\"\u003e科技\u003c/a\u003e \n\u003ca href=\"#/caijing\"\u003e财经\u003c/a\u003e\n\u003ca href=\"#/yule\"\u003e娱乐\u003c/a\u003e\n```\n\n当我们点击这些超链接的时候，就会改变url地址中的hash值，当hash值被改变时，就会触发onhashchange事件  \n在触发onhashchange事件的时候，我们根据hash值来让不同的组件进行显示：\n\n```javascript\nwindow.onhashchange = function() {\n    // 通过 location.hash 获取到最新的 hash 值\n    console.log(location.hash);\n    switch(location.hash.slice(1)){\n        case '/zhuye':\n        //通过更改数据comName来指定显示的组件\n        //因为 \u003ccomponent :is=\"comName\"\u003e\u003c/component\u003e ，组件已经绑定了comName\n        vm.comName = 'zhuye'\n        break\n        case '/keji':\n        vm.comName = 'keji'\n        break\n        case '/caijing':\n        vm.comName = 'caijing'\n        break\n        case '/yule':\n        vm.comName = 'yule'\n        break\n    }\n}\n```\n\n### 3.Vue Router简介\n\n它是一个Vue.js官方提供的路由管理器。是一个功能更加强大的前端路由器，推荐使用。  \nVue Router和Vue.js非常契合，可以一起方便的实现SPA(single page web application,单页应用程序)应用程序的开发。  \nVue Router依赖于Vue，所以需要先引入Vue，再引入Vue Router.\n\nVue Router的特性：  \n支持H5历史模式或者hash模式  \n支持嵌套路由  \n支持路由参数  \n支持编程式路由  \n支持命名路由  \n支持路由导航守卫  \n支持路由过渡动画特效  \n支持路由懒加载  \n支持路由滚动行为\n\n### 4.Vue Router的使用步骤(★★★)\n\nA.导入js文件\n\n`\\\u003cscript src=\"lib/vue_2.5.22.js”\u003e\\\u003c/script\u003e`  \n\\`\u003cscript src=\"lib/vue-router_3.0.2.js”\u003e\\\u003c/script\u003e`\n\nB.添加路由链接:\\\u003crouter-link\u003e是路由中提供的标签，默认会被渲染为a标签，to属性默认被渲染为href属性，to属性的值会被渲染为#开头的hash地址  \n\\\u003crouter-link to=\"/user”\u003eUser\\\u003c/router-link\u003e  \n\\\u003crouter-link to=\"/login”\u003eLogin\\\u003c/router-link\u003e  \nC.添加路由填充位（路由占位符）  \n\\\u003crouter-view\u003e\\\u003c/router-view\u003e  \nD.定义路由组件  \nvar User = { template:”\\\u003cdiv\u003eThis is User\\\u003c/div\u003e\" }  \nvar Login = { template:”\\\u003cdiv\u003eThis is Login\\\u003c/div\u003e\" }  \nE.配置路由规则并创建路由实例  \nvar myRouter = new VueRouter({  \n    //routes是路由规则数组  \n    routes:[  \n        //每一个路由规则都是一个对象，对象中至少包含path和component两个属性  \n        //path表示 路由匹配的hash地址，component表示路由规则对应要展示的组件对象  \n        {path:\"/user\",component:User},  \n        {path:\"/login\",component:Login}  \n    ]  \n})  \nF.将路由挂载到Vue实例中  \nnew Vue({  \n    el:\"#app\",  \n    //通过router属性挂载路由对象  \n    router:myRouter  \n})\n\n小结：  \nVue Router的使用步骤还是比较清晰的，按照步骤一步一步就能完成路由操作  \nA.导入js文件  \nB.添加路由链接  \nC.添加路由占位符(最后路由展示的组件就会在占位符的位置显示)  \nD.定义路由组件  \nE.配置路由规则并创建路由实例  \nF.将路由挂载到Vue实例中\n\n补充：  \n路由重定向：可以通过路由重定向为页面设置默认展示的组件  \n在路由规则中添加一条路由规则即可，如下：  \nvar myRouter = new VueRouter({  \n    //routes是路由规则数组  \n    routes: [  \n    //path设置为/表示页面最初始的地址 / ,redirect表示要被重定向的新地址，设置为一个路由即可  \n        { path:\"/\",redirect:\"/user\"},  \n        { path: \"/user\", component: User },  \n        { path: \"/login\", component: Login }  \n    ]  \n})\n\n### 5.嵌套路由，动态路由的实现方式\n\n#### A.嵌套路由的概念(★★★)\n\n当我们进行路由的时候显示的组件中还有新的子级路由链接以及内容。\n\n嵌套路由最关键的代码在于理解子级路由的概念：  \n比如我们有一个/login的路由  \n那么/login下面还可以添加子级路由，如:  \n/login/account  \n/login/phone\n\n参考代码如下：\n\n```javascript\nvar User = { template: \"\u003cdiv\u003eThis is User\u003c/div\u003e\" }\n//Login组件中的模板代码里面包含了子级路由链接以及子级路由的占位符\nvar Login = { template: `\u003cdiv\u003e\n    \u003ch1\u003eThis is Login\u003c/h1\u003e\n    \u003chr\u003e\n    \u003crouter-link to=\"/login/account\"\u003e账号密码登录\u003c/router-link\u003e\n    \u003crouter-link to=\"/login/phone\"\u003e扫码登录\u003c/router-link\u003e\n    \u003c!-- 子路由组件将会在router-view中显示 --\u003e\n    \u003crouter-view\u003e\u003c/router-view\u003e\n    \u003c/div\u003e` }\n//定义两个子级路由组件\nvar account = { template:\"\u003cdiv\u003e账号：\u003cinput\u003e\u003cbr\u003e密码：\u003cinput\u003e\u003c/div\u003e\"};\nvar phone = { template:\"\u003ch1\u003e扫我二维码\u003c/h1\u003e\"};\nvar myRouter = new VueRouter({\n    //routes是路由规则数组\n    routes: [\n        { path:\"/\",redirect:\"/user\"},\n        { path: \"/user\", component: User },\n        { \n            path: \"/login\", \n            component: Login,\n            //通过children属性为/login添加子路由规则\n            children:[\n                { path: \"/login/account\", component: account },\n                { path: \"/login/phone\", component: phone },\n            ]\n        }\n    ]\n})\nvar vm = new Vue({\n    el: '#app',\n    data: {},\n    methods: {},\n    router:myRouter\n});\n```\n\n页面效果大致如下：\n\n![](03嵌套路由.png)\n\n#### B.动态路由匹配(★★★)\n\n`var User = { template:\"\u003cdiv\u003e用户：{{$route.params.id}}\u003c/div\u003e\"}`\n\n```javascript\nvar myRouter = new VueRouter({\n    //routes是路由规则数组\n    routes: [\n        //通过/:参数名  的形式传递参数 \n        { path: \"/user/:id\", component: User },\n    ]\n\n})\n```\n\n补充：  \n如果使用$route.params.id来获取路径传参的数据不够灵活。  \n1.我们可以通过props来接收参数\n\n```javascript\nvar User = { \n    props:[\"id\"],\n    template:”\\div\u003e用户：{{id}}\\\u003c/div\u003e\"\n}\n\nvar myRouter = new VueRouter({\n    //routes是路由规则数组\n    routes: [\n        //通过/:参数名  的形式传递参数 \n        //如果props设置为true，route.params将会被设置为组件属性\n        { path: \"/user/:id\", component: User,props:true },\n    ]\n\n})\n```\n\n2.还有一种情况，我们可以将props设置为对象，那么就直接将对象的数据传递给  \n组件进行使用\n\n```javascript\nvar User = { \n    props:[\"username\",\"pwd\"],\n    template:\"\u003cdiv\u003e用户：{{username}}---{{pwd}}\u003c/div\u003e\"\n    }\n\nvar myRouter = new VueRouter({\n    //routes是路由规则数组\n    routes: [\n        //通过/:参数名  的形式传递参数 \n        //如果props设置为对象，则传递的是对象中的数据给组件\n        { path: \"/user/:id\", component: User,props:{username:\"jack\",pwd:123} },\n    ]\n\n})\n```\n\n3.如果想要获取传递的参数值还想要获取传递的对象数据，那么props应该设置为  \n函数形式。\n\n```javascript\nvar User = { \n    props:[\"username\",\"pwd\",\"id\"],\n    template:\"\u003cdiv\u003e用户：{{id}} -\u003e {{username}}---{{pwd}}\u003c/div\u003e\"\n    }\n\nvar myRouter = new VueRouter({\n    //routes是路由规则数组\n    routes: [\n        //通过/:参数名  的形式传递参数 \n        //如果props设置为函数，则通过函数的第一个参数获取路由对象\n        //并可以通过路由对象的params属性获取传递的参数\n        //\n        { path: \"/user/:id\", component: User,props:(route)=\u003e{\n            return {username:\"jack\",pwd:123,id:route.params.id}\n            } \n        },\n    ]\n\n})\n```\n\n### 7.命名路由以及编程式导航\n\n#### A.命名路由：给路由取别名\n\n案例：\n\n```javascript\nvar myRouter = new VueRouter({\n    //routes是路由规则数组\n    routes: [\n        //通过name属性为路由添加一个别名\n        { path: \"/user/:id\", component: User, name:\"user\"},\n    ]\n\n})\n```\n\n//添加了别名之后，可以使用别名进行跳转  \n\\\u003crouter-link to=\"/user”\u003eUser\\\u003c/router-link\u003e  \n\\\u003crouter-link :to=\"{ name:'user' , params: {id:123} }”\u003eUser\\\u003c/router-link\u003e\n\n//还可以编程式导航  \nmyRouter.push( { name:'user' , params: {id:123} } )\n\n#### B.编程式导航(★★★)\n\n页面导航的两种方式：  \nA.声明式导航：通过点击链接的方式实现的导航  \nB.编程式导航：调用js的api方法实现导航\n\nVue-Router中常见的导航方式：\n\n```javascript\nthis.$router.push(\"hash地址\");\nthis.$router.push(\"/login\");\nthis.$router.push({ name:'user' , params: {id:123} });\nthis.$router.push({ path:\"/login\" });\nthis.$router.push({ path:\"/login\",query:{username:\"jack\"} });\n\nthis.$router.go( n );//n为数字，参考history.go\nthis.$router.go( -1 );\n```\n\n### 8.实现后台管理案例(★★★)\n\n案例效果：\n\n![](02后台管理系统-4006815.png)\n\n点击左侧的\"用户管理\",\"权限管理\",\"商品管理\",\"订单管理\",\"系统设置\"都会出现对应的组件并展示内容\n\n其中\"用户管理\"组件展示的效果如上图所示，在用户管理区域中的详情链接也是可以点击的，点击之后将会显示用户详情信息。\n\n案例思路：  \n1).先将素材文件夹中的11.基于vue-router的案例.html复制到我们自己的文件夹中。  \n看一下这个文件中的代码编写了一些什么内容，  \n这个页面已经把后台管理页面的基本布局实现了  \n2).在页面中引入vue，vue-router  \n3).创建Vue实例对象，准备开始编写代码实现功能  \n4).希望是通过组件的形式展示页面的主体内容，而不是写死页面结构，所以我们可以定义一个根组件：\n\n```javascript\n//只需要把原本页面中的html代码设置为组件中的模板内容即可\nconst app = {\n    template:`\u003cdiv\u003e\n        \u003c!-- 头部区域 --\u003e\n        \u003cheader class=\"header\"\u003e传智后台管理系统\u003c/header\u003e\n        \u003c!-- 中间主体区域 --\u003e\n        \u003cdiv class=\"main\"\u003e\n          \u003c!-- 左侧菜单栏 --\u003e\n          \u003cdiv class=\"content left\"\u003e\n            \u003cul\u003e\n              \u003cli\u003e用户管理\u003c/li\u003e\n              \u003cli\u003e权限管理\u003c/li\u003e\n              \u003cli\u003e商品管理\u003c/li\u003e\n              \u003cli\u003e订单管理\u003c/li\u003e\n              \u003cli\u003e系统设置\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/div\u003e\n          \u003c!-- 右侧内容区域 --\u003e\n          \u003cdiv class=\"content right\"\u003e\n            \u003cdiv class=\"main-content\"\u003e添加用户表单\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n        \u003c!-- 尾部区域 --\u003e\n        \u003cfooter class=\"footer\"\u003e版权信息\u003c/footer\u003e\n      \u003c/div\u003e`\n  }\n```\n\n5).当我们访问页面的时候，默认需要展示刚刚创建的app根组件，我们可以  \n创建一个路由对象来完成这个事情,然后将路由挂载到Vue实例对象中即可\n\n```javascript\nconst myRouter = new VueRouter({\n    routes:[\n        {path:\"/\",component:app}\n    ]\n})\n\nconst vm = new Vue({\n    el:\"#app\",\n    data:{},\n    methods:{},\n    router:myRouter\n})\n```\n\n补充：到此为止，基本的js代码都处理完毕了，我们还需要设置一个路由占位符\n\n```html\n\u003cbody\u003e\n  \u003cdiv id=\"app\"\u003e\n    \u003crouter-view\u003e\u003c/router-view\u003e\n  \u003c/div\u003e\n\u003c/body\u003e\n```\n\n6).此时我们打开页面应该就可以得到一个VueRouter路由出来的根组件了  \n我们需要在这个根组件中继续路由实现其他的功能子组件  \n先让我们更改根组件中的模板：更改左侧li为子级路由链接，并在右侧内容区域添加子级组件占位符\n\n```javascript\nconst app = {\n    template:`\u003cdiv\u003e\n        ........\n        \u003cdiv class=\"main\"\u003e\n          \u003c!-- 左侧菜单栏 --\u003e\n          \u003cdiv class=\"content left\"\u003e\n            \u003cul\u003e\n              \u003c!-- 注意：我们把所有li都修改为了路由链接 --\u003e\n              \u003cli\u003e\u003crouter-link to=\"/users\"\u003e用户管理\u003c/router-link\u003e\u003c/li\u003e\n              \u003cli\u003e\u003crouter-link to=\"/accesses\"\u003e权限管理\u003c/router-link\u003e\u003c/li\u003e\n              \u003cli\u003e\u003crouter-link to=\"/goods\"\u003e商品管理\u003c/router-link\u003e\u003c/li\u003e\n              \u003cli\u003e\u003crouter-link to=\"/orders\"\u003e订单管理\u003c/router-link\u003e\u003c/li\u003e\n              \u003cli\u003e\u003crouter-link to=\"/systems\"\u003e系统设置\u003c/router-link\u003e\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/div\u003e\n          \u003c!-- 右侧内容区域 --\u003e\n          \u003cdiv class=\"content right\"\u003e\n            \u003cdiv class=\"main-content\"\u003e\n                \u003c!-- 在 --\u003e\n                \u003crouter-view\u003e\u003c/router-view\u003e \n            \u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n        .......\n      \u003c/div\u003e`\n  }\n```\n\n然后，我们要为子级路由创建并设置需要显示的子级组件\n\n```html\n//建议创建的组件首字母大写，和其他内容区分\nconst Users = {template:`\u003cdiv\u003e\n    \u003ch3\u003e用户管理\u003c/h3\u003e\n\u003c/div\u003e`}\nconst Access = {template:`\u003cdiv\u003e\n    \u003ch3\u003e权限管理\u003c/h3\u003e\n\u003c/div\u003e`}\nconst Goods = {template:`\u003cdiv\u003e\n    \u003ch3\u003e商品管理\u003c/h3\u003e\n\u003c/div\u003e`}\nconst Orders = {template:`\u003cdiv\u003e\n    \u003ch3\u003e订单管理\u003c/h3\u003e\n\u003c/div\u003e`}\nconst Systems = {template:`\u003cdiv\u003e\n    \u003ch3\u003e系统管理\u003c/h3\u003e\n\u003c/div\u003e`}\n\n//添加子组件的路由规则\nconst myRouter = new VueRouter({\n    routes:[\n        {path:\"/\",component:app , children:[\n            { path:\"/users\",component:Users },\n            { path:\"/accesses\",component:Access },\n            { path:\"/goods\",component:Goods },\n            { path:\"/orders\",component:Orders },\n            { path:\"/systems\",component:Systems },\n        ]}\n    ]\n})\n\nconst vm = new Vue({\n    el:\"#app\",\n    data:{},\n    methods:{},\n    router:myRouter\n})\n```\n\n7).展示用户信息列表：  \n    A.为Users组件添加私有数据,并在模板中循环展示私有数据  \n    ​```  \n    const Users = {  \n    data(){  \n        return {  \n            userList:[  \n                {id:1,name:\"zs\",age:18},  \n                {id:2,name:\"ls\",age:19},  \n                {id:3,name:\"wang\",age:20},  \n                {id:4,name:\"jack\",age:21},  \n            ]  \n        }  \n    },  \n    template:`\u003cdiv\u003e\n\n        \u003ch3\u003e用户管理\u003c/h3\u003e\n        \u003ctable\u003e\n            \u003cthead\u003e\n                \u003ctr\u003e\n                    \u003cth\u003e编号\u003c/th\u003e\n                    \u003cth\u003e姓名\u003c/th\u003e\n                    \u003cth\u003e年龄\u003c/th\u003e\n                    \u003cth\u003e操作\u003c/th\u003e\n                \u003c/tr\u003e\n            \u003c/thead\u003e\n            \u003ctbody\u003e\n                \u003ctr :key=\"item.id\" v-for=\"item in userList\"\u003e\n                    \u003ctd\u003e{{item.id}}\u003c/td\u003e\n                    \u003ctd\u003e{{item.name}}\u003c/td\u003e\n                    \u003ctd\u003e{{item.age}}\u003c/td\u003e\n                    \u003ctd\u003e\u003ca href=\"javascript:;\"\u003e详情\u003c/a\u003e\u003c/td\u003e\n                \u003c/tr\u003e\n            \u003c/tbody\u003e\n        \u003c/table\u003e\n\n​ \u003c/div\u003e`}  \n​ ​```\n\n8.当用户列表展示完毕之后，我们可以点击列表中的详情来显示用户详情信息，首先我们需要创建一个组件，用来展示详情信息\n\n```\nconst UserInfo = {\n    props:[\"id\"],\n    template:`\u003cdiv\u003e\n      \u003ch5\u003e用户详情\u003c/h5\u003e\n      \u003cp\u003e查看 {{id}} 号用户信息\u003c/p\u003e\n      \u003cbutton @click=\"goBack\"\u003e返回用户详情页\u003c/button\u003e\n    \u003c/div\u003e `,\n    methods:{\n      goBack(){\n        //当用户点击按钮，后退一页\n        this.$router.go(-1);\n      }\n    }\n  }\n```\n\n然后我们需要设置这个组件的路由规则\n\n```\nconst myRouter = new VueRouter({\n    routes:[\n        {path:\"/\",component:app , children:[\n            { path:\"/users\",component:Users },\n            //添加一个/userinfo的路由规则\n            { path:\"/userinfo/:id\",component:UserInfo,props:true},\n            { path:\"/accesses\",component:Access },\n            { path:\"/goods\",component:Goods },\n            { path:\"/orders\",component:Orders },\n            { path:\"/systems\",component:Systems },\n        ]}\n    ]\n})\n\nconst vm = new Vue({\n    el:\"#app\",\n    data:{},\n    methods:{},\n    router:myRouter\n})\n```\n\n再接着给用户列表中的详情a连接添加事件\n\n```javascript\nconst Users = {\n    data(){\n        return {\n            userList:[\n                {id:1,name:\"zs\",age:18},\n                {id:2,name:\"ls\",age:19},\n                {id:3,name:\"wang\",age:20},\n                {id:4,name:\"jack\",age:21},\n            ]\n        }\n    },\n    template:`\u003cdiv\u003e\n        \u003ch3\u003e用户管理\u003c/h3\u003e\n        \u003ctable\u003e\n            \u003cthead\u003e\n                \u003ctr\u003e\n                    \u003cth\u003e编号\u003c/th\u003e\n                    \u003cth\u003e姓名\u003c/th\u003e\n                    \u003cth\u003e年龄\u003c/th\u003e\n                    \u003cth\u003e操作\u003c/th\u003e\n                \u003c/tr\u003e\n            \u003c/thead\u003e\n            \u003ctbody\u003e\n                \u003ctr :key=\"item.id\" v-for=\"item in userList\"\u003e\n                    \u003ctd\u003e{{item.id}}\u003c/td\u003e\n                    \u003ctd\u003e{{item.name}}\u003c/td\u003e\n                    \u003ctd\u003e{{item.age}}\u003c/td\u003e\n                    \u003ctd\u003e\u003ca href=\"javascript:;\" @click=\"goDetail(item.id)\"\u003e详情\u003c/a\u003e\u003c/td\u003e\n                \u003c/tr\u003e\n            \u003c/tbody\u003e\n        \u003c/table\u003e\n    \u003c/div\u003e`,\n    methods:{\n        goDetail(id){\n            this.$router.push(\"/userinfo/\"+id);\n        }\n    }\n}\n```\n\n## Vue工程化\n\n### 1.模块化的分类\n\n#### A.浏览器端的模块化\n\n```txt\n1).AMD(Asynchronous Module Definition,异步模块定义)\n代表产品为：Require.js\n2).CMD(Common Module Definition,通用模块定义)\n代表产品为：Sea.js\n```\n\n#### B.服务器端的模块化\n\n    服务器端的模块化规范是使用CommonJS规范：\n    1).使用require引入其他模块或者包\n    2).使用exports或者module.exports导出模块成员\n    3).一个文件就是一个模块，都拥有独立的作用域\n\n#### C.ES6模块化\n\n    ES6模块化规范中定义：\n        1).每一个js文件都是独立的模块\n        2).导入模块成员使用import关键字\n        3).暴露模块成员使用export关键字\n\n小结：推荐使用ES6模块化，因为AMD，CMD局限使用与浏览器端，而CommonJS在服务器端使用。  \n      ES6模块化是浏览器端和服务器端通用的规范.\n\n### 2.在NodeJS中安装babel\n\n#### A.安装babel\n\n    打开终端，输入命令：npm install --save-dev @babel/core @babel/cli @babel/preset-env @babel/node\n    安装完毕之后，再次输入命令安装：npm install --save @babel/polyfill\n\n#### B.创建babel.config.js\n\n    在项目目录中创建babel.config.js文件。\n    编辑js文件中的代码如下：\n        const presets = [\n            [\"@babel/env\",{\n                targets:{\n                    edge:\"17\",\n                    firefox:\"60\",\n                    chrome:\"67\",\n                    safari:\"11.1\"\n                }\n            }]\n        ]\n        //暴露\n        module.exports = { presets }\n\n#### C.创建index.js文件\n\n    在项目目录中创建index.js文件作为入口文件\n    在index.js中输入需要执行的js代码，例如：\n        console.log(\"ok\");\n\n#### D.使用npx执行文件\n\n    打开终端，输入命令：npx babel-node ./index.js\n\n### 3.设置默认导入/导出\n\n#### A.默认导出\n\n    export default {\n        成员A,\n        成员B,\n        .......\n    },如下：\n    let num = 100;\n    export default{\n        num\n    }\n\n#### B.默认导入\n\n    import 接收名称 from \"模块标识符\"，如下：\n    import test from \"./test.js\"\n\n注意：在一个模块中，只允许使用export default向外默认暴露一次成员，千万不要写多个export default。  \n如果在一个模块中没有向外暴露成员，其他模块引入该模块时将会得到一个**空对象**.\n\n### 4.设置按需导入/导出\n\n#### A.按需导出\n\n```javascript\nexport let num = 998;\nexport let myName = \"jack\";\nexport function fn = function(){ console.log(\"fn\") }\n```\n\n####B.按需导入\n\n```javascript\nimport { num,fn as printFn ,myName } from \"./test.js\"\n//同时导入默认导出的成员以及按需导入的成员\nimport test,{ num,fn as printFn ,myName } from \"./test.js\"\n```\n\n注意：一个模块中既可以按需导入也可以默认导入，一个模块中既可以按需导出也可以默认导出\n\n### 5.直接导入并执行代码\n\n```javascript\nimport \"./test2.js\";\n```\n\n### 6.webpack的概念\n\nwebpack是一个流行的前端项目构建工具，可以解决目前web开发的困境。  \nwebpack提供了模块化支持，代码压缩混淆，解决js兼容问题，性能优化等特性，提高了开发效率和项目的可维护性\n\n### 7.webpack的基本使用\n\n#### A.创建项目目录并初始化\n\n    创建项目，并打开项目所在目录的终端，输入命令：\n    npm init -y\n\n#### B.创建首页及js文件\n\n    在项目目录中创建index.html页面，并初始化页面结构：在页面中摆放一个ul，ul里面放置几个li\n    在项目目录中创建js文件夹，并在文件夹中创建index.js文件\n\n#### C.安装jQuery\n\n    打开项目目录终端，输入命令:\n    npm install jQuery -S\n\n#### D.导入jQuery\n\n```javascript\n//打开index.js文件，编写代码导入jQuery并实现功能：\nimport $ from \"jquery\";\n$(function(){\n    $(\"li:odd\").css(\"background\",\"cyan\");\n    $(\"li:odd\").css(\"background\",\"pink\");\n})\n```\n\n注意：此时项目运行会有错误，因为import $ from \"jquery\";这句代码属于ES6的新语法代码，在浏览器中可能会存在兼容性问题  \n所以我们需要webpack来帮助我们解决这个问题。\n\n#### E.安装webpack\n\n```javascript\n1).打开项目目录终端，输入命令:\nnpm install webpack webpack-cli -D\n2).然后在项目根目录中，创建一个 webpack.config.js 的配置文件用来配置webpack\n在 webpack.config.js 文件中编写代码进行webpack配置，如下：\nmodule.exports = {\n    mode:\"development\"//可以设置为development(开发模式)，production(发布模式)\n}\n补充：mode设置的是项目的编译模式。\n如果设置为development则表示项目处于开发阶段，不会进行压缩和混淆，打包速度会快一些\n如果设置为production则表示项目处于上线发布阶段，会进行压缩和混淆，打包速度会慢一些\n3).修改项目中的package.json文件添加运行脚本dev，如下：\n\"scripts\":{\n    \"dev\":\"webpack\"\n}\n注意：scripts节点下的脚本，可以通过 npm run 运行，如：\n运行终端命令：npm run dev\n将会启动webpack进行项目打包\n4).运行dev命令进行项目打包，并在页面中引入项目打包生成的js文件\n打开项目目录终端，输入命令:\nnpm run dev\n等待webpack打包完毕之后，找到默认的dist路径中生成的main.js文件，将其引入到html页面中。\n浏览页面查看效果。\n```\n\n### 8.设置webpack的打包入口/出口\n\n```javascript\n在webpack 4.x中，默认会将src/index.js 作为默认的打包入口js文件\n                 默认会将dist/main.js 作为默认的打包输出js文件\n如果不想使用默认的入口/出口js文件，我们可以通过改变 webpack.config.js 来设置入口/出口的js文件，如下：\nconst path = require(\"path\");\nmodule.exports = {\n    mode:\"development\",\n    //设置入口文件路径\n    entry: path.join(__dirname,\"./src/xx.js\"),\n    //设置出口文件\n    output:{\n        //设置路径\n        path:path.join(__dirname,\"./dist\"),\n        //设置文件名\n        filename:\"res.js\"\n    }\n}\n```\n\n### 9.设置webpack的自动打包\n\n```javascript\n默认情况下，我们更改入口js文件的代码，需要重新运行命令打包webpack，才能生成出口的js文件\n那么每次都要重新执行命令打包，这是一个非常繁琐的事情，那么，自动打包可以解决这样繁琐的操作。\n实现自动打包功能的步骤如下：\n    A.安装自动打包功能的包:webpack-dev-server\n        npm install webpack-dev-server -D\n    B.修改package.json中的dev指令如下：\n        \"scripts\":{\n            \"dev\":\"webpack-dev-server\"\n        }\n    C.将引入的js文件路径更改为：\u003cscript src=\"/bundle.js\"\u003e\u003c/script\u003e\n    D.运行npm run dev，进行打包\n    E.打开网址查看效果：http://localhost:8080\n\n注意：webpack-dev-server自动打包的输出文件，默认放到了服务器的根目录中.\n```\n\n补充：  \n在自动打包完毕之后，默认打开服务器网页，实现方式就是打开package.json文件，修改dev命令：  \n    \"dev\": \"webpack-dev-server --open --host 127.0.0.1 --port 9999\"\n\n###10.配置html-webpack-plugin\n\n```javascript\n使用html-webpack-plugin 可以生成一个预览页面。\n因为当我们访问默认的 http://localhost:8080/的时候，看到的是一些文件和文件夹，想要查看我们的页面\n还需要点击文件夹点击文件才能查看，那么我们希望默认就能看到一个页面，而不是看到文件夹或者目录。\n实现默认预览页面功能的步骤如下：\n    A.安装默认预览功能的包:html-webpack-plugin\n        npm install html-webpack-plugin -D\n    B.修改webpack.config.js文件，如下：\n        //导入包\n        const HtmlWebpackPlugin = require(\"html-webpack-plugin\");\n        //创建对象\n        const htmlPlugin = new HtmlWebpackPlugin({\n            //设置生成预览页面的模板文件\n            template:\"./src/index.html\",\n            //设置生成的预览页面名称\n            filename:\"index.html\"\n        })\n    C.继续修改webpack.config.js文件，添加plugins信息：\n        module.exports = {\n            ......\n            plugins:[ htmlPlugin ]\n        }\n```\n\n###11.webpack中的加载器\n\n```javascript\n通过loader打包非js模块：默认情况下，webpack只能打包js文件，如果想要打包非js文件，需要调用loader加载器才能打包\n    loader加载器包含：\n        1).less-loader\n        2).sass-loader\n        3).url-loader:打包处理css中与url路径有关的文件\n        4).babel-loader:处理高级js语法的加载器\n        5).postcss-loader\n        6).css-loader,style-loader\n\n注意：指定多个loader时的顺序是固定的，而调用loader的顺序是从后向前进行调用\n\nA.安装style-loader,css-loader来处理样式文件\n    1).安装包\n        npm install style-loader css-loader -D\n    2).配置规则：更改webpack.config.js的module中的rules数组\n    module.exports = {\n        ......\n        plugins:[ htmlPlugin ],\n        module : {\n            rules:[\n                {\n                    //test设置需要匹配的文件类型，支持正则\n                    test:/\\.css$/,\n                    //use表示该文件类型需要调用的loader\n                    use:['style-loader','css-loader']\n                }\n            ]\n        }\n    }\nB.安装less,less-loader处理less文件\n    1).安装包\n        npm install less-loader less -D\n    2).配置规则：更改webpack.config.js的module中的rules数组\n    module.exports = {\n        ......\n        plugins:[ htmlPlugin ],\n        module : {\n            rules:[\n                {\n                    //test设置需要匹配的文件类型，支持正则\n                    test:/\\.css$/,\n                    //use表示该文件类型需要调用的loader\n                    use:['style-loader','css-loader']\n                },\n                {\n                    test:/\\.less$/,\n                    use:['style-loader','css-loader','less-loader']\n                }\n            ]\n        }\n    }\nC.安装sass-loader,node-sass处理less文件\n    1).安装包\n        npm install sass-loader node-sass -D\n    2).配置规则：更改webpack.config.js的module中的rules数组\n    module.exports = {\n        ......\n        plugins:[ htmlPlugin ],\n        module : {\n            rules:[\n                {\n                    //test设置需要匹配的文件类型，支持正则\n                    test:/\\.css$/,\n                    //use表示该文件类型需要调用的loader\n                    use:['style-loader','css-loader']\n                },\n                {\n                    test:/\\.less$/,\n                    use:['style-loader','css-loader','less-loader']\n                },\n                {\n                    test:/\\.scss$/,\n                    use:['style-loader','css-loader','sass-loader']\n                }\n            ]\n        }\n    }\n\n    补充：安装sass-loader失败时，大部分情况是因为网络原因，详情参考：\n    https://segmentfault.com/a/1190000010984731?utm_source=tag-newest\n\nD.安装post-css自动添加css的兼容性前缀（-ie-,-webkit-）\n1).安装包\n    npm install postcss-loader autoprefixer -D\n2).在项目根目录创建并配置postcss.config.js文件\nconst autoprefixer = require(\"autoprefixer\");\nmodule.exports = {\n    plugins:[ autoprefixer ]\n}\n3).配置规则：更改webpack.config.js的module中的rules数组\nmodule.exports = {\n    ......\n    plugins:[ htmlPlugin ],\n    module : {\n        rules:[\n            {\n                //test设置需要匹配的文件类型，支持正则\n                test:/\\.css$/,\n                //use表示该文件类型需要调用的loader\n                use:['style-loader','css-loader','postcss-loader']\n            },\n            {\n                test:/\\.less$/,\n                use:['style-loader','css-loader','less-loader']\n            },\n            {\n                test:/\\.scss$/,\n                use:['style-loader','css-loader','sass-loader']\n            }\n        ]\n    }\n}\n\nE.打包样式表中的图片以及字体文件\n在样式表css中有时候会设置背景图片和设置字体文件，一样需要loader进行处理\n使用url-loader和file-loader来处理打包图片文件以及字体文件\n1).安装包\n    npm install url-loader file-loader -D\n2).配置规则：更改webpack.config.js的module中的rules数组\nmodule.exports = {\n    ......\n    plugins:[ htmlPlugin ],\n    module : {\n        rules:[\n            {\n                //test设置需要匹配的文件类型，支持正则\n                test:/\\.css$/,\n                //use表示该文件类型需要调用的loader\n                use:['style-loader','css-loader']\n            },\n            {\n                test:/\\.less$/,\n                use:['style-loader','css-loader','less-loader']\n            },\n            {\n                test:/\\.scss$/,\n                use:['style-loader','css-loader','sass-loader']\n            },{\n                test:/\\.jpg|png|gif|bmp|ttf|eot|svg|woff|woff2$/,\n                //limit用来设置字节数，只有小于limit值的图片，才会转换\n                //为base64图片\n                use:\"url-loader?limit=16940\"\n            }\n        ]\n    }\n}\n\nF.打包js文件中的高级语法：在编写js的时候，有时候我们会使用高版本的js语法\n有可能这些高版本的语法不被兼容，我们需要将之打包为兼容性的js代码\n我们需要安装babel系列的包\nA.安装babel转换器\n    npm install babel-loader @babel/core @babel/runtime -D\nB.安装babel语法插件包\n    npm install @babel/preset-env @babel/plugin-transform-runtime @babel/plugin-proposal-class-properties -D\nC.在项目根目录创建并配置babel.config.js文件\n    \n    module.exports = {\n        presets:[\"@babel/preset-env\"],\n        plugins:[ \"@babel/plugin-transform-runtime\", \"@babel/plugin-proposal-class-properties\" ]\n    }\nD.配置规则：更改webpack.config.js的module中的rules数组\nmodule.exports = {\n    ......\n    plugins:[ htmlPlugin ],\n    module : {\n        rules:[\n            {\n                //test设置需要匹配的文件类型，支持正则\n                test:/\\.css$/,\n                //use表示该文件类型需要调用的loader\n                use:['style-loader','css-loader']\n            },\n            {\n                test:/\\.less$/,\n                use:['style-loader','css-loader','less-loader']\n            },\n            {\n                test:/\\.scss$/,\n                use:['style-loader','css-loader','sass-loader']\n            },{\n                test:/\\.jpg|png|gif|bmp|ttf|eot|svg|woff|woff2$/,\n                //limit用来设置字节数，只有小于limit值的图片，才会转换\n                //为base64图片\n                use:\"url-loader?limit=16940\"\n            },{\n                test:/\\.js$/,\n                use:\"babel-loader\",\n                //exclude为排除项，意思是不要处理node_modules中的js文件\n                exclude:/node_modules/\n            }\n        ]\n    }\n}\n```\n\n### 12.Vue单文件组件\n\n传统Vue组件的缺陷：  \n全局定义的组件不能重名，字符串模板缺乏语法高亮，不支持css(当html和js组件化时，css没有参与其中)  \n没有构建步骤限制，只能使用H5和ES5，不能使用预处理器（babel）  \n解决方案：  \n使用Vue单文件组件，每个单文件组件的后缀名都是.vue  \n每一个Vue单文件组件都由三部分组成  \n1).template组件组成的模板区域  \n2).script组成的业务逻辑区域  \n3).style样式区域\n\n代码如下：\n\n```vue\n\u003ctemplate\u003e\n    组件代码区域\n\u003c/template\u003e\n\n\u003cscript\u003e\n    js代码区域\n\u003c/script\u003e\n\n\u003cstyle scoped\u003e\n    样式代码区域\n\u003c/style\u003e\n```\n\n补充：安装Vetur插件可以使得.vue文件中的代码高亮\n\n配置.vue文件的加载器  \nA.安装vue组件的加载器  \n    npm install vue-loader vue-template-compiler -D  \nB.配置规则：更改webpack.config.js的module中的rules数组\n\n```javascript\nconst VueLoaderPlugin = require(\"vue-loader/lib/plugin\");\nconst vuePlugin = new VueLoaderPlugin();\nmodule.exports = {\n    ......\n    plugins:[ htmlPlugin, vuePlugin  ],\n    module : {\n        rules:[\n            ...//其他规则\n            { \n                test:/\\.vue$/,\n                loader:\"vue-loader\",\n       \t\t\t}\n    \t\t]\n    }\n}\n```\n\n### 13.在webpack中使用vue\n\n上一节我们安装处理了vue单文件组件的加载器，想要让vue单文件组件能够使用，我们必须要安装vue  \n并使用vue来引用vue单文件组件。  \nA.安装Vue  \n    `npm install vue -S`  \nB.在index.js中引入vue：`import Vue from \"vue\"`  \nC.创建Vue实例对象并指定el，最后使用render函数渲染单文件组件\n\n```javascript\nconst vm = new Vue({\n\tel:\"#first\",\n\trender:h=\u003eh(app)\n})\n```\n\n### 14.使用webpack打包发布项目\n\n在项目上线之前，我们需要将整个项目打包并发布。  \nA.配置package.json\n\n```json\n\"scripts\":{\n    \"dev\":\"webpack-dev-server\",\n    \"build\":\"webpack -p\"\n}\n```\n\nB.在项目打包之前，可以将dist目录删除，生成全新的dist目录\n\n### 15.Vue脚手架\n\nVue脚手架可以快速生成Vue项目基础的架构。  \nA.安装3.x版本的Vue脚手架：  \n    `npm install -g @vue/cli`  \nB.基于3.x版本的脚手架创建Vue项目：  \n    1).使用命令创建Vue项目  \n        命令：`vue create my-project`  \n        选择Manually select features(选择特性以创建项目)  \n        勾选特性可以用空格进行勾选。  \n        是否选用历史模式的路由：n  \n        ESLint选择：ESLint + Standard config  \n        何时进行ESLint语法校验：Lint on save  \n        babel，postcss等配置文件如何放置：In dedicated config files(单独使用文件进行配置)  \n        是否保存为模板：n  \n        使用哪个工具安装包：npm  \n    2).基于ui界面创建Vue项目  \n        命令：`vue ui`  \n        在自动打开的创建项目网页中配置项目信息。  \n    3).基于2.x的旧模板，创建Vue项目  \n        `npm install -g @vue/cli-init`  \n        `vue init webpack my-project`\n\nC.分析Vue脚手架生成的项目结构  \n    node_modules:依赖包目录  \n    public：静态资源目录  \n    src：源码目录  \n    src/assets:资源目录  \n    src/components：组件目录  \n    src/views:视图组件目录  \n    src/App.vue:根组件  \n    src/main.js:入口js  \n    src/router.js:路由js  \n    babel.config.js:babel配置文件  \n    .eslintrc.js:\n\n### 16.Vue脚手架的自定义配置\n\n```json\nA.通过 package.json 进行配置 [不推荐使用]\n    \"vue\":{\n        \"devServer\":{\n            \"port\":\"9990\",\n            \"open\":true\n        }\n    }\nB.通过单独的配置文件进行配置，创建vue.config.js\n    module.exports = {\n        devServer:{\n            port:8888,\n            open:true\n        }\n    }\n```\n","lastmodified":"2023-05-09T16:33:58.287366302Z","tags":[]},"/Vue3%E5%9F%BA%E7%A1%80":{"title":"Vue3快速上手","content":"\n# Vue3快速上手\n\n## 1.Vue3简介\n\n- 2020年9月18日，Vue.js发布3.0版本，代号：One Piece（海贼王）\n- 耗时2年多、[2600+次提交](https://github.com/vuejs/vue-next/graphs/commit-activity)、[30+个RFC](https://github.com/vuejs/rfcs/tree/master/active-rfcs)、[600+次PR](https://github.com/vuejs/vue-next/pulls?q=is%3Apr+is%3Amerged+-author%3Aapp%2Fdependabot-preview+)、[99位贡献者](https://github.com/vuejs/vue-next/graphs/contributors)\n- github上的tags地址：https://github.com/vuejs/vue-next/releases/tag/v3.0.0\n\n## 2.Vue3带来了什么\n\n### 1.性能的提升\n\n- 打包大小减少41%\n- 初次渲染快55%, 更新渲染快133%\n- 内存减少54%  \n  ……\n\n### 2.源码的升级\n\n- 使用Proxy代替defineProperty实现响应式\n- 重写虚拟DOM的实现和Tree-Shaking\n\n  ……\n\n### 3.拥抱TypeScript\n\n- Vue3可以更好的支持TypeScript\n\n### 4.新的特性\n\n1. Composition API（组合API）\n\n   - setup配置\n   - ref与reactive\n   - watch与watchEffect\n   - provide与inject\n   - ……\n2. 新的内置组件\n   - Fragment\n   - Teleport\n   - Suspense\n3. 其他改变\n\n   - 新的生命周期钩子\n   - data 选项应始终被声明为一个函数\n   - 移除keyCode支持作为 v-on 的修饰符\n   - ……\n\n# 一、创建Vue3.0工程\n\n## 1.使用 vue-cli 创建\n\n官方文档：https://cli.vuejs.org/zh/guide/creating-a-project.html#vue-create\n\n```bash\n## 查看@vue/cli版本，确保@vue/cli版本在4.5.0以上\nvue --version\n## 安装或者升级你的@vue/cli\nnpm install -g @vue/cli\n## 创建\nvue create vue_test\n## 启动\ncd vue_test\nnpm run serve\n```\n\n## 2.使用 vite 创建\n\n官方文档：https://v3.cn.vuejs.org/guide/installation.html#vite\n\nvite官网：https://vitejs.cn\n\n- 什么是vite？—— 新一代前端构建工具。\n- 优势如下：\n  - 开发环境中，无需打包操作，可快速的冷启动。\n  - 轻量快速的热重载（HMR）。\n  - 真正的按需编译，不再等待整个应用编译完成。\n- 传统构建 与 vite构建对比图\n\n\u003cimg src=\"https://cn.vitejs.dev/assets/bundler.37740380.png\" style=\"width:500px;height:280px;float:left\" /\u003e\u003cimg src=\"https://cn.vitejs.dev/assets/esm.3070012d.png\" style=\"width:480px;height:280px\" /\u003e\n\n```bash\n## 创建工程\nnpm init vite-app \u003cproject-name\u003e\n## 进入工程目录\ncd \u003cproject-name\u003e\n## 安装依赖\nnpm install\n## 运行\nnpm run dev\n```\n\n# 二、常用 Composition API\n\n官方文档: https://v3.cn.vuejs.org/guide/composition-api-introduction.html\n\n## 1.拉开序幕的setup\n\n1. 理解：Vue3.0中一个新的配置项，值为一个函数。\n2. setup是所有\u003cstrong style=\"color:#DD5145\"\u003eComposition API（组合API）\u003c/strong\u003e\u003ci style=\"color:gray;font-weight:bold\"\u003e“ 表演的舞台 ”\u003c/i\u003e。\n3. 组件中所用到的：数据、方法等等，均要配置在setup中。\n4. setup函数的两种返回值：\n   1. 若返回一个对象，则对象中的属性、方法, 在模板中均可以直接使用。（重点关注！）\n   2. \u003cspan style=\"color:#aad\"\u003e若返回一个渲染函数：则可以自定义渲染内容。（了解）\u003c/span\u003e\n5. 注意点：\n   1. 尽量不要与Vue2.x配置混用\n      - Vue2.x配置（data、methos、computed…）中\u003cstrong style=\"color:#DD5145\"\u003e可以访问到\u003c/strong\u003esetup中的属性、方法。\n      - 但在setup中\u003cstrong style=\"color:#DD5145\"\u003e不能访问到\u003c/strong\u003eVue2.x配置（data、methos、computed…）。\n      - 如果有重名, setup优先。\n   2. setup不能是一个async函数，因为返回值不再是return的对象, 而是promise, 模板看不到return对象中的属性。（后期也可以返回一个Promise实例，但需要Suspense和异步组件的配合）\n\n## 2.ref函数\n\n- 作用: 定义一个响应式的数据\n- 语法: ```const xxx = ref(initValue)```\n  - 创建一个包含响应式数据的\u003cstrong style=\"color:#DD5145\"\u003e引用对象（reference对象，简称ref对象）\u003c/strong\u003e。\n  - JS中操作数据： ```xxx.value```\n  - 模板中读取数据: 不需要.value，直接：```\u003cdiv\u003e{{xxx}}\u003c/div\u003e```\n- 备注：\n  - 接收的数据可以是：基本类型、也可以是对象类型。\n  - 基本类型的数据：响应式依然是靠``Object.defineProperty()``的```get```与```set```完成的。\n  - 对象类型的数据：内部 \u003ci style=\"color:gray;font-weight:bold\"\u003e“ 求助 ”\u003c/i\u003e 了Vue3.0中的一个新函数—— ```reactive```函数。\n\n## 3.reactive函数\n\n- 作用: 定义一个\u003cstrong style=\"color:#DD5145\"\u003e对象类型\u003c/strong\u003e的响应式数据（**基本类型不要用它**，要用`ref`函数）\n- 语法：```const 代理对象= reactive(源对象)```接收一个对象（或数组），返回一个\u003cstrong style=\"color:#DD5145\"\u003e代理对象（Proxy的实例对象，简称proxy对象）\u003c/strong\u003e\n- reactive定义的响应式数据是“深层次的”。\n- 内部基于 ES6 的 Proxy 实现，通过代理对象操作源对象内部数据进行操作。\n\n## 4.Vue3.0中的响应式原理\n\n### vue2.x的响应式\n\n- 实现原理：\n  - 对象类型：通过```Object.defineProperty()```对属性的读取、修改进行拦截（数据劫持）。\n  - 数组类型：通过重写更新数组的一系列方法来实现拦截。（对数组的变更方法进行了包裹）。\n\n    ```js\n    Object.defineProperty(data, 'count', {\n        get () {}, \n        set () {}\n    })\n    ```\n\n- 存在问题：\n  - 新增属性、删除属性, 界面不会更新。\n  - 直接通过下标修改数组, 界面不会自动更新。\n\n### Vue3.0的响应式\n\n- 实现原理:\n  - 通过Proxy（代理）: 拦截对象中任意属性的变化, 包括：属性值的读写、属性的添加、属性的删除等。\n  - 通过Reflect（反射）: 对源对象的属性进行操作。\n  - MDN文档中描述的Proxy与Reflect：\n    - Proxy：https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Proxy\n    - Reflect：https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Reflect\n\n      ```js\n      new Proxy(data, {\n      \t// 拦截读取属性值\n          get (target, prop) {\n          \treturn Reflect.get(target, prop)\n          },\n          // 拦截设置属性值或添加新属性\n          set (target, prop, value) {\n          \treturn Reflect.set(target, prop, value)\n          },\n          // 拦截删除属性\n          deleteProperty (target, prop) {\n          \treturn Reflect.deleteProperty(target, prop)\n          }\n      })\n      \n      proxy.name = 'tom'   \n      ```\n\n## 5.reactive对比ref\n\n- 从定义数据角度对比：\n   - ref用来定义：\u003cstrong style=\"color:#DD5145\"\u003e基本类型数据\u003c/strong\u003e。\n   - reactive用来定义：\u003cstrong style=\"color:#DD5145\"\u003e对象（或数组）类型数据\u003c/strong\u003e。\n   - 备注：ref也可以用来定义\u003cstrong style=\"color:#DD5145\"\u003e对象（或数组）类型数据\u003c/strong\u003e, 它内部会自动通过```reactive```转为\u003cstrong style=\"color:#DD5145\"\u003e代理对象\u003c/strong\u003e。\n- 从原理角度对比：\n   - ref通过``Object.defineProperty()``的```get```与```set```来实现响应式（数据劫持）。\n   - reactive通过使用\u003cstrong style=\"color:#DD5145\"\u003eProxy\u003c/strong\u003e来实现响应式（数据劫持）, 并通过\u003cstrong style=\"color:#DD5145\"\u003eReflect\u003c/strong\u003e操作\u003cstrong style=\"color:orange\"\u003e源对象\u003c/strong\u003e内部的数据。\n- 从使用角度对比：\n   - ref定义的数据：操作数据\u003cstrong style=\"color:#DD5145\"\u003e需要\u003c/strong\u003e```.value```，读取数据时模板中直接读取\u003cstrong style=\"color:#DD5145\"\u003e不需要\u003c/strong\u003e```.value```。\n   - reactive定义的数据：操作数据与读取数据：\u003cstrong style=\"color:#DD5145\"\u003e均不需要\u003c/strong\u003e```.value```。\n\n## 6.setup的两个注意点\n\n- setup执行的时机\n  - 在beforeCreate之前执行一次，this是undefined。\n- setup的参数\n  - props：值为对象，包含：组件外部传递过来，且组件内部声明接收了的属性。\n  - context：上下文对象\n    - attrs: 值为对象，包含：组件外部传递过来，但没有在props配置中声明的属性, 相当于 ```this.$attrs```。\n    - slots: 收到的插槽内容, 相当于 ```this.$slots```。\n    - emit: 分发自定义事件的函数, 相当于 ```this.$emit```。\n\n## 7.计算属性与监视\n\n### 1.computed函数\n\n- 与Vue2.x中computed配置功能一致\n- 写法\n\n  ```js\n  import {computed} from 'vue'\n  \n  setup(){\n      ...\n  \t//计算属性——简写\n      let fullName = computed(()=\u003e{\n          return person.firstName + '-' + person.lastName\n      })\n      //计算属性——完整\n      let fullName = computed({\n          get(){\n              return person.firstName + '-' + person.lastName\n          },\n          set(value){\n              const nameArr = value.split('-')\n              person.firstName = nameArr[0]\n              person.lastName = nameArr[1]\n          }\n      })\n  }\n  ```\n\n### 2.watch函数\n\n- 与Vue2.x中watch配置功能一致\n- 两个小“坑”：\n  - 监视reactive定义的响应式数据时：oldValue无法正确获取、强制开启了深度监视（deep配置失效）。\n  - 监视reactive定义的响应式数据中某个属性时：deep配置有效。\n\n  ```js\n  //情况一：监视ref定义的响应式数据\n  watch(sum,(newValue,oldValue)=\u003e{\n  \tconsole.log('sum变化了',newValue,oldValue)\n  },{immediate:true})\n  \n  //情况二：监视多个ref定义的响应式数据\n  watch([sum,msg],(newValue,oldValue)=\u003e{\n  \tconsole.log('sum或msg变化了',newValue,oldValue)\n  }) \n  \n  /* 情况三：监视reactive定义的响应式数据\n  \t\t\t若watch监视的是reactive定义的响应式数据，则无法正确获得oldValue！！\n  \t\t\t若watch监视的是reactive定义的响应式数据，则强制开启了深度监视 \n  */\n  watch(person,(newValue,oldValue)=\u003e{\n  \tconsole.log('person变化了',newValue,oldValue)\n  },{immediate:true,deep:false}) //此处的deep配置不再奏效\n  \n  //情况四：监视reactive定义的响应式数据中的某个属性\n  watch(()=\u003eperson.job,(newValue,oldValue)=\u003e{\n  \tconsole.log('person的job变化了',newValue,oldValue)\n  },{immediate:true,deep:true}) \n  \n  //情况五：监视reactive定义的响应式数据中的某些属性\n  watch([()=\u003eperson.job,()=\u003eperson.name],(newValue,oldValue)=\u003e{\n  \tconsole.log('person的job变化了',newValue,oldValue)\n  },{immediate:true,deep:true})\n  \n  //特殊情况\n  watch(()=\u003eperson.job,(newValue,oldValue)=\u003e{\n      console.log('person的job变化了',newValue,oldValue)\n  },{deep:true}) //此处由于监视的是reactive素定义的对象中的某个属性，所以deep配置有效\n  ```\n\n### 3.watchEffect函数\n\n- watch的套路是：既要指明监视的属性，也要指明监视的回调。\n- watchEffect的套路是：不用指明监视哪个属性，监视的回调中用到哪个属性，那就监视哪个属性。\n- watchEffect有点像computed：\n  - 但computed注重的计算出来的值（回调函数的返回值），所以必须要写返回值。\n  - 而watchEffect更注重的是过程（回调函数的函数体），所以不用写返回值。\n\n  ```js\n  //watchEffect所指定的回调中用到的数据只要发生变化，则直接重新执行回调。\n  watchEffect(()=\u003e{\n      const x1 = sum.value\n      const x2 = person.age\n      console.log('watchEffect配置的回调执行了')\n  })\n  ```\n\n## 8.生命周期\n\n\u003cdiv style=\"border:1px solid black;width:380px;float:left;margin-right:20px;\"\u003e\u003cstrong\u003evue2.x的生命周期\u003c/strong\u003e\u003cimg src=\"https://cn.vuejs.org/images/lifecycle.png\" alt=\"lifecycle_2\" style=\"zoom:33%;width:1200px\" /\u003e\u003c/div\u003e\u003cdiv style=\"border:1px solid black;width:510px;height:985px;float:left\"\u003e\u003cstrong\u003evue3.0的生命周期\u003c/strong\u003e\u003cimg src=\"https://v3.cn.vuejs.org/images/lifecycle.svg\" alt=\"lifecycle_2\" style=\"zoom:33%;width:2500px\" /\u003e\u003c/div\u003e\n\n1\n\n- Vue3.0中可以继续使用Vue2.x中的生命周期钩子，但有有两个被更名：\n  - ```beforeDestroy```改名为 ```beforeUnmount```\n  - ```destroyed```改名为 ```unmounted```\n- Vue3.0也提供了 Composition API 形式的生命周期钩子，与Vue2.x中钩子对应关系如下：\n  - `beforeCreate`===\u003e`setup()`\n  - `created`=======\u003e`setup()`\n  - `beforeMount` ===\u003e`onBeforeMount`\n  - `mounted`=======\u003e`onMounted`\n  - `beforeUpdate`===\u003e`onBeforeUpdate`\n  - `updated` =======\u003e`onUpdated`\n  - `beforeUnmount` ==\u003e`onBeforeUnmount`\n  - `unmounted` =====\u003e`onUnmounted`\n\n## 9.自定义hook函数\n\n- 什么是hook？—— 本质是一个函数，把setup函数中使用的Composition API进行了封装。\n- 类似于vue2.x中的mixin。\n- 自定义hook的优势: 复用代码, 让setup中的逻辑更清楚易懂。\n\n## 10.toRef\n\n- 作用：创建一个 ref 对象，其value值指向另一个对象中的某个属性。\n- 语法：```const name = toRef(person,'name')```\n- 应用: 要将响应式对象中的某个属性单独提供给外部使用时。\n- 扩展：```toRefs``` 与```toRef```功能一致，但可以批量创建多个 ref 对象，语法：```toRefs(person)```\n\n# 三、其它 Composition API\n\n## 1.shallowReactive 与 shallowRef\n\n- shallowReactive：只处理对象最外层属性的响应式（浅响应式）。\n- shallowRef：只处理基本数据类型的响应式, 不进行对象的响应式处理。\n- 什么时候使用?\n  - 如果有一个对象数据，结构比较深, 但变化时只是外层属性变化 ===\u003e shallowReactive。\n  - 如果有一个对象数据，后续功能不会修改该对象中的属性，而是生新的对象来替换 ===\u003e shallowRef。\n\n## 2.readonly 与 shallowReadonly\n\n- readonly: 让一个响应式数据变为只读的（深只读）。\n- shallowReadonly：让一个响应式数据变为只读的（浅只读）。\n- 应用场景: 不希望数据被修改时。\n\n## 3.toRaw 与 markRaw\n\n- toRaw：\n  - 作用：将一个由```reactive```生成的\u003cstrong style=\"color:orange\"\u003e响应式对象\u003c/strong\u003e转为\u003cstrong style=\"color:orange\"\u003e普通对象\u003c/strong\u003e。\n  - 使用场景：用于读取响应式对象对应的普通对象，对这个普通对象的所有操作，不会引起页面更新。\n- markRaw：\n  - 作用：标记一个对象，使其永远不会再成为响应式对象。\n  - 应用场景:\n    1. 有些值不应被设置为响应式的，例如复杂的第三方类库等。\n    2. 当渲染具有不可变数据源的大列表时，跳过响应式转换可以提高性能。\n\n## 4.customRef\n\n- 作用：创建一个自定义的 ref，并对其依赖项跟踪和更新触发进行显式控制。\n- 实现防抖效果：\n\n  ```vue\n  \u003ctemplate\u003e\n  \t\u003cinput type=\"text\" v-model=\"keyword\"\u003e\n  \t\u003ch3\u003e{{keyword}}\u003c/h3\u003e\n  \u003c/template\u003e\n  \n  \u003cscript\u003e\n  \timport {ref,customRef} from 'vue'\n  \texport default {\n  \t\tname:'Demo',\n  \t\tsetup(){\n  \t\t\t// let keyword = ref('hello') //使用Vue准备好的内置ref\n  \t\t\t//自定义一个myRef\n  \t\t\tfunction myRef(value,delay){\n  \t\t\t\tlet timer\n  \t\t\t\t//通过customRef去实现自定义\n  \t\t\t\treturn customRef((track,trigger)=\u003e{\n  \t\t\t\t\treturn{\n  \t\t\t\t\t\tget(){\n  \t\t\t\t\t\t\ttrack() //告诉Vue这个value值是需要被“追踪”的\n  \t\t\t\t\t\t\treturn value\n  \t\t\t\t\t\t},\n  \t\t\t\t\t\tset(newValue){\n  \t\t\t\t\t\t\tclearTimeout(timer)\n  \t\t\t\t\t\t\ttimer = setTimeout(()=\u003e{\n  \t\t\t\t\t\t\t\tvalue = newValue\n  \t\t\t\t\t\t\t\ttrigger() //告诉Vue去更新界面\n  \t\t\t\t\t\t\t},delay)\n  \t\t\t\t\t\t}\n  \t\t\t\t\t}\n  \t\t\t\t})\n  \t\t\t}\n  \t\t\tlet keyword = myRef('hello',500) //使用程序员自定义的ref\n  \t\t\treturn {\n  \t\t\t\tkeyword\n  \t\t\t}\n  \t\t}\n  \t}\n  \u003c/script\u003e\n  ```\n\n## 5.provide 与 inject\n\n\u003cimg src=\"../../../pics/components_provide.png\" style=\"width:300px\" /\u003e\n\n- 作用：实现\u003cstrong style=\"color:#DD5145\"\u003e祖与后代组件间\u003c/strong\u003e通信\n- 套路：父组件有一个 `provide` 选项来提供数据，后代组件有一个 `inject` 选项来开始使用这些数据\n- 具体写法：\n\n  1. 祖组件中：\n\n     ```js\n     setup(){\n     \t......\n         let car = reactive({name:'奔驰',price:'40万'})\n         provide('car',car)\n         ......\n     }\n     ```\n\n  2. 后代组件中：\n\n     ```js\n     setup(props,context){\n     \t......\n         const car = inject('car')\n         return {car}\n     \t......\n     }\n     ```\n\n## 6.响应式数据的判断\n\n- isRef: 检查一个值是否为一个 ref 对象\n- isReactive: 检查一个对象是否是由 `reactive` 创建的响应式代理\n- isReadonly: 检查一个对象是否是由 `readonly` 创建的只读代理\n- isProxy: 检查一个对象是否是由 `reactive` 或者 `readonly` 方法创建的代理\n\n# 四、Composition API 的优势\n\n## 1.Options API 存在的问题\n\n使用传统OptionsAPI中，新增或者修改一个需求，就需要分别在data，methods，computed里修改 。\n\n\u003cdiv style=\"width:600px;height:370px;overflow:hidden;float:left\"\u003e\n    \u003cimg src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f84e4e2c02424d9a99862ade0a2e4114~tplv-k3u1fbpfcp-watermark.image\" style=\"width:600px;float:left\" /\u003e\n\u003c/div\u003e\n\u003cdiv style=\"width:300px;height:370px;overflow:hidden;float:left\"\u003e\n    \u003cimg src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e5ac7e20d1784887a826f6360768a368~tplv-k3u1fbpfcp-watermark.image\" style=\"zoom:50%;width:560px;left\" /\u003e\n\u003c/div\u003e\n\n## 2.Composition API 的优势\n\n我们可以更加优雅的组织我们的代码，函数。让相关功能的代码更加有序的组织在一起。\n\n\u003cdiv style=\"width:500px;height:340px;overflow:hidden;float:left\"\u003e\n    \u003cimg src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bc0be8211fc54b6c941c036791ba4efe~tplv-k3u1fbpfcp-watermark.image\"style=\"height:360px\"/\u003e\n\u003c/div\u003e\n\u003cdiv style=\"width:430px;height:340px;overflow:hidden;float:left\"\u003e\n    \u003cimg src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6cc55165c0e34069a75fe36f8712eb80~tplv-k3u1fbpfcp-watermark.image\"style=\"height:360px\"/\u003e\n\u003c/div\u003e\n\n# 五、新的组件\n\n## 1.Fragment\n\n- 在Vue2中: 组件必须有一个根标签\n- 在Vue3中: 组件可以没有根标签, 内部会将多个标签包含在一个Fragment虚拟元素中\n- 好处: 减少标签层级, 减小内存占用\n\n## 2.Teleport\n\n- 什么是Teleport？—— `Teleport` 是一种能够将我们的\u003cstrong style=\"color:#DD5145\"\u003e组件html结构\u003c/strong\u003e移动到指定位置的技术。\n\n  ```vue\n  \u003cteleport to=\"移动位置\"\u003e\n  \t\u003cdiv v-if=\"isShow\" class=\"mask\"\u003e\n  \t\t\u003cdiv class=\"dialog\"\u003e\n  \t\t\t\u003ch3\u003e我是一个弹窗\u003c/h3\u003e\n  \t\t\t\u003cbutton @click=\"isShow = false\"\u003e关闭弹窗\u003c/button\u003e\n  \t\t\u003c/div\u003e\n  \t\u003c/div\u003e\n  \u003c/teleport\u003e\n  ```\n\n## 3.Suspense\n\n- 等待异步组件时渲染一些额外内容，让应用有更好的用户体验\n- 使用步骤：\n  - 异步引入组件\n\n    ```js\n    import {defineAsyncComponent} from 'vue'\n    const Child = defineAsyncComponent(()=\u003eimport('./components/Child.vue'))\n    ```\n\n  - 使用```Suspense```包裹组件，并配置好```default``` 与 ```fallback```\n\n    ```vue\n    \u003ctemplate\u003e\n    \t\u003cdiv class=\"app\"\u003e\n    \t\t\u003ch3\u003e我是App组件\u003c/h3\u003e\n    \t\t\u003cSuspense\u003e\n    \t\t\t\u003ctemplate v-slot:default\u003e\n    \t\t\t\t\u003cChild/\u003e\n    \t\t\t\u003c/template\u003e\n    \t\t\t\u003ctemplate v-slot:fallback\u003e\n    \t\t\t\t\u003ch3\u003e加载中.....\u003c/h3\u003e\n    \t\t\t\u003c/template\u003e\n    \t\t\u003c/Suspense\u003e\n    \t\u003c/div\u003e\n    \u003c/template\u003e\n    ```\n\n# 六、其他\n\n## 1.全局API的转移\n\n- Vue 2.x 有许多全局 API 和配置。\n  - 例如：注册全局组件、注册全局指令等。\n\n    ```js\n    //注册全局组件\n    Vue.component('MyButton', {\n      data: () =\u003e ({\n        count: 0\n      }),\n      template: '\u003cbutton @click=\"count++\"\u003eClicked {{ count }} times.\u003c/button\u003e'\n    })\n    \n    //注册全局指令\n    Vue.directive('focus', {\n      inserted: el =\u003e el.focus()\n    }\n    ```\n\n- Vue3.0中对这些API做出了调整：\n  - 将全局的API，即：```Vue.xxx```调整到应用实例（```app```）上\n\n    | 2.x 全局 API（```Vue```） | 3.x 实例 API (`app`) |  \n    | ------------------------- | ------------------------------------------- |  \n    | Vue.config.xxxx | app.config.xxxx |  \n    | Vue.config.productionTip | \u003cstrong style=\"color:#DD5145\"\u003e移除\u003c/strong\u003e |  \n    | Vue.component | app.component |  \n    | Vue.directive | app.directive |  \n    | Vue.mixin | app.mixin |  \n    | Vue.use | app.use |  \n    | Vue.prototype | app.config.globalProperties |\n\n## 2.其他改变\n\n- data选项应始终被声明为一个函数。\n- 过度类名的更改：\n  - Vue2.x写法\n\n    ```css\n    .v-enter,\n    .v-leave-to {\n      opacity: 0;\n    }\n    .v-leave,\n    .v-enter-to {\n      opacity: 1;\n    }\n    ```\n\n  - Vue3.x写法\n\n    ```css\n    .v-enter-from,\n    .v-leave-to {\n      opacity: 0;\n    }\n    \n    .v-leave-from,\n    .v-enter-to {\n      opacity: 1;\n    }\n    ```\n\n- \u003cstrong style=\"color:#DD5145\"\u003e移除\u003c/strong\u003ekeyCode作为 v-on 的修饰符，同时也不再支持```config.keyCodes```\n- \u003cstrong style=\"color:#DD5145\"\u003e移除\u003c/strong\u003e```v-on.native```修饰符\n  - 父组件中绑定事件\n\n    ```vue\n    \u003cmy-component\n      v-on:close=\"handleComponentEvent\"\n      v-on:click=\"handleNativeClickEvent\"\n    /\u003e\n    ```\n\n  - 子组件中声明自定义事件\n\n    ```vue\n    \u003cscript\u003e\n      export default {\n        emits: ['close']\n      }\n    \u003c/script\u003e\n    ```\n\n- \u003cstrong style=\"color:#DD5145\"\u003e移除\u003c/strong\u003e过滤器（filter）\n\n  \u003e 过滤器虽然这看起来很方便，但它需要一个自定义语法，打破大括号内表达式是 “只是 JavaScript” 的假设，这不仅有学习成本，而且有实现成本！建议用方法调用或计算属性去替换过滤器。\n\n- ……\n","lastmodified":"2023-05-09T16:33:58.287366302Z","tags":[]},"/Vuex":{"title":"Vuex","content":"\n##1.Vuex概述\n\nVuex是实现组件全局状态（数据）管理的一种机制，可以方便的实现组件之间的数据共享\n\n使用Vuex管理数据的好处：  \nA.能够在vuex中集中管理共享的数据，便于开发和后期进行维护  \nB.能够高效的实现组件之间的数据共享，提高开发效率  \nC.存储在vuex中的数据是响应式的，当数据发生改变时，页面中的数据也会同步更新\n\n使用 Vue 我们就不可避免的会遇到组件间共享的数据或状态。应用的业务代码逐渐复杂，props、事件、事件总线等通信的方式的弊端就会愈发明显。这个时候我们就需要 Vuex 。Vuex 是一个专门为 Vue 设计的状态管理工具。\n\n状态管理是 Vue 组件解耦的重要手段。\n\n![image-20200719202021432](image-20200719202021432.png)\n\n## 2.Vuex的基本使用\n\n**Vuex 不限制你的代码结构，但需要遵守一些规则：**\n\n1. 应用层级的状态应该集中到单个 store 对象中\n2. 提交 mutation 是更改状态的唯一方法，并且这个过程是同步的\n3. 异步逻辑都应该封装到 action 里面\n\n创建带有vuex的vue项目，打开终端，输入命令：vue ui  \n当项目仪表盘打开之后，我们点击页面左上角的项目管理下拉列表，再点击Vue项目管理器  \n点击创建项目，如下图所示  \n第一步，设置项目名称和包管理器  \n![](Notes/技术/Frontend/VueStack/Vuex.assets/创建vuex项目01.png)  \n第二步，设置手动配置项目  \n![](Notes/技术/Frontend/VueStack/Vuex.assets/创建vuex项目02.png)  \n第三步，设置功能项  \n![](Notes/技术/Frontend/VueStack/Vuex.assets/创建vuex项目03.png)  \n![创建vuex项目04(Vue.assets/创建vuex项目04(1).png)](17-21 Vue.js项目实战开发/20-21vue电商/10.vuex/笔记/images/创建vuex项目04(1).png)  \n第四步，创建项目  \n![](Notes/技术/Frontend/VueStack/Vuex.assets/创建vuex项目05.png)\n\n##3.使用Vuex完成计数器案例\n\n打开刚刚创建的vuex项目，找到src目录中的App.vue组件，将代码重新编写如下：\n\n```vue\n\u003ctemplate\u003e\n  \u003cdiv\u003e\n    \u003cmy-addition\u003e\u003c/my-addition\u003e\n    \u003cp\u003e----------------------------------------\u003c/p\u003e\n    \u003cmy-subtraction\u003e\u003c/my-subtraction\u003e\n  \u003c/div\u003e\n\u003c/template\u003e\n\n\u003cscript\u003e\nimport Addition from './components/Addition.vue'\nimport Subtraction from './components/Subtraction.vue'\n\nexport default {\n  data() {\n    return {}\n  },\n  components: {\n    'my-subtraction': Subtraction,\n    'my-addition': Addition\n  }\n}\n\u003c/script\u003e\n\n\u003cstyle\u003e\n\u003c/style\u003e\n```\n\n在components文件夹中创建Addition.vue组件，代码如下：\n\n```vue\n\u003ctemplate\u003e\n    \u003cdiv\u003e\n        \u003ch3\u003e当前最新的count值为：\u003c/h3\u003e\n        \u003cbutton\u003e+1\u003c/button\u003e\n    \u003c/div\u003e\n\u003c/template\u003e\n\n\u003cscript\u003e\nexport default {\n  data() {\n    return {}\n  }\n}\n\u003c/script\u003e\n\n\u003cstyle\u003e\n\u003c/style\u003e\n```\n\n在components文件夹中创建Subtraction.vue组件，代码如下：\n\n```vue\n\u003ctemplate\u003e\n    \u003cdiv\u003e\n        \u003ch3\u003e当前最新的count值为：\u003c/h3\u003e\n        \u003cbutton\u003e-1\u003c/button\u003e\n    \u003c/div\u003e\n\u003c/template\u003e\n\n\u003cscript\u003e\nexport default {\n  data() {\n    return {}\n  }\n}\n\u003c/script\u003e\n\n\u003cstyle\u003e\n\u003c/style\u003e\n```\n\n最后在项目根目录(与src平级)中创建 .prettierrc 文件，编写代码如下：\n\n```json\n{\n    \"semi\":false,\n    \"singleQuote\":true\n}\n```\n\n##4.Vuex中的核心特性\n\n####A.State\n\nState提供唯一的公共数据源，所有共享的数据都要统一放到Store中的State中存储  \n例如，打开项目中的store.js文件，在State对象中可以添加我们要共享的数据，如：count:0\n\n在组件中访问State的方式：\n\n1. `this.\\$store.state.全局数据名称  如：this.$store.state.count`\n2. 先按需导入mapState函数：` import { mapState } from 'vuex'`\n\n   然后数据映射为计算属性： `computed:{ …mapState(['全局数据名称']) }`\n\n####B.Mutation\n\nMutation用于修改变更$store中的数据  \n使用方式：  \n打开store.js文件，在mutations中添加代码如下\n\n```javascript\nmutations: {\n    add(state,step){\n      //第一个形参永远都是state也就是$state对象\n      //第二个形参是调用add时传递的参数\n      state.count+=step;\n    }\n}\n```\n\n然后在Addition.vue中给按钮添加事件代码如下：\n\n```javascript\n\u003cbutton @click=\"Add\"\u003e+1\u003c/button\u003e\nmethods:{\n  Add(){\n    //使用commit函数调用mutations中的对应函数，\n    //第一个参数就是我们要调用的mutations中的函数名\n    //第二个参数就是传递给add函数的参数\n    this.$store.commit('add',10)\n  }\n}\n```\n\n使用mutations的第二种方式：\n\n```javascript\nimport { mapMutations } from 'vuex'\n\nmethods:{\n  ...mapMutations(['add'])\n}\n```\n\n如下：\n\n```javascript\nimport { mapState,mapMutations } from 'vuex'\n\nexport default {\n  data() {\n    return {}\n  },\n  methods:{\n      //获得mapMutations映射的sub函数\n      ...mapMutations(['sub']),\n      //当点击按钮时触发Sub函数\n      Sub(){\n          //调用sub函数完成对数据的操作\n          this.sub(10);\n      }\n  },\n  computed:{\n      ...mapState(['count'])\n  }\n}\n```\n\n####C.Action\n\n在mutations中不能编写异步的代码，会导致vue调试器的显示出错。  \n在vuex中我们可以使用Action来执行异步操作。  \n操作步骤如下：  \n打开store.js文件，修改Action，如下：\n\n```javascript\nactions: {\n  addAsync(context,step){\n    setTimeout(()=\u003e{\n      context.commit('add',step);\n    },2000)\n  }\n}\n```\n\n然后在Addition.vue中给按钮添加事件代码如下：\n\n```vue\n\u003cbutton @click=\"AddAsync\"\u003e...+1\u003c/button\u003e\n\nmethods:{\n  AddAsync(){\n    this.$store.dispatch('addAsync',5)\n  }\n}\n```\n\n第二种方式：\n\n```javascript\nimport { mapActions } from 'vuex'\n\nmethods:{\n  ...mapActions(['subAsync'])\n}\n```\n\n如下：\n\n```javascript\nimport { mapState,mapMutations,mapActions } from 'vuex'\n\nexport default {\n  data() {\n    return {}\n  },\n  methods:{\n      //获得mapMutations映射的sub函数\n      ...mapMutations(['sub']),\n      //当点击按钮时触发Sub函数\n      Sub(){\n          //调用sub函数完成对数据的操作\n          this.sub(10);\n      },\n      //获得mapActions映射的addAsync函数\n      ...mapActions(['subAsync']),\n      asyncSub(){\n          this.subAsync(5);\n      }\n  },\n  computed:{\n      ...mapState(['count'])\n      \n  }\n}\n```\n\n####D.Getter\n\nGetter用于对Store中的数据进行加工处理形成新的数据  \n它只会包装Store中保存的数据，并不会修改Store中保存的数据，当Store中的数据发生变化时，Getter生成的内容也会随之变化  \n打开store.js文件，添加getters，如下：\n\n```javascript\nexport default new Vuex.Store({\n  .......\n  getters:{\n    //添加了一个showNum的属性\n    showNum : state =\u003e{\n      return '最新的count值为：'+state.count;\n    }\n  }\n})\n```\n\n然后打开Addition.vue中\n\n1. 添加插值表达式使用getters,`{{$store.getters.showNum}}`\n2. 也可以在Addition.vue中，导入mapGetters，并将之映射为计算属性\n\n   ```javascript\n   import { mapGetters } from 'vuex'\n   computed:{\n     ...mapGetters(['showNum'])\n   }\n   ```\n\n##5.vuex案例\n\n####A.初始化案例\n\n首先使用vue ui初始化一个使用vuex的案例  \n然后打开public文件夹，创建一个list.json文件，文件代码如下：\n\n```json\n[\n    {\n        \"id\": 0,\n        \"info\": \"Racing car sprays burning fuel into crowd.\",\n        \"done\": false\n    },\n    {\n        \"id\": 1,\n        \"info\": \"Japanese princess to wed commoner.\",\n        \"done\": false\n    },\n    {\n        \"id\": 2,\n        \"info\": \"Australian walks 100km after outback crash.\",\n        \"done\": false\n    },\n    {\n        \"id\": 3,\n        \"info\": \"Man charged over missing wedding girl.\",\n        \"done\": false\n    },\n    {\n        \"id\": 4,\n        \"info\": \"Los Angeles battles huge wildfires.\",\n        \"done\": false\n    }\n]\n```\n\n再接着，打开main.js,添加store.js的引入，如下：\n\n```javascript\nimport Vue from 'vue'\nimport App from './App.vue'\nimport store from './store.js'\n\n// 1. 导入 ant-design-vue 组件库\nimport Antd from 'ant-design-vue'\n// 2. 导入组件库的样式表\nimport 'ant-design-vue/dist/antd.css'\n\nVue.config.productionTip = false\n// 3. 安装组件库\nVue.use(Antd)\n\nnew Vue({\n  store,\n  render: h =\u003e h(App)\n}).$mount('#app')\n```\n\n再接着打开store.js，添加axios请求json文件获取数据的代码，如下：\n\n```javascript\nimport Vue from 'vue'\nimport Vuex from 'vuex'\nimport axios from 'axios'\n\nVue.use(Vuex)\n\nexport default new Vuex.Store({\n  state: {\n    //所有任务列表\n    list: [],\n    //文本输入框中的值\n    inputValue: 'AAA'\n  },\n  mutations: {\n    initList(state, list) {\n      state.list = list\n    },\n    setInputValue(state,value){\n      state.inputValue = value\n    }\n  },\n  actions: {\n    getList(context) {\n      axios.get('/list.json').then(({ data }) =\u003e {\n        console.log(data);\n        context.commit('initList', data)\n      })\n    }\n  }\n})\n```\n\n最后，代开App.vue文件，将store中的数据获取并展示：\n\n```vue\n\u003ctemplate\u003e\n  \u003cdiv id=\"app\"\u003e\n    \u003ca-input placeholder=\"请输入任务\" class=\"my_ipt\" :value=\"inputValue\" @change=\"handleInputChange\" /\u003e\n    \u003ca-button type=\"primary\"\u003e添加事项\u003c/a-button\u003e\n\n    \u003ca-list bordered :dataSource=\"list\" class=\"dt_list\"\u003e\n      \u003ca-list-item slot=\"renderItem\" slot-scope=\"item\"\u003e\n        \u003c!-- 复选框 --\u003e\n        \u003ca-checkbox :checked=\"item.done\"\u003e{{item.info}}\u003c/a-checkbox\u003e\n        \u003c!-- 删除链接 --\u003e\n        \u003ca slot=\"actions\"\u003e删除\u003c/a\u003e\n      \u003c/a-list-item\u003e\n\n      \u003c!-- footer区域 --\u003e\n      \u003cdiv slot=\"footer\" class=\"footer\"\u003e\n        \u003c!-- 未完成的任务个数 --\u003e\n        \u003cspan\u003e0条剩余\u003c/span\u003e\n        \u003c!-- 操作按钮 --\u003e\n        \u003ca-button-group\u003e\n          \u003ca-button type=\"primary\"\u003e全部\u003c/a-button\u003e\n          \u003ca-button\u003e未完成\u003c/a-button\u003e\n          \u003ca-button\u003e已完成\u003c/a-button\u003e\n        \u003c/a-button-group\u003e\n        \u003c!-- 把已经完成的任务清空 --\u003e\n        \u003ca\u003e清除已完成\u003c/a\u003e\n      \u003c/div\u003e\n    \u003c/a-list\u003e\n  \u003c/div\u003e\n\u003c/template\u003e\n\n\u003cscript\u003e\nimport { mapState } from 'vuex'\n\nexport default {\n  name: 'app',\n  data() {\n    return {\n      // list:[]\n    }\n  },\n  created(){\n    // console.log(this.$store);\n    this.$store.dispatch('getList')\n  },\n  methods:{\n    handleInputChange(e){\n      // console.log(e.target.value)\n      this.$store.commit('setInputValue',e.target.value)\n    }\n  },\n  computed:{\n    ...mapState(['list','inputValue'])\n  }\n}\n\u003c/script\u003e\n\n\u003cstyle scoped\u003e\n#app {\n  padding: 10px;\n}\n\n.my_ipt {\n  width: 500px;\n  margin-right: 10px;\n}\n\n.dt_list {\n  width: 500px;\n  margin-top: 10px;\n}\n\n.footer {\n  display: flex;\n  justify-content: space-between;\n  align-items: center;\n}\n\u003c/style\u003e\n```\n\n####B.完成添加事项\n\n首先，打开App.vue文件，给“添加事项”按钮绑定点击事件，编写处理函数\n\n```javascript\n//绑定事件\n\u003ca-button type=\"primary\" @click=\"addItemToList\"\u003e添加事项\u003c/a-button\u003e\n\n//编写事件处理函数\nmethods:{\n    ......\n    addItemToList(){\n      //向列表中新增事项\n      if(this.inputValue.trim().length \u003c= 0){\n        return this.$message.warning('文本框内容不能为空')\n      }\n\n      this.$store.commit('addItem')\n    }\n  }\n```\n\n然后打开store.js编写addItem\n\n```javascript\nexport default new Vuex.Store({\n  state: {\n    //所有任务列表\n    list: [],\n    //文本输入框中的值\n    inputValue: 'AAA',\n    //下一个id\n    nextId:5\n  },\n  mutations: {\n    ........\n    //添加列表项\n    addItem(state){\n      const obj = {\n        id :state.nextId,\n        info: state.inputValue.trim(),\n        done:false\n      }\n      //将创建好的事项添加到数组list中\n      state.list.push(obj)\n      //将nextId值自增\n      state.nextId++\n      state.inputValue = ''\n    }\n  }\n  ......\n})\n```\n\n####C.完成删除事项\n\n首先，打开App.vue文件，给“删除”按钮绑定点击事件，编写处理函数\n\n```javascript\n//绑定事件\n\u003ca slot=\"actions\" @click=\"removeItemById(item.id)\"\u003e删除\u003c/a\u003e\n\n//编写事件处理函数\nmethods:{\n    ......\n    removeItemById(id){\n      //根据id删除事项\n      this.$store.commit('removeItem',id)\n    }\n  }\n```\n\n然后打开store.js编写addItem\n\n```javascript\nexport default new Vuex.Store({\n  ......\n  mutations: {\n    ........\n    removeItem(state,id){\n      //根据id删除事项数据\n      const index = state.list.findIndex( x =\u003e x.id === id )\n      // console.log(index);\n      if(index != -1) state.list.splice(index,1);\n    }\n  }\n  ......\n})\n```\n\n####D.完成选中状态的改变\n\n首先，打开App.vue文件，给“复选”按钮绑定点击事件，编写处理函数\n\n```javascript\n//绑定事件\n\u003ca-checkbox :checked=\"item.done\" @change=\"cbStateChanged(item.id,$event)\"\u003e{{item.info}}\u003c/a-checkbox\u003e\n\n//编写事件处理函数\nmethods:{\n    ......\n    cbStateChanged(id,e){\n      //复选框状态改变时触发\n      const param = {\n        id:id,\n        status:e.target.checked\n      }\n\n      //根据id更改事项状态\n      this.$store.commit('changeStatus',param)\n    }\n  }\n```\n\n然后打开store.js编写addItem\n\n```javascript\nexport default new Vuex.Store({\n  ......\n  mutations: {\n    ........\n    changeStatus(state,param){\n      //根据id改变对应事项的状态\n      const index = state.list.findIndex( x =\u003e x.id === param.id )\n      if(index != -1) state.list[index].done = param.status\n    }\n  }\n  ......\n})\n```\n\n####E.剩余项统计\n\n打开store.js，添加getters完成剩余项统计\n\n```javascript\ngetters:{\n  unDoneLength(state){\n    const temp = state.list.filter( x =\u003e x.done === false )\n    console.log(temp)\n    return temp.length\n  }\n}\n```\n\n打开App.vue，使用getters展示剩余项\n\n```javascript\n//使用映射好的计算属性展示剩余项\n\u003c!-- 未完成的任务个数 --\u003e\n\u003cspan\u003e{{unDoneLength}}条剩余\u003c/span\u003e\n\n//导入getters\nimport { mapState,mapGetters } from 'vuex'\n//映射\ncomputed:{\n  ...mapState(['list','inputValue']),\n  ...mapGetters(['unDoneLength'])\n}\n```\n\n####F.清除完成事项\n\n首先，打开App.vue文件，给“清除已完成”按钮绑定点击事件，编写处理函数\n\n```javascript\n\u003c!-- 把已经完成的任务清空 --\u003e\n\u003ca @click=\"clean\"\u003e清除已完成\u003c/a\u003e\n\n//编写事件处理函数\nmethods:{\n  ......\n  clean(){\n    //清除已经完成的事项\n    this.$store.commit('cleanDone')\n  }\n}\n```\n\n然后打开store.js编写addItem\n\n```javascript\nexport default new Vuex.Store({\n  ......\n  mutations: {\n    ........\n    cleanDone(state){\n      state.list = state.list.filter( x =\u003e x.done === false )\n    }\n  }\n  ......\n})\n```\n\n####G.点击选项卡切换事项\n\n打开App.vue，给“全部”，“未完成”，“已完成”三个选项卡绑定点击事件，编写处理函数  \n并将列表数据来源更改为一个getters。\n\n```javascript\n\u003ca-list bordered :dataSource=\"infoList\" class=\"dt_list\"\u003e\n  ......\n  \u003c!-- 操作按钮 --\u003e\n  \u003ca-button-group\u003e\n    \u003ca-button :type=\"viewKey ==='all'?'primary':'default'\" @click=\"changeList('all')\"\u003e全部\u003c/a-button\u003e\n    \u003ca-button :type=\"viewKey ==='undone'?'primary':'default'\" @click=\"changeList('undone')\"\u003e未完成\u003c/a-button\u003e\n    \u003ca-button :type=\"viewKey ==='done'?'primary':'default'\" @click=\"changeList('done')\"\u003e已完成\u003c/a-button\u003e\n  \u003c/a-button-group\u003e\n  ......\n\u003c/a-list\u003e\n\n//编写事件处理函数以及映射计算属性\nmethods:{\n  ......\n  changeList( key ){\n    //点击“全部”，“已完成”，“未完成”时触发\n    this.$store.commit('changeKey',key)\n  }\n},\ncomputed:{\n  ...mapState(['list','inputValue','viewKey']),\n  ...mapGetters(['unDoneLength','infoList'])\n}\n```\n\n打开store.js，添加getters，mutations，state\n\n```javascript\nexport default new Vuex.Store({\n  state: {\n    ......\n    //保存默认的选项卡值\n    viewKey:'all'\n  },\n  mutations: {\n    ......\n    changeKey(state,key){\n      //当用户点击“全部”，“已完成”，“未完成”选项卡时触发\n      state.viewKey = key\n    }\n  },\n  ......\n  getters:{\n    .......\n    infoList(state){\n      if(state.viewKey === 'all'){\n        return state.list\n      }\n      if(state.viewKey === 'undone'){\n        return state.list.filter( x =\u003e x.done === false )\n      }\n      if(state.viewKey === 'done'){\n        return state.list.filter( x =\u003e x.done === true )\n      }\n    }\n  }\n})\n```\n\n## Vuex 注入 Vue 生命周期的过程\n\n我们在安装插件的时候，总会像下面一样用 `Vue.use()` 来载入插件，可是 `Vue.use()` 做了什么呢？\n\n```\nimport Vue from 'vue';\nimport Vuex from 'vuex';\n\nVue.use(Vuex);\n```\n\n### Vue.use() 做了什么\n\n\u003e 安装 Vue.js 插件。如果插件是一个对象，必须提供 install 方法。如果插件是一个函数，它会被作为 install 方法。install 方法调用时，会将 Vue 作为参数传入。\n\n以上是 官方文档 的解释。\n\n接下来我们从源码部分来看看 `Vue.use()` 都做了什么。\n\nVue 源码在 `initGlobalAPI` 入口方法中调用了 `initUse (Vue)` 方法，这个方法定义了 `Vue.use()` 需要做的内容。\n\n```javascript\nfunction initGlobalAPI (Vue) {\n  ......\n  initUse(Vue);\n  initMixin$1(Vue); // 下面讲 Vue.mixin 会提到\n  ......\n}\n\nfunction initUse (Vue) {\n  Vue.use = function (plugin) {\n    var installedPlugins = (this._installedPlugins || (this._installedPlugins = []));\n    /* 判断过这个插件是否已经安装 */\n    if (installedPlugins.indexOf(plugin) \u003e -1) {\n      return this\n    }\n    var args = toArray(arguments, 1);\n    args.unshift(this);\n    /* 判断插件是否有 install 方法 */\n    if (typeof plugin.install === 'function') {\n      plugin.install.apply(plugin, args);\n    } else if (typeof plugin === 'function') {\n      plugin.apply(null, args);\n    }\n    installedPlugins.push(plugin);\n    return this\n  };\n}\n```\n\n这段代码主要做了两件事情：\n\n1. 一件是防止重复安装相同的 plugin\n2. 另一件是初始化 plugin\n\n### 插件的 install 方法\n\n看完以上源码，我们知道插件（Vuex）需要提供一个 `install` 方法。那么我们看看 Vuex 源码中是否有这个方法。结果当然是有的：\n\n```javascript\n/* 暴露给外部的 install 方法 */\nfunction install (_Vue) {\n  /* 避免重复安装（Vue.use 内部也会检测一次是否重复安装同一个插件）*/\n  if (Vue \u0026\u0026 _Vue === Vue) {\n    {\n      console.error(\n        '[vuex] already installed. Vue.use(Vuex) should be called only once.'\n      );\n    }\n    return\n  }\n  Vue = _Vue;\n  /* 将 vuexInit 混淆进 Vue 的 beforeCreate(Vue2.0) 或 _init 方法(Vue1.0) */\n  applyMixin(Vue);\n}\n```\n\n这段代码主要做了两件事情：\n\n1. 一件是防止 Vuex 被重复安装\n2. 另一件是执行 `applyMixin`，目的是执行 `vuexInit` 方法初始化 Vuex\n\n接下来 我们看看 `applyMixin(Vue)` 源码：\n\n```javascript\n/* 将 vuexInit 混淆进 Vue 的 beforeCreate */\nfunction applyMixin (Vue) {\n  var version = Number(Vue.version.split('.')[0]);\n  if (version \u003e= 2) {\n    Vue.mixin({ beforeCreate: vuexInit });\n  } else {\n    /* Vue1.0 的处理逻辑，此处省略 */\n    ......\n  }\n  function vuexInit () {\n    ......\n  }\n}\n```\n\n从上面的源码，可以看出 `Vue.mixin` 方法将 `vuexInit` 方法混淆进 `beforeCreate` 钩子中，也是因为这个操作，所以每一个 vm 实例都会调用 `vuexInit` 方法。那么 `vuexInit` 又做了什么呢？\n\n### vuexInit()\n\n我们在使用 Vuex 的时候，需要将 store 传入到 Vue 实例中去。\n\n```javascript\nnew Vue({\n  el: '#app',\n  store\n});\n```\n\n但是我们却在每一个 vm 中都可以访问该 store，这个就需要靠 `vuexInit` 了。\n\n```javascript\n  function vuexInit () {\n    const options = this.$options\n    if (options.store) {\n      /* 根节点存在 stroe 时 */\n      this.$store = typeof options.store === 'function'\n        ? options.store()\n        : options.store\n    } else if (options.parent \u0026\u0026 options.parent.$store) {\n      /* 子组件直接从父组件中获取 $store，这样就保证了所有组件都公用了全局的同一份 store*/\n      this.$store = options.parent.$store\n    }\n  }\n```\n\n\u003e 根节点存在 stroe 时，则直接将 `options.store` 赋值给 `this.$store`。否则则说明不是根节点，从父节点的 `$store` 中获取。\n\n通过这步的操作，我们就以在任意一个 vm 中通过 this.$store 来访问 Store 的实例。接下来我们反过来说说 Vue.mixin()。\n\n### Vue.mixin()\n\n\u003e 全局注册一个混入，影响注册之后所有创建的每个 Vue 实例。插件作者可以使用混入，向组件注入自定义的行为。**不推荐在应用代码中使用。**\n\n在 vue 的 `initGlobalAPI` 入口方法中调用了 `initMixin$1(Vue)` 方法:\n\n```javascript\nfunction initMixin$1 (Vue) {\n  Vue.mixin = function (mixin) {\n    this.options = mergeOptions(this.options, mixin);\n    return this\n  };\n}\n```\n\nVuex 注入 Vue 生命周期的过程大概就是这样，如果你感兴趣的话，你可以直接看看 Vuex 的源码，接下来我们说说 Store。\n\n## Store\n\n上面我们讲到了 `vuexInit` 会从 options 中获取 Store。所以接下来会讲到 Store 是怎么来的呢？\n\n我们使用 Vuex 的时候都会定义一个和下面类似的 Store 实例。\n\n```javascript\nimport Vue from 'vue'\nimport Vuex from 'vuex'\nimport mutations from './mutations'\n\nVue.use(Vuex)\n\nconst state = {\n    showState: 0,                             \n}\n\nexport default new Vuex.Store({\n    strict: true,\n state,\n getters,\n})\n```\n\n\u003e 不要在发布环境下启用严格模式。严格模式会深度监测状态树来检测不合规的状态变更 —— 请确保在发布环境下关闭严格模式，以避免性能损失。\n\n### state 的响应式\n\n你是否关心 state 是如何能够响应式呢？这个主要是通过 Store 的构造函数中调用的 `resetStoreVM(this, state)` 方法来实现的。\n\n这个方法主要是重置一个私有的 _vm（一个 Vue 的实例） 对象。这个 _vm 对象会保留我们的 state 树，以及用计算属性的方式存储了 store 的 getters。现在具体看看它的实现过程。\n\n```javascript\n/* 使用 Vue 内部的响应式注册 state */\nfunction resetStoreVM (store, state, hot) {\n  /* 存放之前的vm对象 */\n  const oldVm = store._vm \n\n  store.getters = {}\n  const wrappedGetters = store._wrappedGetters\n  const computed = {}\n\n  /* 通过 Object.defineProperty 方法为 store.getters 定义了 get 方法。当在组件中调用 this.$store.getters.xxx 这个方法的时候，会访问 store._vm[xxx]*/\n  forEachValue(wrappedGetters, (fn, key) =\u003e {\n    computed[key] = partial(fn, store)\n    Object.defineProperty(store.getters, key, {\n      get: () =\u003e store._vm[key],\n      enumerable: true // for local getters\n    })\n  })\n\n  const silent = Vue.config.silent\n  /* 设置 silent 为 true 的目的是为了取消 _vm 的所有日志和警告 */\n  Vue.config.silent = true\n  /*  这里new了一个Vue对象，运用Vue内部的响应式实现注册state以及computed*/\n  store._vm = new Vue({\n    data: {\n      $$state: state\n    },\n    computed\n  })\n  Vue.config.silent = silent\n\n  /* 使能严格模式，Vuex 中对 state 的修改只能在 mutation 的回调函数里 */\n  if (store.strict) {\n    enableStrictMode(store)\n  }\n\n  if (oldVm) {\n    /* 解除旧 vm 的 state 的引用，并销毁这个旧的 _vm 对象 */\n    if (hot) {\n      store._withCommit(() =\u003e {\n        oldVm._data.$$state = null\n      })\n    }\n    Vue.nextTick(() =\u003e oldVm.$destroy())\n  }\n}\n```\n\nstate 的响应式大概就是这样实现的，也就是初始化 resetStoreVM 方法的过程。\n\n### 看看 Store 的 commit 方法\n\n我们知道 commit 方法是用来触发 mutation 的。\n\n```javascript\ncommit (_type, _payload, _options) {\n  /* unifyObjectStyle 方法校参 */\n  const {\n    type,\n    payload,\n    options\n  } = unifyObjectStyle(_type, _payload, _options)\n\n  const mutation = { type, payload }\n  /* 找到相应的 mutation 方法 */\n  const entry = this._mutations[type]\n  if (!entry) {\n    if (process.env.NODE_ENV !== 'production') {\n      console.error(`[vuex] unknown mutation type: ${type}`)\n    }\n    return\n  }\n  /* 执行 mutation 中的方法 */\n  this._withCommit(() =\u003e {\n    entry.forEach(function commitIterator (handler) {\n      handler(payload)\n    })\n  })\n  /* 通知所有订阅者，传入当前的 mutation 对象和当前的 state */\n  this._subscribers.forEach(sub =\u003e sub(mutation, this.state))\n\n  if (\n    process.env.NODE_ENV !== 'production' \u0026\u0026\n    options \u0026\u0026 options.silent\n  ) {\n    console.warn(\n      `[vuex] mutation type: ${type}. Silent option has been removed. ` +\n      'Use the filter functionality in the vue-devtools'\n    )\n  }\n}\n```\n\n该方法先进行参数风格校验，然后利用 `_withCommit` 方法执行本次批量触发 `mutation` 处理函数。执行完成后，通知所有 `_subscribers`（订阅函数）本次操作的 `mutation` 对象以及当前的 `state`状态。\n","lastmodified":"2023-05-09T16:33:58.287366302Z","tags":[]},"/Web%E6%80%A7%E8%83%BD":{"title":"Web性能","content":"\n## 性能优化涉及到的分类\n\n- 网络层面\n- 构建层面\n- 浏览器渲染层面\n- 服务端层面\n\n## 涉及到的功能点\n\n- 资源的合并与压缩\n- 图片编解码原理和类型选择\n- 浏览器渲染机制\n- 懒加载预加载\n- 浏览器存储\n- 缓存机制\n- `PWA`\n- `Vue-SSR`\n\n## 资源合并与压缩\n\n### `http`请求的过程及潜在的性能优化点\n\n- 理解`减少http请求数量`和`减少请求资源大小`两个优化要点\n- 掌握`压缩`与`合并`的原理\n- 掌握通过`在线网站`和`fis3`两种实现压缩与合并的方法\n\n#### 浏览器的一个请求从发送到返回都经历了什么\n\n动态的加载静态的资源\n\n- `dns`是否可以通过缓存减少`dns`查询时间\n- 网络请求的过程走最近的网络环境\n- 相同的静态资源是否可以缓存\n- 能否减少`http`请求大小\n- 能否减少`http`请求数量\n- 服务端渲染\n\n#### 资源的合并与压缩设计到的性能点\n\n- 减少`http`请求的数量\n- 减少请求的大小\n\n### `html`压缩\n\n\u003e `HTML`代码压缩就是压缩这些在文本文件中有意义，但是在`HTML`中不显示的字符，包括`空格`,`制表符`,`换行符`等，还有一些其他意义的字符，如`HTML`注释也可以被压缩\n\n\u003e 意义\n\n- 大型网站意义比较大\n\n#### 如何进行`html`的压缩\n\n- 使用在线网站进行压缩(走构建工具多，公司级在线网站手动压缩小)\n- `node.js`提供了`html-minifier`工具\n- 后端`模板引擎渲染压缩`\n\n### `css`及`js`压缩\n\n#### `css`的压缩\n\n- 无效代码删除\n  - 注释、无效字符\n- `css`语义合并\n\n**压缩的方式**\n\n- 使用在线网站进行压缩\n- webpack使用`optimize-css-assets-webpack-plugin` `HtmlWebpackPlugin`\n- 使用`html-minifier`对`html`中的`css`进行压缩\n- 使用`clean-css`对`css`进行压缩\n\n#### `js`的压缩与混乱\n\n- 无效字符的删除\n  - 空格、注释、回车等\n- webpack 自动压缩\n- 剔除注释\n- 代码语意的缩减和优化\n  - 变量名缩短(`a`,`b`)等\n- 代码保护\n  - 前端代码是透明的，客户端代码用户是可以直接看到的，可以轻易被窥探到逻辑和漏洞\n\n#### `js`压缩的方式\n\n- 使用在线网站进行压缩\n- 使用`html-minifier`对`html`中的`js`进行压缩\n- 使用`uglifyjs2`对`js`进行压缩\n\n### 不合并文件可能存在的问题\n\n- 文件与文件有插入之间的上行请求，又增加了`N-1`个网络延迟\n- 受丢包问题影响更严重\n- 经过代理服务器时可能会被断开\n\n#### 文件合并缺点\n\n- 首屏渲染问题\n  - 文件合并之后的`js`变大，如果首页的渲染依赖这个`js`的话，整个页面的渲染要等`js`请求完才能执行\n  - 如果首屏只依赖`a.js`，只要等`a.js`完成后就可执行\n  - 没有通过服务器端渲染，现在框架都需要等合并完的文件请求完才能执行，基本都需要等文件合并后的`js`\n- 缓存失效问题\n  - 标记 `js` `md5`戳\n  - 合并之后的`js`,任何一个改动都会导致大面积的缓存失效\n\n#### 文件合并对应缺点的处理\n\n- 公共库合并\n- 不同页面的合并\n  - 不同页面`js`单独打包\n- 见机行事，随机应变\n\n#### 文件合并对应方法\n\n- 使用在线网站进行合并\n- 构建阶段，使用`nodejs`进行文件合并\n\n### CDN减少打包体积\n\n使用 `cdn` 文件来减少工程到打包体积，也可以按需加载。\n\n在 /public/index.html 中引入需要的js和css文件\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184258.png)\n\n去掉 package.json 中对于 vue、element-ui 等相关资源的依赖\n\nsrc/main.js ，去掉 vue、element-ui 等相关资源的 import 和 vue.use 这些语句\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184259.png)\n\n配置externals。由于使用 Vue Cli 3 默认配置，新建出来的项目没有了 build 目录，首先得在项目根目录下，新建 vue.config.js 文件，里面添加以下代码：\n\n```\nmodule.exports = {\n    configureWebpack:{\n        externals:{\n            'Vue': 'Vue',\n            'element-ui': 'element-ui',\n            'clipboard':'VueClipboard'\n        }\n    }\n}\n```\n\n### 去除 SourceMap\n\n由于打包后的文件经过了压缩、合并、混淆、babel编译后的代码不利于定位分析bug。\n\n```\nmodule.exports = {\n  productionSourceMap: false,\n}\n```\n\n### gzip 压缩\n\n`gzip` 压缩效率非常高，通常可以达到 70% 的压缩率，也就是说，如果你的网页有 30K，压缩之后就变成了 9K 左右。\n\n```\n//npm i -D compression-webpack-plugin\nconfigureWebpack: config =\u003e {\n  const CompressionPlugin = require('compression-webpack-plugin')\n  config.plugins.push(new CompressionPlugin())\n}\n```\n\n## 图片相关优化\n\n### 一张`JPG`的解析过程\n\n`jpg`有损压缩：虽然损失一些信息，但是肉眼可见影响并不大\n\n### `png8`/`png24`/`png32`之间的区别\n\n- `png8` ----`256色` + 支持透明\n- `png24` ----`2^24` + 不支持透明\n- `png32` ---`2^24` +支持透明\n\n```\n文件大小`  +   `色彩丰富程度\n```\n\n`png32`是在`png24`上支持了透明，针对不同的业务场景选择不同的图片格式很重要\n\n### 不同的格式图片常用的业务场景\n\n#### 不同格式图片的特点\n\n- `jpg`有损压缩，压缩率高，不支持透明\n- `png`支持透明，浏览器兼容性好\n- `webp`压缩程度更好，在`ios webview`中有兼容性问题\n- `svg`矢量图，代码内嵌，相对较小，图片样式相对简单的场景(尽量使用，绘制能力有限，图片简单用的比较多)\n\n#### 不同格式图片的使用场景\n\n- `jpg`：大部分不需要透明图片的业务场景\n- `png`：大部分需要透明图片的业务场景\n- `webp`：`android`全部(解码速度和压缩率高于`jpg`和`png`，但是`ios` `safari`还没支持)\n- `svg`：图片样式相对简单的业务场景\n\n### 图片压缩的几种情况\n\n- 针对真实图片情况，舍弃一些相对无关紧要的色彩信息\n- `CSS雪碧图`：把你的网站用到的一些图片整合到一张单独的图片中\n  - 优点：减少`HTTP`请求的数量(通过`backgroundPosition`定位所需图片)\n  - 缺点：整合图片比较大时，加载比较慢(如果这张图片没有加载成功，整个页面会失去图片信息)`facebook`官网任然在用，主要`pc`用的比较多，相对性能比较强\n- `Image-inline`：将图片的内容嵌到`html`中(减少网站的`HTTP`请求)\n  - `base64信息`，减少网站的HTTP请求,如果图片比较小比较多，时间损耗主要在请求的骨干网络\n- `使用矢量图`\n  - 使用`SVG`进行矢量图的绘制\n  - 使用`icon-font`解决`icon`问题\n- `在android下使用webp`\n  - `webp`的优势主要体现在它具有更优的图像数据压缩算法，能带来更小的图片体积，而且拥有肉眼识别无差异的图像质量；\n  - 同时具备了无损和有损的压缩模式、`Alpha`透明以及动画的特性，在`JPEG`和`PNG`上的转化效果都非常优秀、稳定和统一\n\n## css和js的装载与执行\n\n### HTML页面加载渲染的过程\n\n#### 一个网站在浏览器端是如何进行渲染的\n\n### HTML渲染过程中的一些特点\n\n- 顺序执行，并发加载\n  - 词法分析：从上到下依次解析\n    - 通过`HTML`生成`Token对象`（当前节点的所有子节点生成后，才会通过`next token`获取到当前节点的兄弟节点），最终生成`Dom Tree`\n  - 并发加载：资源请求是并发请求的\n  - 并发上限\n    - 浏览器中可以支持并发请求，不同浏览器所支持的并发数量不同（以域名划分），以`Chrome`为例，并发上限为6个\n    - 优化点： 把CDN资源分布在多个域名下\n- 是否阻塞\n  - css阻塞\n    - css 在head中通过link引入会阻塞页面的渲染\n      - 如果我们把`css`代码放在`head`中去引入的话，那么我们整个页面的渲染实际上就会等待`head`中`css`加载并生成`css树`，最终和`DOM`整合生成`RanderTree`之后才会进行渲染\n    - 为了浏览器的渲染，能让页面显示的时候视觉上更好。 避免某些情况，如：假设你放在页面最底部，用户打开页面时，有可能出现，页面先是显示一大堆文字或图片，自上而下，丝毫没有排版和样式可言。最后，页面又恢复所要的效果\n    \n  - `css`不阻塞`js`的加载，但阻塞`js`的执行\n    \n    - `css`不阻塞外部脚步的加载(`webkit preloader 预资源加载器`)\n  - js阻塞\n    - 直接通过\\\u003cscript src\u003e引入会阻塞后面节点的渲染\n      - `html parse`认为`js`会动态修改文档结构(`document.write`等方式)，没有进行后面文档的变化\n      - async defer(async放弃了依赖关系)\n        - `defer`属性（``） (这是延迟执行引入的`js`脚本（即脚本加载是不会导致解析停止，等到`document`全部解析完毕后，`defer-script`也加载完毕后，在执行所有的`defer-script`加载的`js`代码，再触发`Domcontentloaded`）``\n        - async属性（\\\u003cscript src=\"\" async\u003e\\\u003c/script\u003e）\n          - 这是异步执行引入的`js`脚本文件\n          - 与`defer`的区别是`async`会在加载完成后就执行，但是不会影响阻塞到解析和渲染。但是还是会阻塞`load`事件，所以`async-script`会可能在`DOMcontentloaded`触发前或后执行，但是一定会在`load`事件前触发。\n\n## 懒加载与预加载\n\n### 懒加载\n\n- 图片进入可视区域之后请求图片资源\n- 对于电商等图片很多，页面很长的业务场景适用\n- 减少无效资源的加载\n- 并发加载的资源过多会会阻塞js的加载，影响网站的正常使用\n\n`img src`被设置之后，`webkit`解析到之后才去请求这个资源。所以我们希望图片到达可视区域之后，`img src`才会被设置进来，没有到达可视区域前并不现实真正的`src`，而是类似一个`1px`的占位符。\n\n```\n场景：电商图片\n```\n\n### 预加载\n\n- 图片等静态资源在使用之前的提前请求\n- 资源使用到时能从缓存中加载，提升用户体验\n- 页面展示的依赖关系维护\n\n```\n场景：抽奖\n```\n\n### 懒加载原生`js`和`zepto.lazyload`\n\n\u003e ```\n\u003e 原理\n\u003e ```\n\n先将`img`标签中的`src`链接设为同一张图片（空白图片），将其真正的图片地址存储再`img`标签的自定义属性中（比如`data-src`）。当`js`监听到该图片元素进入可视窗口时，即将自定义属性中的地址存储到`src`属性中，达到懒加载的效果。\n\n\u003e 注意问题：\n\n- 关注首屏处理,因为还没滑动\n- 占位，图片大小首先需要预设高度，如果没有设置的话，会全部显示出来\n\n```javascript\nvar viewheight = document.documentElement.clientHeight   //可视区域高度\n\nfunction lazyload(){\n    var eles = document.querySelectorAll('img[data-original][lazyload]')\n\n    Array.prototype.forEach.call(eles,function(item,index){\n        var rect;\n        if(item.dataset.original === '') return;\n        rect = item.getBoundingClientRect(); //返回元素的大小及其相对于视口的\n\n        if(rect.bottom \u003e= 0 \u0026\u0026 rect.top \u003c viewheight){\n            !function(){\n                var img = new Image();\n                img.src = item.dataset.url;\n                img.onload = function(){\n                    item.src = img.src\n                }\n                item.removeAttribute('data-original');\n                item.removeAttribute('lazyload');\n            }()\n        }\n    })\n}\n\nlazyload()\ndocument.addEventListener('scroll',lazyload)\n```\n\n### 预加载原生`js`和`preloadJS`实现\n\n#### 预加载实现的几种方式\n\n- 第一种方式：直接请求下来\n\n```html\n\u003cimg src=\"https://user-gold-cdn.xitu.io/2019/2/21/1690d1b216cbfa18\" style=\"display: none\"/\u003e\n\u003cimg src=\"https://user-gold-cdn.xitu.io/2019/2/21/1690d1b21b70c8d2\" style=\"display: none\"/\u003e\n\u003cimg src=\"https://user-gold-cdn.xitu.io/2019/2/21/1690d1b216e17e26\" style=\"display: none\"/\u003e\n\u003cimg src=\"https://user-gold-cdn.xitu.io/2019/2/21/1690d1b217b3ae59\" style=\"display: none\"/\u003e\n```\n\n- 第二种方式：`image`对象\n\n```javascript\nvar image = new Image();\nimage.src = \"www.pic26.com/dafdafd/safdas.jpg\"；\n```\n\n- 第三种方式：\n\n  ```javascript\n  xmlhttprequest\n  ```\n\n  - 缺点：存在跨域问题\n  - 优点：好控制\n\n```javascript\nvar xmlhttprequest = new XMLHttpRequest();\n\nxmlhttprequest.onreadystatechange = callback;\n\nxmlhttprequest.onprogress = progressCallback;\n\nxmlhttprequest.open(\"GET\",\"http:www.xxx.com\",true);\n\nxmlhttprequest.send();\n\nfunction callback(){\n    if(xmlhttprequest.readyState == 4 \u0026\u0026 xmlhttprequest.status == 200){\n        var responseText = xmlhttprequest.responseText;\n    }else{\n        console.log(\"Request was unsuccessful:\" + xmlhttprequest.status);\n    }\n}\n\nfunction progressCallback(){\n    e = e || event;\n    if(e.lengthComputable){\n        console.log(\"Received\"+e.loaded+\"of\"+e.total+\"bytes\")\n    }\n}   \n```\n\n```\nPreloadJS模块\n```\n\n- **本质**：**权衡浏览器加载能力，让它尽可能饱和利用起来**\n\n## 重绘与回流\n\n### `css`性能让`javascript`变慢\n\n要把`css`相关的外部文件引入放进`head`中，加载`css`时，整个页面的渲染是阻塞的，同样的执行`javascript`代码的时候也是阻塞的，例如`javascript`死循环。\n\n```\n一个线程   =\u003e  javascript解析\n一个线程   =\u003e  UI渲染\n```\n\n这两个线程是互斥的，当`UI`渲染的时候，`javascript`的代码被终止。当`javascript`代码执行，`UI`线程被冻结。所以`css`的性能让`javascript`变慢。\n\n```\n频繁触发重绘与回流，会导致UI频繁渲染，最终导致js变慢\n```\n\n### 什么是重绘和回流\n\n#### 回流\n\n- 当`render tree`中的一部分(或全部)因为元素的`规模尺寸`，`布局`，`隐藏`等改变而需要`重新构建`。这就成为回流(`reflow`)\n- 当页面布局和几何属性改变时，就需要`回流`。\n\n#### 重绘\n\n- 当`render tree`中的一些元素需要更新属性，而这些属性只是影响元素的`外观`，`风格`，而不影响布局，比如`background-color`。就称重绘。\n\n#### 关系\n\n用到`chrome` 分析 `performance`\n\n```\n回流必将引起重绘，但是重绘不一定会引起回流\n```\n\n### 避免重绘、回流的两种方法\n\n#### 触发页面重布局的一些css属性\n\n- 盒子模型相关属性会触发重布局\n  - `width` `height` `padding` `margin` `display` `border-width` `border` `min-height`\n- 定位属性及浮动也会触发重布局\n  - `top` `bottom` `left` `right` `position` `float` `clear`\n- 改变节点内部文字结构也会触发重布局\n  - `text-align` `overflow-y` `font-weight` `overflow` `font-family` `line-height` `vertical-align` `white-space` `font-size`\n\n```\n优化点：使用不触发回流的方案替代触发回流的方案\n```\n\n#### 只触发重绘不触发回流\n\n- `color`\n- `border-style`、`border-radius`\n- `visibility`\n- `text-decoration`\n- `background`、`background-image`、`background-position`、`background-repeat`、`background-size`\n- `outline`、`outline-color`、`outline-style`、`outline-width`\n- `box-shadow`\n\n#### 新建DOM的过程\n\n- 获取`DOM`后分割为多个图层\n- 对每个图层的节点计算样式结果(`Recalculate style` 样式重计算)\n- 为每个节点生成图形和位置(`Layout` 回流和重布局)\n- 将每个节点绘制填充到图层位图中(`Paint Setup`和`Paint` `重绘`)\n- 图层作为纹理上传至`gpu`\n- 符合多个图层到页面上生成最终屏幕图像(`Composite Layers` 图层重组)\n\n### 浏览器绘制`DOM`的过程是这样子的：\n\n- 获取 DOM 并将其分割为多个层（`layer`），将每个层独立地绘制进位图（`bitmap`）中\n- 将层作为纹理（`texture`）上传至 `GPU`，复合（`composite`）多个层来生成最终的屏幕图像\n- `left/top/margin`之类的属性会影响到元素在文档中的布局，当对布局（`layout`）进行动画时，该元素的布局改变可能会影响到其他元素在文档中的位置，就导致了所有被影响到的元素都要进行重新布局，浏览器需要为整个层进行重绘并重新上传到 `GPU`，造成了极大的性能开销。\n- `transform` 属于合成属性（`composite property`），对合成属性进行 `transition/animation` 动画将会创建一个合成层（`composite layer`），这使得被动画元素在一个独立的层中进行动画。\n- 通常情况下，浏览器会将一个层的内容先绘制进一个位图中，然后再作为纹理（`texture`）上传到 `GPU`，只要该层的内容不发生改变，就没必要进行重绘（`repaint`），浏览器会通过重新复合（`recomposite`）来形成一个新的帧。\n\n#### `chrome`创建图层的条件\n\n```\n将频繁重绘回流的DOM元素单独作为一个独立图层，那么这个DOM元素的重绘和回流的影响只会在这个图层中\n```\n\n- `3D`或透视变换\n- `CSS`属性使用加速视频解码的 `\u003cvideo\u003e` 元素\n- 拥有 `3D` (`WebGL`) 上下文或加速的 `2D` 上下文的 `\u003ccanvas\u003e` 元素\n- 复合插件(如 `Flash`)\n- 进行 `opacity/transform` 动画的元素拥有加速\n- `CSS filters` 的元素元素有一个包含复合层的后代节点(换句话说，就是一个元素拥有一个子元素，该子元素在自己的层里)\n- 元素有一个 `z-index` 较低且包含一个复合层的兄弟元素(换句话说就是该元素在复合层上面渲染)\n\n\u003e 总结：对布局属性进行动画，浏览器需要为每一帧进行重绘并上传到 `GPU` 中对合成属性进行动画，浏览器会为元素创建一个独立的复合层，当元素内容没有发生改变，该层就不会被重绘，浏览器会通过重新复合来创建动画帧\n\n```\ngif图\n```\n\n#### 总结\n\n- 尽量避免使用触发`回流`、`重绘`的`CSS`属性\n- 将`重绘`、`回流`的影响范围限制在单独的图层(`layers`)之内\n- 图层合成过程中消耗很大页面性能，这时候需要平衡考虑重绘回流的性能消耗\n\n### 实战优化点总结\n\n- 用`translate`替代`top`属性\n  - `top`会触发`layout`，但`translate`不会\n- 用`opacity`代替`visibility`\n  - `opacity`不会触发重绘也不会触发回流，只是改变图层`alpha`值，但是必须要将这个图片独立出一个图层\n  - `visibility`会触发重绘\n- 不要一条一条的修改`DOM`的样式，预先定义好`class`，然后修改`DOM`的`className`\n- `把DOM`离线后修改，比如：先把`DOM`给`display:none`（有一次`reflow`），然后你修改100次，然后再把它显示出来\n- 不要把`DOM`节点的属性值放在一个循环里当成循环的变量\n  - `offsetHeight`、`offsetWidth`每次都要刷新缓冲区，缓冲机制被破坏\n  - 先用变量存储下来\n- 不要使用`table`布局，可能很小的一个小改动会造成整个`table`的重新布局\n  - `div`只会影响后续样式的布局\n- 动画实现的速度的选择\n  - 选择合适的动画速度\n  - 根据`performance`量化性能优化\n- 对于动画新建图层\n  - 启用`gpu`硬件加速(并行运算)，`gpu加速`意味着数据需要从`cpu`走总线到`gpu`传输，需要考虑传输损耗.\n- `transform:translateZ(0)`\n    - `transform:translate3D(0)`\n\nhttps://lavas.baidu.com/guide/v1/foundation/lavas-start)\n\n## 缓存\n\n缓存算是很重要了，面试经常问到，故整理。\n\n总体感知一下它的匹配流程，如下：\n\n1. 浏览器发送请求前，根据请求头的expires和cache-control判断是否命中（包括是否过期）强缓存策略，如果命中，直接从缓存获取资源，并不会发送请求。如果没有命中，则进入下一步。\n2. 没有命中强缓存规则，浏览器会发送请求，根据请求头的last-modified和etag判断是否命中协商缓存，如果命中，直接从缓存获取资源。如果没有命中，则进入下一步。\n3. 如果前两步都没有命中，则直接从服务端获取资源。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184300.jpg)\n\n### 强缓存\n\n强缓存：不会向服务器发送请求，直接从缓存中读取资源。\n\n#### 强缓存原理\n\n强制缓存就是向浏览器缓存查找该请求结果，并根据该结果的缓存规则来决定是否使用该缓存结果的过程，强制缓存的情况主要有三种(暂不分析协商缓存过程)，如下：\n\n- 第一次请求，不存在缓存结果和缓存标识，直接向服务器发送请求\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184301.jpg)\n\n- 存在缓存标识和缓存结果，但是已经失效，强制缓存是啊比，则使用协商缓存（暂不分析）\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184302.jpg)\n\n- 存在该缓存结果和缓存标识，且该结果尚未失效，强制缓存生效，直接返回该结果\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184303.jpg)\n\n那么强制缓存的缓存规则是什么？ 当浏览器向服务器发起请求时，服务器会将缓存规则放入HTTP响应报文的HTTP头中和请求结果一起返回给浏览器，控制强制缓存的字段分别是`Expires`和`Cache-Control`，其中`Cache-Control`优先级比`Expires`高。\n\n#### Expires\n\n缓存过期时间，用来指定资源到期的时间，是服务器端的具体的时间点。也就是说，`Expires=max-age + 请求时间`，需要和`Last-modified`结合使用。`Expires`是Web服务器响应消息头字段，在响应http请求时告诉浏览器在过期时间前浏览器可以直接从浏览器缓存取数据，而无需再次请求。\n\n\u003e Expires 是 HTTP/1 的产物，受限于本地时间，如果修改了本地时间，可能会造成缓存失效。\n\n#### Cache-Control\n\n在HTTP/1.1中，Cache-Control是最重要的规则，主要用于控制网页缓存，主要取值为： - public：所有内容都将被缓存（客户端和代理服务器都可缓存） - private：所有内容只有客户端可以缓存，`Cache-Control`的默认取值 - no-cache：客户端缓存内容，但是是否使用缓存则需要经过**协商缓存**来验证决定 - no-store：所有内容都不会被缓存，即不使用强制缓存，也不使用协商缓存 - max-age=xxx (xxx is numeric)：缓存内容将在xxx秒后失效\n\n\u003e 需要注意的是，`no-cache`这个名字有一点误导。设置了`no-cache`之后，并不是说浏览器就不再缓存数据，只是浏览器在使用缓存数据时，需要先确认一下数据是否还跟服务器保持一致，也就是协商缓存。而`no-store`才表示不会被缓存，即不使用强制缓存，也不使用协商缓存\n\n#### nginx设置\n\n强缓存需要服务端设置`expires`和`cache-control`。 `nginx`代码参考，设置了一年的缓存时间：\n\n```text\nlocation ~ .*\\.(ico|svg|ttf|eot|woff)(.*) {\n  proxy_cache               pnc;\n  proxy_cache_valid         200 304 1y;\n  proxy_cache_valid         any 1m;\n  proxy_cache_lock          on;\n  proxy_cache_lock_timeout  5s;\n  proxy_cache_use_stale     updating error timeout invalid_header http_500 http_502;\n  expires                   1y;\n}\n```\n\n浏览器的缓存存放在哪里，如何在浏览器中判断强制缓存是否生效？这就是下面我们要讲到的`from disk cache`和`from memory cache`。\n\n#### from disk cache和from memory cache\n\n细心地同学在开发的时候应该注意到了Chrome的网络请求的Size会出现三种情况`from disk cache(磁盘缓存)`、`from memory cache(内存缓存)`、以及资源大小数值。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184304.jpg)\n\n浏览器读取缓存的顺序为memory –\u003e disk。 以访问`https://github.com/xiangxingchen/blog`为例 我们第一次访问时`https://github.com/xiangxingchen/blog`\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184305.jpg)\n\n关闭标签页，再此打开`https://github.com/xiangxingchen/blog`时\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184306.jpg)\n\nF5刷新时\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184307.jpg)\n\n简单的对比一下\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184308.jpg)\n\n### 协商缓存\n\n协商缓存就是强制缓存失效后，浏览器携带缓存标识向服务器发起请求，由服务器根据缓存标识决定是否使用缓存的过程，主要有以下两种情况：\n\n- 协商缓存生效，返回304和Not Modified\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184309.jpg)\n\n- 协商缓存失效，返回200和请求结果\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184310.jpg)\n\n#### Last-Modified和If-Modified-Since\n\n1. 浏览器首先发送一个请求，让服务端在`response header`中返回请求的资源上次更新时间，就是`last-modified`，浏览器会缓存下这个时间。\n2. 然后浏览器再下次请求中，`request header`中带上`if-modified-since`:`[保存的last-modified的值]`。根据浏览器发送的修改时间和服务端的修改时间进行比对，一致的话代表资源没有改变，服务端返回正文为空的响应，让浏览器中缓存中读取资源，这就大大减小了请求的消耗。\n\n由于last-modified依赖的是保存的绝对时间，还是会出现误差的情况：\n\n1. 保存的时间是以秒为单位的，1秒内多次修改是无法捕捉到的；\n2. 各机器读取到的时间不一致，就有出现误差的可能性。为了改善这个问题，提出了使用etag。\n\n#### ETag和If-None-Match\n\n`etag`是`http`协议提供的若干机制中的一种`Web`缓存验证机制，并且允许客户端进行缓存协商。生成etag常用的方法包括对资源内容使用抗碰撞散列函数，使用最近修改的时间戳的哈希值，甚至只是一个版本号。 和`last-modified`一样. - 浏览器会先发送一个请求得到`etag`的值，然后再下一次请求在`request header`中带上`if-none-match`:`[保存的etag的值]`。 - 通过发送的`etag`的值和服务端重新生成的`etag`的值进行比对，如果一致代表资源没有改变，服务端返回正文为空的响应，告诉浏览器从缓存中读取资源。\n\n\u003e etag能够解决last-modified的一些缺点，但是etag每次服务端生成都需要进行读写操作，而last-modified只需要读取操作，从这方面来看，etag的消耗是更大的。\n\n二者对比 - 精确度上：`Etag`要优于`Last-Modified`。 - 优先级上：服务器校验优先考虑`Etag`。 - 性能上：`Etag`要逊于`Last-Modified`\n\n#### 用户行为对浏览器缓存的影响\n\n1. 打开网页，地址栏输入地址： 查找 `disk cache` 中是否有匹配。如有则使用；如没有则发送网络请求。\n2. 普通刷新 (F5)：因为 TAB 并没有关闭，因此 `memory cache` 是可用的，会被优先使用(如果匹配的话)。其次才是 `disk cache`。\n3. 强制刷新 (Ctrl + F5)：浏览器不使用缓存，因此发送的请求头部均带有 `Cache-control:no-cache`(为了兼容，还带了 `Pragma:no-cache`),服务器直接返回 200 和最新内容。\n\n### 总结\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184311.jpg)\n\n\u003e 期望大规模数据能自动化缓存，而不是手动进行缓存，需要浏览器端和服务器端协商一种缓存机制\n\n\u003e - Cache-Control所控制的缓存策略\n\u003e - last-modified 和 etage以及整个服务端浏览器端的缓存流程\n\u003e - 基于node实践以上缓存方式\n\n### httpheader\n\n#### 可缓存性\n\n- `public`:表明响应可以被任何对象（包括：发送请求的客户端，代理服务器，等等）缓存。\n- `private`:表明响应只能被单个用户缓存，不能作为共享缓存（即代理服务器不能缓存它）。\n- `no-cache`:强制所有缓存了该响应的缓存用户，在使用已存储的缓存数据前，发送带验证器的请求到原始服务器\n- `only-if-cached`:表明如果缓存存在，只使用缓存，无论原始服务器数据是否有更新\n\n#### 到期\n\n- `max-age=`:设置缓存存储的最大周期，超过这个时间缓存被认为过期(单位秒)。与 `Expires`相反，时间是相对于请求的时间。\n- `s-maxage=`:覆盖`max-age` 或者 `Expires` 头，但是仅适用于共享缓存(比如各个代理)，并且私有缓存中它被忽略。`cdn`缓存\n- `max-stale[=]` 表明客户端愿意接收一个已经过期的资源。 可选的设置一个时间(单位秒)，表示响应不能超过的过时时间。\n- `min-fresh=` 表示客户端希望在指定的时间内获取最新的响应。\n\n#### 重新验证和重新加载\n\n\u003e 重新验证\n\n- `must-revalidate`：缓存必须在使用之前验证旧资源的状态，并且不可使用过期资源。\n- `proxy-revalidate`：与`must-revalidate`作用相同，但它仅适用于共享缓存（例如代理），并被私有缓存忽略。\n- `immutable` ：表示响应正文不会随时间而改变。资源（如果未过期）在服务器上不发生改变，因此客户端不应发送重新验证请求头（例如`If-None-Match`或`If-Modified-Since`）来检查更新，即使用户显式地刷新页面。在`Firefox`中，`immutable`只能被用在 `https:// transactions`.\n\n\u003e 重新加载\n\n- `no-store`:缓存不应存储有关客户端请求或服务器响应的任何内容。\n- `no-transform`:不得对资源进行转换或转变。`Content-Encoding`,`Content-Range`, `Content-Type`等`HTTP`头不能由代理修改。例如，非透明代理可以对图像格式进行转换，以便节省缓存空间或者减少缓慢链路上的流量。 `no-transform`指令不允许这样做。\n\n### Expires\n\n- 缓存过期时间，用来指定资源到期的时间，是服务器端的时间点\n- 告诉浏览器在过期时间前浏览器可以直接从浏览器缓存中存取数据，而无需再次请求\n- `expires`是`http1.0`的时候的\n- `http1.1`时候，我们希`望cache`的管理统一进行，`max-age`优先级高于`expires`，当有`max-age`在的时候`expires`可能就会被忽略。\n- 如果没有设置`cache-control`时候会使用`expires`\n\n### Last-modified和If-Modified-since\n\n- 基于客户端和服务器端协商的缓存机制\n- `last-modified` --\u003e `response header`  \n   `if-modified-since` --\u003e `request header`\n- 需要与`cache-control`共同使用\n\n\u003e `last-modified`有什么缺点？\n\n- 某些服务端不能获取精确的修改时间\n- 文件修改时间改了，但文件的内容却没有变\n\n### Etag和 If-none-match\n\n- 文件内容的hash值\n- `etag` --\u003e`reponse header`  \n   `if-none-match` --\u003e`request header`\n- 需要与`cache-control`共同使用\n\n好处：\n\n- 比`if-modified-since`更加准确\n- 优先级比`etage`更高\n\n### Service Worker\n\n`ServiceWorker` 是运行在浏览器后台进程里的一段 JS，它可以做许多事情，比如拦截客户端的请求、向客户端发送消息、向服务器发起请求等等，其中最重要的作用之一就是离线资源缓存。\n\n`ServiceWorker` 拥有对缓存流程丰富灵活的控制能力，当页面请求到 `ServiceWorker`时，`ServiceWorker` 同时请求缓存和网络，把缓存的内容直接给用户，而后覆盖缓存。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184312.png)\n\n**注意：需要HTTPS才可以使用 ServiceWorker**\n\n### 缓存流程图\n\n## 服务端性能优化\n\n```\n服务端用的node.js因为和前端用的同一种语言，可以利用服务端运算能力来进行相关的运算而减少前端的运算\n```\n\n- `vue`渲染遇到的问题\n- `vue-ssr`和原理和引用\n\n### vue渲染面临的问题\n\n```\n    先加载vue.js\n=\u003e  执行vue.js代码\n=\u003e  生成html\n```\n\n\u003e 以前没有前端框架时，\n\n- 用`jsp/php`在服务端进行数据的填充`，发送给客户端就是已经`填充好数据`的html\n- 使用`jQuery`异步加载数据\n- 使用`React`和`Vue`前端框架\n  - 代价：需要框架全部加载完，才能把页面渲染出来，页面的首屏性能不好\n\n### 多层次的优化方案\n\n- 构建层的模板编译。`runtime`,`compile`拆开,构建层做模板编译工作。`webpack`构建时候，统一，直接编译成`runtime`可以执行的代码\n- 数据无关的`prerender`的方式\n- 服务端渲染\n\n## Web性能监控\n\ngoogle 开发者提出了一种 RAIL 模型来衡量应用性能，即：Response、Animation、Idle、Load，分别代表着 web 应用生命周期的四个不同方面。并指出最好的性能指标是：100ms 内响应用户输入；动画或者滚动需在 10ms 内产生下一帧；最大化空闲时间；页面加载时长不超过 5 秒。\n\n![0](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184313.jpg)\n\n我们可转化为三个方面来看：响应速度、页面稳定性、外部服务调用\n\n- 响应速度：页面初始访问速度 + 交互响应速度\n- 页面稳定性：页面出错率\n- 外部服务调用：网络请求访问速度\n\n### 1. 页面访问速度：白屏、首屏时间、可交互时间\n\n我们来看看 google 开发者针对用户体验，提出的几个性能指标\n\n![1](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184314.jpg)\n\n这几个指标其实都是根据用户体验，提炼出对应的性能指标\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184315.jpg)\n\n#### 1）first paint (FP) and first contentful paint (FCP)\n\n首次渲染、首次有内容的渲染  \n这两个指标浏览器已经标准化了，从 performance 的 The Paint Timing API 可以获取到，一般来说两个时间相同，但也有情况下两者不同。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184316.jpg)\n\n#### 2）First meaningful paint and hero element timing\n\n首次有意义的渲染、页面关键元素  \n我们假设当一个网页的 DOM 结构发生剧烈的变化的时候，就是这个网页主要内容出现的时候，那么在这样的一个时间点上，就是首次有意义的渲染。这个指标浏览器还没有规范，毕竟很难统一一个标准来定义网站的主体内容。  \ngoogle lighthouse 定义的 first meaningful paint：  \nhttps://docs.google.com/document/d/1BR94tJdZLsin5poeet0XoTW60M0SjvOJQttKT-JK8HI/view\n\n#### 3）Time to interactive\n\n可交互时间\n\n#### 4）长任务\n\n浏览器是单线程的，如果长任务过多，那必然会影响着用户响应时长。好的应用需要最大化空闲时间，以保证能最快响应用户的输入。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184317.jpg)\n\n### 2. 页面稳定性：页面出错情况\n\n资源加载错误  \nJS 执行报错\n\n### 3. 外部服务调用\n\nCGI 耗时  \nCGI 成功率  \nCDN 资源耗时\n\n### 监控的分类？\n\nweb 性能监控可分为两类，一类是合成监控（Synthetic Monitoring，SYN），另一类是真实用户监控（Real User Monitoring，RUM）\n\n#### 1.合成监控\n\n合成监控是采用 web 浏览器模拟器来加载网页，通过模拟终端用户可能的操作来采集对应的性能指标，最后输出一个网站性能报告。例如：Lighthouse、PageSpeed、WebPageTest、Pingdom、PhantomJS 等。\n\n##### 1. Lighthouse\n\nLighthouse 是 google 一个开源的自动化工具，运行 Lighthouse 的方式有两种：一种是作为 Chrome 扩展程序运行；另一种作为命令行工具运行。 Chrome 扩展程序提供了一个对用户更友好的界面，方便读取报告。通过命令行工具可以将 Lighthouse 集成到持续集成系统。  \n展示了白屏、首屏、可交互时间等性能指标和 SEO、PWA 等。  \n腾讯文档移动端官网首页测速结果：\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184318.jpg)\n\n##### 2. PageSpeed\n\nhttps://developers.google.com/speed/pagespeed/insights/  \n不仅展示了一些主要的性能指标数据，还给出了部分性能优化建议。  \n腾讯文档移动端首页测速结果和性能优化建议：\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184319.jpg)\n\n##### 3. WebPageTest\n\nWebPageTest  \n给出性能测速结果和资源加载的瀑布图。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184320.jpg)\n\n##### 4. Pingdom\n\nhttps://www.pingdom.com/  \n注意：Pingdom 不仅提供合成监控，也提供真实用户监控。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184321.jpg)  \n合成监控方式的优缺点：  \n优点：  \n无侵入性。  \n简单快捷。  \n缺点：  \n不是真实的用户访问情况，只是模拟的。  \n没法考虑到登录的情况，对于需要登录的页面就无法监控到。\n\n#### 2.真实用户监控\n\n真实用户监控是一种被动监控技术，是一种应用服务，被监控的 web 应用通过 sdk 等方式接入该服务，将真实的用户访问、交互等性能指标数据收集上报、通过数据清洗加工后形成性能分析报表。例如 FrontJs、oneapm、Datadog 等。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184322.jpg)\n\n##### 1. oneapm\n\nhttps://www.oneapm.com/bi/feature.html  \n功能包括：大盘数据、特征统计、慢加载追踪、访问页面、脚本错误、AJAX、组合分析、报表、告警等。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184323.jpg)\n\n##### 2. Datadog\n\nhttps://www.datadoghq.com/rum/\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184324.jpg)\n\n##### 3. FrontJs\n\nhttps://www.frontjs.com/  \n功能包括：访问性能、异常监控、报表、趋势等。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184325.jpg)  \n**这种监控方式的优缺点：**  \n**优点：**  \n是真实用户访问情况。  \n可以观察历史性能趋势。  \n有一些额外的功能：报表推送、监控告警等等。  \n**缺点：**  \n有侵入性，会一定程度上响应 web 性能。\n\n### performance 分析\n\n在讲如何监控之前，先来看看浏览器提供的 performance api，这也是性能监控数据的主要来源。  \nperformance 提供高精度的时间戳，精度可达纳秒级别，且不会随操作系统时间设置的影响。  \n目前市场上的支持情况：主流浏览器都支持，大可放心使用。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184326.jpg)\n\n#### 基本属性\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184327.jpg)\n\nperformance.navigation: 页面是加载还是刷新、发生了多少次重定向\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184328.jpg)\n\nperformance.timing: 页面加载的各阶段时长\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184329.jpg)\n\n各阶段的含义：\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184330.jpg)\n\nperformance.memory： 基本内存使用情况，Chrome 添加的一个非标准扩展\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184331.jpg)\n\nperformance.timeorigin: 性能测量开始时的时间的高精度时间戳\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184332.jpg)\n\n#### 基本方法\n\nperformance.getEntries()  \n通过这个方法可以获取到所有的 performance 实体对象，通过 getEntriesByName 和 getEntriesByType 方法可对所有的 performance 实体对象 进行过滤，返回特定类型的实体。  \nmark 方法 和 measure 方法的结合可打点计时，获取某个函数执行耗时等。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184333.jpg)\n\nperformance.getEntriesByName()  \nperformance.getEntriesByType()  \nperformance.mark()  \nperformance.clearMarks()  \nperformance.measure()  \nperformance.clearMeasures()  \nperformance.now()  \n…\n\n#### 提供的 API\n\nperformance 也提供了多种 API，不同的 API 之间可能会有重叠的部分。\n\n##### 1. PerformanceObserver API\n\n用于检测性能的事件，这个 API 利用了观察者模式。  \n获取资源信息\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184334.jpg)\n\n监测 TTI\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184335.jpg)\n\n监测 长任务\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184336.jpg)\n\n##### 2. Navigation Timing API\n\nhttps://www.w3.org/TR/navigation-timing-2/  \nperformance.getEntriesByType(\"navigation\");\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184337.jpg)\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184338.jpg)\n\n不同阶段之间是连续的吗? —— 不连续  \n每个阶段都一定会发生吗？—— 不一定\n\n重定向次数：performance.navigation.redirectCount  \n重定向耗时: redirectEnd - redirectStart  \nDNS 解析耗时: domainLookupEnd - domainLookupStart  \nTCP 连接耗时: connectEnd - connectStart  \nSSL 安全连接耗时: connectEnd - secureConnectionStart  \n网络请求耗时 (TTFB): responseStart - requestStart  \n数据传输耗时: responseEnd - responseStart  \nDOM 解析耗时: domInteractive - responseEnd  \n资源加载耗时: loadEventStart - domContentLoadedEventEnd  \n首包时间: responseStart - domainLookupStart  \n白屏时间: responseEnd - fetchStart  \n首次可交互时间: domInteractive - fetchStart  \nDOM Ready 时间: domContentLoadEventEnd - fetchStart  \n页面完全加载时间: loadEventStart - fetchStart  \nhttp 头部大小： transferSize - encodedBodySize\n\n##### 3. Resource Timing API\n\nhttps://w3c.github.io/resource-timing/  \nperformance.getEntriesByType(\"resource\");\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184339.jpg)\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184340.jpg)\n\n| 123456 | // 某类资源的加载时间，可测量图片、js、css、XHRresourceListEntries.forEach(resource =\u003e {  if (resource.initiatorType == 'img') {  console.info(`Time taken to load ${resource.name}: `, resource.responseEnd - resource.startTime);  }}); |\n| ------ | ------------------------------------------------------------ |\n|        |                                                              |\n\n这个数据和 chrome 调式工具里 network 的瀑布图数据是一样的。\n\n##### 4. paint Timing API\n\nhttps://w3c.github.io/paint-timing/  \n首屏渲染时间、首次有内容渲染时间\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184341.jpg)\n\n##### 5. User Timing API\n\nhttps://www.w3.org/TR/user-timing-2/#introduction  \n主要是利用 mark 和 measure 方法去打点计算某个阶段的耗时，例如某个函数的耗时等。\n\n##### 6. High Resolution Time API\n\nhttps://w3c.github.io/hr-time/#dom-performance-timeorigin  \n主要包括 now() 方法和 timeOrigin 属性。\n\n##### 7. Performance Timeline API\n\nhttps://www.w3.org/TR/performance-timeline-2/#introduction\n\n#### 总结\n\n基于 performance 我们可以测量如下几个方面：  \nmark、measure、navigation、resource、paint、frame。\n\nlet p = window.performance.getEntries();  \n重定向次数：performance.navigation.redirectCount  \nJS 资源数量：p.filter(ele =\u003e ele.initiatorType === \"script\").length  \nCSS 资源数量：p.filter(ele =\u003e ele.initiatorType === \"css\").length  \nAJAX 请求数量：p.filter(ele =\u003e ele.initiatorType === \"xmlhttprequest\").length  \nIMG 资源数量：p.filter(ele =\u003e ele.initiatorType === \"img\").length  \n总资源数量: window.performance.getEntriesByType(\"resource\").length\n\n**不重复的耗时时段区分：**  \n重定向耗时: redirectEnd - redirectStart  \nDNS 解析耗时: domainLookupEnd - domainLookupStart  \nTCP 连接耗时: connectEnd - connectStart  \nSSL 安全连接耗时: connectEnd - secureConnectionStart  \n网络请求耗时 (TTFB): responseStart - requestStart  \nHTML 下载耗时：responseEnd - responseStart  \nDOM 解析耗时: domInteractive - responseEnd  \n资源加载耗时: loadEventStart - domContentLoadedEventEnd\n\n**其他组合分析：**  \n白屏时间: domLoading - fetchStart  \n粗略首屏时间: loadEventEnd - fetchStart 或者 domInteractive - fetchStart  \nDOM Ready 时间: domContentLoadEventEnd - fetchStart  \n页面完全加载时间: loadEventStart - fetchStart\n\n**JS 总加载耗时:**\n\n| 123  | const p = window.performance.getEntries();let cssR = p.filter(ele =\u003e ele.initiatorType === \"script\");Math.max(…cssR.map((ele) =\u003e ele.responseEnd)) - Math.min(…cssR.map((ele) =\u003e ele.startTime)); |\n| ---- | ------------------------------------------------------------ |\n|      |                                                              |\n\n**CSS 总加载耗时:**\n\n| 123  | const p = window.performance.getEntries();let cssR = p.filter(ele =\u003e ele.initiatorType === \"css\");Math.max(…cssR.map((ele) =\u003e ele.responseEnd)) - Math.min(…cssR.map((ele) =\u003e ele.startTime)); |\n| ---- | ------------------------------------------------------------ |\n|      |                                                              |\n\n### 如何监控？\n\n在了解了 performance 之后，我们来看看，具体是如何监控的？\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184342.jpg)\n\n总体流程：性能指标收集与数据上报—数据存储—数据聚合—分析展示—告警、报表推送\n\n这里主要讲述如何收集性能数据。  \n性能指标收集注意项：1）保证数据的准确性 2）尽量不影响应用的性能\n\n#### 1. 基本性能上报\n\n采集数据：将 performance navagation timing 中的所有点都上报，其余的上报内容可参考 performance 分析一节中截取部分上报。例如：白屏时间，JS 和 CSS 总数，以及加载总时长。  \n其余可参考的上报：是否有缓存？是否启用 gzip 压缩、页面加载方式。  \n在收集好性能数据后，即可将数据上报。  \n那选择什么时机上报？  \ngoogle 开发者推荐的上报方式：\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184343.jpg)\n\n#### 2. 首屏时间计算\n\n我们知道首屏时间是一项重要指标，但是又很难从 performance 中拿到，来看下首屏时间计算主要有哪些方式？  \nhttps://web.dev/first-meaningful-paint/  \n1）用户自定义打点—最准确的方式（只有用户自己最清楚，什么样的时间才算是首屏加载完成）  \n2）lighthouse 中使用的是 chrome 渲染过程中记录的 trace event  \n3）可利用 [Chrome DevTools Protocol](https://chromedevtools.github.io/devtools-protocol/) 拿到页面布局节点数目。思想是：获取到当页面具有最大布局变化的时间点  \n4）aegis 的方法：利用 MutationObserver 接口，监听 document 对象的节点变化。  \n检查这些变化的节点是否显示在首屏中，若这些节点在首屏中，那当前的时间点即为首屏渲染时间。但是还有首屏内图片的加载时间需要考虑，遍历 performance.getEntries() 拿到的所有图片实体对象，根据图片的初始加载时间和加载完成时间去更新首屏渲染时间。  \n5）利用 MutationObserver 接口提供了监视对 DOM 树所做更改的能力，是 DOM3 Events 规范的一部分。  \n方法：在首屏内容模块插入一个 div，利用 Mutation Observer API 监听该 div 的 dom 事件，判断该 div 的高度是否大于 0 或者大于指定值，如果大于了，就表示主要内容已经渲染出来，可计算首屏时间。  \n6）某个专利：在 loading 状态下循环判断当前页面高度是否大于屏幕高度，若大于，则获取到当前页面的屏幕图像，通过逐像素对比来判断页面渲染是否已满屏。https://patentimages.storage.googleapis.com/bd/83/3d/f65775c31c7120/CN103324521A.pdf\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/webperformance/20210410184344.jpg)\n\n#### 3. 异常上报\n\n1）js error  \n监听 window.onerror 事件  \n2）promise reject 的异常  \n监听 unhandledrejection 事件\n\n```javascript\nwindow.addEventListener(\"unhandledrejection\", function (event) {\n    console.warn(\"WARNING: Unhandled promise rejection. Shame on you! Reason: \"\n        + event.reason);\n});\n```\n\n3）资源加载失败  \nwindow.addEventListener('error')  \n4）网络请求失败  \n重写 window.XMLHttpRequest 和 window.fetch 捕获请求错误  \n5）iframe 异常  \nwindow.frames[0].onerror  \n6）window.console.error\n\n#### 4. CGI 上报\n\n大致原理：拦截 ajax 请求  \n数据存储与聚合  \n一个用户访问，可能会上报几十条数据，每条数据都是多维度的。即：当前访问时间、平台、网络、ip 等。这些一条条的数据都会被存储到数据库中，然后通过数据分析与聚合，提炼出有意义的数据。例如：某日所有用户的平均访问时长、pv 等。\n\n数据统计分析的方法：平均值统计法、百分位数统计法、样本分布统计法。\n","lastmodified":"2023-05-09T16:33:58.287366302Z","tags":[]},"/YAML-Front-matter":{"title":"YAML Front matter","content":"\n这可以说是能建立自动构建分类部署的最重要的东西了  \n每篇文章的开头都有被---包裹的 yaml 数据，而这两个三横杠包括的区域一般被称为 `Front-matter`。\n\n```yaml\n---\nkey: value\nkey2: value2\nmultiple: [one, two, three]\nmultiple:\n- one\n- two\n- three\n---\n你的所有参数都应当放在同一个 Front matter 中，例如其中的 Key、Key2、Key3 可以是多个参数名，如果是一个 Key 对应多个参数的话，就需要用 [ ] 将他们概括起来，或者用 - 来将它们从上到下排列。\n```\n\nObsidian 自带的 Front matter 主要只有两个：\n\n```yaml\n---\nalias: 别名1\naliases: [别名2, 别名3, 别名4]\naliases:\n- 别名2\n- 别名3\n- 别名4\n---\n```\n\n```yaml\n---\ntag: 标签1\ntags: [标签1, 标签2, 标签3] #还可以是\"#pkm\"等\ntags:\n- 标签1\n- 标签2\n- 标签3\n---\n```\n\n作用显而易见，而既然自动发布是和 hugo 相关的，那么了解 hugo 相关的 key 就很有必要了[URL Management | Hugo](https://gohugo.io/content-management/urls/#aliases)\n\n```yaml\n---\nslug: my-first-post \ntitle: \"My First Post\"\nurl: /articles/my-first-article\n---\n\n```\n","lastmodified":"2023-05-09T16:33:58.287366302Z","tags":["obsidian"]},"/browser":{"title":"browser","content":"\n### JS 的单线程\n\n很多人都知道的是，JavaScript 是一门**动态的解释型的语言**，具有**跨平台性**。在被问到 JavaScript 为什么是一门单线程的语言，有的人可能会这么回答：“语言特性决定了 JavaScript 是一个单线程语言，JavaScript 天生是一个单线程语言”，这只不过是一层糖衣罢了。\n\nJavaScript 从诞生起就是单线程，原因大概是不想让浏览器变得太复杂，因为多线程需要共享资源、且有可能修改彼此的运行结果，对于一种网页脚本语言来说，这就太复杂了。\n\n准确的来说，我认为 JavaScript 的单线程是指 **JavaScript 引擎是单线程**的，JavaScript 的引擎并不是独立运行的，跨平台意味着 JavaScript 依赖其运行的宿主环境 --- 浏览器(大部分情况下是浏览器)。\n\n浏览器需要渲染 DOM，JavaScript 可以修改 DOM 结构，JavaScript 执行时，浏览器 DOM 渲染停止。如果 JavaScript 引擎线程不是单线程的，那么可以同时执行多段 JavaScript，如果这多段 JavaScript 都操作 DOM，那么就会出现 DOM 冲突。\n\n举个例子来说，在同一时刻执行两个 script 对同一个 DOM 元素进行操作，一个修改 DOM，一个删除 DOM，那这样话浏览器就会懵逼了，它就不知道到底该听谁的，会有资源竞争，这也是 JavaScript 单线程的原因之一。\n\n### 浏览器的多线程\n\n之前说过，JavaScript 运行的宿主环境浏览器是多线程的。\n\n以 Chrome 来说，我们可以通过 Chrome 的任务管理器来看看。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172110.png)\n\n当你打开一个 Tab 页面的时候，就创建了一个进程。如果从一个页面打开了另一个页面，打开的页面和当前的页面属于同一站点的话，那么这个页面会复用父页面的渲染进程。\n\n### 浏览器主线程常驻线程\n\n1. GUI 渲染线程\n\n   - 绘制页面，解析 HTML、CSS，构建 DOM 树，布局和绘制等\n   - 页面重绘和回流\n   - 与 JS 引擎线程互斥，也就是所谓的 JS 执行阻塞页面更新\n\n2. JS 引擎线程\n\n   - 负责 JS 脚本代码的执行\n   - 负责准执行准备好待执行的事件，即定时器计数结束，或异步请求成功并正确返回的事件\n   - 与 GUI 渲染线程互斥，执行时间过长将阻塞页面的渲染\n\n3. 事件触发线程\n\n   - 负责将准备好的事件交给 JS 引擎线程执行\n   - 多个事件加入任务队列的时候需要排队等待(JS 的单线程)\n\n4. 定时器触发线程\n\n   - 负责执行异步的定时器类的事件，如 setTimeout、setInterval\n   - 定时器到时间之后把注册的回调加到任务队列的队尾\n\n5. HTTP 请求线程\n\n   - 负责执行异步请求\n    - 主线程执行代码遇到异步请求的时候会把函数交给该线程处理，当监听到状态变更事件，如果有回调函数，该线程会把回调函数加入到任务队列的队尾等待执行\n\n### Chrome 多进程架构的好处\n\n之前有提到，Chrome 用的是多进程的渲染方式，最容易想到的场景就是每个窗口（Tab）都有一个独立的渲染进程。假设你打开了三个浏览器窗口，当其中一个窗口因为某种原因崩掉的时候，你大可以直接关闭这个不再响应的窗口并继续你在其他窗口的工作。我们换一个浏览器，所有的窗口都共享同一个进程，当一个窗口挂掉的时候，所有的窗口都直接挂掉了（像不像理财的时候人们总是说：“不要把所有的钱放在同一个钱包里”？）。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172111.png)\n\n将浏览器的工作拆分成不同的进程还有一个好处，就是安全。由于操作系统提供了一种限制进程“权限”的方法，因此浏览器可以将特定的功能和进程有效的隔离开。比如，Chrome 会限制用来处理用户输入的渲染进程去直接访问文件。\n\n每个进程都有各自的内存空间，因此它们常会各自拥有一份基础功能的拷贝。正因为它们之间不像同一进程中的线程那样能够共享资源，所以就需要更多的内存占用。为了节省内存，Chrome 对其自身可调用的进程在数量上做了限制。具体的限制大小在不同性能的机器上各不相同，唯一确定的是，当达到了这个上限后，Chrome 会将同站点的多个窗口交给同一个进程来管理。\n\n### Chrome 服务化 —节省更多的内存\n\n浏览器进程也应用了相同的方案。Chrome 正在进行架构层面的整改，目的是将浏览器的各部分功能变成独立的服务，这样就能轻松的将其拆分为不同的进程，也能更加灵活的互相组合。\n\n总的来说，当 Chrome 在较高性能的设备上运行时，它会将每个服务分配至不同的进程，以此来获得更强的运行时稳定性和健壮性；反之，如果 Chrome 运行在一台资源受限的设备上时，Chrome 会将服务整合在一个进程中，以此来节省内存的占用。像这种通过整合进程资源以此来节省内存的手段，已经被用于 Android 上了。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172112.png)\n\n### Chrome站点隔离（Site Isolation）\n\n站点隔离是 Chrome 在其 67 桌面版上新增的特性，基本原则是不同的站点各自运行在自己的沙箱环境中，独享进程，并且不允许通信。我们已经讨论过每个窗口一个进程的模型，在这个模型中，浏览器允许跨站点的 iframe 独立进程共享不同站点之间的内存空间。早先在一个渲染进程中（窗口）同时运行 a.com 和 b.com 看起来没有什么问题，因为有同源策略，确保一个站点未经同意就无法访问其他站点的数据。绕过同源策略基本上成为了所有安全攻击的指导方针。而进程间的相互隔离是将站点分开的最佳途径（感兴趣的同学可以去了解一下 Meltdown 和 Spectre 攻击）。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172113.png)\n\n经过多年的工程上的努力，如今的站点隔离已经默认为用户开启了。事实上，站点隔离并不仅仅是为站点分配不同的渲染进程这么简单，它从根本上改变了 iframe 之间的通信方式。打开运行有不同站点 iframe 的开发者工具，意味着浏览器必须做很多看不到的幕后工作，才能让这一切看起来和以前没有什么区别，即使是简简单单的 ctrl+F 在这个场景下也意味着在不同的渲染进程中查询字符串。网上有很多文章介绍浏览器的站点隔离策略，当你看完那些之后就会意识到，为什么站点隔离值得 Chrome 团队为其发布一个独立版本了。\n\n### 输入URL后发生了什么\n\n输入一个 url，浏览器会从服务端获取数据并将页面展示出来。本文会聚焦在用户通过浏览器向一个站点发起访问请求以及浏览器准备渲染这个页面的部分，这个过程我称之为导航。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172114.png)\n\n我们在上一篇文章中提过，所有处于窗口之外的部分都由同一个浏览器进程进行掌管。浏览器的进程又同时拥有许多线程，掌管浏览器的不同部分：UI 线程用来绘制顶部的操作按钮和输入框、网络线程负责处理并接收来自互联网的数据、存储线程控制着访问本地文件的权限等。当你将一个网站的 url 输入到浏览器的地址栏时，此刻正是浏览器进程中的 UI 线程在起作用。\n\n##### Step 1：处理用户输入\n\n当用户开始在地址栏输入时，UI 线程首先会问：“大兄弟，你输入的是个查询字符串还是网站地址？”。因为 Chrome 的地址栏同时还是个搜索框，所以 UI 线程需要解析用户的输入，才能决定该直接访问网址还是把用户的输入丢给搜索引擎处理。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172115.png)\n\n##### Step 2：开始导航\n\n当用户按下回车键后，UI 线程要求网络线程去获取网站的内容。窗口的 Tab 上会开始转菊花，网络线程会采用一系列的协议和操作（比如 DNS）查询必要的信息并为请求建立连接。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172116.png)\n\n此时，网络线程可能会收到来自服务器的一个标记着重定向指令的头部比如 HTTP 301，在这种情况下，网络线程会把这件事情告诉 UI 线程，之后则会发起一次指向重定向地址的新的网络请求。\n\n##### Step 3：读取响应\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172116.png)\n\n当响应的数据开始传送到浏览器时，网络线程会在必要的情况下检查一些来自响应的字段。响应数据的 `Content-Type` 字段会表示当前返回的是哪种类型的数据，但它也不完全靠谱，经常会出现丢失或者干脆不准确的情况，但也不用担心，**MIME 嗅探**[3]会完成缺失的工作。正如**源码**[4]的注释中写道，这是一个可以被解释为 hack 的方案，如果感兴趣的话，你也可以去阅读这些注释，这样就能了解不同的浏览器是如何将实际的数据与 `Content-Type` 匹配了。\n\n如果响应数据是一个 HTML 文件，那么接下来的一步会是把数据传递给浏览器的渲染进程；但如果数据是 zip 压缩文件或其他类型的文件，意味着这将被定位成一次下载动作，于是浏览器会将数据转交给下载管理器去处理。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172117.png)\n\n通常这一步也是**安全检测**[5]发生的时候：如果域名或响应数据和已知的恶意网站匹配时，网络进程会抛出一个警告，并展现一个告警的页面。另外，**CORB**[6] 检测也会开始工作，确保那些来自敏感站点的跨站响应数据不会进入到浏览器的渲染进程中。\n\n##### Step 4：渲染进程\n\n网络线程以获取了全部的数据，并完成了所有需要的检查，此刻它自信的告诉 UI 线程：“小兄弟，数据准备好了！”。接着，UI 线程会唤起一个渲染进程去渲染页面。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172117.png)\n\n由于网络情况的不可控，一个请求可能会花上好几百毫秒才能把响应数据拿回来，所以这里浏览器默认开启了用来加速这一过程的优化。在 Step 2 中，当 UI 线程将需要请求的 url 告诉网络线程时，其实它本身已经知道要导航到哪个网站了，于是 UI 线程在把 url 传递给网络线程的同时，会尝试启动一个渲染进程。如果一切都按照预期正常进行的话，当网络线程拿到数据时，渲染进程就已经处于待命状态了。也会有例外的情况：比如导航重定向到一个另外的站点，那么预先启动好的渲染进程将不会被使用，这导致 UI 线程需要重新启动一个渲染进程。\n\n##### Step 5：触发导航\n\n现在我们假设数据和渲染进程都准备好了，浏览器进程通过 IPC 告知渲染进程可以出发本次导航了。与此同时，数据流也将传递给渲染进程，这样后者就能继续接收 HTML 数据。一旦浏览器收到了来自渲染进程的导航启动信号，这次导航也就完成了，下一步进入文档的加载阶段。\n\n到这会儿，浏览器的地址栏更新，安全指示符和站点的设置 UI 会将新页面的信息呈现出来。当前窗口的 session 将会更新，刚导航到的页面会被后退/前进按钮记录到窗口的页面历史中。为了便于在关闭窗口时恢复页面，历史的会话记录会保存在本地的磁盘上。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172118.png)\n\n##### Extra Step：初始加载完成\n\n当导航触发后，渲染进程会持续接收资源并渲染页面。我们将在下一篇文章中讨论这一步的更多细节。当渲染进程“完成”渲染后，它会通过 IPC 告知浏览器进程（页面的 onload 事件均已执行完毕后），UI 线程也就不再在 tab 上转菊花了。\n\n上面的“完成”两个字，之所以打了双引号，因为在实际场景中，它通常并不真正意味着完成，因为客户端的 JavaScript 可能在此时持续地加载资源并渲染新的视图。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172119.png)\n\n##### 导航到另一个网站\n\n一次简单的导航截至目前已经完成了。假如这时用户输入了一个不同的 url 会发生什么呢？其实也没啥，浏览器进程会按照上面的步骤导航到这个网站。但在这一切开始之前，浏览器会检查当前已经渲染好了的网站是否需要在网页卸载之前搞一点事情，这就是 `beforeunload` 事件。\n\n在 `beforeunload` 事件中，我们可以在用户即将跳转至其他页面或者关闭 Tab 的时候发起一个“确认离开当前页面？”的二次确认。Tab 中的所有东西都由渲染进程控制着，当然也包括开发者编写的 JavaScript，所以当一个新的导航请求即将到来时，浏览器进程会对当前的渲染进程做最后的检查。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172120.png)\n\n我们应当尽量避免在 `beforeunload` 中添加总会执行的事件代码，这会造成更多的交互延时，毕竟它们总会在新的导航开始之前执行。**只在需要的时候添加这些代码**，比如提醒用户如果进入新的页面那么当前页面的数据会丢失。\n\n如果导航是在渲染进程中被创建的（比如用户点击了页面上的某一链接或者在 JavaScript 运行了 `window.location.href = 'https://kyrieliu.cn'` ），则当前的渲染进程会首先检查是 `beforeunload` 中是否有东西需要执行。之后，它会经历与浏览器进程直接发起导航后一样的导航过程。\n\n当新的导航将发往与当前页面不同的站点时，浏览器将会创建一个新的渲染进程去处理这些新工作，旧的渲染进程则则用来在剩余的时间里处理诸如 `unload` 的页面事件。如果你想了解更多的话，可以看看**页面生命周期概览**[7]和**页面生命周期 API**[8]这两篇文章。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172121.png)\n\n##### 如果有 Service Worker…\n\n**Service Worker**[9] 的引入会对页面的导航流程带来一些改变。Service Worker 是一种可以在应用代码中编写网络代理的方法；增强了开发者对于本地缓存以及何时发起网络请求的控制。如果 Service Worker 提前设置了从本地缓存中读取某一页面的数据，那么也就不需要发起网络请求了。\n\n需要明确的一点是，即使 Service Worker 提供了听起来很高端的功能，但它实质上也是运行在渲染进程中的 JavaScript 代码。那么问题来了：当用户发起一次导航时，浏览器进程是如何知道目标站点存在一个 Service Worker 的呢？\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172122.png)\n\n当一个 Service Worker 注册后，它的作用域会保存在一个引用中（你可以通过 **Service Worker 的生命周期**[10] 这篇文章了解我所说的“作用域”）。当导航发生时，网络线程会依据域名在已注册的 Service Worker 作用域集合中查询，如果找到某个对应的 Service Worker，UI 线程会发起一个渲染进程去执行 Service Worker 中的代码。Service Worker 可以从本地缓存中加载数据（无需发起网络请求），也可以选择通过网络请求获取最新的资源和数据。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172123.png)\n\n##### 导航预加载\n\n相信你可以发现，如果 Service Worker 最终决定从网络中请求数据，那么之前在浏览器进程和渲染进程之间所发生的通信都将成为导致响应延时的罪魁祸首。**导航预加载**[11]就是用来加速这一进程的机制：与 Service Worker 并行启动去加载资源。它将为这些请求设置一个 Header，由服务端来决定为这些请求发送不同的内容；比如，仅返回更新的数据而不是整个文档。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172124.png)\n\n### 浏览器端的 Event Loop\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172125.png)\n\n上图是一张 JS 的运行机制图，Js 运行时大致会分为几个部分：\n\n1. Call Stack：调用栈(执行栈)，所有同步任务在主线程上执行，形成一个执行栈，因为 JS 单线程的原因，所以调用栈中每次只能执行一个任务，当遇到的同步任务执行完之后，由任务队列提供任务给调用栈执行。\n2. Task Queue：任务队列，存放着异步任务，当异步任务可以执行的时候，任务队列会通知主线程，然后该任务会进入主线程执行。任务队列中的都是已经完成的异步操作，而不是说注册一个异步任务就会被放在这个任务队列中。\n\n说到这里，Event Loop 也可以理解为：不断地从任务队列中取出任务执行的一个过程。\n\n#### 同步任务和异步任务\n\n上文已经说过了 JavaScript 是一门单线程的语言，一次只能执行一个任务，如果所有的任务都是同步任务，那么程序可能因为等待会出现假死状态，这对于一个用户体验很强的语言来说是非常不友好的。\n\n比如说向服务端请求资源，你不可能一直不停的循环判断有没有拿到数据，就好像你点了个外卖，点完之后就开始一直打电话问外卖有没有送到，外卖小哥都会抄着锅铲来打你(狗头)。因此，在 JavaScript 中任务有了同步任务和异步任务，异步任务通过注册回调函数，等到数据来了就通知主程序。\n\n1. 同步任务：必须等到结果来了之后才能做其他的事情，举例来说就是你烧水的时候一直等在水壶旁边等水烧开，期间不做其他的任何事情。\n2. 异步任务：不需要等到结果来了才能继续往下走，等结果期间可以做其他的事情，结果来了会收到通知。举例来说就是你烧水的时候可以去做自己想做的事情，听到水烧开的声音之后再去处理。\n\n从概念就可以看出来，异步任务从一定程度上来看比同步任务更高效一些，核心是提高了用户体验。\n\n#### Event Loop\n\nEvent Loop 很好的调度了任务的运行，宏任务和微任务也知道了，现在我们就来看看它的调度运行机制。\n\nJavaScript 的代码执行时，主线程会从上到下一步步的执行代码，同步任务会被依次加入执行栈中先执行，异步任务会在拿到结果的时候将注册的回调函数放入任务队列，当执行栈中的没有任务在执行的时候，引擎会从任务队列中读取任务压入执行栈(Call Stack)中处理执行。\n\n#### 宏任务和微任务\n\n现在就有一个问题了，任务队列是一个消息队列，先进先出，那就是说，后来的事件都是被加在队尾等到前面的事件执行完了才会被执行。如果在执行的过程中突然有重要的数据需要获取，或是说有事件突然需要处理一下，**按照队列的先进先出顺序这些是无法得到及时处理的。这个时候就催生了宏任务和微任务，微任务使得一些异步任务得到及时的处理**。\n\n曾经看到的一个例子很好，宏任务和微任务形象的来说就是：你去营业厅办一个业务会有一个排队号码，当叫到你的号码的时候你去窗口办充值业务(宏任务执行)，在你办理充值的时候你又想改个套餐(微任务)，这个时候工作人员会直接帮你办，不可能让你重新排队。\n\n所以上文说过的异步任务又分为宏任务和微任务，JS 运行时任务队列会分为宏任务队列和微任务队列，分别对应宏任务和微任务。\n\n先介绍一下(浏览器环境的)宏任务和微任务大致有哪些：\n\n- 宏任务：\n- 1. script(整体的代码)\n  2. setTimeout\n  3. setInterval\n  4. I/O 操作\n  5. UI 渲染 (对这个笔者持保留意见)\n\n- 微任务：\n- 1. Promise.then\n  2. MutationObserver\n\n#### 事件运行顺序\n\n1. 执行同步任务，同步任务不需要做特殊处理，直接执行(下面的步骤中遇到同步任务都是一样处理) --- 第一轮从 script开始\n2. 从宏任务队列中取出队头任务执行\n3. 如果产生了宏任务，将宏任务放入宏任务队列，下次轮循的时候执行\n4. 如果产生了微任务，将微任务放入微任务队列\n5. 执行完当前宏任务之后，取出微任务队列中的所有任务依次执行\n6. 如果微任务执行过程中产生了新的微任务，则继续执行微任务，直到微任务的队列为空\n7. 轮循，循环以上 2 - 6\n\n总的来说就是：同步任务/宏任务 -\u003e 执行产生的所有微任务(包括微任务产生的微任务) -\u003e 同步任务/宏任务 -\u003e 执行产生的所有微任务(包括微任务产生的微任务) -\u003e 循环……\n\n注意：微任务队列\n\n#### 举个栗子\n\n光说不练假把式，现在就来看一个例子：\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172126.png)举个栗子\n\n放图的原因是为了让大家在看解析之前可以先自己按照运行顺序走一遍，写好答案之后再来看解析。  \n解析：  \n(用绿色的表示同步任务和宏任务，红色表示微任务)\n\n```javascript\n+  console.log('script start')\n+  setTimeout(function() {\n+    console.log('setTimeout')\n+  }, 0)\n+  new Promise((resolve, reject)=\u003e{\n+    console.log(\"promise1\")\n+    resolve()\n+  })\n-  .then(()=\u003e{\n-    console.log(\"then11\")\n+    new Promise((resolve, reject)=\u003e{\n+      console.log(\"promise2\")\n+      resolve();\n+    })\n-    .then(() =\u003e {\n-      console.log(\"then2-1\")\n-    })\n-    .then(() =\u003e {\n-      console.log(\"then2-2\")\n-    })\n-  })\n-  .then(()=\u003e{\n-    console.log(\"then12\")\n-  })\n+  console.log('script end')\n```\n\n1. 首先遇到 console.log()，输出 `script start`\n2. 遇到 setTimeout 产生宏任务，注册到**宏任务队列[setTimeout]**，下一轮 Event Loop 的时候在执行\n3. 然后遇到 new Promise 构造声明(同步)，log 输出 `promise1`，然后 resolve\n4. resolve 匹配到 **promise1 的第一个 then**，把这个 then 注册到**微任务队列[then11]中**，继续当前整体脚本的执行\n5. 遇到最后的一个 log，输出 `script end`，**当前执行栈清空**\n6. **从微任务队列中取出队头任务'then11'** 进行执行，其中有一个 log，输出 `then11`\n7. 往下遇到 new Promise 构造声明(同步)，log 输出 `promise2`，然后 resolve\n8. resolve 匹配到 **promise2 的第一个 then**，把这个 then 注册到**微任务队列[then2-1]**，当前 then11 可执行部分结束，然后产生了 **promise1 的第二个 then**，把这个 then 注册到**微任务队列[then2-1, then12]**\n9. **拿出微任务队头任务'then2-1'** 执行，log 输出 `then2-1`，触发 **promise2 的第二个 then**，注册到**微任务队列[then12, then2-2]**\n10. **拿出微任务队头任务'then12'**，log 输出 `then12`\n11. **拿出微任务队头任务'then2-2'**，log 输出 `then2-2`\n12. 微任务队列执行完毕，别忘了宏任务队列中的 setTimeout，log 输出 `setTimeout`\n\n经过以上一番缜(xia)密(gao)分析，希望没有绕晕你，最后的输出结果就是：  \n`script start -\u003e promise1 -\u003e script end -\u003e then11 -\u003e promise2 -\u003e then2-1 -\u003e then12 -\u003e then2-2 -\u003e setTimeout`\n\n#### 宏任务？微任务？\n\n不知道大家看了宏任务和微任务之后会不会有一个疑惑，宏任务和微任务都是异步任务，微任务之前说过了是为了及时解决一些必要事件而产生的。\n\n- 为什么要有微任务？  \n  为什么要有微任务的原因前面已经说了，这里就不再赘述，简单说一下就是为了及时处理一些任务，不然等到最后再执行的时候拿到的数据可能已经是被污染的数据达不到预期目标了。\n\n- 是什么宏任务？什么是微任务？  \n  相信大家在学习 Event Loop 查找资料的时候，肯定各种资料里面都会讲到宏任务和微任务，但是不知道你有没有灵魂拷问过你自己：`什么是宏任务？什么是微任务？怎么区分宏任务和微任务？`不能只是默许接受这个概念，在这里，我根据我的个人理解进行一番说(hu)明(che)\n\n- 宏任务和微任务的真面目  \n  其实在 Chrome 的源码中并没有什么宏任务和微任务的代码或是说明，在 **JS 大会**[3]上提到过微任务这个名词，但是也没有说到底什么是微任务。\n\n  宏任务  \n  文章最开始的时候说过，在 chrome 里，每个页面都对应一个进程。而该进程又有多个线程，比如 JS 线程、渲染线程、IO 线程、网络线程、定时器线程等等，这些线程之间的通信是通过向对象的任务队列中添加一个任务（postTask）来实现的。**宏任务的本质可以认为是多线程事件循环或消息循环，也就是线程间通信的一个消息队列。**\n\n  就拿 setTimeout 举例来说，当遇到它的时候，浏览器就会对 Event Loop 说：嘿，我有一个任务交给你，Event Loop 就会说：好的，我会把它加到我的 todoList 中，之后我会执行它，它是需要调用 API 的。\n\n  **宏任务的真面目是浏览器派发，与 JS 引擎无关的，参与了 Event Loop 调度的任务**\n\n  微任务  \n  微任务是在运行宏任务/同步任务的时候产生的，是属于当前任务的，所以它不需要浏览器的支持，内置在 JS 当中，直接在 JS 的引擎中就被执行掉了。\n\n#### 特殊的点\n\n1. async 隐式返回 Promise 作为结果\n2. 执行完 await 之后直接跳出 async 函数，让出执行的所有权\n3. 当前任务的其他代码执行完之后再次获得执行权进行执行\n4. 立即 resolve 的 Promise 对象，是在本轮\"事件循环\"的结束时执行，而不是在下一轮\"事件循环\"的开始时\n\n#### 再举个栗子\n\n```javascript\nconsole.log('script start')\n\n  async function async1() {\n      await async2()\n      console.log('async1 end')\n  }\n  async function async2() {\n      console.log('async2 end')\n  }\n  async1()\n\n  setTimeout(function() {\n      console.log('setTimeout')\n  }, 0)\n\n  newPromise(resolve =\u003e {\n      console.log('Promise')\n      resolve()\n  })\n  .then(function() {\n      console.log('promise1')\n  })\n  .then(function() {\n      console.log('promise2')\n  })\n\n  console.log('script end')\n```\n\n按照之前的分析方法去分析之后就会得出一个结果：  \n`script start =\u003e async2 end =\u003e Promise =\u003e script end =\u003e promise1 =\u003e promise2 =\u003e async1 end =\u003e setTimeout`\n\n可以看出 async1 函数获取执行权是作为微任务的队尾，但是，在 Chrome73(金丝雀) 版本之后，async 的执行优化了，它会在 promise1 和 promise2 的输出之前执行。笔者大概了解了一下应该是用 PromiseResolve 对 await 进行了优化，减少了 Promise 的再次创建，有兴趣的小伙伴可以看看 Chrome 的源码。\n\n### Node 中的 Event Loop\n\nNode 中也有宏任务和微任务，与浏览器中的事件循环类似。Node 与浏览器事件循环不同，其中有多个宏任务队列，而浏览器是只有一个宏任务队列。\n\nNode 的架构底层是有 libuv，它是 Node 自身的动力来源之一，通过它可以去调用一些底层操作，Node 中的 Event Loop 功能就是在 libuv 中封装实现的。\n\n#### 宏任务和微任务\n\nNode 中的宏任务和微任务在浏览器端的 JS 相比增加了一些，这里只列出浏览器端没有的：\n\n- 宏任务\n- 1. setImmediate\n- 微任务\n- 1. process.nextTick\n\n#### **事件循环机制的六个阶段**\n\n![image-20200402223627057](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172127.png)\n\nNode 的事件循环分成了六个阶段，每个阶段对应一个宏任务队列，相当于是宏任务进行了一个分类。\n\n1. timers(计时器)  \n   执行 setTimeout 以及 setInterval 的回调\n2. I/O callbacks  \n   处理网络、流、TCP 的错误回调\n3. idel, prepare --- 闲置阶段  \n   node 内部使用\n4. poll(轮循)  \n   执行 poll 中的 I/O 队列，检查定时器是否到时间\n5. check(检查)  \n   存放 setImmediate 回调\n6. close callbacks  \n   关闭回调，例如 sockect.on('close')\n\n#### 轮循顺序\n\n执行的轮循顺序 --- 每个阶段都要等对应的宏任务队列执行完毕才会进入到下一个阶段的宏任务队列\n\n1. timers\n2. I/O callbacks\n3. poll\n4. setImmediate\n5. close events\n\n每两个阶段之间执行微任务队列\n\n#### Event Loop 过程\n\n1. 执行全局的 script 同步代码\n2. 执行微任务队列，先执行所有 Next Tick 队列中的所有任务，再执行其他的微任务队列中的所有任务\n3. 开始执行宏任务，共六个阶段，从第一个阶段开始执行自己宏任务队列中的所有任务(浏览器是从宏任务队列中取第一个执行！！)\n4. 每个阶段的宏任务执行完毕之后，开始执行微任务\n5. TimersQueue -\u003e 步骤2 -\u003e I/O Queue -\u003e 步骤2 -\u003e Check Queue -\u003e 步骤2 -\u003e Close Callback Queue -\u003e 步骤2 -\u003e TimersQueue …\n\n这里要注意的是，nextTick 事件是一个单独的队列，它的优先级会高于微任务，所以在当前宏任务/同步任务执行完成之后，会先执行 nextTick 队列中的所有任务，再去执行微任务队列中的所有任务。\n\n### Node 与浏览器的 Event Loop 差异\n\n**浏览器环境下，microtask 的任务队列是每个 macrotask 执行完之后执行。而在 Node.js 中，microtask 会在事件循环的各个阶段之间执行，也就是一个阶段执行完毕，就会去执行 microtask 队列的任务**。\n\n[![img](Browser.assets/2019-01-14-006.png)](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172128.png)\n\n接下我们通过一个例子来说明两者区别：\n\n```javascript\nsetTimeout(()=\u003e{\n    console.log('timer1')\n    Promise.resolve().then(function() {\n        console.log('promise1')\n    })\n}, 0)\nsetTimeout(()=\u003e{\n    console.log('timer2')\n    Promise.resolve().then(function() {\n        console.log('promise2')\n    })\n}, 0)\n```\n\n浏览器端运行结果：`timer1=\u003epromise1=\u003etimer2=\u003epromise2`\n\n浏览器端的处理过程如下：\n\n[![img](Browser.assets/2019-01-14-007.gif)](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172129.gif)\n\nNode 端运行结果：`timer1=\u003etimer2=\u003epromise1=\u003epromise2`\n\n- 全局脚本（main()）执行，将 2 个 timer 依次放入 timer 队列，main()执行完毕，调用栈空闲，任务队列开始执行；\n- 首先进入 timers 阶段，执行 timer1 的回调函数，打印 timer1，并将 promise1.then 回调放入 microtask 队列，同样的步骤执行 timer2，打印 timer2；\n- 至此，timer 阶段执行结束，event loop 进入下一个阶段之前，执行 microtask 队列的所有任务，依次打印 promise1、promise2\n\nNode 端的处理过程如下：\n\n[![img](Browser.assets/2019-01-14-008.gif)](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172130.gif)\n\n浏览器和 Node 环境下，microtask 任务队列的执行时机不同\n\n- Node 端，microtask 在事件循环的各个阶段之间执行\n- 浏览器端，microtask 在事件循环的 macrotask 执行完之后执行\n\n### setTimeout 和 setImmediate\n\n在这里要单独说一下 setTimeout 和 setImmediate，setTimeout 定时器很熟悉，那就说说 setImmediate\n\nsetImmediate() 方法用于把一些需要长时间运行的操作放在一个回调函数里，并在浏览器完成其他操作（如事件和显示更新）后立即运行回调函数。从定义来看就是为了防止一些耗时长的操作阻塞后面的操作，这也是为什么 check 阶段运行顺序排的比较后。\n\n#### 举个栗子\n\n我们来看这样的一个例子：\n\n```\nsetTimeout(() =\u003e {\n  console.log('setTimeout')\n}, 0)\n\nsetImmediate(() =\u003e {\n  console.log('setImmediate')\n})\n```\n\n这里涉及 timers 阶段和 check 阶段，按照上面的运行顺序来说，timers 阶段是在第一个执行的，会早于 check 阶段。运行这段程序可以看到如下的结果：\n\n![image-20200402224016596](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172131.png)\n\n可是再多运行几次，你就会看到如下的结果：\n\n![image-20200402224028075](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172132.png)\n\nsetImmediate 的输出跑到 setTimeout 前面去了，这时候就是：小朋友你是否有很多的问号❓\n\n#### 分析\n\n我们来分析一下原因，timers 阶段确实是在 check 阶段之前，但是在 timers 阶段时候，这里的 setTimeout 真的到了执行的时间吗？\n\n这里就要先看看 `setTiemout(fn, 0)`，这个语句的意思不是指不延迟的执行，而是指在可以执行 setTimeout 的时候就立即执行它的回调，也就是处理完当前事件的时候立即执行回调。\n\n在 Node 中 setTimeout 第二个时间参数的最小值是 1ms，小于 1ms 会被初始化为 1(浏览器中最小值是 4ms)，所以在这里 `setTimeout(fn, 0) === setTimeout(fn, 1)`\n\nsetTimeout 的回调函数在 timers 阶段执行，setImmediate 的回调函数在 check 阶段执行，Event Loop 的开始会先检查 timers 阶段，但是在代码开始运行之前到 timers 阶段(代码的启动、运行)会消耗一定的时间，所以会出现两种情况：\n\n1. timers 前的准备时间超过 1ms，满足 loop -\u003e timers \u003e= 1，setTimeout 的时钟周期到了，则执行 timers 阶段(setTimeout)的回调函数\n2. timers 前的准备时间小于 1ms，还没到 setTimeout 预设的时间，则先执行 check 阶段(setImmediate)的回调函数，下一次 Event Loop 再进入 timers 阶段执行 timer 阶段(setTimeout)的回调函数\n\n最开始就说了，一个优秀的程序员要让自己的代码按照自己想要的顺序运行，下面我们就来控制一下 setTimeout 和 setImediate 的运行。\n\n- 让 setTimeout 先执行  \n  上面代码运行顺序不同无非就是因为 Node 准备时间的不确定性，我们可以直接手动延长准备时间👇\n\n```\nconst start = Date.now()\n  while (Date.now() - start \u003c 10)\n  setTimeout(() =\u003e {\n  console.log('setTimeout')\n  }, 0)\n\n  setImmediate(() =\u003e {\n    console.log('setImmediate')\n  })\n```\n\n- 让 setImmediate 先执行  \n  setImmediate 是在 check 阶段执行，相对于 setTimeout 来说是在 timers 阶段之后，只需要想办法把程序的运行环境控制在 timers 阶段之后就可以了。\n\n  让程序至少从 I/O callbacks 阶段开始 --- 可以套一层文件读写把把程序控制在 I/O callbacks 阶段的运行环境中👇\n\n```\nconst fs = require('fs')\n\nfs.readFile(__dirname, () =\u003e {\n  setTimeout(() =\u003e {\n    console.log('setTimeout')\n  }, 0)\n  \n  setImmediate(() =\u003e {\n    console.log('setImmediate')\n  })\n})\n```\n\n### Node 11.x 的变化\n\ntimers 阶段的执行有所变化\n\n```\nsetTimeout(() =\u003econsole.log('timeout1'))\nsetTimeout(() =\u003e {\n console.log('timeout2')\n Promise.resolve().then(() =\u003econsole.log('promise resolve'))\n})\n```\n\n1. node 10 及之前的版本：  \n   要考虑上一个定时器执行完成时，下一个定时器是否到时间加入了任务队列中，如果未到时间，先执行其他的代码。  \n   比如：  \n   timer1 执行完之后 timer2 到了任务队列中，顺序为 `timer1 -\u003e timer2 -\u003e promise resolve`  \n   timer2 执行完之后 timer2 还没到任务队列中，顺序为 `timer1 -\u003e promise resolve -\u003e timer2`\n2. node 11 及其之后的版本：  \n   `timeout1 -\u003e timeout2 -\u003e promise resolve`  \n   一旦执行某个阶段里的一个宏任务之后就立刻执行微任务队列，这和浏览器端运行是一致的。\n\n### 小结\n\nNode 和端浏览器端有什么不同\n\n1. 浏览器端的 Event Loop 和 Node.js 中的 Event Loop 是不同的，实现机制也不一样\n2. Node.js 可以理解成有4个宏任务队列和2个微任务队列，但是执行宏任务时有6个阶段\n3. Node.js 中限制性全局 script 代码，执行完同步代码后，先从微任务队列 Next Tick Queue 中取出所有任务放入调用栈执行，再从其他微任务队列中取出所有任务放入调用栈中执行，然后开始宏任务的6个阶段，每个阶段都将其宏任务队列中的所有任务都取出来执行(浏览器是只取第一个执行)，每个宏任务阶段执行完毕之后开始执行微任务，再开始执行下一阶段宏任务，以此构成事件循环\n4. 宏任务包括 ….\n5. 微任务包括 ….\n\n看到这里，你应该对浏览器端和 Node 端的 Event Loop 有了一定的了解，那就留一个题目。\n\n![image-20200402224502960](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172133.png)\n\n不直接放代码是想让大家先自己思考然后在敲代码运行一遍\n\n## 浏览器存储\n\n### cookies\n\n#### 多种浏览器存储方式并存，如何选择？\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172134)\n\n- 因为`http`请求无状态，所以需要`cookie`去维持客户端状态\n- cookie的生成方式：\n  - `http`--\u003e`response header`--\u003e`set-cookie`\n- `js`中可以通过`document.cookie`可以读写`cookie`\n- cookie的使用用处：\n  - 用于浏览器端和服务器端的交互(用户状态)\n  - 客户端自身数据的存储\n- `expire`：过期时间\n- cookie的限制：\n  - 作为浏览器存储，大小`4kb`左右\n  - 需要设置过期时间 `expire`\n- 重要属性：`httponly` 不支持`js`读写(防止收到模拟请求攻击)\n- 不太作为存储方案而是用于维护客户关系\n- 优化点：cookie中在相关域名下面：\n  - `cdn`的流量损耗\n  - 解决方案：`cdn`的域名和主站域名要分开\n\n**API**\n\n服务端向客户端发送的cookie(HTTP头,不带参数)：  \n`Set-Cookie: =` (name可选)\n\n服务端向客户端发送的cookie(HTTP头，带参数)：  \n`Set-Cookie: =;(可选参数1);(可选参数2)`\n\n客户端设置cookie：\n\n```\ndocument.cookie = \"\u003ccookie-name\u003e=\u003ccookie-value\u003e;(可选参数1);(可选参数2)\"\n```\n\n可选参数：  \n`Expires=`：cookie的最长有效时间，若不设置则cookie生命期与会话期相同\n\n`Max-Age=`：cookie生成后失效的秒数\n\n`Domain=`：指定cookie可以送达的主机域名，若一级域名设置了则二级域名也能获取。\n\n`Path=`：指定一个URL，例如指定path=/docs，则”/docs”、”/docs/Web/“、”/docs/Web/Http”均满足匹配条件\n\n`Secure`：必须在请求使用SSL或HTTPS协议的时候cookie才回被发送到服务器\n\n`HttpOnly`：客户端无法更改Cookie，客户端设置cookie时不能使用这个参数，一般是服务器端使用\n\n示例：\n\n```\nSet-Cookie: sessionid=aes7a8; HttpOnly; Path=/\n\ndocument.cookie = \"KMKNKK=1234;Sercure\"\n```\n\n可选前缀：  \n`__Secure-`：以`__Secure-`为前缀的cookie，必须与secure属性一同设置，同时必须应用于安全页面（即使用HTTPS）\n\n`__Host-`：以`__Host-`为前缀的cookie，必须与secure属性一同设置，同时必须应用于安全页面（即使用HTTPS）。必须不能设置domian属性（这样可以防止二级域名获取一级域名的cookie），path属性的值必须为”/“。\n\n前缀使用示例：\n\n```\nSet-Cookie: __Secure-ID=123; Secure; Domain=example.com\nSet-Cookie: __Host-ID=123; Secure; Path=/\n\ndocument.cookie = \"__Secure-KMKNKK=1234;Sercure\"\ndocument.cookie = \"__Host-KMKNKK=1234;Sercure;path=/\"\n```\n\n### local storage\n\n- `HTML5`设计出来专门用于浏览器存储的\n- 大小为`5M`左右\n- 仅在客户端使用，不和服务端进行通信\n- 接口封装较好\n- 浏览器本地缓存方案\n\n**API**\n\n```javascript\n//sessionStorage用法相同\nlocalStorage.setItem(\"name\",1);   // 以\"x\"为名字存储一个数值\nlocalStorage.getItem(\"name\");     // 获取数值\nlocalStorage.key(i);              // 获取第i对的名字\nlocalStorage.removeItem(\"name\");  // 获取该对的值\nlocalStorage.clear();             // 全部删除\n```\n\n### session storage\n\n- 会话级别的浏览器存储\n- 大小为`5M`左右\n- 仅在客户端使用，不和服务器端进行通信\n- 接口封装较好\n- 对于表单信息的维护\n\n### indexedDB\n\n- `IndexedDB`是一种低级`API`，用于客户端存储大量结构化数据。该`API`使用索引来实现对该数据的高性能搜索。虽然`Web`\n- `Storage`对于存储叫少量的数据很管用，但对于存储更大量的结构化数据来说，这种方法不太有用。`IndexedDB`提供了一个解决方案。\n\n```\n为应用创建离线版本\n```\n\n- `cdn`域名不要带`cookie`\n- `localstorage`存库、图片\n\n`cookie`种在主站下，二级域名也会携带这个域名，造成流量的浪费\n\n### Service Worker产生的意义\n\n### PWA与Service Worker\n\n- `PWA`(`Progressive Web Apps`)是一种`Web App`新模型，并不是具体指某一种前言的技术或者某一个单一的知识点，我们从英文缩写来看就能看出来，这是一个渐进式的`Web App`，是通过一系列新的`Web特性`，配合优秀的`UI`交互设计，逐步增强`Web App`的用户体验\n\n#### chrom插件lighthouse\n\n\u003e 检测是不是一个渐进式`web app`\n\n- 当前手机在弱网环境下能不能加载出来\n- 离线环境下能不能加载出来\n\n\u003e 特点\n\n- 可靠：没有网络的环境中也能提供基本的页面访问，而不会出现“未连接到互联网”的页面\n- 快速：针对网页渲染及网络数据访问有较好的优化\n- 融入(`Engaging`)：应用可以被增加到手机桌面，并且和普通应用一样有全屏、推送等特性\n\n#### service worker\n\n\u003e `service worker`是一个脚本，浏览器独立于当前页面，将其在后台运行，为实现一些不依赖页面的或者用户交互的特性打开了一扇大门。在未来这些特性将包括消息推送，背景后台同步，`geofencing`(地理围栏定位)，但他将推出的第一个首要的特性，就是拦截和处理网络请求的能力，包括以编程方式来管理被缓存的响应。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/notes/frontend/browser/20210410172135)\n\n#### 案例分析\n\n[Service Worker学习与实践](https://juejin.im/post/5ba0fe356fb9a05d2c43a25c)\n\n[了解servie worker](http://kailian.github.io/2017/03/01/service-worker#🐉)\n\n```\nchrome://serviceworker-internals/`  \n `chrome://inspect/#service-worker/\n```\n\n`service worker`网络拦截能力，存储`Cache Storage`，实现离线应用\n\n#### Service Worker离线应用\n\n`serviceworker`需要`https`协议\n\n#### 如何实现ServiceWorker与主页面之间的通信\n\n[lavas](\n","lastmodified":"2023-05-09T16:33:58.287366302Z","tags":[]},"/css":{"title":"css","content":"\n## 1. 怎么让一个 div 水平垂直居中\n\n```html\n\u003cdiv class=\"parent\"\u003e\n  \u003cdiv class=\"child\"\u003e\u003c/div\u003e\n\u003c/div\u003e\n```\n\n- 简单粗暴\n\n  ```css\n  div.parent {\n    background-color:red;\n    height:300px;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n  }\n  div.child {\n    background-color:blue;\n    height:50px;\n    width:20%;\n  }\n  ```\n\n- ```css\n  div.parent {\n      background-color:red;\n    \theight:300px;\n      position: relative; \n  }\n  div.child {\n      background-color:blue;\n    \theight:50px;\n    \twidth:20%;\n      position: absolute; \n      top: 50%;\n      left: 50%;\n      transform: translate(-50%, -50%);  \n      //主要原理是先top后child位于父亲的150-200高度，left后位于50%-70%宽度，再自身transform50%后,就能向左上平移半个自己的身位，正好居中\n  }\n  ```\n- ```csharp\n  div.parent {\n      display: grid;\n  }\n  div.child {\n      justify-self: center;\n      align-self: center;\n  }\n  ```\n\n## 2. 分析比较 opacity: 0、visibility: hidden、display: none 优劣和适用场景。\n\n- `display: none;`\n\n1. **DOM 结构**：浏览器不会渲染 `display` 属性为 `none` 的元素，不占据空间；\n2. **事件监听**：无法进行 DOM 事件监听；\n3. **性能**：动态改变此属性时会引起重排，性能较差；\n4. **继承**：不会被子元素继承，毕竟子类也不会被渲染；\n5. **transition**：`transition` 不支持 `display`。\n\n- `visibility: hidden;`\n\n1. **DOM 结构**：元素被隐藏，但是会被渲染不会消失，占据空间；\n2. **事件监听**：无法进行 DOM 事件监听；\n3. **性 能**：动态改变此属性时会引起重绘，性能较高；\n4. **继 承**：会被子元素继承，子元素可以通过设置 `visibility: visible;` 来取消隐藏；\n5. **transition**：`transition` 不支持 `display`。\n\n- opacity: 0;\n\n1. **DOM 结构**：透明度为 100%，元素隐藏，占据空间；\n2. **事件监听**：可以进行 DOM 事件监听；\n3. **性 能**：提升为合成层，不会触发重绘，性能较高；\n4. **继 承**：会被子元素继承,且，子元素并不能通过 `opacity: 1` 来取消隐藏；\n5. **transition**：`transition` 不支持 `opacity`。\n\n## 3. 如何修改才能让图片宽度为 300px ？注意下面代码不可修改。\n\n`\u003cimg src=\"1.jpg\" style=\"width:480px!important;\"\u003e`\n\n\u003chr\u003e\n\n增加`max-width:300px`或`transform: scale(0.625)`,但后者元素本身大小还是480。\n\n## 4. BFC、IFC、GFC 和 FFC\n\n**BFC（Block formatting contexts）：块级格式上下文** 页面上的一个隔离的渲染区域，那么他是如何产生的呢？可以触发BFC的元素有float、position、overflow、display：table-cell/ inline-block/table-caption ；BFC有什么作用呢？比如说实现多栏布局’\n\n**IFC（Inline formatting contexts）：内联格式上下文** IFC的line box（线框）高度由其包含行内元素中最高的实际高度计算而来（不受到竖直方向的padding/margin影响)IFC中的line box一般左右都贴紧整个IFC，但是会因为float元素而扰乱。float元素会位于IFC与与line box之间，使得line box宽度缩短。 同个ifc下的多个line box高度会不同 IFC中时不可能有块级元素的，当插入块级元素时（如p中插入div）会产生两个匿名块与div分隔开，即产生两个IFC，每个IFC对外表现为块级元素，与div垂直排列。 那么IFC一般有什么用呢？ 水平居中：当一个块要在环境中水平居中时，设置其为inline-block则会在外层产生IFC，通过text-align则可以使其水平居中。 垂直居中：创建一个IFC，用其中一个元素撑开父元素的高度，然后设置其vertical-align:middle，其他行内元素则可以在此父元素下垂直居中。\n\n**GFC（GrideLayout formatting contexts）：网格布局格式化上下文** 当为一个元素设置display值为grid的时候，此元素将会获得一个独立的渲染区域，我们可以通过在网格容器（grid container）上定义网格定义行（grid definition rows）和网格定义列（grid definition columns）属性各在网格项目（grid item）上定义网格行（grid row）和网格列（grid columns）为每一个网格项目（grid item）定义位置和空间。那么GFC有什么用呢，和table又有什么区别呢？首先同样是一个二维的表格，但GridLayout会有更加丰富的属性来控制行列，控制对齐以及更为精细的渲染语义和控制。\n\n**FFC（Flex formatting contexts）:自适应格式上下文** display值为flex或者inline-flex的元素将会生成自适应容器（flex container），可惜这个牛逼的属性只有谷歌和火狐支持，不过在移动端也足够了，至少safari和chrome还是OK的，毕竟这俩在移动端才是王道。Flex Box 由伸缩容器和伸缩项目组成。通过设置元素的 display 属性为 flex 或 inline-flex 可以得到一个伸缩容器。设置为 flex 的容器被渲染为一个块级元素，而设置为 inline-flex 的容器则渲染为一个行内元素。伸缩容器中的每一个子元素都是一个伸缩项目。伸缩项目可以是任意数量的。伸缩容器外和伸缩项目内的一切元素都不受影响。简单地说，Flexbox 定义了伸缩容器内伸缩项目该如何布局。\n\n## 5. 如何用 css 或 js 实现多行文本溢出省略效果，考虑兼容性\n\n- 单行： overflow: hidden; text-overflow:ellipsis; white-space: nowrap;\n- 多行： display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 3; //行数 overflow: hidden;\n- 兼容： p{position: relative; line-height: 20px; max-height: 40px;overflow: hidden;} p:: after{content: \"…\"; position: absolute; bottom: 0; right: 0; padding-left: 40px; background: -webkit-linear-gradient(left, transparent, \\#fff 55%); background: -o-linear-gradient(right, transparent, \\#fff55%); background: -moz-linear-gradient(right, transparent, \\#fff55%); background: linear-gradient(to right, transparent, \\#fff55%); }\n\n## 6.介绍下BFC及其应用\n\nBFC特性：\n\n1. 内部box会在垂直方向，一个接一个地放置。\n2. Box垂直方向的距离由margin决定，在一个BFC中，两个相邻的块级盒子的垂直外边距会产生折叠。\n3. 在BFC中，每一个盒子的左外边缘（margin-left）会触碰到容器的左边缘(border-left)（对于从右到左的格式来说，则触碰到右边缘）\n4. 形成了BFC的区域不会与float box重叠\n5. 计算BFC高度时，浮动元素也参与计算\n\n生成BFC除了 @webproblem 童鞋所说的还有：行内块元素、网格布局、contain值为layout、content或 strict的元素等。 更多生成BFC的方法：[传送门](https://developer.mozilla.org/zh-CN/docs/Web/Guide/CSS/Block_formatting_context)\n\nBFC作用：\n\n1. 利用特性4可实现左图右文之类的效果：\n\n```html\n\u003cimg src='image.png'\u003e\n\u003cp\u003e我是超长的文字\u003cp\u003e\nimg {\n    float:left\n}\np {\n    overflow:hidden\n}\n```\n\n1. 利用特性5可以解决浮动元素造成的父元素高度塌陷问题：\n\n```html\n\u003cdiv class='parent'\u003e\n    \u003cdiv class='float'\u003e浮动元素\u003c/div\u003e\n\u003c/div\u003e\n.parent {\n    overflow:hidden;\n}\n.float {\n    float:left;\n}\n```\n\n## 7.页面导入样式时，使用link和@import有什么区别？\n\n相同的地方，都是外部引用CSS方式，区别：\n\n1. link是xhtml标签，除了加载css外，还可以定义RSS等其他事务；@import属于CSS范畴，只能加载CSS\n2. link引用CSS时候，页面载入时同时加载；@import需要在页面完全加载以后加载，而且@import被引用的CSS会等到引用它的CSS文件被加载完才加载\n3. link是xhtml标签，无兼容问题；@import是在css2.1提出来的，低版本的浏览器不支持\n4. link支持使用javascript控制去改变样式，而@import不支持\n5. link方式的样式的权重高于@import的权重\n6. import在html使用时候需要``标签\n\n## 8.**无样式内容闪烁（FOUC）Flash of Unstyle Content**\n\n@import导入CSS文件会等到文档加载完后再加载CSS样式表。因此，在页面DOM加载完成到CSS导入完成之间会有一段时间页面上的内容是没有样式的。\n\n解决方法：使用link标签加载CSS样式文件。因为link是顺序加载的，这样页面会等到CSS下载完之后再下载HTML文件，这样先布局好，就不会出现FOUC问题。\n\n## 9.CSS3有哪些新特性\n\n- 新增各种`css`选择器\n- 圆角 `border-radius`\n- 多列布局\n- 阴影和反射\n- 文字特效`text-shadow`\n- 线性渐变\n- 旋转`transform`\n\n## **10.CSS3新增伪类有那些？**\n\n- `:after`在元素之前添加内容,也可以用来做清除浮动。\n- `:before`在元素之后添加内容。\n- `:enabled`已启用的表单元素。\n- `:disabled`已禁用的表单元素。\n- `:checked`单选框或复选框被选中。\n\n## 11.CSS常用选择器\n\n```\n通配符：*\nID选择器：#ID\n类选择器：.class\n元素选择器：p、a    等\n后代选择器：p span、div a   等\n伪类选择器：a:hover 等\n属性选择器：input[type=\"text\"]  等\n```\n\n## 12.`display:inline-block`什么时候不会显示间隙？\n\n- 移除空格\n- 使用`margin`负值\n- 使用`font-size:0`\n- `letter-spacing`\n- `word-spacing`\n\n13. ## 为什么要初始化CSS样式?\n\n    - 因为浏览器的兼容问题，不同浏览器对有些标签的默认值是不同的，如果没对`CSS`初始化往往会出现浏览器之间的页面显示差异。\n    - 当然，初始化样式会对`SEO`有一定的影响，但鱼和熊掌不可兼得，但力求影响最小的情况下初始化\n\n## 14.`position`有哪些值？有什么作用？\n\n- `static`。默认值，不脱离文档流，`top`，`right`，`bottom`，`left`等属性不生效。\n- `relative`。不脱离文档流，依据自身位置进行偏离，当子元素设置`absolute`，将依据它进行偏离。\n- `absolute`。脱离文档流，依据`top，right，bottom，left`等属性在正常文档流中偏移位置。\n- `fixed`。通过浏览器窗口进行定位，出现滚动条的时候，不会随之滚动。\n\n## 15.垂直居中有哪些方法？\n\n- 单行文本的话可以使用`height`和`line-height`设置同一高度。\n- `position+margin`：设置父元素:`position: relative;`，子元素`height: 100px;position:absolute;top: 50%; margin: -50px 0 0 0;`（定高）\n- `position+transform`：设置父元素`position:relative`,子元素：`position: absolute;top: 50%;transform: translate(0, -50%);`（不定高）\n- 百搭flex布局(ie10+)，设置父元素`display:flex;align-items: center;`（不定高）\n\n## 16.水平居中的方法\n\n- 元素为行内元素，设置父元素`text-align:center`\n- 如果元素宽度固定，可以设置左右`margin`为`auto`;\n- 如果元素为绝对定位，设置父元素`position`为`relative`，元素设`left:0;right:0;margin:auto;`\n- 使用`flex-box`布局，指定`justify-content`属性为`center` `display`设置为`tabel-ceil`\n\n## 17.Flex布局\n\n```\ndisplay: flex  //设置Flex模式\nflex-direction: column  //决定元素是横排还是竖着排\nflex-wrap: wrap     //决定元素换行格式\njustify-content: space-between  //同一排下对齐方式，空格如何隔开各个元素\nalign-items: center     //同一排下元素如何对齐\nalign-content: space-between    //多行对齐方式\n```\n\n## 18.`stylus/sass/less`区别\n\n- 均具有“变量”、“混合”、“嵌套”、“继承”、“颜色混合”五大基本特性\n- `Scss`和`LESS`语法较为严谨，`LESS`要求一定要使用大括号`“{}”，Scss`和`Stylus`可以通过缩进表示层次与嵌套关系\n- `Scss`无全局变量的概念，`LESS`和`Stylus`有类似于其它语言的作用域概念\n- `Sass`是基于`Ruby`语言的，而`LESS`和`Stylus`可以基于`NodeJS NPM`下载相应库后进行编译；\n\n## 19.知道css有个content属性吗？有什么作用？有什么应用？\n\n`css`的`content`属性专门应用在 `before/after`伪元素上，用于来插入生成内容。最常见的应用是利用伪类清除浮动。\n\n## 20.重置（resetting）CSS 和 标准化（normalizing）CSS 的区别是什么？你会选择哪种方式，为什么？\n\n- **重置（Resetting）**： 重置意味着除去所有的浏览器默认样式。对于页面所有的元素，像`margin`、`padding`、`font-size`这些样式全部置成一样。你将必须重新定义各种元素的样式。\n- **标准化（Normalizing）**： 标准化没有去掉所有的默认样式，而是保留了有用的一部分，同时还纠正了一些常见错误。\n\n当需要实现非常个性化的网页设计时，我会选择重置的方式，因为我要写很多自定义的样式以满足设计需求，这时候就不再需要标准化的默认样式了。\n\n### 参考\n\n- [https://stackoverflow.com/questions/6887336/what-is-the-difference-between-normalize-css-and-reset-css](https://link.zhihu.com/?target=https%3A//stackoverflow.com/questions/6887336/what-is-the-difference-between-normalize-css-and-reset-css)\n\n## 21.请阐述`Float`定位的工作原理。\n\n浮动（float）是 CSS 定位属性。浮动元素从网页的正常流动中移出，但是保持了部分的流动性，会影响其他元素的定位（比如文字会围绕着浮动元素）。**这一点与绝对定位不同，绝对定位的元素完全从文档流中脱离。**\n\nCSS 的`clear`属性通过使用`left`、`right`、`both`，让该元素向下移动（清除浮动）到浮动元素下面。\n\n如果父元素只包含浮动元素，那么该父元素的高度将塌缩为 0。我们可以通过清除（clear）从浮动元素后到父元素关闭前之间的浮动来修复这个问题。\n\n有一种 hack 的方法，是自定义一个`.clearfix`类，利用伪元素选择器`::after`清除浮动。[另外还有一些方法](https://link.zhihu.com/?target=https%3A//css-tricks.com/all-about-floats/%23article-header-id-4)，比如添加空的``和设置浮动元素父元素的`overflow`属性。与这些方法不同的是，`clearfix`方法，只需要给父元素添加一个类，定义如下：\n\n```css\n.clearfix::after {\n  content: '';\n  display: block;\n  clear: both;\n}\n```\n\n值得一提的是，把父元素属性设置为`overflow: auto`或`overflow: hidden`，会使其内部的子元素形成块格式化上下文（Block Formatting Context），并且父元素会扩张自己，使其能够包围它的子元素。\n\n### 参考\n\n- [https://css-tricks.com/all-about-floats/](https://link.zhihu.com/?target=https%3A//css-tricks.com/all-about-floats/)\n\n## 22.请阐述`z-index`属性，并说明如何形成层叠上下文（stacking context）。\n\nCSS 中的`z-index`属性控制重叠元素的垂直叠加顺序。`z-index`只能影响`position`值不是`static`的元素。\n\n没有定义`z-index`的值时，元素按照它们出现在 DOM 中的顺序堆叠（层级越低，出现位置越靠上）。非静态定位的元素（及其子元素）将始终覆盖静态定位（static）的元素，而不管 HTML 层次结构如何。\n\n层叠上下文是包含一组图层的元素。 在一组层叠上下文中，其子元素的`z-index`值是相对于该父元素而不是 document root 设置的。每个层叠上下文完全独立于它的兄弟元素。如果元素 B 位于元素 A 之上，则即使元素 A 的子元素 C 具有比元素 B 更高的`z-index`值，元素 C 也永远不会在元素 B 之上.\n\n每个层叠上下文是自包含的：当元素的内容发生层叠后，整个该元素将会在父层叠上下文中按顺序进行层叠。少数 CSS 属性会触发一个新的层叠上下文，例如`opacity`小于 1，`filter`不是`none`，`transform`不是`none`。\n\n### 参考\n\n- [https://css-tricks.com/almanac/properties/z/z-index/](https://link.zhihu.com/?target=https%3A//css-tricks.com/almanac/properties/z/z-index/)\n- [https://philipwalton.com/articles/what-no-one-told-you-about-z-index/](https://link.zhihu.com/?target=https%3A//philipwalton.com/articles/what-no-one-told-you-about-z-index/)\n- [https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Positioning/Understanding_z_index/The_stacking_context](https://link.zhihu.com/?target=https%3A//developer.mozilla.org/en-US/docs/Web/CSS/CSS_Positioning/Understanding_z_index/The_stacking_context)\n\n### 23.如何解决不同浏览器的样式兼容性问题？\n\n- 在确定问题原因和有问题的浏览器后，使用单独的样式表，仅供出现问题的浏览器加载。这种方法需要使用服务器端渲染。\n- 使用已经处理好此类问题的库，比如 Bootstrap。\n- 使用 `autoprefixer` 自动生成 CSS 属性前缀。\n- 使用 Reset CSS 或 Normalize.css。\n\n## 24.编写高效的 CSS 应该注意什么？\n\n首先，浏览器从最右边的选择器，即关键选择器（key selector），向左依次匹配。根据关键选择器，浏览器从 DOM 中筛选出元素，然后向上遍历被选元素的父元素，判断是否匹配。选择器匹配语句链越短，浏览器的匹配速度越快。避免使用标签和通用选择器作为关键选择器，因为它们会匹配大量的元素，浏览器必须要进行大量的工作，去判断这些元素的父元素们是否匹配。\n\n## 25.使用 CSS 预处理的优缺点分别是什么？\n\n优点：\n\n- 提高 CSS 可维护性。\n- 易于编写嵌套选择器。\n- 引入变量，增添主题功能。可以在不同的项目中共享主题文件。\n- 通过混合（Mixins）生成重复的 CSS。\n- Splitting your code into multiple files. CSS files can be split up too but doing so will require a HTTP request to download each CSS file.\n- 将代码分割成多个文件。不进行预处理的 CSS，虽然也可以分割成多个文件，但需要建立多个 HTTP 请求加载这些文件。\n\n缺点：\n\n- 需要预处理工具。\n- 重新编译的时间可能会很慢。\n\n### 对于你使用过的 CSS 预处理，说说喜欢和不喜欢的地方？\n\n喜欢：\n\n- 绝大部分优点上题以及提过。\n- Less 用 JavaScript 实现，与 NodeJS 高度结合。\n\n**Dislikes:**\n\n- 我通过`node-sass`使用 Sass，它用 C ++ 编写的 LibSass 绑定。在 Node 版本切换时，我必须经常重新编译。\n- Less 中，变量名称以`@`作为前缀，容易与 CSS 关键字混淆，如`@media`、`@import`和`@font-face`。\n\n## 26.如何实现一个使用非标准字体的网页设计？\n\n使用`@font-face`并为不同的`font-weight`定义`font-family`。\n\n## 27.解释浏览器如何确定哪些元素与 CSS 选择器匹配。\n\n这部分与上面关于编写高效的 CSS 有关。浏览器从最右边的选择器（关键选择器）根据关键选择器，浏览器从 DOM 中筛选出元素，然后向上遍历被选元素的父元素，判断是否匹配。选择器匹配语句链越短，浏览器的匹配速度越快。\n\n例如，对于形如`p span`的选择器，浏览器首先找到所有``元素，并遍历它的父元素直到根元素以找到``元素。对于特定的``，只要找到一个``，就知道'`已经匹配并停止继续匹配。\n\n## 28.描述伪元素及其用途。\n\nCSS 伪元素是添加到选择器的关键字，去选择元素的特定部分。它们可以用于装饰（`:first-line`，`:first-letter`）或将元素添加到标记中（与 content:…组合），而不必修改标记（`:before`，`:after`）。\n\n- `:first-line`和`:first-letter`可以用来修饰文字。\n- 上面提到的`.clearfix`方法中，使用`clear: both`来添加不占空间的元素。\n- 使用`:before`和`after`展示提示中的三角箭头。鼓励关注点分离，因为三角被视为样式的一部分，而不是真正的 DOM。如果不使用额外的 HTML 元素，只用 CSS 样式绘制三角形是不太可能的。\n\n## 29.说说你对盒模型的理解，以及如何告知浏览器使用不同的盒模型渲染布局。\n\nCSS 盒模型描述了以文档树中的元素而生成的矩形框，并根据排版模式进行布局。每个盒子都有一个内容区域（例如文本，图像等）以及周围可选的`padding`、`border`和`margin`区域。\n\nCSS 盒模型负责计算：\n\n- 块级元素占用多少空间。\n- 边框是否重叠，边距是否合并。\n- 盒子的尺寸。\n\n盒模型有以下规则：\n\n- 块级元素的大小由`width`、`height`、`padding`、`border`和`margin`决定。\n- 如果没有指定`height`，则块级元素的高度等于其包含子元素的内容高度加上`padding`（除非有浮动元素，请参阅下文）。\n- 如果没有指定`width`，则非浮动块级元素的宽度等于其父元素的宽度减去父元素的`padding`。\n- 元素的`height`是由内容的`height`来计算的。\n- 元素的`width`是由内容的`width`来计算的。\n- 默认情况下，`padding`和`border`不是元素`width`和`height`的组成部分。\n\n## 30.响应式设计与自适应设计有何不同？\n\n响应式设计和自适应设计都以提高不同设备间的用户体验为目标，根据视窗大小、分辨率、使用环境和控制方式等参数进行优化调整。\n\n响应式设计的适应性原则：网站应该凭借一份代码，在各种设备上都有良好的显示和使用效果。响应式网站通过使用媒体查询，自适应栅格和响应式图片，基于多种因素进行变化，创造出优良的用户体验。就像一个球通过膨胀和收缩，来适应不同大小的篮圈。\n\n自适应设计更像是渐进式增强的现代解释。与响应式设计单一地去适配不同，自适应设计通过检测设备和其他特征，从早已定义好的一系列视窗大小和其他特性中，选出最恰当的功能和布局。与使用一个球去穿过各种的篮筐不同，自适应设计允许使用多个球，然后根据不同的篮筐大小，去选择最合适的一个。\n\n### 参考\n\n- [https://developer.mozilla.org/en-US/docs/Archive/Apps/Design/UI_layout_basics/Responsive_design_versus_adaptive_design](https://link.zhihu.com/?target=https%3A//developer.mozilla.org/en-US/docs/Archive/Apps/Design/UI_layout_basics/Responsive_design_versus_adaptive_design)\n- [http://mediumwell.com/responsive-adaptive-mobile/](https://link.zhihu.com/?target=http%3A//mediumwell.com/responsive-adaptive-mobile/)\n- [https://css-tricks.com/the-difference-between-responsive-and-adaptive-design/](https://link.zhihu.com/?target=https%3A//css-tricks.com/the-difference-between-responsive-and-adaptive-design/)\n\n## 31.你有没有使用过视网膜分辨率的图形？当中使用什么技术？\n\n我倾向于使用更高分辨率的图形（显示尺寸的两倍）来处理视网膜显示。更好的方法是使用媒体查询，像`@media only screen and (min-device-pixel-ratio: 2) { … }`，然后改变`background-image`。\n\n对于图标类的图形，我会尽可能使用 svg 和图标字体，因为它们在任何分辨率下，都能被渲染得十分清晰。\n\n还有一种方法是，在检查了`window.devicePixelRatio`的值后，利用 JavaScript 将``的`src`属性修改，用更高分辨率的版本进行替换。\n\n## 32.什么情况下，用`translate()`而不用绝对定位？什么时候，情况相反。\n\n`translate()`是`transform`的一个值。改变`transform`或`opacity`不会触发浏览器重新布局（reflow）或重绘（repaint），只会触发复合（compositions）。而改变绝对定位会触发重新布局，进而触发重绘和复合。`transform`使浏览器为元素创建一个 GPU 图层，但改变绝对定位会使用到 CPU。 因此`translate()`更高效，可以缩短平滑动画的绘制时间。\n\n当使用`translate()`时，元素仍然占据其原始空间（有点像`position：relative`），这与改变绝对定位不同。\n\n### 33.如何用css实现瀑布流布局\n\n利用column-count和break-inside这两个CSS3属性即可，复制如下代码即可察看效果\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cstyle\u003e\n        body {\n            margin: 0;\n        }\n        .waterfall-container {\n            /*分几列*/\n            column-count: 2;\n            width: 100%;\n            /* 列间距 */\n            column-gap: 10px;\n        }\n\n        .waterfall-item {\n            break-inside: avoid;\n            width: 100%;\n            height: 100px;\n            margin-bottom: 10px;\n            background: #ddd;\n            column-gap: 0;\n            text-align: center;\n            color: #fff;\n            font-size: 40px;\n        }\n    \u003c/style\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n    \u003cdiv class=\"waterfall-container\"\u003e\n        \u003cdiv class=\"waterfall-item\" style=\"height: 100px\"\u003e1\u003c/div\u003e\n        \u003cdiv class=\"waterfall-item\" style=\"height: 300px\"\u003e2\u003c/div\u003e\n        \u003cdiv class=\"waterfall-item\" style=\"height: 400px\"\u003e3\u003c/div\u003e\n        \u003cdiv class=\"waterfall-item\" style=\"height: 100px\"\u003e4\u003c/div\u003e\n        \u003cdiv class=\"waterfall-item\" style=\"height: 300px\"\u003e5\u003c/div\u003e\n        \u003cdiv class=\"waterfall-item\" style=\"height: 600px\"\u003e6\u003c/div\u003e\n        \u003cdiv class=\"waterfall-item\" style=\"height: 400px\"\u003e7\u003c/div\u003e\n        \u003cdiv class=\"waterfall-item\" style=\"height: 300px\"\u003e8\u003c/div\u003e\n        \u003cdiv class=\"waterfall-item\" style=\"height: 700px\"\u003e9\u003c/div\u003e\n        \u003cdiv class=\"waterfall-item\" style=\"height: 100px\"\u003e10\u003c/div\u003e\n    \u003c/div\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n```\n\n## 33.文本超出部分显示省略号\n\n### 单行\n\n```text\noverflow: hidden;\ntext-overflow:ellipsis;\nwhite-space: nowrap;\n```\n\n### 多行\n\n```text\ndisplay: -webkit-box;\n-webkit-box-orient: vertical;\n-webkit-line-clamp: 3; // 最多显示几行\noverflow: hidden;\n```\n\n## 34.去除inline-block元素间间距的方法\n\n- 移除空格\n- 使用margin负值\n- 使用font-size:0\n- letter-spacing\n- word-spacing\n","lastmodified":"2023-05-09T16:33:58.287366302Z","tags":[]},"/hexo%E4%B9%8B%E7%9C%8B%E6%9D%BF%E5%A8%98":{"title":"hexo二次元看板娘初探","content":"\n# hexo二次元看板娘初探\n\n基本配置好hexo后，便迫不及待地想把这萌萌的东西放在自己的博客中，配置使用整理一哈~\n\n## 安装\n\n^93febc\n\n根据其[官方文档](https://github.com/EYHN/hexo-helper-live2d/blob/master/README.zh-CN.md)的描述：\n\n```bash\nnpm install --save hexo-helper-live2d\n```\n\n安装后打开博客，就能发现默认的看板娘已经出现在页面中了。\n\n## 配置\n\n整理了一份基本配置，放置在根目录下的`_config.yml`中：\n\n```yaml\n# Live2D\n## https://github.com/EYHN/hexo-helper-live2d\nlive2d:\n  enable: true\n  # enable: false\n  scriptFrom: local # 默认\n  pluginRootPath: live2dw/ # 插件在站点上的根目录(相对路径)\n  pluginJsPath: lib/ # 脚本文件相对与插件根目录路径\n  pluginModelPath: assets/ # 模型文件相对与插件根目录路径\n  # scriptFrom: jsdelivr # jsdelivr CDN\n  # scriptFrom: unpkg # unpkg CDN\n  # scriptFrom: https://cdn.jsdelivr.net/npm/live2d-widget@3.x/lib/L2Dwidget.min.js # 你的自定义 url\n  tagMode: false # 标签模式, 是否仅替换 live2d tag标签而非插入到所有页面中\n  debug: false # 调试, 是否在控制台输出日志\n  model:\n    use: live2d-widget-model-wanko # npm-module package name\n    # use: wanko # 博客根目录/live2d_models/ 下的目录名\n    # use: ./wives/wanko # 相对于博客根目录的路径\n    # use: https://cdn.jsdelivr.net/npm/live2d-widget-model-wanko@1.0.5/assets/wanko.model.json # 你的自定义 url\n    scale: 1\n    hHeadPos: 0.5\n    vHeadPos: 0.618\n  display:\n    superSample: 2\n    width: 125\n    height: 125\n    position: left\n    hOffset: 30\n    vOffset: -20\n  mobile:\n    show: false\n    scale: 0.05A  q`3\n  react:\n    opacityDefault: 1\n    opacityOnHover: 0.2\n\n\n```\n\n下面罗列部分配置的含义，`hexo-helper-live2d`是在`live2d-widget.js`的基础上封装的，接受`live2d-widget.js`的配置以及`hexo-helper-live2d`的特有配置，其他详细配置可访问：[live2d-widget.js](https://l2dwidget.js.org/docs/class/src/index.js~L2Dwidget.html#instance-method-init)\n\n| 配置项               | 类型    | 属性                               | 备注                                                  |\n| :------------------- | :------ | :--------------------------------- | :---------------------------------------------------- |\n| enable               | Boolean | `true`或者`false`                  | 控制live2d插件是否生效。                              |\n| scriptFrom           | String  | `local`或者`jsdelivr`或者`unpkg`   | l2dwidget.js使用的CDN地址，local表示使用本地地址。    |\n| pluginRootPath       | String  | 例如：`live2dw/`                   | 插件在站点上根目录的相对路径。                        |\n| pluginJsPath         | String  | 例如：`lib/`                       | 脚本文件相对与插件根目录路径。                        |\n| pluginModelPath      | String  | 例如：`assets/`                    | 模型文件相对与插件根目录路径。                        |\n| tagMode              | Boolean | `true`或者`false`                  | 标签模式, 控制是否仅替换tag标签而非插入到所有页面中。 |\n| debug                | Boolean | `true`或者`false`                  | 调试模式, 控制是否在控制台输出日志。                  |\n| model.use            | String  | 例如：`live2d-widget-model-hijiki` | npm 模块包名（上文例中即使用的这个方式）。            |\n| model.use            | String  | 例如：`hijiki`                     | 博客根目录/live2d_models/ 下的目录名。                |\n| model.use            | String  | 例如：`./wives/hijiki`             | 相对于博客根目录的路径。                              |\n| model.use            | String  | 例如：`https://域名/model.json`    | 你自定义live2d模型json文件的url。                     |\n| model.scale          | Number  | 可选值，默认值为 `1`               | 模型与canvas的缩放。                                  |\n| model.hHeadPos       | Number  | 可选值，默认值为 `0.5`             | 模型头部横坐标。                                      |\n| model.vHeadPos       | Number  | 可选值，默认值为 `0.618`           | 模型头部横坐标。                                      |\n| display.superSample  | Number  | 可选值，默认值为 `2`               | 超采样等级。                                          |\n| display.width        | Number  | 可选值，默认值为 `150`             | canvas的长度。                                        |\n| display.height       | String  | 可选值，默认值为 `300`             | canvas的高度。                                        |\n| display.position     | Number  | 可选值，默认值为 `right`           | 显示位置：左或右。                                    |\n| display.hOffset      | Number  | 可选值，默认值为 `0`               | canvas水平偏移。                                      |\n| display.vOffset      | Number  | 可选值，默认值为 `-20`             | canvas水平偏移。                                      |\n| mobile.show          | Boolean | 可选值，默认值为 `true`            | 控制是否在移动设备上显示。                            |\n| mobile.scale         | Number  | 可选值，默认值为 `0.5`             | 移动设备上的缩放。                                    |\n| react.opacityDefault | Number  | 可选值，默认值为 `0.7`             | 默认透明度。                                          |\n| react.opacityOnHover | Number  | 可选值，默认值为 `0.2`             | 鼠标移上透明度（此项貌似没有效果）。                  |\n\n经过以上配置（很多配置不需要写，默认即可）后，就能更好的展示自己的看板娘啦。\n\n## Model\n\n### 使用提供的\n\n配置完成后，还得选择自己心仪的看板娘，不要说什么成小孩子才。。。官方给我们提供了一些。https://github.com/xiazeyu/live2d-widget-models\n\n### 使用自己的\n\n占坑。。很遗憾我想要的没有官方model，正在自己寻找制作中。。。\n\n### 使用方法：\n\n### a. live2d_models子目录名称\n\n1. 在您博客根目录下创建一个 `live2d_models` 文件夹.\n2. 在此文件夹内新建一个子文件夹.\n3. 将你的 Live2D 模型复制到这个子文件夹中.\n4. 将子文件夹的名称输入 `_config.yml` 的 `model.use` 中.\n\n\u003e 你的模型叫 `mymiku`.\n\u003e\n\u003e 在博客根目录 (应当有 `_config.yml` 、`sources` 、 `themes` ) 新建名为 `mymiku` 的子文件夹.\n\u003e\n\u003e 将模型复制到 `/live2d_models/mymiku/` 中.\n\u003e\n\u003e 现在, 在这里应当有一个 `.model.json` 文件 (例如 `mymiku.model.json`)\n\u003e\n\u003e 在 `/live2d_models/mymiku/` 中.\n\u003e\n\u003e 将 `mymiku` 输入到位于 `_config.yml` 的 `model.use` 中.\n\n### b. 相对于博客根目录的自定义路径\n\n您可直接输入**相对于博客根目录**的自定义路径到 `model.use` 中.\n\n示例: `./wives/wanko`\n","lastmodified":"2023-05-09T16:33:58.291366399Z","tags":[]},"/html":{"title":"html","content":"\n## iframe框架有那些优缺点\n\n优点：\n\n- iframe能够原封不动的把嵌入的网页展现出来。\n- 如果有多个网页引用iframe，那么你只需要修改iframe的内容，就可以实现调用的每一个页面内容的更改，方便快捷。\n- 网页如果为了统一风格，头部和版本都是一样的，就可以写成一个页面，用iframe来嵌套，可以增加代码的可重用。\n- 如果遇到加载缓慢的第三方内容如图标和广告，这些问题可以由iframe来解决。\n\n缺点：\n\n- 搜索引擎的爬虫程序无法解读这种页面\n- 框架结构中出现各种滚动条\n- 使用框架结构时，保证设置正确的导航链接。\n- iframe页面会增加服务器的http请求\n\n## label标签有什么作用\n\n`label` 标签通常是写在表单内，它关联一个控件，使用 `label` 可以实现点击文字选取对应的控件。\n\n```html\n\u003cinput type=\"checkbox\" id=\"test\"\u003e\n\u003clabel for=\"test\" \u003etest\u003c/label\u003e\n```\n\n## HTML5的form如何关闭自动完成功能\n\n将不想要自动完成的 `form` 或 `input` 设置为 `autocomplete=off`\n\n[MDN](https://link.zhihu.com/?target=https%3A//developer.mozilla.org/zh-CN/docs/Web/Security/Securing_your_site/Turning_off_form_autocompletion)\n\n## DOM和BOM有什么区别\n\n- DOM\n\nDocument Object Model，文档对象模型\n\nDOM 是为了操作文档出现的 API，document 是其的一个对象\n\nDOM和文档有关，这里的文档指的是网页，也就是html文档。DOM和浏览器无关，他关注的是网页本身的内容。\n\n- BOM\n\nBrowser Object Model，浏览器对象模型\n\nBOM 是为了操作浏览器出现的 API，window 是其的一个对象\n\nwindow 对象既为 javascript 访问浏览器提供API，同时在 ECMAScript 中充当 Global 对象\n\n## 行内元素和块级元素有哪些\n\n### 行内元素\n\n一个行内元素只占据它对应标签的边框所包含的空间  \n一般情况下，行内元素只能包含数据和其他行内元素\n\n```text\nb, big, i, small, tt\nabbr, acronym, cite, code, dfn, em, kbd, strong, samp, var\na, bdo, br, img, map, object, q, script, span, sub, sup\nbutton, input, label, select, textarea\n```\n\n### 块级元素\n\n占据一整行，高度、行高、内边距和外边距都可以改变，可以容纳块级标签和其他行内标签\n\n```text\nheader,form,ul,ol,table,article,div,hr,aside,figure,canvas,video,audio,footer\n```\n\n## **DOCTYPE**\n\n**1 DOCTYPE有什么作用？标准模式与混杂模式如何区分？它们有何意义?**\n\n告诉浏览器使用哪个版本的HTML规范来渲染文档。DOCTYPE不存在或形式不正确会导致HTML文档以混杂模式呈现。  \n标准模式（Standards mode）以浏览器支持的最高标准运行；混杂模式（Quirks mode）中页面是一种比较宽松的向后兼容的方式显示。\n\n**2 HTML5为什么只需要写 **!DOCTYPE HTML**？**\n\nHTML5不基于SGML（Standard Generalized Markup Language 标准通用标记语言），因此不需要对DTD（DTD 文档类型定义）进行引用，但是需要DOCTYPE来规范浏览器行为。\n\nHTML4.01基于SGML，所以需要引用DTD。才能告知浏览器文档所使用的文档类型，如下：\n\n\\\u003c!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01//EN\" \"http://www.w3.org/TR/html4/strict.dtd\"\u003e\n\n## **简述一下你对HTML语义化的理解？**\n\n1. 去掉或丢失样式的时候能够让页面呈现出清晰的结构。\n2. 有利于SEO和搜索引擎建立良好沟通，有助于爬虫抓取更多的信息，爬虫依赖于标签来确定上下文和各个关键字的权重。\n3. 方便其它设备解析。\n4. 便于团队开发和维护，语义化根据可读性。\n\n## **HTML5的文件离线储存怎么使用，工作原理是什么？**\n\n在线情况下，浏览器发现HTML头部有manifest属性，它会请求manifest文件，如果是第一次访问，那么浏览器就会根据manifest文件的内容下载相应的资源，并进行离线存储。如果已经访问过并且资源已经离线存储了，那么浏览器就会使用离线的资源加载页面。然后浏览器会对比新的manifest文件与旧的manifest文件，如果文件没有发生改变，就不会做任何操作，如果文件改变了，那么就会重新下载文件中的资源，并且进行离线存储。例如，\n\n在页面头部加入manifest属性\n\n```\n\u003chtml manifest='cache.manifest'\u003e\n```\n\n在cache.manifest文件中编写离线存储的资源\n\n```\nCACHE MANIFEST\n#v0.11\nCACHE:\njs/app.js\ncss/style.css\nNETWORK:\nResourse/logo.png\nFALLBACK:\n //offline.html\n```\n\n## **如何在页面上实现一个圆形的可点击区域？**\n\n1. map+area或者svg\n2. border-radius\n3. 纯js实现，一个点不在圆上的算法\n\n## **实现不使用 border 画出1px高的线，在不同浏览器的Quirks mode和CSS Compat模式下都能保持同一效果**\n\n```\n\u003cdiv style=\"height:1px;overflow:hidden;background:red\"\u003e\u003c/div\u003e\n```\n\n## HTML5有哪些新特性、移除了那些元素？\n\n`HTML5`现在已经不是 `SGML` 的子集，主要是关于图像，位置，存储，多任务等功能的增加\n\n- 绘画 `canvas`\n- 用于媒介回放的 `video` 和 `audio` 元素\n- 本地离线存储 `localStorage` 长期存储数据，浏览器关闭后数据不丢失\n- `sessionStorage` 的数据在浏览器关闭后自动删除\n- 语意化更好的内容元素，比如`article`、`footer`、`header`、`nav`、`section`\n- 表单控件，`calendar`、`date`、`time`、`email`、`url`、`search`\n- 新的技术`webworker`、 `websocket`、 `Geolocation`\n\n**移除的元素**：\n\n- 纯表现的元素：`basefont`、`big`、`center`、`font`、`s`、`strike`、 `tt、u`\n- 对可用性产生负面影响的元素：`frame`、`frameset`、`noframes`\n\n## HTML全局属性(global attribute)有哪些\n\n- `class`:为元素设置类标识\n- `data-*`: 为元素增加自定义属性\n- `draggable`: 设置元素是否可拖拽\n- `id`: 元素id，文档内唯一\n- `lang`: 元素内容的的语言\n- `style`: 行内css样式\n- `title`: 元素相关的建议信息\n\n## 渲染优化\n\n1、使用`CSS3`代码代替`JS`动画（尽可能避免重绘重排以及回流）\n\n2、页面中空的`href`和 `src`会阻塞页面其他资源的加载 (阻塞下载进程)\n\n3、用`innerHTML`代替`DOM`操作，减少`DOM`操作次数，优化`javascript`性能\n\n4、当需要设置的样式很多时设置`className`而不是直接操作`style`\n\n5、少用全局变量、缓存`DOM`节点查找的结果。减少`IO`读取操作\n\n6、图片预加载，将样式表放在顶部，将脚本放在底部 加上时间戳\n\n作者：蛙哇  \n链接：https://juejin.im/post/5d9cca32f265da5b6e0a34fd  \n来源：掘金  \n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n","lastmodified":"2023-05-09T16:33:58.291366399Z","tags":[]},"/javascript":{"title":"javascript","content":"\n## 1. mouseenter和mouseover的区别\n\nmouseover事件：不论鼠标指针穿过被选元素或其子元素，都会触发 mouseover 事件。\n\nmouseenter事件：只有在鼠标指针穿过被选元素时，才会触发 mouseenter 事件。\n\n以及\n\nmouseout事件：不论鼠标指针离开被选元素还是任何子元素，都会触发 mouseout 事件。\n\nmouseleave事件：只有在鼠标指针离开被选元素时，才会触发 mouseleave 事件。\n\n## 2. alert（1\u0026\u00262），alert（1||0）\n\n\u0026\u0026运算符，前面的true，返回后面的。前面的为false，返回前面的。\n\n||运算符，前面的为true，返回前面的。前面的为false，返回后面的。\n\n## 3. 为什么TCP连接需要三次握手，两次不可以吗，为什么\n\n感觉自己还没懂，先占坑，可看知乎\n\n## 4. js字符串两边截取空白的trim的原型方法的实现\n\n## 5. `['1', '2', '3'].map(parseInt)` what \u0026 why ?\n\n```js\n['10','10','10','10','10'].map(parseInt);\n// [10, NaN, 2, 3, 4]\n```\n\n奇怪吧\n\n首先需要知道parseInt：\n\n**parseInt(string, radix)** 将一个字符串 string 转换为 radix 进制的整数， radix 为介于2-36之间的数。即将string看作是radix进制的数，并返回其对应的十进制数。\n\n- `string`\n\n  要被解析的值。如果参数不是一个字符串，则将其转换为字符串(使用 `ToString `抽象操作)。字符串开头的空白符将会被忽略。\n\n- `radix`\n\n  一个介于2和36之间的整数(数学系统的基础)，表示上述字符串的**基数**。比如参数 10 表示使用十进制数值系统。**始终指定此参数**可以消除阅读该代码时的困惑并且保证转换结果可预测。当未指定基数时，不同的实现会产生不同的结果，通常认为其值默认为**10**，但是如果你的代码运行在过时的浏览器中，那么请在使用时**总是显式地指定 radix**。\n\n返回解析后的整数值（十进制）。 如果被解析参数的第一个字符无法被转化成数值类型，则返回 [`NaN`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/NaN)。\n\n注意：\n\n- `radix`参数为n将会把第一个参数看作是一个数的n进制表示，而返回的值则是十进制的。例如：\n\n```html\nparseInt('123', 5) // 将'123'看作5进制数，返回十进制数38 =\u003e 1*5^2 + 2*5^1 + 3*5^0 = 38\n```\n\n- 如果`parseInt`的字符不是指定基数中的数字，则忽略该字符和所有后续字符，并返回解析到该点的整数值。`parseInt`将数字截断为整数值。允许使用前导空格和尾随空格。\n- 使用parseInt去截取包含e字符数值部分会造成难以预料的结果。例如：\n\n  parseInt(\"6.022e23\", 10); // 返回 6  \n  parseInt(6.022e2, 10); // 返回 602\n\n- 在基数为 `undefined`，或者基数为 0 或者没有指定的情况下，JavaScript 作如下处理：\n  - 如果字符串 `string` 以\"0x\"或者\"0X\"开头, 则基数是16 (16进制).\n  - 如果字符串 `string` 以\"0\"开头, 基数是8（八进制）或者10（十进制），那么具体是哪个基数由实现环境决定。ECMAScript 5 规定使用10，但是并不是所有的浏览器都遵循这个规定。因此，**永远都要明确给出radix参数的值**。\n  - 如果字符串 `string` 以其它任何值开头，则基数是10 (十进制)。\n\n  如果第一个字符不能被转换成数字，`parseInt`返回`NaN`。\n\n  算术上， `NaN` 不是任何一个进制下的数。 你可以调用[`isNaN`](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/isNaN) 来判断 `parseInt` 是否返回 `NaN`。`NaN` 参与的数学运算其结果总是 `NaN`。\n\n  将整型数值以特定基数转换成它的字符串值可以使用 `intValue.toString(radix)`.\n\n其次还得知道map():`map()` 方法创建一个新数组，其结果是该数组中的每个元素都调用一个提供的函数后返回的结果。\n\n```js\nvar new_array = arr.map(function callback(currentValue[,index[, array]]) {\n // Return element for new_array\n }[, thisArg])\n```\n\n\u003e可以看到`callback`回调函数需要三个参数, 我们通常只使用第一个参数 (其他两个参数是可选的)。\n\u003e\n\u003e`currentValue` 是callback 数组中正在处理的当前元素。\n\u003e\n\u003e`index`可选, 是callback 数组中正在处理的当前元素的索引。\n\u003e\n\u003e`array`可选, 是callback map 方法被调用的数组。\n\u003e\n\u003e另外还有`thisArg`可选, 执行 callback 函数时使用的this 值。\n\n```js\n['1', '2', '3'].map(parseInt)\n```\n\n对于每个迭代`map`, `parseInt()`传递两个参数: **字符串和基数**。 所以实际执行的的代码是：\n\n```js\n['1', '2', '3'].map((item, index) =\u003e {\n\treturn parseInt(item, index)\n})\n```\n\n即返回的值分别为：\n\n```js\nparseInt('1', 0) // 1\nparseInt('2', 1) // NaN\nparseInt('3', 2) // NaN, 3 不是二进制\n```\n\n所以：\n\n```js\n['1', '2', '3'].map(parseInt)\n// 1, NaN, NaN\n```\n\n由此，加里·伯恩哈德例子也就很好解释了，这里不再赘述\n\n```js\n['10','10','10','10','10'].map(parseInt);\n// [10, NaN, 2, 3, 4]\n```\n\n如下解决\n\n```js\n['10','10','10','10','10'].map((val,index)=\u003e{console.log(val+','+index);return parseInt(val,10)})\n//\n['10','10','10','10','10'].map(Number);\n```\n\n## 6. 防抖与节流\n\n[参考](https://github.com/mqyqingfeng/Blog/issues/22)\n\n1. **防抖**\n\n\u003e 触发高频事件后n秒内函数只会执行一次，如果n秒内高频事件再次被触发，则重新计算时间\n\n- 思路：\n\n\u003e 每次触发事件时都取消之前的延时调用方法\n\n```js\nfunction debounce(fn) {\n  let timeout = null // 创建一个标记用来存放定时器的返回值\n  return function() {\n    clearTimeout(timeout) // 每当用户输入的时候把前一个 setTimeout clear 掉\n    timeout = setTimeout(() =\u003e {\n      // 然后又创建一个新的 setTimeout, 这样就能保证输入字符后的 interval 间隔内如果还有字符输入的话，就不会执行 fn 函数\n      fn.apply(this, arguments)\n    }, 500)\n  }\n}\nfunction sayHi() {\n  console.log('防抖成功')\n}\n\nvar inp = document.getElementById('inp')\ninp.addEventListener('input', debounce(sayHi)) // 防抖\n```\n\n1. **节流**\n\n\u003e 高频事件触发，但在n秒内只会执行一次，所以节流会稀释函数的执行频率\n\n- 思路：\n\n\u003e 每次触发事件时都判断当前是否有等待执行的延时函数\n\n```js\nfunction throttle(fn) {\n  let canRun = true // 通过闭包保存一个标记\n  return function() {\n    if (!canRun) return // 在函数开头判断标记是否为true，不为true则return\n    canRun = false // 立即设置为false\n    setTimeout(() =\u003e {\n      // 将外部传入的函数的执行放在setTimeout中\n      fn.apply(this, arguments)\n      // 最后在setTimeout执行完毕后再把标记设置为true(关键)表示可以执行下一次循环了。当定时器没有执行的时候标记永远是false，在开头被return掉\n      canRun = true\n    }, 500)\n  }\n}\nfunction sayHi(e) {\n  console.log(e.target.innerWidth, e.target.innerHeight)\n}\nwindow.addEventListener('resize', throttle(sayHi))\n```\n\n## 7. 介绍下 Set、Map、WeakSet 和 WeakMap 的区别？\n\nSet 和 Map 主要的应用场景在于 **数据重组** 和 **数据储存**\n\nSet 是一种叫做**集合**的数据结构，Map 是一种叫做**字典**的数据结构\n\n#### 1. 集合（Set）\n\nES6 新增的一种新的数据结构，类似于数组，但成员是唯一且无序的，没有重复的值。\n\n**Set 本身是一种构造函数，用来生成 Set 数据结构。**\n\n```js\nnew Set([iterable])\n```\n\n举个例子：\n\n```js\nconst s = new Set()\n[1, 2, 3, 4, 3, 2, 1].forEach(x =\u003e s.add(x))\n\nfor (let i of s) {\n    console.log(i)\t// 1 2 3 4\n}\n\n// 去重数组的重复对象\nlet arr = [1, 2, 3, 2, 1, 1]\n[... new Set(arr)]\t// [1, 2, 3]\n```\n\nSet 对象允许你储存任何类型的唯一值，无论是原始值或者是对象引用。\n\n向 Set 加入值的时候，不会发生类型转换，所以`5`和`\"5\"`是两个不同的值。Set 内部判断两个值是否不同，使用的算法叫做“Same-value-zero equality”，它类似于**精确相等**运算符（`===`），主要的区别是**`NaN`等于自身，而精确相等运算符认为`NaN`不等于自身。**\n\n```js\nlet set = new Set();\nlet a = NaN;\nlet b = NaN;\nset.add(a);\nset.add(b);\nset // Set {NaN}\n\nlet set1 = new Set()\nset1.add(5)\nset1.add('5')\nconsole.log([...set1])\t// [5, \"5\"]\n```\n\n- Set 实例属性\n  - constructor： 构造函数\n  - size：元素数量\n\n    ```js\n    let set = new Set([1, 2, 3, 2, 1])\n    \n    console.log(set.length)\t// undefined\n    console.log(set.size)\t// 3\n    ```\n\n- Set 实例方法\n  - 操作方法\n    - add(value)：新增，相当于 array里的push\n    - delete(value)：存在即删除集合中value\n    - has(value)：判断集合中是否存在 value\n    - clear()：清空集合\n\n      ------\n\n      ```js\n      let set = new Set()\n      set.add(1).add(2).add(1)\n      \n      set.has(1)\t// true\n      set.has(3)\t// false\n      set.delete(1)\t\n      set.has(1)\t// false\n      ```\n\n      `Array.from` 方法可以将 Set 结构转为数组\n\n      ```js\n      const items = new Set([1, 2, 3, 2])\n      const array = Array.from(items)\n      console.log(array)\t// [1, 2, 3]\n      // 或\n      const arr = [...items]\n      console.log(arr)\t// [1, 2, 3]\n      ```\n\n  - 遍历方法（遍历顺序为插入顺序）\n    - keys()：返回一个包含集合中所有键的迭代器\n    - values()：返回一个包含集合中所有值得迭代器\n    - entries()：返回一个包含Set对象中所有元素得键值对迭代器\n    - forEach(callbackFn, thisArg)：用于对集合成员执行callbackFn操作，如果提供了 thisArg 参数，回调中的this会是这个参数，**没有返回值**\n\n      ```js\n      let set = new Set([1, 2, 3])\n      console.log(set.keys())\t// SetIterator {1, 2, 3}\n      console.log(set.values())\t// SetIterator {1, 2, 3}\n      console.log(set.entries())\t// SetIterator {1, 2, 3}\n      \n      for (let item of set.keys()) {\n        console.log(item);\n      }\t// 1\t2\t 3\n      for (let item of set.entries()) {\n        console.log(item);\n      }\t// [1, 1]\t[2, 2]\t[3, 3]\n      \n      set.forEach((value, key) =\u003e {\n          console.log(key + ' : ' + value)\n      })\t// 1 : 1\t2 : 2\t3 : 3\n      console.log([...set])\t// [1, 2, 3]\n      ```\n\n      Set 可默认遍历，默认迭代器生成函数是 values() 方法\n\n      ```js\n      Set.prototype[Symbol.iterator] === Set.prototype.values\t// true\n      ```\n\n      所以， Set可以使用 map、filter 方法\n\n      ```js\n      let set = new Set([1, 2, 3])\n      set = new Set([...set].map(item =\u003e item * 2))\n      console.log([...set])\t// [2, 4, 6]\n      \n      set = new Set([...set].filter(item =\u003e (item \u003e= 4)))\n      console.log([...set])\t//[4, 6]\n      ```\n\n      因此，Set 很容易实现交集（Intersect）、并集（Union）、差集（Difference）\n\n      ```js\n      let set1 = new Set([1, 2, 3])\n      let set2 = new Set([4, 3, 2])\n      \n      let intersect = new Set([...set1].filter(value =\u003e set2.has(value)))\n      let union = new Set([...set1, ...set2])\n      let difference = new Set([...set1].filter(value =\u003e !set2.has(value)))\n      \n      console.log(intersect)\t// Set {2, 3}\n      console.log(union)\t\t// Set {1, 2, 3, 4}\n      console.log(difference)\t// Set {1}\n      ```\n\n#### 2. WeakSet\n\nWeakSet 对象允许你将**弱引用对象**储存在一个集合中\n\nWeakSet 与 Set 的区别：\n\n- WeakSet 只能储存对象引用，不能存放值，而 Set 对象都可以\n- WeakSet 对象中储存的对象值都是被弱引用的，即垃圾回收机制不考虑 WeakSet 对该对象的应用，如果没有其他的变量或属性引用这个对象值，则这个对象将会被垃圾回收掉（不考虑该对象还存在于 WeakSet 中），所以，WeakSet 对象里有多少个成员元素，取决于垃圾回收机制有没有运行，运行前后成员个数可能不一致，遍历结束之后，有的成员可能取不到了（被垃圾回收了），WeakSet 对象是无法被遍历的（ES6 规定 WeakSet 不可遍历），也没有办法拿到它包含的所有元素\n\n属性：\n\n- constructor：构造函数，任何一个具有 Iterable 接口的对象，都可以作参数\n\n  ```js\n  const arr = [[1, 2], [3, 4]]\n  const weakset = new WeakSet(arr)\n  console.log(weakset)\n  ```\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/面试/JavaScript/20210405162005.png)\n\n方法：\n\n- add(value)：在WeakSet 对象中添加一个元素value\n- has(value)：判断 WeakSet 对象中是否包含value\n- delete(value)：删除元素 value\n- clear()：清空所有元素，**注意该方法已废弃**\n\n```js\nvar ws = new WeakSet()\nvar obj = {}\nvar foo = {}\n\nws.add(window)\nws.add(obj)\n\nws.has(window)\t// true\nws.has(foo)\t// false\n\nws.delete(window)\t// true\nws.has(window)\t// false\n```\n\n#### 3. 字典（Map）\n\n集合 与 字典 的区别：\n\n- 共同点：集合、字典 可以储存不重复的值\n- 不同点：集合 是以 [value, value]的形式储存元素，字典 是以 [key, value] 的形式储存\n\n```js\nconst m = new Map()\nconst o = {p: 'haha'}\nm.set(o, 'content')\nm.get(o)\t// content\n\nm.has(o)\t// true\nm.delete(o)\t// true\nm.has(o)\t// false\n```\n\n**任何具有 Iterator 接口、且每个成员都是一个双元素的数组的数据结构**都可以当作`Map`构造函数的参数，例如：\n\n```js\nconst set = new Set([\n  ['foo', 1],\n  ['bar', 2]\n]);\nconst m1 = new Map(set);\nm1.get('foo') // 1\n\nconst m2 = new Map([['baz', 3]]);\nconst m3 = new Map(m2);\nm3.get('baz') // 3\n```\n\n如果读取一个未知的键，则返回`undefined`。\n\n```js\nnew Map().get('asfddfsasadf')\n// undefined\n```\n\n注意，只有对同一个对象的引用，Map 结构才将其视为同一个键。这一点要非常小心。\n\n```js\nconst map = new Map();\n\nmap.set(['a'], 555);\nmap.get(['a']) // undefined\n```\n\n上面代码的`set`和`get`方法，表面是针对同一个键，但实际上这是两个值，内存地址是不一样的，因此`get`方法无法读取该键，返回`undefined`。\n\n由上可知，Map 的键实际上是跟内存地址绑定的，只要内存地址不一样，就视为两个键。这就解决了同名属性碰撞（clash）的问题，我们扩展别人的库的时候，如果使用对象作为键名，就不用担心自己的属性与原作者的属性同名。\n\n如果 Map 的键是一个简单类型的值（数字、字符串、布尔值），则只要两个值严格相等，Map 将其视为一个键，比如`0`和`-0`就是一个键，布尔值`true`和字符串`true`则是两个不同的键。另外，`undefined`和`null`也是两个不同的键。虽然`NaN`不严格相等于自身，但 Map 将其视为同一个键。\n\n```js\nlet map = new Map();\n\nmap.set(-0, 123);\nmap.get(+0) // 123\n\nmap.set(true, 1);\nmap.set('true', 2);\nmap.get(true) // 1\n\nmap.set(undefined, 3);\nmap.set(null, 4);\nmap.get(undefined) // 3\n\nmap.set(NaN, 123);\nmap.get(NaN) // 123\n```\n\nMap 的属性及方法\n\n属性：\n\n- constructor：构造函数\n- size：返回字典中所包含的元素个数\n\n  ```js\n  const map = new Map([\n    ['name', 'An'],\n    ['des', 'JS']\n  ]);\n  \n  map.size // 2\n  ```\n\n操作方法：\n\n- set(key, value)：向字典中添加新元素\n- get(key)：通过键查找特定的数值并返回\n- has(key)：判断字典中是否存在键key\n- delete(key)：通过键 key 从字典中移除对应的数据\n- clear()：将这个字典中的所有元素删除\n\n遍历方法\n\n- Keys()：将字典中包含的所有键名以迭代器形式返回\n- values()：将字典中包含的所有数值以迭代器形式返回\n- entries()：返回所有成员的迭代器\n- forEach()：遍历字典的所有成员\n\n```js\nconst map = new Map([\n            ['name', 'An'],\n            ['des', 'JS']\n        ]);\nconsole.log(map.entries())\t// MapIterator {\"name\" =\u003e \"An\", \"des\" =\u003e \"JS\"}\nconsole.log(map.keys()) // MapIterator {\"name\", \"des\"}\n```\n\nMap 结构的默认遍历器接口（`Symbol.iterator`属性），就是`entries`方法。\n\n```js\nmap[Symbol.iterator] === map.entries\n// true\n```\n\nMap 结构转为数组结构，比较快速的方法是使用扩展运算符（`…`）。\n\n对于 forEach ，看一个例子\n\n```js\nconst reporter = {\n  report: function(key, value) {\n    console.log(\"Key: %s, Value: %s\", key, value);\n  }\n};\n\nlet map = new Map([\n    ['name', 'An'],\n    ['des', 'JS']\n])\nmap.forEach(function(value, key, map) {\n  this.report(key, value);\n}, reporter);\n// Key: name, Value: An\n// Key: des, Value: JS\n```\n\n在这个例子中， forEach 方法的回调函数的 this，就指向 reporter\n\n**与其他数据结构的相互转换**\n\n1. Map 转 Array\n\n   ```js\n   const map = new Map([[1, 1], [2, 2], [3, 3]])\n   console.log([...map])\t// [[1, 1], [2, 2], [3, 3]]\n   ```\n\n2. Array 转 Map\n\n   ```js\n   const map = new Map([[1, 1], [2, 2], [3, 3]])\n   console.log(map)\t// Map {1 =\u003e 1, 2 =\u003e 2, 3 =\u003e 3}\n   ```\n\n3. Map 转 Object\n\n   因为 Object 的键名都为字符串，而Map 的键名为对象，所以转换的时候会把非字符串键名转换为字符串键名。\n\n   ```js\n   function mapToObj(map) {\n       let obj = Object.create(null)\n       for (let [key, value] of map) {\n           obj[key] = value\n       }\n       return obj\n   }\n   const map = new Map().set('name', 'An').set('des', 'JS')\n   mapToObj(map)  // {name: \"An\", des: \"JS\"}\n   ```\n\n4. Object 转 Map\n\n   ```js\n   function objToMap(obj) {\n       let map = new Map()\n       for (let key of Object.keys(obj)) {\n           map.set(key, obj[key])\n       }\n       return map\n   }\n   \n   objToMap({'name': 'An', 'des': 'JS'}) // Map {\"name\" =\u003e \"An\", \"des\" =\u003e \"JS\"}\n   ```\n\n5. Map 转 JSON\n\n   ```js\n   function mapToJson(map) {\n       return JSON.stringify([...map])\n   }\n   \n   let map = new Map().set('name', 'An').set('des', 'JS')\n   mapToJson(map)\t// [[\"name\",\"An\"],[\"des\",\"JS\"]]\n   ```\n\n6. JSON 转 Map\n\n   ```js\n   function jsonToStrMap(jsonStr) {\n     return objToMap(JSON.parse(jsonStr));\n   }\n   \n   jsonToStrMap('{\"name\": \"An\", \"des\": \"JS\"}') // Map {\"name\" =\u003e \"An\", \"des\" =\u003e \"JS\"}\n   ```\n\n#### 4. WeakMap\n\nWeakMap 对象是一组键值对的集合，其中的**键是弱引用对象，而值可以是任意**。\n\n**注意，WeakMap 弱引用的只是键名，而不是键值。键值依然是正常引用。**\n\nWeakMap 中，每个键对自己所引用对象的引用都是弱引用，在没有其他引用和该键引用同一对象，这个对象将会被垃圾回收（相应的key则变成无效的），所以，WeakMap 的 key 是不可枚举的。\n\n属性：\n\n- constructor：构造函数\n\n方法：\n\n- has(key)：判断是否有 key 关联对象\n- get(key)：返回key关联对象（没有则则返回 undefined）\n- set(key)：设置一组key关联对象\n- delete(key)：移除 key 的关联对象\n\n```js\nlet myElement = document.getElementById('logo');\nlet myWeakmap = new WeakMap();\n\nmyWeakmap.set(myElement, {timesClicked: 0});\n\nmyElement.addEventListener('click', function() {\n  let logoData = myWeakmap.get(myElement);\n  logoData.timesClicked++;\n}, false);\n```\n\n#### 5. 总结\n\n- Set\n  - 成员唯一、无序且不重复\n  - [value, value]，键值与键名是一致的（或者说只有键值，没有键名）\n  - 可以遍历，方法有：add、delete、has\n- WeakSet\n  - 成员都是对象\n  - 成员都是弱引用，可以被垃圾回收机制回收，可以用来保存DOM节点，不容易造成内存泄漏\n  - 不能遍历，方法有add、delete、has\n- Map\n  - 本质上是键值对的集合，类似集合\n  - 可以遍历，方法很多可以跟各种数据格式转换\n- WeakMap\n  - 只接受对象作为键名（null除外），不接受其他类型的值作为键名\n  - 键名是弱引用，键值可以是任意的，键名所指向的对象可以被垃圾回收，此时键名是无效的\n  - 不能遍历，方法有get、set、has、delete\n\n#### 6. 扩展：Object与Set、Map\n\n1. Object 与 Set\n\n   ```js\n   // Object\n   const properties1 = {\n       'width': 1,\n       'height': 1\n   }\n   console.log(properties1['width']? true: false) // true\n   \n   // Set\n   const properties2 = new Set()\n   properties2.add('width')\n   properties2.add('height')\n   console.log(properties2.has('width')) // true\n   ```\n\n2. Object 与 Map\n\nJS 中的对象（Object），本质上是键值对的集合（hash 结构）\n\n```js\nconst data = {};\nconst element = document.getElementsByClassName('App');\n\ndata[element] = 'metadata';\nconsole.log(data['[object HTMLCollection]']) // \"metadata\"\n```\n\n但当以一个DOM节点作为对象 data 的键，对象会被自动转化为字符串[Object HTMLCollection]，所以说，Object 结构提供了 **字符串-值** 对应，Map则提供了 **值-值** 的对应\n\n## 8. ES5/ES6 的继承除了写法以外还有什么区别？\n\n这个问题比较复杂，暂时还不懂[url](https://muyiy.cn/question/js/7.html)\n\nES5 和 ES6 子类 `this` 生成顺序不同。ES5 的继承先生成了子类实例，再调用父类的构造函数修饰子类实例，ES6 的继承先生成父类实例，再调用子类的构造函数修饰父类实例。这个差别使得 ES6 可以继承内置对象。\n\n```javascript\nfunction MyES5Array() {\n  Array.call(this, arguments);\n}\n\n// it's useless\nconst arrayES5 = new MyES5Array(3); // arrayES5: MyES5Array {}\n\nclass MyES6Array extends Array {}\n\n// it's ok\nconst arrayES6 = new MyES6Array(3); // arrayES6: MyES6Array(3) []\n```\n\n## 9. 3 个判断数组的方法，请分别介绍它们之间的区别和优劣\n\n```js\nObject.prototype.toString.call()\ninstanceof \nArray.isArray()\n```\n\n```js\n1. Object.prototype.toString.call()\n每一个继承 Object 的对象都有 toString 方法，如果 toString 方法没有重写的话，会返回 [Object type]，其中 type 为对象的类型。但当除了 Object 类型的对象外，其他类型直接使用 toString 方法时，会直接返回都是内容的字符串，所以我们需要使用call或者apply方法来改变toString方法的执行上下文。\n\nconst an = ['Hello','An'];\nan.toString(); // \"Hello,An\"\nObject.prototype.toString.call(an); // \"[object Array]\"\n这种方法对于所有基本的数据类型都能进行判断，即使是 null 和 undefined 。\n\nObject.prototype.toString.call('An') // \"[object String]\"\nObject.prototype.toString.call(1) // \"[object Number]\"\nObject.prototype.toString.call(Symbol(1)) // \"[object Symbol]\"\nObject.prototype.toString.call(null) // \"[object Null]\"\nObject.prototype.toString.call(undefined) // \"[object Undefined]\"\nObject.prototype.toString.call(function(){}) // \"[object Function]\"\nObject.prototype.toString.call({name: 'An'}) // \"[object Object]\"\nObject.prototype.toString.call() 常用于判断浏览器内置对象时。\n\n更多实现可见 谈谈 Object.prototype.toString\n\n2. instanceof\ninstanceof  的内部机制是通过判断对象的原型链中是不是能找到类型的 prototype。\n\n使用 instanceof判断一个对象是否为数组，instanceof 会判断这个对象的原型链上是否会找到对应的 Array 的原型，找到返回 true，否则返回 false。\n\n[]  instanceof Array; // true\n但 instanceof 只能用来判断对象类型，原始类型不可以。并且所有对象类型 instanceof Object 都是 true。\n\n[]  instanceof Object; // true\n3. Array.isArray()\n功能：用来判断对象是否为数组\n\ninstanceof 与 isArray\n\n当检测Array实例时，Array.isArray 优于 instanceof ，因为 Array.isArray 可以检测出 iframes\n\nvar iframe = document.createElement('iframe');\ndocument.body.appendChild(iframe);\nxArray = window.frames[window.frames.length-1].Array;\nvar arr = new xArray(1,2,3); // [1,2,3]\n\n// Correctly checking for Array\nArray.isArray(arr);  // true\nObject.prototype.toString.call(arr); // true\n// Considered harmful, because doesn't work though iframes\narr instanceof Array; // false\nArray.isArray() 与 Object.prototype.toString.call()\n\nArray.isArray()是ES5新增的方法，当不存在 Array.isArray() ，可以用 Object.prototype.toString.call() 实现。\n\nif (!Array.isArray) {\n  Array.isArray = function(arg) {\n    return Object.prototype.toString.call(arg) === '[object Array]';\n  };\n}\n```\n\ninstanceof是判断类型的prototype是否出现在对象的原型链中，但是对象的原型可以随意修改，所以这种判断并不准确。\n\n```javascript\nconst obj = {}\nobj.__proto__ = Array.prototype\n// Object.setPrototypeOf(obj, Array.prototype)\nobj instanceof Array // true\n```\n\n## 10.介绍模块化发展历程\n\n可从IIFE、AMD、CMD、CommonJS、UMD、webpack(require.ensure)、ES Module、`` 这几个角度考虑。\n\nhttps://www.processon.com/view/link/5c8409bbe4b02b2ce492286a#map\n\n模块化主要是用来抽离公共代码，隔离作用域，避免变量冲突等。\n\n**IIFE**： 使用自执行函数来编写模块化，特点：**在一个单独的函数作用域中执行代码，避免变量冲突**。\n\n```js\n(function(){\n  return {\n\tdata:[]\n  }\n})()\n```\n\n**AMD**： 使用requireJS 来编写模块化，特点：**依赖必须提前声明好**。\n\n```js\ndefine('./index.js',function(code){\n\t// code 就是index.js 返回的内容\n})\n```\n\n**CMD**： 使用seaJS 来编写模块化，特点：**支持动态引入依赖文件**。\n\n```js\ndefine(function(require, exports, module) {  \n  var indexCode = require('./index.js');\n});\n```\n\n**CommonJS**： nodejs 中自带的模块化。\n\n```js\nvar fs = require('fs');\n```\n\n**UMD**：兼容AMD，CommonJS 模块化语法。\n\n**webpack(require.ensure)**：webpack 2.x 版本中的代码分割。\n\n**ES Modules**： ES6 引入的模块化，支持import 来引入另一个 js 。\n\n```js\nimport a from 'a';\n```\n\n## 11.全局作用域中，用 const 和 let 声明的变量不在 window 上，那到底在哪里？如何去获取？\n\n[关于let声明的变量在window里无法获取到的问题](https://blog.csdn.net/fang_ze_zhang/article/details/83419022)\n\n主要还是得好好学学js作用域那一块，同时es5、es6区别语法等各种坑特别多得注意\n\n\u003chr\u003e\n\n在ES5中，顶层对象的属性和全局变量是等价的，var 命令和 function 命令声明的全局变量，自然也是顶层对象。\n\n```js\nvar a = 12;\nfunction f(){};\n\nconsole.log(window.a); // 12\nconsole.log(window.f); // f(){}\n```\n\n但ES6规定，var 命令和 function 命令声明的全局变量，依旧是顶层对象的属性，但 let命令、const命令、class命令声明的全局变量，不属于顶层对象的属性。\n\n```js\nlet aa = 1;\nconst bb = 2;\n\nconsole.log(window.aa); // undefined\nconsole.log(window.bb); // undefined\n```\n\n在哪里？怎么获取？通过在设置断点，看看浏览器是怎么处理的：\n\n![letandconst](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/面试/JavaScript/20210405162006.png)\n\n通过上图也可以看到，在全局作用域中，用 let 和 const 声明的全局变量并没有在全局对象中，只是一个块级作用域（Script）中\n\n怎么获取？在定义变量的块级作用域中就能获取啊，既然不属于顶层对象，那就不加 window（global）呗。\n\n```js\nlet aa = 1;\nconst bb = 2;\n\nconsole.log(aa); // 1\nconsole.log(bb); // 2\n```\n\n## 12.使用 sort() 对数组 [3, 15, 8, 29, 102, 22] 进行排序，输出结果\n\n`sort` 函数，可以接收一个函数，返回值是比较两个数的相对顺序的值\n\n1. 默认没有函数 是按照 `UTF-16` 排序的，对于字母数字 你可以利用 `ASCII` 进行记忆\n\n```js\n [3, 15, 8, 29, 102, 22].sort();\n\n// [102, 15, 22, 29, 3, 8]\n```\n\n1. 带函数的比较\n\n```js\n [3, 15, 8, 29, 102, 22].sort((a,b) =\u003e {return a - b});\n```\n\n- 返回值大于0 即a-b \u003e 0 ， a 和 b 交换位置\n- 返回值大于0 即a-b \u003c 0 ， a 和 b 位置不变\n- 返回值等于0 即a-b = 0 ， a 和 b 位置不变\n\n\u003e 对于函数体返回 `b-a` 可以类比上面的返回值进行交换位置\n\n## 13. JS **JavaScript Demo: Function.call() **JavaScript Demo: Function.apply()\n\n `call()` 方法接受的是**一个参数列表**，而 `apply()` 方法接受的是**一个包含多个参数的数组**。\n\n## 14.输出以下代码的执行结果并解释为什么\n\n```js\nvar a = {n: 1};\nvar b = a;\na.x = a = {n: 2};\n\nconsole.log(a.x) \t\nconsole.log(b.x)\n```\n\n结果: undefined {n:2}\n\n首先，a和b同时引用了{n:2}对象，接着执行到a.x = a = {n：2}语句，尽管赋值是从右到左的没错，但是.的优先级比=要高，所以这里首先执行a.x，相当于为a（或者b）所指向的{n:1}对象新增了一个属性x，即此时对象将变为{n:1;x:undefined}。之后按正常情况，从右到左进行赋值，此时执行a ={n:2}的时候，a的引用改变，指向了新对象{n：2},而b依然指向的是旧对象。之后执行a.x = {n：2}的时候，并不会重新解析一遍a，而是沿用最初解析a.x时候的a，也即旧对象，故此时旧对象的x的值为{n：2}，旧对象为 {n:1;x:{n：2}}，它被b引用着。 后面输出a.x的时候，又要解析a了，此时的a是指向新对象的a，而这个新对象是没有x属性的，故访问时输出undefined；而访问b.x的时候，将输出旧对象的x的值，即{n:2}。\n\n![image](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/面试/JavaScript/20210405162007.png)\n\n## 15. 数组里10万个数据，取第一个元素和第99999个元素时间相差多少\n\njs 中数组元素的存储方式并不是连续的，而是哈希映射关系。哈希映射关系，可以通过键名 key，直接计算出值存储的位置，所以查找起来很快。推荐一下这篇文章：[深究 JavaScript 数组](https://juejin.im/entry/59ae664d518825244d207196)\n\n## 16.输出以下代码运行结果\n\n```js\n// example 1\nvar a={}, b='123', c=123;  \na[b]='b';\na[c]='c';  \nconsole.log(a[b]);\n\n---------------------\n// example 2\nvar a={}, b=Symbol('123'), c=Symbol('123');  \na[b]='b';\na[c]='c';  \nconsole.log(a[b]);\n\n---------------------\n// example 3\nvar a={}, b={key:'123'}, c={key:'456'};  \na[b]='b';\na[c]='c';  \nconsole.log(a[b]);\n```\n\n这题考察的是对象的键名的转换。\n\n- 对象的键名只能是字符串和 Symbol 类型。\n- 其他类型的键名会被转换成字符串类型。\n- 对象转字符串默认会调用 toString 方法。\n\n```js\n// example 1\nvar a={}, b='123', c=123;\na[b]='b';\n\n// c 的键名会被转换成字符串'123'，这里会把 b 覆盖掉。\na[c]='c';  \n\n// 输出 c\nconsole.log(a[b]);\n// example 2\nvar a={}, b=Symbol('123'), c=Symbol('123');  \n\n// b 是 Symbol 类型，不需要转换。\na[b]='b';\n\n// c 是 Symbol 类型，不需要转换。任何一个 Symbol 类型的值都是不相等的，所以不会覆盖掉 b。\na[c]='c';\n\n// 输出 b\nconsole.log(a[b]);\n// example 3\nvar a={}, b={key:'123'}, c={key:'456'};  \n\n// b 不是字符串也不是 Symbol 类型，需要转换成字符串。\n// 对象类型会调用 toString 方法转换成字符串 [object Object]。\na[b]='b';\n\n// c 不是字符串也不是 Symbol 类型，需要转换成字符串。\n// 对象类型会调用 toString 方法转换成字符串 [object Object]。这里会把 b 覆盖掉。\na[c]='c';  \n\n// 输出 c\nconsole.log(a[b]);\n```\n\n前面说的很清楚了，除了Symbol，如果想要不被覆盖 可以使用ES6提供的Map\n\n```js\nvar a=new Map(), b='123', c=123;\na.set(b,'b');\na.set(c,'c');\na.get(b);  // 'b'\na.get(c);  // 'c'\n```\n\n## 17.var、let 和 const 区别的实现原理是什么\n\n- var：遇到有var的作用域，**在任何语句执行前都已经完成了声明和初始化**，也就是变量提升而且拿到undefined的原因由来\n- function： 声明、初始化、赋值一开始就全部完成，所以函数的变量提升优先级更高\n- let：解析器进入一个块级作用域，发现let关键字，变量只是先完成**声明**，并没有到**初始化**那一步。此时如果在此作用域提前访问，则报错xx is not defined，这就是暂时性死区的由来。等到解析到有let那一行的时候，才会进入**初始化**阶段。如果let的那一行是赋值操作，则初始化和赋值同时进行\n- const、class都是同let一样的道理\n\n比如解析如下代码步骤：\n\n```javascript\n{\n// 没用的第一行\n// 没用的第二行\nconsole.log(a) // 如果此时访问a报错 a is not defined\nlet a = 1\n}\n```\n\n步骤：\n\n1. 发现作用域有let a，先注册个a，仅仅注册\n2. 没用的第一行\n3. 没用的第二行\n4. a is not defined，暂时性死区的表现\n5. 假设前面那行不报错，a初始化为undefined\n6. a赋值为1\n\n对比于var，let、const只是解耦了声明和初始化的过程，var是在任何语句执行前都已经完成了声明和初始化，let、const仅仅是在任何语句执行前只完成了声明\n\n## 18.Async/Await 如何通过同步的方式实现异步\n\n看了第一个回答，讲的挺深入底层的，但我现在还不太看得懂，先占个坑。\n\n[网址](https://muyiy.cn/question/async/9.html)\n\n## 19.输出运行结果\n\n```js\nfunction Foo() {\nFoo.a = function() {\nconsole.log(1)\n}\nthis.a = function() {\nconsole.log(2)\n}\n}\nFoo.prototype.a = function() {\nconsole.log(3)\n}\nFoo.a = function() {\nconsole.log(4)\n}\nFoo.a();\nlet obj = new Foo();\nobj.a();\nFoo.a();\n```\n\n结果：\n\n```javascript\nfunction Foo() {\n    Foo.a = function() {\n        console.log(1)\n    }\n    this.a = function() {\n        console.log(2)\n    }\n}\n// 以上只是 Foo 的构建方法，没有产生实例，此刻也没有执行\n\nFoo.prototype.a = function() {\n    console.log(3)\n}\n// 现在在 Foo 上挂载了原型方法 a ，方法输出值为 3\n\nFoo.a = function() {\n    console.log(4)\n}\n// 现在在 Foo 上挂载了直接方法 a ，输出值为 4\n\nFoo.a();\n// 立刻执行了 Foo 上的 a 方法，也就是刚刚定义的，所以\n// # 输出 4\n\nlet obj = new Foo();\n/* 这里调用了 Foo 的构建方法。Foo 的构建方法主要做了两件事：\n1. 将全局的 Foo 上的直接方法 a 替换为一个输出 1 的方法。\n2. 在新对象上挂载直接方法 a ，输出值为 2。\n*/\n\nobj.a();\n// 因为有直接方法 a ，不需要去访问原型链，所以使用的是构建方法里所定义的 this.a，\n// # 输出 2\n\nFoo.a();\n// 构建方法里已经替换了全局 Foo 上的 a 方法，所以\n// # 输出 1\n```\n\n同理\n\n```javascript\nfunction Foo() {\n    getName = function () { alert (1); };\n    return this;\n}\nFoo.getName = function () { alert (2);};\nFoo.prototype.getName = function () { alert (3);};\nvar getName = function () { alert (4);};\nfunction getName() { alert (5);}\n \n//请写出以下输出结果：\nFoo.getName();\ngetName();\nFoo().getName();\ngetName();\nnew Foo.getName();\nnew Foo().getName();\nnew new Foo().getName();\n```\n\n先看此题的上半部分做了什么，首先定义了一个叫Foo的函数，之后为Foo创建了一个叫getName的静态属性存储了一个匿名函数，之后为Foo的原型对象新创建了一个叫getName的匿名函数。之后又通过函数变量表达式创建了一个getName的函数，最后再声明一个叫getName函数。\n\n第一问的Foo.getName自然是访问Foo函数上存储的静态属性，答案自然是2，这里就不需要解释太多的，一般来说第一问对于稍微懂JS基础的同学来说应该是没问题的,当然我们可以用下面的代码来回顾一下基础，先加深一下了解\n\n1. Foo.getName();\n\n   自然是访问Foo函数上存储的静态属性，答案自然是2，这里就不需要解释太多的，一般来说第一问对于稍微懂JS基础的同学来说应该是没问题的,当然我们可以用下面的代码来回顾一下基础，先加深一下了解\n\n   ```javascript\n   function User(name) {\n   \tvar name = name; //私有属性\n   \tthis.name = name; //公有属性\n   \tfunction getName() { //私有方法\n   \t\treturn name;\n   \t}\n   }\n   User.prototype.getName = function() { //公有方法\n   \treturn this.name;\n   }\n   User.name = 'Wscats'; //静态属性\n   User.getName = function() { //静态方法\n   \treturn this.name;\n   }\n   var Wscat = new User('Wscats'); //实例化\n   ```\n\n   注意下面这几点：\n\n   - 调用公有方法，公有属性，我们必需先实例化对象，也就是用new操作符实化对象，就可构造函数实例化对象的方法和属性，并且公有方法是不能调用私有方法和静态方法的\n   - 静态方法和静态属性就是我们无需实例化就可以调用\n   - 而对象的私有方法和属性,外部是不可以访问的\n\n2. getName();\n\n   既然是直接调用那么就是访问当前上文作用域内的叫getName的函数，所以这里应该直接把关注点放在4和5上，跟1 2 3都没什么关系。当然后来我问了我的几个同事他们大多数回答了5。此处其实有两个坑，一是变量声明提升，二是函数表达式和函数声明的区别。\n\n   我们来看看为什么，可参考(1)关于Javascript的函数声明和函数表达式 (2)关于JavaScript的变量提升\n\n   在Javascript中，定义函数有两种类型\n\n   函数声明\n\n   ```javascript\n   // 函数声明\n   function wscat(type) {\n   \treturn type === \"wscat\";\n   }\n   ```\n\n   函数表达式\n\n   ```javascript\n   // 函数表达式\n   var oaoafly = function(type) {\n   \treturn type === \"oaoafly\";\n   }\n   ```\n\n   先看下面这个经典问题，在一个程序里面同时用函数声明和函数表达式定义一个名为getName的函数\n\n   ```javascript\n   getName() //oaoafly\n   var getName = function() {\n   \tconsole.log('wscat')\n   }\n   getName() //wscat\n   function getName() {\n   \tconsole.log('oaoafly')\n   }\n   getName() //wscat\n   ```\n\n   上面的代码看起来很类似，感觉也没什么太大差别。但实际上，Javascript函数上的一个“陷阱”就体现在Javascript两种类型的函数定义上。\n\n   - JavaScript 解释器中存在一种变量声明被提升的机制，也就是说函数声明会被提升到作用域的最前面，即使写代码的时候是写在最后面，也还是会被提升至最前面。\n   - 而用函数表达式创建的函数是在运行时进行赋值，且要等到表达式赋值完成后才能调用\n\n   ```javascript\n   var getName //变量被提升，此时为undefined\n   \n   getName() //oaoafly 函数被提升 这里受函数声明的影响，虽然函数声明在最后可以被提升到最前面了\n   var getName = function() {\n   \tconsole.log('wscat')\n   } //函数表达式此时才开始覆盖函数声明的定义\n   getName() //wscat\n   function getName() {\n   \tconsole.log('oaoafly')\n   }\n   getName() //wscat 这里就执行了函数表达式的值\n   ```\n\n   所以可以分解为这两个简单的问题来看清楚区别的本质\n\n   ```javascript\n   var getName;\n   console.log(getName) //undefined\n   getName() //Uncaught TypeError: getName is not a function\n   var getName = function() {\n   \tconsole.log('wscat')\n   }\n   var getName;\n   console.log(getName) //function getName() {console.log('oaoafly')}\n   getName() //oaoafly\n   function getName() {\n   \tconsole.log('oaoafly')\n   }\n   ```\n\n   这个区别看似微不足道，但在某些情况下确实是一个难以察觉并且“致命“的陷阱。出现这个陷阱的本质原因体现在这两种类型在函数提升和运行时机（解析时/运行时）上的差异。\n\n   当然我们给一个总结：Javascript中函数声明和函数表达式是存在区别的，函数声明在JS解析时进行函数提升，因此在同一个作用域内，不管函数声明在哪里定义，该函数都可以进行调用。而函数表达式的值是在JS运行时确定，并且在表达式赋值完成后，该函数才能调用。\n\n   所以第二问的答案就是4，5的函数声明被4的函数表达式覆盖了\n\n3. Foo().getName();\n\n   先执行了Foo函数，然后调用Foo函数的返回值对象的getName属性函数。\n\n   Foo函数的第一句`getName = function () { alert (1); };`是一句函数赋值语句，注意它没有var声明，所以先向当前Foo函数作用域内寻找getName变量，没有。再向当前函数作用域上层，即外层作用域内寻找是否含有getName变量，找到了，也就是第二问中的alert(4)函数，将此变量的值赋值为`function(){alert(1)}`。\n\n   此处实际上是将外层作用域内的getName函数修改了。\n\n   \u003e 注意：此处若依然没有找到会一直向上查找到window对象，若window对象中也没有getName属性，就在window对象中创建一个getName变量。\n\n   之后Foo函数的返回值是this，而JS的this问题已经有非常多的文章介绍，这里不再多说。\n\n   简单的讲，this的指向是由所在函数的调用方式决定的。而此处的直接调用方式，this指向window对象。\n\n   遂Foo函数返回的是window对象，相当于执行`window.getName()`，而window中的getName已经被修改为alert(1)，所以最终会输出1  \n   此处考察了两个知识点，一个是变量作用域问题，一个是this指向问题  \n   我们可以利用下面代码来回顾下这两个知识点:\n\n   ```javascript\n   var name = \"Wscats\"; //全局变量\n   window.name = \"Wscats\"; //全局变量\n   function getName() {\n   \tname = \"Oaoafly\"; //去掉var变成了全局变量\n   \tvar privateName = \"Stacsw\";\n   \treturn function() {\n   \t\tconsole.log(this); //window\n   \t\treturn privateName\n   \t}\n   }\n   var getPrivate = getName(\"Hello\"); //当然传参是局部变量，但函数里面我没有接受这个参数\n   console.log(name) //Oaoafly\n   console.log(getPrivate()) //Stacsw\n   ```\n\n   因为JS没有块级作用域，但是函数是能产生一个作用域的，函数内部不同定义值的方法会直接或者间接影响到全局或者局部变量，函数内部的私有变量可以用闭包获取，函数还真的是第一公民呀~\n\n   而关于this，this的指向在函数定义的时候是确定不了的，只有函数执行的时候才能确定this到底指向谁，实际上this的最终指向的是那个调用它的对象\n\n   所以第三问中实际上就是window在调用**Foo()**函数，所以this的指向是window\n\n   ```javascript\n   window.Foo().getName();\n   //-\u003ewindow.getName();\n   ```\n\n4. getName();\n\n   直接调用getName函数，相当于`window.getName()`，因为这个变量已经被Foo函数执行时修改了，遂结果与第三问相同，为1，也就是说Foo执行后把全局的getName函数给重写了一次，所以结果就是Foo()执行重写的那个getName函数\n\n5. new Foo.getName()\n\n   下面是JS运算符的优先级表格，从高到低排列。可参考MDN运算符优先级\n\n   | 优先级 | 运算类型             | 关联性   | 运算符         |\n   | ------ | -------------------- | -------- | -------------- |\n   | 19     | 圆括号               | n/a      | ( … )          |\n   | 18     | 成员访问             | 从左到右 | … . …          |\n   |        | 需计算的成员访问     | 从左到右 | … [ … ]        |\n   |        | new (带参数列表)     | n/a new  | … ( … )        |\n   | 17     | 函数调用             | 从左到右 | … ( … )        |\n   |        | new (无参数列表)     | 从右到左 | new …          |\n   | 16     | 后置递增(运算符在后) | n/a      | … ++           |\n   |        | 后置递减(运算符在后) | n/a      | … --           |\n   | 15     | 逻辑非               | 从右到左 | ! …            |\n   |        | 按位非               | 从右到左 | ~ …            |\n   |        | 一元加法             | 从右到左 | + …            |\n   |        | 一元减法             | 从右到左 | - …            |\n   |        | 前置递增             | 从右到左 | ++ …           |\n   |        | 前置递减             | 从右到左 | -- …           |\n   |        | typeof               | 从右到左 | typeof …       |\n   |        | void                 | 从右到左 | void …         |\n   |        | delete               | 从右到左 | delete …       |\n   | 14     | 乘法                 | 从左到右 | … * …          |\n   |        | 除法                 | 从左到右 | … / …          |\n   |        | 取模                 | 从左到右 | … % …          |\n   | 13     | 加法                 | 从左到右 | … + …          |\n   |        | 减法                 | 从左到右 | … - …          |\n   | 12     | 按位左移             | 从左到右 | … \u003c\u003c …         |\n   |        | 按位右移             | 从左到右 | … \u003e\u003e …         |\n   |        | 无符号右移           | 从左到右 | … \u003e\u003e\u003e …        |\n   | 11     | 小于                 | 从左到右 | … \u003c …          |\n   |        | 小于等于             | 从左到右 | … \u003c= …         |\n   |        | 大于                 | 从左到右 | … \u003e …          |\n   |        | 大于等于             | 从左到右 | … \u003e= …         |\n   |        | in                   | 从左到右 | … in …         |\n   |        | instanceof           | 从左到右 | … instanceof … |\n   | 10     | 等号                 | 从左到右 | … == …         |\n   |        | 非等号               | 从左到右 | … != …         |\n   |        | 全等号               | 从左到右 | … === …        |\n   |        | 非全等号             | 从左到右 | … !== …        |\n   | 9      | 按位与               | 从左到右 | … \u0026 …          |\n   | 8      | 按位异或             | 从左到右 | … ^ …          |\n   | 7      | 按位或               | 从左到右 | … 按位或 …     |\n   | 6      | 逻辑与               | 从左到右 | … \u0026\u0026 …         |\n   | 5      | 逻辑或               | 从左到右 | … 逻辑或 …     |\n   | 4      | 条件运算符           | 从右到左 | … ? … : …      |\n   | 3      | 赋值                 | 从右到左 | … = …          |\n   |        |                      |          | … += …         |\n   |        |                      |          | … -= …         |\n   |        |                      |          | … *= …         |\n   |        |                      |          | … /= …         |\n   |        |                      |          | … %= …         |\n   |        |                      |          | … \u003c\u003c= …        |\n   |        |                      |          | … \u003e\u003e= …        |\n   |        |                      |          | … \u003e\u003e\u003e= …       |\n   |        |                      |          | … \u0026= …         |\n   |        |                      |          | … ^= …         |\n   |        |                      |          | … 或= …        |\n   | 2      | yield                | 从右到左 | yield …        |\n   |        | yield*               | 从右到左 | yield* …       |\n   | 1      | 展开运算符           | n/a      | … …          |\n   | 0      | 逗号                 | 从左到右 | … , …          |\n\n \n\n这题首先看优先级的第18和第17都出现关于new的优先级，new (带参数列表)比new (无参数列表)高比函数调用高，跟成员访问同级\n\n   `new Foo.getName();`的优先级是这样的\n\n   相当于是:\n\n   ```\n   new (Foo.getName)();\n   ```\n\n   - 点的优先级(18)比new无参数列表(17)优先级高\n   - 当点运算完后又因为有个括号`()`，此时就是变成new有参数列表(18)，所以直接执行new，当然也可能有朋友会有疑问为什么遇到()不函数调用再new呢，那是因为函数调用(17)比new有参数列表(18)优先级低\n\n   \u003e .成员访问(18)-\u003enew有参数列表(18)\n\n   所以这里实际上将getName函数作为了构造函数来执行，遂弹出2。\n\n6. 这一题比上一题的唯一区别就是在Foo那里多出了一个括号，这个有括号跟没括号我们在第五问的时候也看出来优先级是有区别的\n\n   ```\n   (new Foo()).getName()\n   ```\n\n   那这里又是怎么判断的呢？首先new有参数列表(18)跟点的优先级(18)是同级，同级的话按照从左向右的执行顺序，所以先执行new有参数列表(18)再执行点的优先级(18)，最后再函数调用(17)\n\n   \u003e new有参数列表(18)-\u003e.成员访问(18)-\u003e()函数调用(17)\n\n   这里还有一个小知识点，Foo作为构造函数有返回值，所以这里需要说明下JS中的构造函数返回值问题。\n\n   ### 构造函数的返回值\n\n   在传统语言中，构造函数不应该有返回值，实际执行的返回值就是此构造函数的实例化对象。  \n   而在JS中构造函数可以有返回值也可以没有。\n\n   1. 没有返回值则按照其他语言一样返回实例化对象。\n\n   ```\n   function Foo(name) {\n   this.name = name\n   }\n   console.log(new Foo('wscats'))\n   ```\n\n   1. 若有返回值则检查其返回值是否为引用类型。如果是非引用类型，如基本类型（String,Number,Boolean,Null,Undefined）则与无返回值相同，实际返回其实例化对象。\n\n   ```\n   function Foo(name) {\n   this.name = name\n   return 520\n   }\n   console.log(new Foo('wscats'))\n   ```\n\n   1. 若返回值是引用类型，则实际返回值为这个引用类型。\n\n   ```\n   function Foo(name) {\n   this.name = name\n   return {\n   age: 16\n   }\n   }\n   console.log(new Foo('wscats'))\n   ```\n\n   原题中，由于返回的是this，而this在构造函数中本来就代表当前实例化对象，最终Foo函数返回实例化对象。\n\n   之后调用实例化对象的getName函数，因为在Foo构造函数中没有为实例化对象添加任何属性，当前对象的原型对象(prototype)中寻找getName函数。\n\n   当然这里再拓展个题外话，如果构造函数和原型链都有相同的方法，如下面的代码，那么默认会拿构造函数的公有方法而不是原型链，这个知识点在原题中没有表现出来，后面改进版我已经加上。\n\n   ```javascript\n   function Foo(name) {\n   this.name = name\n   this.getName = function() {\n   return this.name\n   }\n   }\n   Foo.prototype.name = 'Oaoafly';\n   Foo.prototype.getName = function() {\n   return 'Oaoafly'\n   }\n   console.log((new Foo('Wscats')).name) //Wscats\n   console.log((new Foo('Wscats')).getName()) //Wscats\n   ```\n\n7. `new new Foo().getName();`\n\n   同样是运算符优先级问题。做到这一题其实我已经觉得答案没那么重要了，关键只是考察面试者是否真的知道面试官在考察我们什么。  \n   最终实际执行为:\n\n   ```\n   new ((new Foo()).getName)();\n   ```\n\n   \u003e new有参数列表(18)-\u003enew有参数列表(18)\n\n   先初始化Foo的实例化对象，然后将其原型上的getName函数作为构造函数再次new，所以最终结果为3\n\n进阶版\n\n```javascript\nfunction Foo() {\nthis.getName = function() {\nconsole.log(3);\nreturn {\ngetName: getName //这个就是第六问中涉及的构造函数的返回值问题\n}\n}; //这个就是第六问中涉及到的，JS构造函数公有方法和原型链方法的优先级\ngetName = function() {\nconsole.log(1);\n};\nreturn this\n}\nFoo.getName = function() {\nconsole.log(2);\n};\nFoo.prototype.getName = function() {\nconsole.log(6);\n};\nvar getName = function() {\nconsole.log(4);\n};\n\nfunction getName() {\nconsole.log(5);\n} //答案：\nFoo.getName(); //2\ngetName(); //4\nconsole.log(Foo())\nFoo().getName(); //1\ngetName(); //1\nnew Foo.getName(); //2\nnew Foo().getName(); //3\n//多了一问\nnew Foo().getName().getName(); //3 1\nnew new Foo().getName(); //3\n```\n\n## 20.写出结果\n\n```js\nString('11') == new String('11');\nString('11') === new String('11');\n```\n\nTrue false\n\nnew String() 返回的是对象\n\n`==` 的时候，实际运行的是 String('11') == new String('11').toString();\n\n`===` 不再赘述。\n\n```javascript\nvar str1 = String('11')\nvar str2 = new String('11')\nstr1 == str2 // true\nstr1 === str2 // false\ntypeof str1  // \"string\"\ntypeof str2 // \"object\"\n```\n\n## 21.写出结果\n\n```js\n1 + \"1\"\n\n2 * \"2\"\n\n[1, 2] + [2, 1]\n\n\"a\" + + \"b\"\n//答案为\n//'11'\n//4\n//'1,22,1'\n//'aNaN'\n```\n\n- 1 + \"1\"\n\n加性操作符：如果只有一个操作数是字符串，则将另一个操作数转换为字符串，然后再将两个字符串拼接起来\n\n所以值为：“11”\n\n- 2 * \"2\"\n\n乘性操作符：如果有一个操作数不是数值，则在后台调用 Number()将其转换为数值\n\n- [1, 2] + [2, 1]\n\nJavascript中所有对象基本都是先调用`valueOf`方法，如果不是数值，再调用`toString`方法。\n\n所以两个数组对象的toString方法相加，值为：\"1,22,1\"\n\n- \"a\" + + \"b\"\n\n后边的“+”将作为一元操作符，如果操作数是字符串，将调用Number方法将该操作数转为数值，如果操作数无法转为数值，则为NaN。\n\n所以值为：\"aNaN\"\n\n以上均参考：《Javascript高级程序设计》\n\n**稍稍补充一小下：** 加号作为一元运算符时，其后面的表达式将进行[ToNumber(参考es规范)](http://www.ecma-international.org/ecma-262/6.0/#sec-tonumber)的抽象操作：\n\n- true -\u003e 1\n- false -\u003e 0\n- undefined -\u003e NaN\n- null -\u003e 0\n- ’字符串‘ -\u003e 字符串为纯数字时返回转换后的数字（十六进制返回十进制数），否则返回NaN\n- 对象 -\u003e 通过[ToPrimitive](http://www.ecma-international.org/ecma-262/6.0/#sec-toprimitive)拿到基本类型值，然后再进行[ToNumber](http://www.ecma-international.org/ecma-262/6.0/#sec-tonumber)操作\n\n```javascript\n+true  // 1\n+false // 0\n+undefined // NaN\n+null // 0\n+'b'    // NaN\n+'0x10' // 16\n+{valueOf: ()=\u003e 5} // 5\n```\n\n## 22.为什么 for 循环嵌套顺序会影响性能？\n\n```js\nvar t1 = new Date().getTime()\nfor (let i = 0; i \u003c 100; i++) {\n  for (let j = 0; j \u003c 1000; j++) {\n    for (let k = 0; k \u003c 10000; k++) {\n    }\n  }\n}\nvar t2 = new Date().getTime()\nconsole.log('first time', t2 - t1)\n\nfor (let i = 0; i \u003c 10000; i++) {\n  for (let j = 0; j \u003c 1000; j++) {\n    for (let k = 0; k \u003c 100; k++) {\n\n    }\n  }\n}\nvar t3 = new Date().getTime()\nconsole.log('two time', t3 - t2)\n```\n\n想起来之前在书上看到的，let每个循环都会初始化，所以外层循环次数越大，内层变量初始化次数越多，影响性能。\n\n## 23.输出以下代码执行结果\n\n```js\nfunction wait() {\n  return new Promise(resolve =\u003e\n    setTimeout(resolve, 10 * 1000)\n  )\n}\n\nasync function main() {\n  console.time();\n  const x = wait();\n  const y = wait();\n  const z = wait();\n  await x;\n  await y;\n  await z;\n  console.timeEnd();\n}\nmain();\n```\n\n\u003chr\u003e\n\n## 24.理解任务队列(消息队列)\n\n一种是同步任务（synchronous），另一种是异步任务（asynchronous）\n\n```js\n    // 请问最后的输出结果是什么？\n    console.log(\"A\");\n    while(true){ }\n    console.log(\"B\");\n```\n\n如果你的回答是A,恭喜你答对了，因为这是同步任务，程序由上到下执行，遇到while()死循环，下面语句就没办法执行。\n\n```js\n    // 请问最后的输出结果是什么？\n    console.log(\"A\");\n    setTimeout(function(){\n    \tconsole.log(\"B\");\n    },0);\n    while(true){}\n```\n\n如果你的答案是A，恭喜你现在对js运行机制已经有个粗浅的认识了！ 题目中的setTimeout()就是个异步任务。在所有同步任务执行完之前，任何的异步任务是不会执行的\n\n```js\n// new Promise(xx)相当于同步任务, 会立即执行, .then后面的是微任务\nconsole.log('----------------- start -----------------');\nsetTimeout(() =\u003e {\n    console.log('setTimeout');\n}, 0)\nnew Promise((resolve, reject) =\u003e{  // new Promise(xx)相当于同步任务, 会立即执行, .then后面的是微任务\n    for (var i = 0; i \u003c 5; i++) {\n        console.log(i);\n    }\n    resolve();  \n}).then(() =\u003e {  \n    console.log('promise实例成功回调执行');\n})\nconsole.log('----------------- end -----------------');\n\n\u003e ----------------- start -----------------\n\u003e 0\n\u003e 1\n\u003e 2\n\u003e 3\n\u003e 4\n\u003e ----------------- end -----------------\n\u003e promise实例成功回调执行\n\u003e setTimeout\n```\n\nnew Promise(xx)相当于同步任务, 会立即执行\n\n所以: x,y,z 三个任务是几乎同时开始的, 最后的时间依然是10*1000ms (比这稍微大一点点, 超出部分在1x1000ms之内)\n\n**但如果稍稍修改**\n\n```javascript\nfunction wait() {\n\treturn new Promise(resolve =\u003e\n\t\tsetTimeout(resolve, 10 * 1000)\n\t)\n}\n\nasync function main() {\n\tconsole.time();\n\tconst x = await wait(); // 每个都是都执行完才结,包括setTimeout（10*1000）的执行时间\n\tconst y = await wait(); // 执行顺序 x-\u003ey-\u003ez 同步执行，x 与 setTimeout 属于同步执行\n\tconst z = await wait();\n\tconsole.timeEnd(); // default: 30099.47705078125ms\n\t\n\tconsole.time();\n\tconst x1 = wait(); // x1,y1,z1 同时异步执行， 包括setTimeout（10*1000）的执行时间\n\tconst y1 = wait(); // x1 与 setTimeout 属于同步执行\n\tconst z1 = wait();\n\tawait x1;\n\tawait y1;\n\tawait z1;\n\tconsole.timeEnd(); // default: 10000.67822265625ms\n\t\n\tconsole.time();\n\tconst x2 = wait(); // x2,y2,z2 同步执行，但是不包括setTimeout（10*1000）的执行时间\n\tconst y2 = wait(); // x2 与 setTimeout 属于异步执行\n\tconst z2 = wait();\n\tx2,y2,z2;\n\tconsole.timeEnd(); // default: 0.065185546875ms\n}\nmain();\n```\n\n## 25.for in 和for of的区别\n\nfor..of适用遍历数/数组对象/字符串/map/set等拥有迭代器对象的集合.但是不能遍历对象,因为没有迭代器对象.与forEach()不同的是，它可以正确响应break、continue和return语句\n\nfor-of循环不支持普通对象，但如果你想迭代一个对象的属性，你可以用for-in循环（这也是它的本职工作）或内建的Object.keys()方法：\n\n## 26.数组扁平化处理：实现一个flatten方法，使得输入一个数组，该数组里面的元素也可以是数组，该方法会输出一个扁平化的数组。\n\n```\n// Example\nlet givenArr = [[1, 2, 2], [3, 4, 5, 5], [6, 7, 8, 9, [11, 12, [12, 13, [14]]]], 10];\nlet outputArr = [1,2,2,3,4,5,5,6,7,8,9,11,12,12,13,14,10]\n\n// 实现flatten方法使得\nflatten(givenArr)——\u003eoutputArr\n复制代码\n```\n\n年轻的我是用递归实现的QAQ，我的答案\n\n```javascript\nfunction flatten(arr){\n    var res = [];\n    for(var i=0;i\u003carr.length;i++){\n        if(Array.isArray(arr[i])){\n            res = res.concat(flatten(arr[i]));\n        }else{\n            res.push(arr[i]);\n        }\n    }\n    return res;\n}\n```\n\n其实你还可以这样\n\n```javascript\nfunction flatten(arr){\n    return arr.reduce(function(prev,item){\n        return prev.concat(Array.isArray(item)?flatten(item):item);\n    },[]);\n}\n```\n\n还可以使用ES6拓展运算符\n\n```javascript\nfunction flatten(arr){\n    while(arr.some(item=\u003eArray.isArray(item)){\n        arr = [].concat(...arr);\n    }\n    return arr;\n}\n```\n\n这是李魁昊写的：\n\n```javascript\nlet flatArr = (arr) =\u003e\n  arr.reduce(\n    (acc, value, index, arr) =\u003e\n      acc.concat(Array.isArray(value) ? flatArr(value) : value),\n    []\n  )\n```\n\n## 26.Async/await和Promise\n\nAsync/await是generator和Promise的语法糖，但仅仅是语法糖吗？ 它们两个的性能有没有区别呢， 又或者 promise.then()和await 同为微任务，但是它们的执行顺序是怎样的呢？\n\nAsync/Await与Promise最大区别在于：await b()会暂停所在的async函数的执行；而Promise.then(b)将b函数加入回调链中之后，会继续执行当前函数。对于堆栈来说，这个不同点非常关键。\n\n当一个Promise链抛出一个未处理的错误时，无论我们使用await b()还是Promise.then(b)，JavaScript引擎都需要打印错误信息及其堆栈。对于JavaScript引擎来说，两者获取堆栈的方式是不同的。\n\n### Promise.then()\n\n观察下面代码, 假设b()返回一个promise\n\n```\nconst a = () =\u003e {\n    b().then(() =\u003e c())\n}\n复制代码\n```\n\n当调用a()函数时，这些事情同步发生，b()函数产生一个promise对象，调用then方法，Promise会在将来的某个时刻resolve，也就是把then里的回调函数添加到回调链。(如果这一块不太明白，可以仔细学习promise，或者读一读promise源码并尝试写一写，相信你更通透)，这样，a()函数就执行完了，在这个过程中，a()函数并不会暂停，因此在异步函数resolve的时候，a()的作用域已经不存在了，那要如何生成包含a()的堆栈信息呢？ 为了解决这个问题，JavaScripts引擎要做一些额外的工作；它会及时记录并保存堆栈信息。对于V8引擎来说，这些堆栈信息随着Promise在Promise链中传递，这样c()函数在需要的时候也能获取堆栈信息。但是这无疑造成了额外的开销，会降低性能；保存堆栈信息会占用额外的内存。\n\n### Await\n\n我们可以用Async/await来实现一下\n\n```\nconst a = () =\u003e {\n    await b()\n    c()\n}\n复制代码\n```\n\n使用await的时候，无需存储堆栈信息，因为存储b()到a()的指针的足够了。当b()函数执行的时候，a()函数被暂停了，因此a()函数的作用域还在内存可以访问。如果b()抛出一个错误，堆栈通过指针迅速生成。如果c()函数抛出一个错误，堆栈信息也可以像同步函数一样生成，因为c()是在a()中执行的。不论是b()还是c()，我们都不需要去存储堆栈信息，因为堆栈信息可以在需要的时候立即生成。而存储指针，显然比存储堆栈更加节省内存\n\n### 结论\n\n很多ECMAScript语法特性看起来都只是些语法糖，其实并非如此，至少Async/await绝不仅仅是语法糖 为了让JavaScript引擎处理堆栈的方式性能更高，请尽量使用Async/await，而不是直接使用Promise。\n\n## 27.CommonJS和ES6模块化的区别以及如何解决让CommonJS导出的模块也能改变其内部变量\n\n### ES6 模块化\n\n1.export  \nexport可以输出变量、函数和类，切记不可直接输出值，否则会报错  \n2.export default  \n一个模块只能有一个默认输出，因此export default命令只能使用一次。所以，import命令后面才不用加大括号，因为只可能唯一对应export default命令  \n3.import  \nimport命令接受一对大括号，里面指定要从其他模块导入的变量名。大括号里面的变量名，必须与被导入模块对外接口的名称相同。如果想为输入的变量重新取一个名字，import命令要使用as关键字，将输入的变量重命名。\n\n`import {sum} from 'index.js';  \nimport {sum,age,name} from 'index.js';  \nimport {sum as hg, age as nl, name as xm} from 'index.js';`\n\nimport只会导入一次，无论你引入多少次  \n有提升效果，import会自动提升到顶部，首先执行  \nimport命令输入的变量都是只读的，因为它的本质是输入接口。也就是说，不允许在加载模块的脚本里面，改写接口。如果脚本加载了变量，对其重新赋值就会报错，因为变量是一个只读的接口。但是，如果是一个对象，改写对象的属性是允许的。（对象只能改变值但不能改变引用）  \n由于import是静态执行，所以不能使用表达式和变量，这些只有在运行时才能得到结果的语法结构。  \nimport后面的from指定模块文件的位置，可以是相对路径，也可以是绝对路径，.js后缀可以省略。如果只是模块名，不带有路径，那么必须有配置文件，告诉  \nJavaScript 引擎该模块的位置。  \n循环加载时，ES6模块是动态引用。只要两个模块之间存在某个引用，代码就能够执行。\n\n### CommonJs\n\n1.module.exports  \n2.require  \nCommonJs模块的特点\n\n所有代码都运行在模块作用域，不会污染全局作用域。  \n模块可以多次加载，但是只会在第一次加载时运行一次，然后运行结果就被缓存了，以后再加载，就直接读取缓存结果。要想让模块再次运行，必须清除缓存。  \n模块加载的顺序，按照其在代码中出现的顺序。  \nCommonJs规范加载模块是同步的，即只有加载完成，才能执行后面的操作  \nCommonJs模块的加载机制是，输入的是被输出的值的拷贝，即，一旦输出一个值，模块内部的变化影响不到这个值(关于这一条详细看下方举例1⃣️)  \n对于基本数据类型，属于复制。即会被模块缓存。同时，在另一个模块可以对该模块输出的变量重新赋值。对于复杂数据类型，属于浅拷贝。由于两个模块引用的对象指向同一个内存空间，因此对该模块的值做修改时会影响另一个模块。  \n当使用require命令加载某个模块时，就会运行整个模块的代码。  \n循环加载时，属于加载时执行。即脚本代码在require的时候，就会全部执行。一旦出现某个模块被\"循环加载\"，就只输出已经执行的部分，还未执行的部分不会输出。\n\n```javascript\n// lib.js\nvar counter = 3;\nfunction incCounter() {\n  counter++;\n}\nmodule.exports = {\n  counter: counter,\n  incCounter: incCounter,\n};\n// main.js\nvar mod = require('./lib');\n \nconsole.log(mod.counter);  // 3\nmod.incCounter();\nconsole.log(mod.counter); // 3\n```\n\n经过事实的检验我们可以得出，在CommonJs中，输入的是被输出的值的拷贝。  \n上面代码说明，lib.js模块加载以后，它的内部变化就影响不到输出的mod.counter了。这是因为mod.counter是一个原始类型的值，会被缓存。除非写成一个函数，才能得到内部变动后的值。  \n那commonJs怎么办呢 当然有！\n\n```javascript\nvar counter = 3;\nfunction incCounter() {\n    counter++;\n}\nmodule.exports = {\n    get counter() {\n        return counter\n    },\n    incCounter: incCounter,\n};\n```\n\n再看ES6 模块化\n\n```javascript\n// lib.js\nexport let counter = 3;\nexport function incCounter() {\n  counter++;\n}\n \n// main.js\nimport { counter, incCounter } from './lib';\n```\n\n从上面我们看出，CommonJS 模块输出的是值的拷贝，也就是说，一旦输出一个值，模块内部的变化就影响不到这个值。而ES6 模块是动态地去被加载的模块取值，并且变量总是绑定其所在的模块。\n\n另外CommonJS 加载的是一个对象（即module.exports属性），该对象只有在脚本运行完才会生成。而 ES6 模块不是对象，它的对外接口只是一种静态定义，在代码静态解析阶段就会生成。\n\n## 28.[webpack 中 loader 和 plugin 的区别是什么](https://github.com/Advanced-Frontend/Daily-Interview-Question/issues/308#)\n\n#### 主要区别\n\nloader 用于加载某些资源文件。 因为webpack 本身只能打包commonjs规范的js文件，对于其他资源例如 css，图片，或者其他的语法集，比如 jsx， coffee，是没有办法加载的。 这就需要对应的loader将资源转化，加载进来。从字面意思也能看出，loader是用于加载的，它作用于一个个文件上。\n\nplugin 用于扩展webpack的功能。它直接作用于 webpack，扩展了它的功能。当然loader也时变相的扩展了 webpack ，但是它只专注于转化文件（transform）这一个领域。而plugin的功能更加的丰富，而不仅局限于资源的加载。\n\n## 29.手写call、apply、bind实现及详解\n\napply接收两个参数，第一个参数为函数上下文this，第二个参数为函数参数只不过是通过一个数组的形式传入的。\n\n```\nallName.apply(obj, ['我是', '前端'])//我的全名是“我是一个前端” this指向obj\n```\n\ncall 接收多个参数，第一个为函数上下文也就是this，后边参数为函数本身的参数。\n\n```\n        let obj = {\n            name: \"一个\"\n        }\n\n        function allName(firstName, lastName) {\n            console.log(this)\n            console.log(`我的全名是“${firstName}${this.name}${lastName}”`)\n        }\n        // 很明显此时allName函数是没有name属性的\n        allName('我是', '前端') //我的全名是“我是前端”  this指向window\n        allName.call(obj, '我是', '前端') //我的全名是“我是一个前端” this指向ob\n```\n\n![image.png](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/面试/JavaScript/20210405162008.webp)\n\nbind 接收多个参数，第一个是bind返回值*返回值是一个函数*上下文的this，不会立即执行。\n\n```\n        let obj = {\n            name: \"一个\"\n        }\n\n        function allName(firstName, lastName, flag) {\n            console.log(this)\n            console.log(`我的全名是\"${firstName}${this.name}${lastName}\"我的座右铭是\"${flag}\"`)\n        }\n        allName.bind(obj) //不会执行\n        let fn = allName.bind(obj)\n        fn('我是', '前端', '好好学习天天向上')\n\n        // 也可以这样用，参数可以分开传。bind后的函数参数默认排列在原函数参数后边\n        fn = allName.bind(obj, \"你是\")\n        fn('前端', '好好学习天天向上')\n复制代码\n```\n\n接下来搓搓手实现call、apply和bind\n\n### call\n\n#### 定义与使用\n\n\u003e Function.prototype.call(): [developer.mozilla.org/zh-CN/docs/…](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Function/call)\n\n```\n// Function.prototype.call()样例\nfunction fun(arg1, arg2) {\n  console.log(this.name)\n  console.log(arg1 + arg2)\n}\nconst _this = { name: 'YIYING' }\n// 接受的是一个参数列表;方法立即执行\nfun.call(_this, 1, 2)\n复制代码\n// 输出：\nYIYING\n3\n复制代码\n```\n\n#### 手写实现\n\n```\n/**\n * 自定义call实现\n * @param context   上下文this对象\n * @param args      动态参数\n */\nFunction.prototype.ownCall = function(context, ...args) {\n  context = (typeof context === 'object' ? context : window)\n  // 防止覆盖掉原有属性\n  const key = Symbol()\n  // 这里的this为需要执行的方法\n  context[key] = this\n  // 方法执行\n  const result = context[key](...args)\n  delete context[key]\n  return result\n}\n复制代码\n// 验证样例\nfunction fun(arg1, arg2) {\n  console.log(this.name)\n  console.log(arg1 + arg2)\n}\nconst _this = { name: 'YIYING' }\n// 接受的是一个参数列表;方法立即执行\nfun.ownCall(_this, 1, 2)\n复制代码\n// 输出：\nYIYING\n3\n复制代码\n```\n\n### apply\n\n#### 定义与使用\n\n\u003e Function.prototype.apply(): [developer.mozilla.org/zh-CN/docs/…](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Function/apply)\n\n```\n// Function.prototype.apply()样例\nfunction fun(arg1, arg2) {\n  console.log(this.name)\n  console.log(arg1 + arg2)\n}\nconst _this = { name: 'YIYING' }\n// 参数为数组;方法立即执行\nfun.apply(_this, [1, 2])\n复制代码\n// 输出：\nYIYING\n3\n复制代码\n```\n\n#### 手写实现\n\n```javascript\n/**\n * 自定义Apply实现\n * @param context   上下文this对象\n * @param args      参数数组\n */\nFunction.prototype.ownApply = function(context, args) {\n  context = (typeof context === 'object' ? context : window)\n  // 防止覆盖掉原有属性\n  const key = Symbol()\n  // 这里的this为需要执行的方法\n  context[key] = this\n  // 方法执行\n  const result = context[key](...args)\n  delete context[key]\n  return result\n}\n复制代码\n// 验证样例\nfunction fun(arg1, arg2) {\n  console.log(this.name)\n  console.log(arg1 + arg2)\n}\nconst _this = { name: 'YIYING' }\n// 参数为数组;方法立即执行\nfun.ownApply(_this, [1, 2])\n复制代码\n// 输出：\nYIYING\n3\n```\n\n### bind\n\n#### 定义与使用\n\n\u003e Function.prototype.bind() : [developer.mozilla.org/zh-CN/docs/…](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Function/bind)\n\n```\n// Function.prototype.bind()样例\nfunction fun(arg1, arg2) {\n  console.log(this.name)\n  console.log(arg1 + arg2)\n}\nconst _this = { name: 'YIYING' }\n// 只变更fun中的this指向，返回新function对象\nconst newFun = fun.bind(_this)\nnewFun(1, 2)\n复制代码\n// 输出：\nYIYING\n3\n复制代码\n```\n\n#### 手写实现\n\n```\n/**\n * 自定义bind实现\n * @param context     上下文\n * @returns {Function}\n */\nFunction.prototype.ownBind = function(context) {\n  context = (typeof context === 'object' ? context : window)\n  return (...args)=\u003e{\n    this.call(context, ...args)\n  }\n}\n复制代码\n// 验证样例\nfunction fun(arg1, arg2) {\n  console.log(this.name)\n  console.log(arg1 + arg2)\n}\nconst _this = { name: 'YIYING' }\n// 只变更fun中的this指向，返回新function对象\nconst newFun = fun.ownBind(_this)\nnewFun(1, 2)\n复制代码\n// 输出：\nYIYING\n3\n```\n","lastmodified":"2023-05-09T16:33:58.291366399Z","tags":[]},"/kafka":{"title":"kafka","content":"\n**Kafka 概念**  \nKafka 是一种高吞吐量、分布式、基于发布/订阅的消息系统，最初由 LinkedIn 公司开发，使用 Scala 语言编写，目前是 Apache 的开源项目。![image.png](https://cdn.jsdelivr.net/gh/jieye-ericx/rax-picbed/PicList/obsidian/20230505_202305051420471_c028f.jpg)\n\n- **Kafka 主要组件broker：**Kafka 服务器，负责消息存储和转发\n- **topic：**消息类别，Kafka 按照 topic 来分类消息\n\n![图文详解：Kafka到底有哪些秘密让我对它情有独钟呢？-开源基础软件社区](https://dl-harmonyoscto.com/images/202204/e9b42d951c2b0680ce176138d46f033ca33039.png \"图文详解：Kafka到底有哪些秘密让我对它情有独钟呢？-开源基础软件社区\")\n\n**partition：**topic 的分区，一个 topic 可以包含多个 partition，topic 消息保存在各个 partition 上![图文详解：Kafka到底有哪些秘密让我对它情有独钟呢？-开源基础软件社区](https://dl-harmonyoscto.com/images/202204/134ee83537b5218c51f6639f77da19876228af.png \"图文详解：Kafka到底有哪些秘密让我对它情有独钟呢？-开源基础软件社区\")\n\n**offset：**消息在日志中的位置，可以理解是消息在 partition 上的偏移量，也是代表该消息的唯一序号![图文详解：Kafka到底有哪些秘密让我对它情有独钟呢？-开源基础软件社区](https://dl-harmonyoscto.com/images/202204/d60de4259ba0219fa60515692eb8c5830f81d9.png \"图文详解：Kafka到底有哪些秘密让我对它情有独钟呢？-开源基础软件社区\")\n\n- **Producer：**消息生产者\n- **Consumer：**消息消费者\n- **Consumer Group：**消费者分组，每个 Consumer 必须属于一个 group\n- **Zookeeper：**保存着集群 broker、topic、partition 等 meta 数据;另外，还负责 broker 故障发现，partition leader 选举，负载均衡等功能\n\n**Kafka 优点**\n\n- **解耦：**消息系统在处理过程中间插入了一个隐含的、基于数据的接口层，两边的处理过程都要实现这一接口。这允许你独立的扩展或修改两边的处理过程，只要确保**它们遵守同样的接口约束。**\n- **冗余：**消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的\"插入-获取-删除\"范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。\n- **扩展性：**因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。不需要改变代码、不需要调节参数。扩展就像调大电力按钮一样简单。\n- 灵活性 \u0026 峰值处理能力：使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。\n- **可恢复性：**消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。\n- **顺序保证：**大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。Kafka保证一个Partition内的消息的有序性。\n- **缓冲：**消息队列通过一个缓冲层来帮助任务最高效率的执行。写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。\n- **异步通信：**消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。\n\n**Kafka 应用场景**\n\n- **活动追踪：**跟踪网站⽤用户与前端应⽤用程序发⽣生的交互，如：网站PV/UV分析\n- **传递消息：**系统间异步的信息交互，如：营销活动(注册后发送券码福利利)\n- **日志收集：**收集系统及应⽤用程序的度量量指标及⽇日志，如：应用监控和告警\n- **提交日志：**将数据库的更更新发布到kafka上，如：交易统计\n\n![图文详解：Kafka到底有哪些秘密让我对它情有独钟呢？-开源基础软件社区](https://dl-harmonyoscto.com/images/202204/22c7772103d214751ad5438c118b2f44c0747d.png \"图文详解：Kafka到底有哪些秘密让我对它情有独钟呢？-开源基础软件社区\")\n\n**Kafka 数据存储设计**\n\n**partition 的数据文件**\n\npartition 中的每条 Message 包含三个属性：offset，MessageSize，data，其中 offset 表 示 Message 在这个 partition 中的偏移量，offset 不是该 Message 在 partition 数据文件中的实际存储位置，而是逻辑上一个值，它唯一确定了 partition 中的一条 Message，可以认为 offset 是 partition 中 Message 的 id;MessageSize 表示消息内容 data 的大小;data 为 Message 的具体内容。\n\n**数据文件分段 segment**\n\npartition 物理上由多个 segment 文件组成，每个 segment 大小相等，顺序读写。每个 segment数据文件以该段中最小的 offset 命名，文件扩展名为.log。这样在查找指定 offset 的 Message 的时候，用二分查找就可以定位到该 Message 在哪个 segment 数据文件中。\n\n**数据文件索引**\n\nKafka 为每个分段后的数据文件建立了索引文件，文件名与数据文件的名字是一样的，只是文件扩展名为.index。index 文件中并没有为数据文件中的每条 Message 建立索引，而是采用了稀疏存储的方式，每隔一定字节的数据建立一条索引。这样避免了索引文件占用过多的空间，从而可以将索引文件保留在内存中。![图文详解：Kafka到底有哪些秘密让我对它情有独钟呢？-开源基础软件社区](https://dl-harmonyoscto.com/images/202204/36ee4e6026e184d67f06907e8adeb754788c6a.png \"图文详解：Kafka到底有哪些秘密让我对它情有独钟呢？-开源基础软件社区\")\n\n**Zookeeper 在 kafka 的作用**\n\n无论是 kafka 集群，还是 producer 和 consumer 都依赖于 zookeeper 来保证系统可用性集群保存一些meta信息。\n\nKafka 使用 zookeeper 作为其分布式协调框架，很好的将消息生产、消息存储、消息消费的过程结合在一起。\n\n同时借助 zookeeper，kafka 能够生产者、消费者和 broker 在内的所以组件在无状态的情况下，建立起生产者和消费者的订阅关系，并实现生产者与消费者的负载均衡。\n\n**生产者设计**\n\n**负载均衡**\n\n由于消息 topic 由多个 partition 组成，且 partition 会均衡分布到不同 broker 上，因此，为了有效利用 broker 集群的性能，提高消息的吞吐量，producer 可以通过随机或者 hash 等方式，将消息平均发送到多个 partition 上，以实现负载均衡。![图文详解：Kafka到底有哪些秘密让我对它情有独钟呢？-开源基础软件社区](https://dl-harmonyoscto.com/images/202204/d6b296e2256a3949c44414f1e171760ce79c4b.png \"图文详解：Kafka到底有哪些秘密让我对它情有独钟呢？-开源基础软件社区\")\n\n**批量发送**\n\n是提高消息吞吐量重要的方式，Producer 端可以在内存中合并多条消息后，以一次请求的方式发送了批量的消息给 broker，从而大大减少 broker 存储消息的 IO 操作次数。但也一定程度上影响了消息的实时性，相当于以时延代价，换取更好的吞吐量。\n\n**压缩**\n\nKafka支持以集合(batch)为单位发送消息，在此基础上，Kafka还支持对消息集合进行压缩，Producer 端可以通过 GZIP 或 Snappy 格式对消息集合进行压缩。Producer 端进行压缩之后，在Consumer 端需进行解压。压缩的好处就是减少传输的数据量，减轻对网络传输的压力，在对大数据处理上，瓶颈往往体现在网络上而不是 CPU(压缩和解压会耗掉部分 CPU 资源)。\n\n那么如何区分消息是压缩的还是未压缩的呢，Kafka在消息头部添加了一个描述压缩属性字节，这个字节的后两位表示消息的压缩采用的编码，如果后两位为0，则表示消息未被压缩。\n\n**消费者设计**\n\n**Consumer Group**\n\n同一 Consumer Group 中的多个 Consumer 实例，不同时消费同一个 partition，等效于队列模式。partition 内消息是有序的，Consumer 通过 pull 方式消费消息。Kafka 不删除已消费的消息对于 partition，顺序读写磁盘数据，以时间复杂度 O(1)方式提供消息持久化能力。![图文详解：Kafka到底有哪些秘密让我对它情有独钟呢？-开源基础软件社区](https://dl-harmonyoscto.com/images/202204/164d96892f6d43900796113d1477b5395770fd.png \"图文详解：Kafka到底有哪些秘密让我对它情有独钟呢？-开源基础软件社区\")\n\n**实践应用**\n\n**Kafka 作为消息系统**\n\nkafka 通过在主题中具有并行性概念 - 分区 - ，Kafka能够在消费者流程池中提供订购保证和负载平衡。这是通过将主题中的分区分配给使用者组中的使用者来实现的，以便每个分区仅由该组中的一个使用者使用。通过这样做，我们确保使用者是该分区的唯一读者并按顺序使用数据。由于有许多分区，这仍然可以平衡许多消费者实例的负载。但请注意，消费者组中的消费者实例不能超过分区。\n\n**Kafka 作为存储系统**\n\nKafka是一个非常好的存储系统。写入Kafka的数据将写入磁盘并进行复制以实现容错。Kafka允许生产者等待确认，以便在完全复制之前写入不被认为是完整的，并且即使写入的服务器失败也保证写入仍然存在。\n\n磁盘结构Kafka很好地使用了规模 - 无论服务器上有50 KB还是50 TB的持久数据，Kafka都会执行相同的操作。\n\n由于认真对待存储并允许客户端控制其读取位置，您可以将Kafka视为一种专用于高性能，低延迟提交日志存储，复制和传播的专用分布式文件系统。\n\n**Kafka 用于流处理**\n\n对于复杂的转换，Kafka提供了完全集成的Streams API。这允许构建执行非平凡处理的应用程序，这些应用程序可以计算流的聚合或将流连接在一起。\n\n此工具有助于解决此类应用程序面临的难题：处理无序数据，在代码更改时重新处理输入，执行有状态计算等。\n\n流 API 构建在 Kafka 提供的核心原理上：它使用生产者和消费者 API 进行输入，使用 Kafka 进行8有状态存储，并在流处理器实例之间使用相同的组机制来实现容错*。\n","lastmodified":"2023-05-09T16:33:58.291366399Z","tags":["kafka","消息中间件"]},"/log":{"title":"log","content":"\n## Undolog\n\n- **实现事务回滚，保障事务的原子性**。事务处理过程中，如果出现了错误或者用户执行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。\n- **实现 MVCC（多版本并发控制）关键因素之一**。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。\n  ![[Pasted image 20230504002111.png]]\n\n### 持久化\n\nundo log 和数据页的刷盘策略是一样的，都需要通过 redo log 保证持久化。\n\nbuffer pool 中有 undo 页，对 undo 页的修改也都会记录到 redo log。redo log 会每秒刷盘，提交事务时也会刷盘，数据页和 undo 页都是靠这个机制保证持久化的。\n\n## Redolog\n\n**WAL （Write-Ahead Logging）技术指的是， MySQL 的写操作并不是立刻写到磁盘上，而是先写日志，然后在合适的时间再写到磁盘上**。 ^6b023b\n\n- **实现事务的持久性，让 MySQL 有 crash-safe 的能力**，能够保证 MySQL 在任何时间段突然崩溃，重启后之前已提交的记录都不会丢失；\n- **将写操作从「随机写」变成了「顺序写」**，提升 MySQL 写入磁盘的性能。\n  ![[Pasted image 20230504002840.png]]\n\n### 持久化\n\n![[Pasted image 20230504003619.png]]\n\n## binlog\n\n### 持久化\n\n一个事务的 binlog 是不能被拆开的，因此无论这个事务有多大（比如有很多条语句），也要保证一次性写入，MySQL 给每个线程分配了一片内存用于缓冲 binlog ，该内存叫 binlog cache\n![[Pasted image 20230504004446.png]]\nMySQL 提供一个 sync_binlog 参数来控制数据库的 binlog 刷到磁盘上的频率（每次提交事务都只 write，不 fsync **or** 每次提交事务都会 write，然后马上执行 fsync **or** 每次提交事务都 write，但累积 N 个事务后才 fsync）\n\n## 主从复制\n\n![[Pasted image 20230504004122.png]]\n在实际使用中，一个主库一般跟 2 ～ 3 个从库（1 套数据库，1 主 2 从 1 备主），这就是一主多从的 MySQL 集群结构。\n**三种模型**\n\n- **同步复制**：MySQL 主库提交事务的线程要等待所有从库的复制成功响应，才返回客户端结果。这种方式在实际项目中，基本上没法用，原因有两个：一是性能很差，因为要复制到所有节点才返回响应；二是可用性也很差，主库和所有从库任何一个数据库出问题，都会影响业务。\n- **异步复制**（默认模型）：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。\n- **半同步复制**：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。这种**半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险**。\n\n## update 语句的执行过程\n\n1. 执行器调用存储引擎的接口，通过**主键索引**树搜索记录：\n   - 数据页本来就在 buffer pool 中，就直接返回给执行器更新；\n   - 记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。\n2. 执行器得到聚簇索引记录后，会看一下更新前的记录和更新后的记录是否一样：\n   - 如果一样的话就不进行后续更新流程；\n   - 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的执行更新记录的操作；\n3. 开启事务， InnoDB 层更新记录前，首先要记录相应 undo log，旧值会写入 **Buffer Pool 中的 Undo 页面**，不过在内存修改该 Undo 页面后，需要**记录对应的 redo log**。\n4. InnoDB 层开始更新记录，会先更新内存（同时标记为**脏页**），然后将记录写到 redo log 里面，这个时候更新就算完成了。[[log#^6b023b|WAL]]!\n5. 至此，一条记录更新完了。\n6. 一条更新语句执行完后，开始记录该语句对应的 **binlog 被保存到 binlog cache**，并**没有**刷新到硬盘上的 binlog 文件，在**事务提交时**才会统一将该事务运行过程中的所有 binlog 刷新到硬盘。\n7. 事务提交，剩下的就是**两阶段提交**\n\n![[两阶段提交]]\n","lastmodified":"2023-05-09T16:33:58.291366399Z","tags":["mysql"]},"/nodejs":{"title":"Node 定时器","content":"\n## child_process\n\n在介绍child_process模块之前，先来看一个例子。\n\n```javascript\nconst http = require('http');\nconst longComputation = () =\u003e {\n  let sum = 0;\n  for (let i = 0; i \u003c 1e10; i++) {\n    sum += i;\n  };\n  return sum;\n};\nconst server = http.createServer();\nserver.on('request', (req, res) =\u003e {\n  if (req.url === '/compute') {\n    const sum = longComputation();\n    return res.end(`Sum is ${sum}`);\n  } else {\n    res.end('Ok')\n  }\n});\n\nserver.listen(3000);\n```\n\n可以试一下使用上面的代码启动Node.js服务，然后打开两个浏览器选项卡分别访问/compute和/，可以发现node服务接收到/compute请求时会进行大量的数值计算，导致无法响应其他的请求（/）。\n\n在Java语言中可以通过多线程的方式来解决上述的问题，但是Node.js在代码执行的时候是单线程的，那么Node.js应该如何解决上面的问题呢？其实Node.js可以创建一个子进程执行密集的cpu计算任务（例如上面例子中的longComputation）来解决问题，而child_process模块正是用来创建子进程的。\n\n### 创建子进程的方式\n\nchild_process提供了几种创建子进程的方式\n\n- 异步方式：spawn、exec、execFile、fork\n- 同步方式：spawnSync、execSync、execFileSync\n\n首先介绍一下spawn方法\n\n```javascript\nchild_process.spawn(command[, args][, options])\n\ncommand： 要执行的指令\nargs：    传递参数\noptions： 配置项\n\nconst { spawn } = require('child_process');\nconst child = spawn('pwd');\n```\n\npwd是shell的命令，用于获取当前的目录，上面的代码执行完控制台并没有任何的信息输出，这是为什么呢？\n\n控制台之所以不能看到输出信息的原因是由于子进程有自己的stdio流（stdin、stdout、stderr），控制台的输出是与当前进程的stdio绑定的，因此如果希望看到输出信息，可以通过在子进程的stdout 与当前进程的stdout之间建立管道实现\n\n```javascript\nchild.stdout.pipe(process.stdout);\n```\n\n也可以监听事件的方式（子进程的stdio流都是实现了EventEmitter API的，所以可以添加事件监听）\n\n```javascript\nchild.stdout.on('data', function(data) {\n  process.stdout.write(data);\n});\n```\n\n*在Node.js代码里使用的console.log其实底层依赖的就是process.stdout*\n\n除了建立管道之外，还可以通过子进程和当前进程共用stdio的方式来实现\n\n```javascript\nconst { spawn } = require('child_process');\nconst child = spawn('pwd', {\n  stdio: 'inherit'\n});\n```\n\nstdio选项用于配置父进程和子进程之间建立的管道，由于stdio管道有三个(stdin, stdout, stderr）因此stdio的三个可能的值其实是数组的一种简写\n\n- pipe 相当于['pipe', 'pipe', 'pipe']（默认值）\n- ignore 相当于['ignore', 'ignore', 'ignore']\n- inherit 相当于[process.stdin, process.stdout, process.stderr]\n\n由于inherit方式使得子进程直接使用父进程的stdio，因此可以看到输出\n\nignore用于忽略子进程的输出（将/dev/null指定为子进程的文件描述符了），因此当ignore时child.stdout是null。\n\nspawn默认情况下并不会创建子shell来执行命令，因此下面的代码会报错\n\n```javascript\nconst { spawn } = require('child_process');\nconst child = spawn('ls -l');\nchild.stdout.pipe(process.stdout);\n\n// 报错\nevents.js:167\n      throw er; // Unhandled 'error' event\n      ^\n\nError: spawn ls -l ENOENT\n    at Process.ChildProcess._handle.onexit (internal/child_process.js:229:19)\n    at onErrorNT (internal/child_process.js:406:16)\n    at process._tickCallback (internal/process/next_tick.js:63:19)\n    at Function.Module.runMain (internal/modules/cjs/loader.js:746:11)\n    at startup (internal/bootstrap/node.js:238:19)\n    at bootstrapNodeJSCore (internal/bootstrap/node.js:572:3)\nEmitted 'error' event at:\n    at Process.ChildProcess._handle.onexit (internal/child_process.js:235:12)\n    at onErrorNT (internal/child_process.js:406:16)\n    [... lines matching original stack trace ...]\n    at bootstrapNodeJSCore (internal/bootstrap/node.js:572:3)\n```\n\n如果需要传递参数的话，应该采用数组的方式传入\n\n```javascript\nconst { spawn } = require('child_process');\nconst child = spawn('ls', ['-l']);\nchild.stdout.pipe(process.stdout);\n```\n\n如果要执行`ls -l | wc -l`命令的话可以采用创建两个spawn命令的方式\n\n```javascript\nconst { spawn } = require('child_process');\nconst child = spawn('ls', ['-l']);\nconst child2 = spawn('wc', ['-l']);\nchild.stdout.pipe(child2.stdin);\nchild2.stdout.pipe(process.stdout);\n```\n\n也可以使用`exec`\n\n```javascript\nconst { exec } = require('child_process');\nexec('ls -l | wc -l', function(err, stdout, stderr) {\n  console.log(stdout);\n});\n```\n\n由于exec会创建子shell，所以可以直接执行shell管道命令。spawn采用流的方式来输出命令的执行结果，而exec也是将命令的执行结果缓存起来统一放在回调函数的参数里面，因此exec只适用于命令执行结果数据小的情况。\n\n其实spawn也可以通过配置shell option的方式来创建子shell进而支持管道命令，如下所示\n\n```javascript\nconst { spawn, execFile } = require('child_process');\nconst child = spawn('ls -l | wc -l', {\n  shell: true\n});\nchild.stdout.pipe(process.stdout);\n```\n\n配置项除了stdio、shell之外还有cwd、env、detached等常用的选项\n\ncwd用于修改命令的执行目录\n\n```javascript\nconst { spawn, execFile, fork } = require('child_process');\nconst child = spawn('ls -l | wc -l', {\n  shell: true,\n  cwd: '/usr'\n});\nchild.stdout.pipe(process.stdout);\n```\n\nenv用于指定子进程的环境变量（如果不指定的话，默认获取当前进程的环境变量）\n\n```javascript\nconst { spawn, execFile, fork } = require('child_process');\nconst child = spawn('echo $NODE_ENV', {\n  shell: true,\n  cwd: '/usr'\n});\nchild.stdout.pipe(process.stdout);\nNODE_ENV=randal node b.js\n\n// 输出结果\nrandal\n```\n\n如果指定env的话就会覆盖掉默认的环境变量，如下\n\n```javascript\nconst { spawn, execFile, fork } = require('child_process');\nspawn('echo $NODE_TEST $NODE_ENV', {\n  shell: true,\n  stdio: 'inherit',\n  cwd: '/usr',\n  env: {\n    NODE_TEST: 'randal-env'\n  }\n});\n\nNODE_ENV=randal node b.js\n\n// 输出结果\nrandal-env\n```\n\ndetached用于将子进程与父进程断开连接\n\n例如假设存在一个长时间运行的子进程\n\n```javascript\n// timer.js\nwhile(true) {\n\n}\n```\n\n但是主进程并不需要长时间运行的话就可以用detached来断开二者之间的连接\n\n```javascript\nconst { spawn, execFile, fork } = require('child_process');\nconst child = spawn('node', ['timer.js'], {\n  detached: true,\n  stdio: 'ignore'\n});\nchild.unref();\n```\n\n当调用子进程的unref方法时，同时配置子进程的stdio为ignore时，父进程就可以独立退出了\n\nexecFile与exec不同，execFile通常用于执行文件，而且并不会创建子shell环境\n\nfork方法是spawn方法的一个特例，fork用于执行js文件创建Node.js子进程。而且fork方式创建的子进程与父进程之间建立了IPC通信管道，因此子进程和父进程之间可以通过send的方式发送消息。\n\n*注意：fork方式创建的子进程与父进程是完全独立的，它拥有单独的内存，单独的V8实例，因此并不推荐创建很多的Node.js子进程*\n\nfork方式的父子进程之间的通信参照下面的例子\n\nparent.js\n\n```javascript\nconst { fork } = require('child_process');\n\nconst forked = fork('child.js');\n\nforked.on('message', (msg) =\u003e {\n  console.log('Message from child', msg);\n});\n\nforked.send({ hello: 'world' });\n```\n\nchild.js\n\n```javascript\nprocess.on('message', (msg) =\u003e {\n  console.log('Message from parent:', msg);\n});\n\nlet counter = 0;\n\nsetInterval(() =\u003e {\n  process.send({ counter: counter++ });\n}, 1000);\n//------------------\nnode parent.js\n\n// 输出结果\nMessage from parent: { hello: 'world' }\nMessage from child { counter: 0 }\nMessage from child { counter: 1 }\nMessage from child { counter: 2 }\nMessage from child { counter: 3 }\nMessage from child { counter: 4 }\nMessage from child { counter: 5 }\nMessage from child { counter: 6 }\n```\n\n回到本文初的那个问题，我们就可以将密集计算的逻辑放到单独的js文件中，然后再通过fork的方式来计算，等计算完成时再通知主进程计算结果，这样避免主进程繁忙的情况了。\n\ncompute.js\n\n```javascript\nconst longComputation = () =\u003e {\n  let sum = 0;\n  for (let i = 0; i \u003c 1e10; i++) {\n    sum += i;\n  };\n  return sum;\n};\n\nprocess.on('message', (msg) =\u003e {\n  const sum = longComputation();\n  process.send(sum);\n});\n```\n\nindex.js\n\n```javascript\nconst http = require('http');\nconst { fork } = require('child_process');\n\nconst server = http.createServer();\n\nserver.on('request', (req, res) =\u003e {\n  if (req.url === '/compute') {\n    const compute = fork('compute.js');\n    compute.send('start');\n    compute.on('message', sum =\u003e {\n      res.end(`Sum is ${sum}`);\n    });\n  } else {\n    res.end('Ok')\n  }\n});\n\nserver.listen(3000);\n```\n\n### 监听进程事件\n\n通过前述几种方式创建的子进程都实现了EventEmitter，因此可以针对进程进行事件监听\n\n常用的事件包括几种：close、exit、error、message\n\nclose事件当子进程的stdio流关闭的时候才会触发，并不是子进程exit的时候close事件就一定会触发，因为多个子进程可以共用相同的stdio。\n\nclose与exit事件的回调函数有两个参数code和signal，code代码子进程最终的退出码，如果子进程是由于接收到signal信号终止的话，signal会记录子进程接受的signal值。\n\n先看一个正常退出的例子\n\n```javascript\nconst { spawn, exec, execFile, fork } = require('child_process');\nconst child = exec('ls -l', {\n  timeout: 300\n});\nchild.on('exit', function(code, signal) {\n  console.log(code);\n  console.log(signal);\n});\n\n// 输出结果\n0\nnull\n```\n\n再看一个因为接收到signal而终止的例子，应用之前的timer文件，使用exec执行的时候并指定timeout\n\n```javascript\nconst { spawn, exec, execFile, fork } = require('child_process');\nconst child = exec('node timer.js', {\n  timeout: 300\n});\nchild.on('exit', function(code, signal) {\n  console.log(code);\n  console.log(signal);\n});\n// 输出结果\nnull\nSIGTERM\n```\n\n*注意：由于timeout超时的时候error事件并不会触发，并且当error事件触发时exit事件并不一定会被触发*\n\nerror事件的触发条件有以下几种：\n\n- 无法创建进程\n- 无法结束进程\n- 给进程发送消息失败\n\n*注意当代码执行出错的时候，error事件并不会触发，exit事件会触发，code为非0的异常退出码*\n\n```javascript\nconst { spawn, exec, execFile, fork } = require('child_process');\nconst child = exec('ls -l /usrs');\nchild.on('error', function(code, signal) {\n  console.log(code);\n  console.log(signal);\n});\nchild.on('exit', function(code, signal) {\n  console.log('exit');\n  console.log(code);\n  console.log(signal);\n});\n\n// 输出结果\nexit\n1\nnull\n```\n\nmessage事件适用于父子进程之间建立IPC通信管道的时候的信息传递，传递的过程中会经历序列化与反序列化的步骤，因此最终接收到的并不一定与发送的数据相一致。\n\n```javascript\n//sub.js\nprocess.send({ foo: 'bar', baz: NaN });\n//main.js\nconst cp = require('child_process');\nconst n = cp.fork(`${__dirname}/sub.js`);\n\nn.on('message', (m) =\u003e {\n  console.log('got message:', m);   // got message: { foo: 'bar', baz: null }\n});\n```\n\n关于message有一种特殊情况要注意，下面的message并不会被子进程接收到\n\n```javascript\nconst { fork } = require('child_process');\n\nconst forked = fork('child.js');\n\nforked.send({\n  cmd: \"NODE_foo\",\n  hello: 'world'\n});\n```\n\n当发送的消息里面包含cmd属性，并且属性的值是以`NODE_`开头的话，这样的消息是提供给Node.js本身保留使用的，因此并不会发出`message`事件，而是会发出`internalMessage`事件，开发者应该避免这种类型的消息，并且应当避免监听`internalMessage`事件。\n\nmessage除了发送字符串、object之外还支持发送server对象和socket对象，正因为支持socket对象才可以做到多个Node.js进程监听相同的端口号。\n\n未完待续……\n\n## cluster（集群）\n\n单个 Node.js 实例运行在单个线程中。 为了充分利用多核系统，有时需要启用一组 Node.js 进程去处理负载任务。\n\n集群有以下两种常见的实现方案，而node自带的cluster模块，**采用了方案二**。\n\n1. 方案一：多个node实例+多个端口\n\n集群内的node实例，各自监听不同的端口，再由反向代理实现请求到多个端口的分发。\n\n- 优点：实现简单，各实例相对独立，这对服务稳定性有好处。\n- 缺点：增加端口占用，进程之间通信比较麻烦。\n\n2. 方案二：主进程向子进程转发请求\n\n集群内，创建一个主进程(master)，以及若干个子进程(worker)。由master监听客户端连接请求，并根据特定的策略，转发给worker。\n\n- 优点：通常只占用一个端口，通信相对简单，转发策略更灵活。\n- 缺点：实现相对复杂，对主进程的稳定性要求较高。\n\n`cluster` 模块可以创建共享服务器端口的子进程。\n\n```js\nconst cluster = require('cluster');\nconst http = require('http');\nconst numCPUs = require('os').cpus().length;\n\nif (cluster.isMaster) {\n  console.log(`主进程 ${process.pid} 正在运行`);\n\n  // 衍生工作进程。\n  for (let i = 0; i \u003c numCPUs; i++) {\n    cluster.fork();\n  }\n\n  cluster.on('exit', (worker, code, signal) =\u003e {\n    console.log(`工作进程 ${worker.process.pid} 已退出`);\n  });\n} else {\n  // 工作进程可以共享任何 TCP 连接。\n  // 在本例子中，共享的是 HTTP 服务器。\n  http.createServer((req, res) =\u003e {\n    res.writeHead(200);\n    // res.end('你好世界\\n');\n    res.end(`response from worker ${process.pid}`);\n  }).listen(8000);\n\n  console.log(`工作进程 ${process.pid} 已启动`);\n}\n```\n\n运行代码，则工作进程会共享 8000 端口：\n\n```console\n$ node server.js\n主进程 3596 正在运行\n工作进程 4324 已启动\n工作进程 4520 已启动\n工作进程 6056 已启动\n工作进程 5644 已启动\n```\n\n在 Windows 上，尚无法在工作进程中设置命名管道服务器。\n\n创建批处理脚本：./req.sh。\n\n```shell\n#!/bin/bash\n\n# req.sh\nfor((i=1;i\u003c=4;i++)); do   \n  curl http://127.0.0.1:3000\n  echo \"\"\ndone \n```\n\n输出如下。可以看到，响应来自不同的进程。\n\n```\nresponse from worker 23735\nresponse from worker 23731\nresponse from worker 23729\nresponse from worker 23730\n```\n\n### 实现原理\n\n#### 问题1：master、worker如何通信\n\n这个问题比较简单。master进程通过 cluster.fork() 来创建 worker进程。cluster.fork() 内部 是通过 child_process.fork() 来创建子进程。\n\n也就是说：\n\n1. master进程、worker进程是父、子进程的关系。\n2. master进程、woker进程可以通过**IPC通道**进行通信。（重要）\n\n#### 问题2：如何实现端口共享\n\n在前面的例子中，多个woker中创建的server监听了同个端口3000。通常来说，多个进程监听同个端口，系统会报错。\n\n为什么我们的例子没问题呢？\n\n秘密在于，net模块中，对 listen() 方法进行了特殊处理。根据当前进程是master进程，还是worker进程：\n\n1. master进程：在该端口上正常监听请求。（没做特殊处理）\n2. worker进程：创建server实例。然后通过IPC通道，向master进程发送消息，让master进程也创建 server 实例，并在该端口上监听请求。当请求进来时，master进程将请求转发给worker进程的server实例。\n\n归纳起来，就是：master进程监听特定端口，并将客户请求转发给worker进程。\n\n如下图所示：\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/node/20210410171339.png)\n\n#### 问题3：如何将请求分发到多个worker\n\n每当worker进程创建server实例来监听请求，都会通过IPC通道，在master上进行注册。当客户端请求到达，master会负责将请求转发给对应的worker。\n\n具体转发给哪个worker？这是由转发策略决定的。可以通过环境变量NODE_CLUSTER_SCHED_POLICY设置，也可以在cluster.setupMaster(options)时传入。\n\n默认的转发策略是轮询（SCHED_RR）。\n\n当有客户请求到达，master会轮询一遍worker列表，找到第一个空闲的worker，然后将该请求转发给该worker。\n\n## process\n\n**[文档](http://nodejs.cn/api/process.html)**\n\n`process` 对象是一个全局变量，它提供有关当前 Node.js 进程的信息并对其进行控制。 作为一个全局变量，它始终可供 Node.js 应用程序使用，无需使用 `require()`。 它也可以使用 `require()` 显式地访问：\n\n```js\nconst process = require('process');\n```\n\n### process.env\n\n`process.env` 属性返回包含用户环境的对象。\n\n此对象的示例如下所示：\n\n```js\n{\n  TERM: 'xterm-256color',\n  SHELL: '/usr/local/bin/bash',\n  USER: 'maciej',\n  PATH: '~/.bin/:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin',\n  PWD: '/Users/maciej',\n  EDITOR: 'vim',\n  SHLVL: '1',\n  HOME: '/Users/maciej',\n  LOGNAME: 'maciej',\n  _: '/usr/local/bin/node'\n}\n```\n\n可以修改此对象，但这些修改不会反映到 Node.js 进程之外，或者（除非明确请求）反映到其他 [`Worker`](http://nodejs.cn/s/S15DqM) 线程。 换句话说，以下示例不起作用：\n\n```console\n$ node -e 'process.env.foo = \"bar\"' \u0026\u0026 echo $foo\n```\n\n以下示例则起作用：\n\n```js\nprocess.env.foo = 'bar';\nconsole.log(process.env.foo);\n```\n\n在 `process.env` 上分配属性将**隐式地将值转换为字符串**。 不推荐使用此行为。 当值不是字符串、数字或布尔值时，Node.js 的未来版本可能会抛出错误。\n\n```js\nprocess.env.test = null;\nconsole.log(process.env.test);\n// =\u003e 'null'\nprocess.env.test = undefined;\nconsole.log(process.env.test);\n// =\u003e 'undefined'\n```\n\n使用 `delete` 可以从 `process.env` 中删除属性。\n\n```js\nprocess.env.TEST = 1;\ndelete process.env.TEST;\nconsole.log(process.env.TEST);\n// =\u003e undefined\n```\n\n在 Windows 操作系统上，环境变量不区分大小写。\n\n```js\nprocess.env.TEST = 1;\nconsole.log(process.env.test);\n// =\u003e 1\n```\n\n除非在创建 [`Worker`](http://nodejs.cn/s/S15DqM) 实例时明确指定，否则每个 [`Worker`](http://nodejs.cn/s/S15DqM) 线程都有自己的 `process.env` 副本，基于其父线程的 `process.env`，或者指定为 [`Worker`](http://nodejs.cn/s/S15DqM) 构造函数的 `env` 选项的任何内容。 对于 `process.env` 的更改将在 [`Worker`](http://nodejs.cn/s/S15DqM) 线程中不可见，并且只有主线程可以进行对操作系统或本机加载项可见的更改。\n\n### 资源使用\n\n资源使用指运行此进程所消耗的机器资源。例如内存、cpu\n\n#### 内存\n\n```\nprocess.memoryUsage())\n\n{ rss: 21848064,\n  heapTotal: 7159808,\n  heapUsed: 4431688,\n  external: 8224 \n }\n```\n\nrss(常驻内存)的组成见下图\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/node/20210410171340.png)\n\n`code segment` 对应当前运行的代码\n\n`external` 对应的是C++对象（与V8管理的JS对象绑定）的占用的内存，比如Buffer的使用\n\n```\nBuffer.allocUnsafe(1024 * 1024 * 1000);\nconsole.log(process.memoryUsage());\n\n{ rss: 22052864,\n  heapTotal: 6635520,\n  heapUsed: 4161376,\n  external: 1048584224 }\n复制代码\n```\n\n#### cpu\n\n```javascript\nconst startUsage = process.cpuUsage();\nconsole.log(startUsage);\n\nconst now = Date.now();\nwhile (Date.now() - now \u003c 500);\n\nconsole.log(process.cpuUsage());\nconsole.log(process.cpuUsage(startUsage)); //相对时间\n\n// { user: 59459, system: 18966 }\n// { user: 558135, system: 22312 }\n// { user: 498432, system: 3333 }\n```\n\nuser对应用户时间，system代表系统时间\n\n### 运行环境\n\n运行环境指此进程运行的宿主环境包括运行目录、node环境、CPU架构、用户环境、系统平台\n\n#### 运行目录\n\n```javascript\nconsole.log(`Current directory: ${process.cwd()}`);\n\n// Current directory: /Users/xxxx/workspace/learn/node-basic/process\n```\n\n#### node环境\n\n```javascript\nconsole.log(process.version)\n\n// v9.1.0\n```\n\n如果不仅仅希望获得node的版本信息，还希望v8、zlib、libuv版本等信息的话就需要使用process.versions了\n\n```javascript\nconsole.log(process.versions);\n{ http_parser: '2.7.0',\n  node: '9.1.0',\n  v8: '6.2.414.32-node.8',\n  uv: '1.15.0',\n  zlib: '1.2.11',\n  ares: '1.13.0',\n  modules: '59',\n  nghttp2: '1.25.0',\n  openssl: '1.0.2m',\n  icu: '59.1',\n  unicode: '9.0',\n  cldr: '31.0.1',\n  tz: '2017b' }\n```\n\n#### cpu架构\n\n```javascript\nconsole.log(`This processor architecture is ${process.arch}`);\n\n// This processor architecture is x64\n```\n\n支持的值包括：`'arm'`, `'arm64'`, `'ia32'`, `'mips'`, `'mipsel'`, `'ppc'`, `'ppc64'`, `'s390'`, `'s390x'`, `'x32'` `'x64'`\n\n#### 用户环境\n\n```javascript\nconsole.log(process.env.NODE_ENV); // dev\n\nNODE_ENV=dev node b.js\n```\n\n除了启动时的自定义信息之外，process.env还可以获得其他的用户环境信息（比如PATH、SHELL、HOME等），感兴趣的可以自己打印一下试试\n\n#### 系统平台\n\n```javascript\nconsole.log(`This platform is ${process.platform}`);\n\nThis platform is darwin\n```\n\n支持的系统平台包括：`'aix'` `'darwin'` `'freebsd'` `'linux'` `'openbsd'` `'sunos'` `'win32'`\n\nandroid目前还处于试验阶段\n\n### 运行状态\n\n运行状态指当前进程的运行相关的信息包括启动参数、执行目录、主文件、PID信息、运行时间\n\n#### 启动参数\n\n获取启动参数有三个方法，execArgv获取Node.js的命令行选项（见[官网文档](https://nodejs.org/api/cli.html)）\n\nargv获取非命令行选项的信息，argv0则获取argv[0]的值（略有差异)\n\n```javascript\nconsole.log(process.argv)\nconsole.log(process.argv0)\nconsole.log(process.execArgv)\n\nnode --harmony  b.js foo=bar --version\n\n// 输出结果\n[ '/Users/xiji/.nvm/versions/node/v9.1.0/bin/node',\n  '/Users/xiji/workspace/learn/node-basic/process/b.js',\n  'foo=bar',\n  '--version' ]\nnode\n[ '--harmony' ]\n```\n\n#### 执行目录\n\n```javascript\nconsole.log(process.execPath);\n\n// /Users/xxxx/.nvm/versions/node/v9.1.0/bin/node\n```\n\n#### 运行时间\n\n```javascript\nvar date = new Date();\nwhile(new Date() - date \u003c 500) {}\nconsole.log(process.uptime()); // 0.569\n```\n\n#### 主文件\n\n除了require.main之外也可以通过process.mainModule来判断一个模块是否是主文件\n\n```javascript\n//a.js\nconsole.log(`module A: ${process.mainModule === module}`);\n\n//b.js\nrequire('./a');\nconsole.log(`module B: ${process.mainModule === module}`);\n\nnode b.js\n// 输出\nmodule A: false\nmodule B: true\n```\n\nPID信息\n\n```javascript\nconsole.log(`This process is pid ${process.pid}`); //This process is pid 12554\n```\n\n### 监听事件\n\nprocess是EventEmiiter的实例对象，因此可以使用process.on('eventName', () =\u003e {})来监听事件。 常用的事件类型分两种：\n\n- 进程状态 比如：beforeExit、exit、uncaughtException、message\n- 信号事件 比如：SIGTERM、SIGKILL、SIGUSR1\n\nbeforeExit与exit的区别有两方面：\n\n- beforeExit里面可以执行异步代码、exit只能是同步代码\n- 手动调用process.exit()或者触发uncaptException导致进程退出不会触发beforeExit事件、exit事件会触发。\n\n因此下面的代码console都不会被执行\n\n```javascript\nprocess.on('beforeExit', function(code) {\n  console.log('before exit: '+ code);\n});\nprocess.on('exit', function(code) {\n  setTimeout(function() {\n    console.log('exit: ' + code);\n  }, 0);\n});\na.b();\n```\n\n当异常一直没有被捕获处理的话，最后就会触发'uncaughtException'事件。默认情况下，Node.js会打印堆栈信息到stderr然后退出进程。不要试图阻止uncaughtException退出进程，因此此时程序的状态可能已经不稳定了，建议的方式是及时捕获处理代码中的错误，uncaughtException里面只做一些清理工作。\n\n**注意：node的9.3版本增加了process.setUncaughtExceptionCaptureCallback方法**\n\n当process.setUncaughtExceptionCaptureCallback(fn)指定了监听函数的时候，uncaughtException事件将会不再被触发。\n\n```javascript\nprocess.on('uncaughtException', function() {\n  console.log('uncaught listener');\n});\n\nprocess.setUncaughtExceptionCaptureCallback(function() {\n  console.log('uncaught fn');\n});\n\na.b();\n// uncaught fn\n```\n\nmessage适用于父子进程之间发送消息，关于如何创建父子进程请参见[child_process模块解读](https://juejin.im/post/5b10a814f265da6e2a08a6f7)。\n\nSIGTERM信号虽然也是用于请求终止Node.js进程，但是它与SIGKILL有所不同，进程可以选择响应还是忽略此信号。 SIGTERM会以一种友好的方式来结束进程，在进程结束之前先释放已分配的资源（比如数据库连接），因此这种方式被称为优雅关闭(graceful shutdown) 具体的执行步骤如下：\n\n- 应用程序被通知需要关闭（接收到SIGTERM信号）\n- 应用程序通知负载均衡不再接收新的请求\n- 应用程序完成正在进行中的请求\n- 释放资源（例如数据库连接）\n- 应用程序正常退出，退出状态码为0\n\nSIGUSR1 Node.js当接收到SIGUSR1信号时会启动内置的调试器，当执行下列操作时\n\n```shell\nkill -USR1 PID_OF_THE_NODE_JS_PROCESS\n```\n\n可以看到node.js会启动调试器代理，端口是9229\n\n```javascript\nserver is listening 8089\nDebugger listening on ws://127.0.0.1:9229/7ef98ccb-02fa-451a-8954-4706bd74105f\nFor help, see: https://nodejs.org/en/docs/inspector\n```\n\n也可以在服务启动时使用--inspect 来启动调试代理\n\n```shell\nnode --inspect index.js\n```\n\n### 调度任务 process.nextTick(fn)\n\n详见目录 定时器\n\n`process.nextTick(fn)`\n\n通过process.nextTick调度的任务是异步任务，EventLoop是分阶段的，每个阶段执行特定的任务，而nextTick的任务在阶段切换的时候就会执行，因此nextTick会比setTimeout(fn, 0)更快的执行，关于EventLoop见下。\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/node/20210410171341.png)\n\nNode.js是单线程的，除了系统IO之外，在它的事件轮询过程中，同一时间只会处理一个事件。你可以把事件轮询想象成一个大的队列，在每个**时间点**上，系统只会处理一个事件。即使你的电脑有多个CPU核心，你也无法同时并行的处理多个事件。但也就是这种特性使得node.js适合处理I／O型的应用，不适合那种CPU运算型的应用。在每个I／O型的应用中，你只需要给每一个输入输出定义一个回调函数即可，他们会自动加入到事件轮询的处理队列里。当I／O操作完成后，这个回调函数会被触发。然后系统会继续处理其他的请求。\n\n在这种处理模式下，process.nextTick()的意思就是定义出一个动作，并且让这个动作在下一个事件轮询的时间点上执行。我们来看一个例子。例子中有一个foo()，你想在下一个时间点上调用他，可以这么做：\n\n```javascript\nfunction foo() {\n    console.error('foo');\n}\n\nprocess.nextTick(foo);\nconsole.error('bar');\n```\n\n运行上面的代码，你从下面终端打印的信息会看到，\"bar\"的输出在“foo”的前面。这就验证了上面的说法，foo()是在下一个时间点运行的。\n\n```javascript\nbar\nfoo \n```\n\n你也可以使用setTimeout()函数来达到貌似同样的执行效果：\n\n```javascript\nsetTimeout(foo, 0);\nconsole.log('bar');\n```\n\n但在内部的处理机制上，process.nextTick()和setTimeout(fn, 0)是不同的，process.nextTick()不是一个单纯的延时，他有更多的 [特性](https://gist.github.com/1257394)。\n\n更精确的说，process.nextTick()定义的调用会创建一个新的子堆栈。在当前的栈里，你可以执行任意多的操作。但一旦调用netxTic\tk，函数就必须返回到父堆栈。然后事件轮询机制又重新等待处理新的事件，如果发现nextTick的调用，就会创建一个新的栈。\n\n#### Process.nextTick 和 setImmediate 的区别\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/node/20210410171342.png)\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/node/20210410171343.png)\n\n### 发出警告\n\n```javascript\nprocess.emitWarning('Something warning happened!', {\n  code: 'MY_WARNING',\n  type: 'XXXX'\n});\n\n// (node:14771) [MY_WARNING] XXXX: Something warning happened!\n```\n\n当type为DeprecationWarning时，可以通过命令行选项施加影响\n\n- `--throw-deprecation` 会抛出异常\n- `--no-deprecation` 不输出DeprecationWarning\n- `--trace-deprecation` 打印详细堆栈信息\n\n```javascript\nprocess.emitWarning('Something warning happened!', {\n  type: 'DeprecationWarning'\n});\nconsole.log(4);\n\nnode --throw-deprecation index.js\nnode --no-deprecation index.js\nnode --trace-deprecation index.js\n```\n\n# Node 定时器\n\nJavaScript 是单线程运行，异步操作特别重要。\n\n只要用到引擎之外的功能，就需要跟外部交互，从而形成异步操作。由于异步操作实在太多，JavaScript 不得不提供很多异步语法。这就好比，有些人老是受打击， 他的抗打击能力必须变得很强，否则他就完蛋了。\n\nNode 的异步语法比浏览器更复杂，因为它可以跟内核对话，不得不搞了一个专门的库 [libuv](http://thlorenz.com/learnuv/book/history/history_1.html) 做这件事。这个库负责各种回调函数的执行时间，毕竟异步任务最后还是要回到主线程，一个个排队执行。\n\n![](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/node/20210410171344.png)\n\n为了协调异步任务，Node 居然提供了四个定时器，让任务可以在指定的时间运行。\n\n\u003e - setTimeout()\n\u003e - setInterval()\n\u003e - setImmediate()\n\u003e - process.nextTick()\n\n前两个是语言的标准，后两个是 Node 独有的。它们的写法差不多，作用也差不多，不太容易区别。\n\n你能说出下面代码的运行结果吗？\n\n\u003e ```javascript\n\u003e // test.js\n\u003e setTimeout(() =\u003e console.log(1));\n\u003e setImmediate(() =\u003e console.log(2));\n\u003e process.nextTick(() =\u003e console.log(3));\n\u003e Promise.resolve().then(() =\u003e console.log(4));\n\u003e (() =\u003e console.log(5))();\n\u003e ```\n\n运行结果如下。\n\n\u003e ```bash\n\u003e $ node test.js\n\u003e 5\n\u003e 3\n\u003e 4\n\u003e 1\n\u003e 2\n\u003e ```\n\n如果你能一口说对，可能就不需要再看下去了。本文详细解释，Node 怎么处理各种定时器，或者更广义地说，libuv 库怎么安排异步任务在主线程上执行。\n\n## 同步任务和异步任务\n\n首先，同步任务总是比异步任务更早执行。\n\n前面的那段代码，只有最后一行是同步任务，因此最早执行。\n\n\u003e ```javascript\n\u003e (() =\u003e console.log(5))();\n\u003e ```\n\n## 本轮循环和次轮循环\n\n异步任务可以分成两种。\n\n\u003e - 追加在**本轮循环**的异步任务\n\u003e - 追加在**次轮循环**的异步任务\n\n所谓\"循环\"，指的是事件循环（event loop）。这是 JavaScript 引擎处理异步任务的方式，后文会详细解释。这里只要理解，**本轮循环一定早于次轮循环**执行即可。\n\nNode 规定，**`process.nextTick`和`Promise`的回调函数，追加在本轮循环**，即同步任务一旦执行完成，就开始执行它们。而`setTimeout`、`setInterval`、`setImmediate`的回调函数，追加在次轮循环。\n\n这就是说，文首那段代码的第三行和第四行，一定比第一行和第二行更早执行。\n\n\u003e ```javascript\n\u003e // 下面两行，次轮循环执行\n\u003e setTimeout(() =\u003e console.log(1));\n\u003e setImmediate(() =\u003e console.log(2));\n\u003e // 下面两行，本轮循环执行\n\u003e process.nextTick(() =\u003e console.log(3));\n\u003e Promise.resolve().then(() =\u003e console.log(4));\n\u003e ```\n\n## process.nextTick()\n\n`process.nextTick`这个名字有点误导，它是在本轮循环执行的，而且是所有异步任务里面最快执行的。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/node/20210410171345.png)\n\nNode 执行完所有同步任务，接下来就会执行`process.nextTick`的任务队列。所以，下面这行代码是第二个输出结果。\n\n\u003e ```javascript\n\u003e process.nextTick(() =\u003e console.log(3));\n\u003e ```\n\n基本上，如果你希望异步任务尽可能快地执行，那就使用`process.nextTick`。\n\n## 微任务\n\n根据语言规格，`Promise`对象的回调函数，会进入异步任务里面的\"微任务\"（microtask）队列。\n\n**微任务队列追加在`process.nextTick`队列的后面，也属于本轮循环**。所以，下面的代码总是先输出`3`，再输出`4`。\n\n\u003e ```javascript\n\u003e process.nextTick(() =\u003e console.log(3));\n\u003e Promise.resolve().then(() =\u003e console.log(4));\n\u003e // 3\n\u003e // 4\n\u003e ```\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/node/20210410171346.png)\n\n注意，只有前一个队列全部清空以后，才会执行下一个队列。\n\n\u003e ```javascript\n\u003e process.nextTick(() =\u003e console.log(1));\n\u003e Promise.resolve().then(() =\u003e console.log(2));\n\u003e process.nextTick(() =\u003e console.log(3));\n\u003e Promise.resolve().then(() =\u003e console.log(4));\n\u003e // 1\n\u003e // 3\n\u003e // 2\n\u003e // 4\n\u003e ```\n\n上面代码中，全部`process.nextTick`的回调函数，执行都会早于`Promise`的。\n\n至此，本轮循环的执行顺序就讲完了。\n\n\u003e 1. 同步任务\n\u003e 2. process.nextTick()\n\u003e 3. 微任务\n\n## 事件循环的概念\n\n下面开始介绍次轮循环的执行顺序，这就必须理解什么是事件循环（event loop）了。\n\nNode 的[官方文档](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/)是这样介绍的。\n\n\u003e \"When Node.js starts, it initializes the event loop, processes the provided input script which may make async API calls, schedule timers, or call process.nextTick(), then begins processing the event loop.\"\n\n这段话很重要，需要仔细读。它表达了三层意思。\n\n首先，有些人以为，除了主线程，还存在一个单独的事件循环线程。不是这样的，只有一个主线程，事件循环是在主线程上完成的。\n\n其次，Node 开始执行脚本时，会先进行事件循环的初始化，但是这时事件循环还没有开始，会先完成下面的事情。\n\n\u003e - 同步任务\n\u003e - 发出异步请求\n\u003e - 规划定时器生效的时间\n\u003e - 执行`process.nextTick()`等等\n\n最后，上面这些事情都干完了，事件循环就正式开始了。\n\n## 事件循环的六个阶段\n\n事件循环会无限次地执行，一轮又一轮。只有异步任务的回调函数队列清空了，才会停止执行。\n\n每一轮的事件循环，分成六个阶段。这些阶段会依次执行。\n\n\u003e 1. timers\n\u003e 2. I/O callbacks\n\u003e 3. idle, prepare\n\u003e 4. poll\n\u003e 5. check\n\u003e 6. close callbacks\n\n每个阶段都有一个先进先出的回调函数队列。只有一个阶段的回调函数队列清空了，该执行的回调函数都执行了，事件循环才会进入下一个阶段。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/node/20210410171347.png)\n\n下面简单介绍一下每个阶段的含义，详细介绍可以看[官方文档](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/)，也可以参考 libuv 的[源码解读](https://jsblog.insiderattack.net/handling-io-nodejs-event-loop-part-4-418062f917d1)。\n\n**（1）timers**\n\n这个是定时器阶段，处理`setTimeout()`和`setInterval()`的回调函数。进入这个阶段后，主线程会检查一下当前时间，是否满足定时器的条件。如果满足就执行回调函数，否则就离开这个阶段。\n\n**（2）I/O callbacks**\n\n除了以下操作的回调函数，其他的回调函数都在这个阶段执行。\n\n\u003e - `setTimeout()`和`setInterval()`的回调函数\n\u003e - `setImmediate()`的回调函数\n\u003e - 用于关闭请求的回调函数，比如`socket.on('close', …)`\n\n**（3）idle, prepare**\n\n该阶段只供 libuv 内部调用，这里可以忽略。\n\n**（4）Poll**\n\n这个阶段是轮询时间，用于等待还未返回的 I/O 事件，比如服务器的回应、用户移动鼠标等等。\n\n这个阶段的时间会比较长。如果没有其他异步任务要处理（比如到期的定时器），会一直停留在这个阶段，等待 I/O 请求返回结果。\n\n**（5）check**\n\n该阶段执行`setImmediate()`的回调函数。\n\n**（6）close callbacks**\n\n该阶段执行关闭请求的回调函数，比如`socket.on('close', …)`。\n\n## 事件循环的示例\n\n下面是来自官方文档的一个示例。\n\n\u003e ```javascript\n\u003e const fs = require('fs');\n\u003e \n\u003e const timeoutScheduled = Date.now();\n\u003e \n\u003e // 异步任务一：100ms 后执行的定时器\n\u003e setTimeout(() =\u003e {\n\u003e   const delay = Date.now() - timeoutScheduled;\n\u003e   console.log(`${delay}ms`);\n\u003e }, 100);\n\u003e \n\u003e // 异步任务二：文件读取后，有一个 200ms 的回调函数\n\u003e fs.readFile('test.js', () =\u003e {\n\u003e   const startCallback = Date.now();\n\u003e   while (Date.now() - startCallback \u003c 200) {\n\u003e     // 什么也不做\n\u003e   }\n\u003e });\n\u003e ```\n\n上面代码有两个异步任务，一个是 100ms 后执行的定时器，一个是文件读取，它的回调函数需要 200ms。请问运行结果是什么？\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/node/20210410171348.jpg)\n\n脚本进入第一轮事件循环以后，没有到期的定时器，也没有已经可以执行的 I/O 回调函数，所以会进入 Poll 阶段，等待内核返回文件读取的结果。由于读取小文件一般不会超过 100ms，所以在定时器到期之前，Poll 阶段就会得到结果，因此就会继续往下执行。\n\n第二轮事件循环，依然没有到期的定时器，但是已经有了可以执行的 I/O 回调函数，所以会进入 I/O callbacks 阶段，执行`fs.readFile`的回调函数。这个回调函数需要 200ms，也就是说，在它执行到一半的时候，100ms 的定时器就会到期。但是，必须等到这个回调函数执行完，才会离开这个阶段。\n\n第三轮事件循环，已经有了到期的定时器，所以会在 timers 阶段执行定时器。最后输出结果大概是200多毫秒。\n\n## setTimeout 和 setImmediate\n\n由于`setTimeout`在 timers 阶段执行，而`setImmediate`在 check 阶段执行。所以，`setTimeout`会早于`setImmediate`完成。\n\n\u003e ```javascript\n\u003e setTimeout(() =\u003e console.log(1));\n\u003e setImmediate(() =\u003e console.log(2));\n\u003e ```\n\n上面代码应该先输出`1`，再输出`2`，但是实际执行的时候，结果却是不确定，有时还会先输出`2`，再输出`1`。\n\n这是因为`setTimeout`的第二个参数默认为`0`。但是实际上，Node 做不到0毫秒，最少也需要1毫秒，根据[官方文档](https://nodejs.org/api/timers.html#timers_settimeout_callback_delay_args)，第二个参数的取值范围在1毫秒到2147483647毫秒之间。也就是说，`setTimeout(f, 0)`等同于`setTimeout(f, 1)`。\n\n实际执行的时候，进入事件循环以后，有可能到了1毫秒，也可能还没到1毫秒，取决于系统当时的状况。如果没到1毫秒，那么 timers 阶段就会跳过，进入 check 阶段，先执行`setImmediate`的回调函数。\n\n但是，下面的代码一定是先输出2，再输出1。\n\n\u003e ```javascript\n\u003e const fs = require('fs');\n\u003e \n\u003e fs.readFile('test.js', () =\u003e {\n\u003e   setTimeout(() =\u003e console.log(1));\n\u003e   setImmediate(() =\u003e console.log(2));\n\u003e });\n\u003e ```\n\n上面代码会先进入 I/O callbacks 阶段，然后是 check 阶段，最后才是 timers 阶段。因此，`setImmediate`才会早于`setTimeout`执行。\n","lastmodified":"2023-05-09T16:33:58.295366497Z","tags":[]},"/react":{"title":"第1章：React入门","content":"\n# 第1章：React入门\n\n## 1.1. React简介\n\n1.1.1. 官网\n\n1. 英文官网:[https://reactjs.org/](https://reactjs.org/)\n2. 中文官网: https://react.docschina.org/\n\n1.1.2. 介绍描述\n\n1. 用于动态构建用户界面的 JavaScript 库(只关注于视图)\n2. 由Facebook开源\n\n1.1.3. React的特点\n\n1. 声明式编码\n2. 组件化编码\n3. React Native 编写原生应用\n4. 高效（优秀的Diffing算法）\n\n1.1.4. React高效的原因\n\n1. 使用虚拟(virtual)DOM, 不总是直接操作页面真实DOM。\n2. DOM Diffing算法, 最小化页面重绘。\n\n## 1.2. React的基本使用\n\n### 1.2.1. 效果\n\n![image-20210113103850309](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113103850.png)\n\n### 1.2.2. 相关js库\n\n1. react.js：React核心库。\n2. react-dom.js：提供操作DOM的react扩展库。\n3. babel.min.js：解析JSX语法代码转为JS代码的库。\n\n### 1.2.3. 创建虚拟DOM的两种方式\n\n1. 纯JS方式(一般不用)\n2. JSX方式\n\n### 1.2.4. 虚拟DOM与真实DOM\n\n1. React提供了一些API来创建一种 “特别” 的一般js对象\n\n![image-20210113103942744](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113103942.png)\n\n创建的就是一个简单的虚拟DOM对象\n\n2. 虚拟DOM对象最终都会被React转换为真实的DOM\n3. 我们编码时基本只需要操作react的虚拟DOM相关数据, react会转换为真实DOM变化而更新界。\n\n## 1.3. React JSX\n\n### 1.3.1. 效果\n\n![image-20210113104020994](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113104021.png)\n\n### 1.3.2. JSX\n\n1. 全称: JavaScript XML\n2. react定义的一种类似于XML的JS扩展语法: JS + XML本质是![image-20210113104045723](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113104045.png)方法的语法糖\n3. 作用: 用来简化创建虚拟DOM\n\n   1) 写法：`var ele = \u003ch1\u003eHello JSX!\u003c/h1\u003e`\n\n   2) 注意1：它不是字符串, 也不是HTML/XML标签\n\n   3) 注意2：**它最终产生的就是一个JS对象**\n\n4. 标签名任意: HTML标签或其它标签\n5. 标签属性任意: HTML标签属性或其它\n6. 基本语法规则\n\n   1) 遇到 \u003c开头的代码, 以标签的语法解析: html同名标签转换为html同名元素, 其它标签需要特别解析\n\n   2) 遇到以 { 开头的代码，以JS语法解析: 标签中的js表达式必须**用{ }包含**\n\n7. babel.js的作用\n\n   1) 浏览器不能直接解析JSX代码, 需要babel转译为纯JS的代码才能运行\n\n   2) 只要用了JSX，都要加上type=\"text/babel\", 声明需要babel来处理\n\n### 1.3.3. 渲染虚拟DOM(元素)\n\n1. 语法: `ReactDOM.render(virtualDOM, containerDOM)`\n2. 作用: 将虚拟DOM元素渲染到页面中的真实容器DOM中显示\n3. 参数说明\n\n   1) 参数一: 纯js或jsx创建的虚拟dom对象\n\n   2) 参数二: 用来包含虚拟DOM元素的真实dom元素对象(一般是一个div)\n\n### 1.3.4. JSX练习\n\n需求: 动态展示如下列表\n\n![image-20210113105711179](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113105711.png)\n\n## 1.4. 模块与组件、模块化与组件化的理解\n\n### 1.4.1. 模块\n\n1. 理解：向外提供特定功能的js程序, 一般就是一个js文件\n2. 为什么要拆成模块：随着业务逻辑增加，代码越来越多且复杂。\n3. 作用：复用js, 简化js的编写, 提高js运行效率\n\n### 1.4.2. 组件\n\n1. 理解：用来实现局部功能效果的代码和资源的集合(html/css/js/image等等)\n2. 为什么要用组件： 一个界面的功能更复杂\n3. 作用：复用编码, 简化项目编码, 提高运行效率\n\n### 1.4.3. 模块化\n\n当应用的js都以模块来编写的, 这个应用就是一个模块化的应用\n\n### 1.4.4. 组件化\n\n当应用是以多组件的方式实现, 这个应用就是一个组件化的应用\n\n ![image-20210113105809103](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113105809.png)\n\n# 第2章：React面向组件编程\n\n## 2.1. 基本理解和使用\n\n### 2.1.1. 使用React开发者工具调试\n\n### 2.1.2. 效果\n\n![image-20210113105843606](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113105843.png)  \n\n### 2.1.3. 注意\n\n1. 组件名必须首字母大写\n2. 虚拟DOM元素只能有一个根元素\n3. 虚拟DOM元素必须有结束标签\n\n### 2.1.4. 渲染类组件标签的基本流程\n\n1. React内部会创建组件实例对象\n2. 调用render()得到虚拟DOM, 并解析为真实DOM\n3. 插入到指定的页面元素内部\n\n## 2.2. 组件三大核心属性1: state\n\n### 2.2.1. 效果\n\n*需求:* 定义一个展示天气信息的组件\n\n*1.* *默认展示天气炎热* *或* *凉爽*\n\n*2.* *点击文字切换天气*\n\n### 2.2.2. 理解\n\n1. state是组件对象最重要的属性, 值是对象(可以包含多个key-value的组合)\n2. 组件被称为\"状态机\", 通过更新组件的state来更新对应的页面显示(重新渲染组件)\n\n### 2.2.3. 强烈注意\n\n1. 组件中render方法中的this为组件实例对象\n2. 组件自定义的方法中this为undefined，如何解决？\n\n   a) 强制绑定this: 通过函数对象的bind()\n\n   b) 箭头函数\n\n3. 状态数据，不能直接修改或更新\n\n## 2.3. 组件三大核心属性2: props\n\n### 2.3.1. 效果\n\n*需求:* *自定义用来显示一个人员信息的组件*\n\n*1.* *姓名必须指定，且为字符串类型；*\n\n*2.* *性别为字符串类型，如果性别没有指定，默认为男*\n\n*3.* *年龄为字符串类型，且为数字类型，默认值为**18*  \n\n### 2.3.2. 理解\n\n1. 每个组件对象都会有props(properties的简写)属性\n2. 组件标签的所有属性都保存在props中\n\n### 2.3.3. 作用\n\n1. 通过标签属性从组件外向组件内传递变化的数据\n2. 注意: **组件内部不要修改props数据**\n\n### 2.3.4. 编码操作\n\n1. 内部读取某个属性值\n\n​ **this**.**props**.**name**\n\n2. 对props中的属性值进行类型限制和必要性限制\n\n   ![image-20210113112413459](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113112413.png)\n\n3. 扩展属性: 将对象的所有属性通过props传递\n\n![image-20210113112430463](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113112430.png)\n\n4. 默认属性值：\n\n![image-20210113112443390](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113112443.png)\n\n5. 组件类的构造函数\n\n## 2.4. 组件三大核心属性3: refs与事件处理\n\n### 2.4.1. 效果\n\n*需求:* *自定义组件,* *功能说明如下:*\n\n *1.* *点击按钮,* *提示第一个输入框中的值*\n\n *2.* *当第**2**个输入框失去焦点时,* *提示这个输入框中的值*\n\n### 2.4.2. 理解\n\n组件内的标签可以定义ref属性来标识自己\n\n### 2.4.3. 编码\n\n![image-20210113112907110](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113112907.png)\n\n### 2.4.4. 事件处理\n\n1. 通过onXxx属性指定事件处理函数(注意大小写)\n\n   1) React使用的是自定义(合成)事件, 而不是使用的原生DOM事件\n\n   2) React中的事件是通过事件委托方式处理的(委托给组件最外层的元素)\n\n2. 通过event.target得到发生事件的DOM元素对象\n\n## 2.5. 收集表单数据\n\n### 2.5.1. 效果\n\n*需求:* *定义一个包含表单的组件*\n\n *输入用户名密码后,* *点击登录提示输入信息*\n\n### 2.5.2. 理解\n\n包含表单的组件分类\n\n1. 受控组件\n\n   ```javascript\n   class Login extends React.Component{\n   \n   \t\t\t//初始化状态\n   \t\t\tstate = {\n   \t\t\t\tusername:'', //用户名\n   \t\t\t\tpassword:'' //密码\n   \t\t\t}\n   \n   \t\t\t//保存用户名到状态中\n   \t\t\tsaveUsername = (event)=\u003e{\n   \t\t\t\tthis.setState({username:event.target.value})\n   \t\t\t}\n   \n   \t\t\t//保存密码到状态中\n   \t\t\tsavePassword = (event)=\u003e{\n   \t\t\t\tthis.setState({password:event.target.value})\n   \t\t\t}\n   \n   \t\t\t//表单提交的回调\n   \t\t\thandleSubmit = (event)=\u003e{\n   \t\t\t\tevent.preventDefault() //阻止表单提交\n   \t\t\t\tconst {username,password} = this.state\n   \t\t\t\talert(`你输入的用户名是：${username},你输入的密码是：${password}`)\n   \t\t\t}\n   \n   \t\t\trender(){\n   \t\t\t\treturn(\n   \t\t\t\t\t\u003cform onSubmit={this.handleSubmit}\u003e\n   \t\t\t\t\t\t用户名：\u003cinput onChange={this.saveUsername} type=\"text\" name=\"username\"/\u003e\n   \t\t\t\t\t\t密码：\u003cinput onChange={this.savePassword} type=\"password\" name=\"password\"/\u003e\n   \t\t\t\t\t\t\u003cbutton\u003e登录\u003c/button\u003e\n   \t\t\t\t\t\u003c/form\u003e\n   \t\t\t\t)\n   \t\t\t}\n   \t\t}\n   ```\n\n2. 非受控组件\n\n## 2.6. 组件的生命周期\n\n### 2.6.1. 效果\n\n*需求**:**定义组件实现以下功能：*\n\n *1.* *让指定的文本做显示* */* *隐藏的渐变动画*\n\n *2.* *从完全可见，到彻底消失，耗时**2S*\n\n *3.* *点击“不活了”按钮从界面中卸载组件*\n\n### 2.6.2. 理解\n\n1. 组件从创建到死亡它会经历一些特定的阶段。\n2. React组件中包含一系列勾子函数(生命周期回调函数), 会在特定的时刻调用。\n3. 我们在定义组件时，会在特定的生命周期回调函数中，做特定的工作。\n\n### 2.6.3. 生命周期流程图(旧)\n\n ![image-20210113113031023](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113113031.png)\n\n生命周期的三个阶段（旧）\n\n**1.** **初始化阶段:** 由ReactDOM.render()触发---初次渲染\n\n1. constructor()\n2. componentWillMount()\n3. render()\n4. componentDidMount()\n\n **2.** **更新阶段:** 由组件内部this.setSate()或父组件重新render触发\n\n1. shouldComponentUpdate()\n2. componentWillUpdate()\n3. render()\n4. componentDidUpdate()\n\n **3.** **卸载组件:** 由ReactDOM.unmountComponentAtNode()触发\n\n1. componentWillUnmount()\n\n### 2.6.4. 生命周期流程图(新)\n\n ![image-20210113113130146](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113113130.png)\n\n生命周期的三个阶段（新）\n\n**1.** **初始化阶段:** 由ReactDOM.render()触发---初次渲染\n\n1. constructor()\n\n**2.** **getDerivedStateFromProps**\n\n3. render()\n4. componentDidMount()\n\n**2.** **更新阶段:** 由组件内部this.setSate()或父组件重新render触发\n\n1. **getDerivedStateFromProps**\n2. shouldComponentUpdate()\n3. render()\n4. **getSnapshotBeforeUpdate**\n5. componentDidUpdate()\n\n**3.** **卸载组件:** 由ReactDOM.unmountComponentAtNode()触发\n\n1. componentWillUnmount()\n\n### 2.6.5. 重要的勾子\n\n1. render：初始化渲染或更新渲染调用\n2. componentDidMount：**开启监听, 发送ajax请求**\n3. componentWillUnmount：做一些收尾工作, 如: 清理定时器\n\n### 2.6.6. 即将废弃的勾子\n\n1. componentWillMount\n2. componentWillReceiveProps\n3. componentWillUpdate\n\n现在使用会出现警告，下一个大版本需要加上UNSAFE_前缀才能使用，以后可能会被彻底废弃，不建议使用。\n\n## 2.7. 虚拟DOM与DOM Diffing算法\n\n### 2.7.1. 效果\n\n需求：验证虚拟**DOM Diffing**算法的存在\n\n### 2.7.2. 基本原理图\n\n![image-20210113114017515](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113114017.png)\n\n# 第3章：React应用(基于React脚手架)\n\n## 3.1. 使用create-react-app创建react应用\n\n### 3.1.1. react脚手架\n\n1. xxx脚手架: 用来帮助程序员快速创建一个基于xxx库的模板项目\n1. 包含了所有需要的配置（语法检查、jsx编译、devServer…）\n2. 下载好了所有相关的依赖\n3. 可以直接运行一个简单效果\n2. react提供了一个用于创建react项目的脚手架库: create-react-app\n3. 项目的整体技术架构为: react + webpack + es6 + eslint\n4. 使用脚手架开发的项目的特点: 模块化, 组件化, 工程化\n\n### 3.1.2. 创建项目并启动\n\n​\t**第一步**，全局安装：`npm i -g create-react-app`\n\n​\t**第二步**，切换到想创项目的目录，使用命令：`create-react-app hello-react`\n\n​\t**第三步**，进入项目文件夹：`cd hello-react`\n\n​\t**第四步**，启动项目：`npm start`\n\n### 3.1.3. react脚手架项目结构\n\n```\npublic ---- 静态资源文件夹\n\n​            favicon.icon ------ 网站页签图标\n\n​            index.html --------** **主页面\n\n​            logo192.png ------- logo图\n\n​            logo512.png ------- logo图\n\n​            manifest.json ----- 应用加壳的配置文件\n\n​            robots.txt -------- 爬虫协议文件\n\nsrc ---- 源码文件夹\n\n​            App.css -------- App组件的样式\n\n​            App.js --------- App组件\n\n​            App.test.js ---- 用于给App做测试\n\n​            index.css ------ 样式\n\n​            index.js -------入口文件\n\n​            logo.svg ------- logo图\n\n​            reportWebVitals.js\n\n​                    --- 页面性能分析文件(需要web-vitals库的支持)\n\n​            setupTests.js\n\n​                    ---- 组件单元测试的文件(需要jest-dom库的支持)\n```\n\n### 3.1.4. 功能界面的组件化编码流程（通用）\n\n1. 拆分组件: 拆分界面,抽取组件\n2. 实现静态组件: 使用组件实现静态页面效果\n3. 实现动态组件\n\n   ​\t3.1 动态显示初始化数据\n\n   ​\t3.1.1 数据类型\n\n   ​\t3.1.2 数据名称\n\n   ​\t3.1.2 保存在哪个组件?\n\n   3.2 交互(从绑定事件监听开始)\n\n## 3.2. 组件的组合使用-TodoList\n\n*功能:* *组件化实现此功能*\n\n *1.* *显示所有**todo**列表*\n\n *2.* *输入文本,* *点击按钮显示到列表的首位,* *并清除输入的文本*\n\n# 第7章：redux\n\n## 7.1. redux理解\n\n### 1.求和案例_redux精简版\n\n1. 去除Count组件自身的状态\n2. src下建立:\n\n​ -redux\n\n​ -store.js\n\n​ -count_reducer.js\n\n3. store.js：\n\n​ 1.引入redux中的createStore函数，创建一个store\n\n​ 2.createStore调用时要传入一个为其服务的reducer\n\n​ 3.记得暴露store对象\n\n4. count_reducer.js：\n\n​ 1. reducer的本质是一个函数，接收：preState,action，返回加工后的状态\n\n​ 2. reducer有两个作用：初始化状态，加工状态\n\n​ 3. reducer被第一次调用时，是store自动触发的，\n\n​ 传递的preState是undefined,\n\n​ 传递的action是:{type:'@@REDUX/INIT_a.2.b.4}\n\n5. 在index.js中监测store中状态的改变，一旦发生改变重新渲染\u003cApp/\u003e\n\n​ 备注：redux只负责管理状态，至于状态的改变驱动着页面的展示，要靠我们自己写。\n\n### 2.求和案例_redux完整版\n\n  新增文件：\n\n1. count_action.js 专门用于创建action对象\n2. constant.js 放置容易写错的type值\n\n### 3.求和案例_redux异步action版\n\n1. 明确：延迟的动作不想交给组件自身，想交给action\n2. 何时需要异步action：想要对状态进行操作，但是具体的数据靠异步任务返回。\n3. 具体编码：\n\n​ 1. yarn add redux-thunk，并配置在store中\n\n​ 2. 创建action的函数不再返回一般对象，而是一个函数，该函数中写异步任务。\n\n​ 3. 异步任务有结果后，分发一个同步的action去真正操作数据。\n\n4. 备注：异步action不是必须要写的，完全可以自己等待异步任务的结果了再去分发同步action。\n\n### 4.求和案例_react-redux基本使用\n\n1. 明确两个概念：\n\n​ 1.UI组件:不能使用任何redux的api，只负责页面的呈现、交互等。\n\n​ 2.容器组件：负责和redux通信，将结果交给UI组件。\n\n2. 如何创建一个容器组件————靠react-redux 的 connect函数\n\n​ connectmapStateToProps,mapDispatchToPropsUI组件\n\n​ -mapStateToProps:映射状态，返回值是一个对象\n\n​ -mapDispatchToProps:映射操作状态的方法，返回值是一个对象\n\n3. 备注1：容器组件中的store是靠props传进去的，而不是在容器组件中直接引入\n4. 备注2：mapDispatchToProps，也可以是一个对象\n\n### 5.求和案例_react-redux优化\n\n1. 容器组件和UI组件整合一个文件\n2. 无需自己给容器组件传递store，给\u003cApp/\u003e包裹一个\u003cProvider store={store}\u003e即可。\n3. 使用了react-redux后也不用再自己检测redux中状态的改变了，容器组件可以自动完成这个工作。\n4. mapDispatchToProps也可以简单的写成一个对象\n5. 一个组件要和redux“打交道”要经过哪几步？\n\n​ 1. 定义好UI组件---不暴露\n\n​ 2. 引入connect生成一个容器组件，并暴露，写法如下：\n\n​ connect\n\n​ state =\u003e {key:value}, //映射状态\n\n​ {key:xxxxxAction} //映射操作状态的方法\n\n​ UI组件\n\n​ 4. 在UI组件中通过this.props.xxxxxxx读取和操作状态\n\n### 6.求和案例_react-redux数据共享版\n\n1. 定义一个Pserson组件，和Count组件通过redux共享数据。\n2. 为Person组件编写：reducer、action，配置constant常量。\n3. 重点：Person的reducer和Count的Reducer要使用combineReducers进行合并，\n\n​ 合并后的总状态是一个对象！！！\n\n4. 交给store的是总reducer，最后注意在组件中取出状态的时候，记得“取到位”。\n\n### 7.求和案例_react-redux开发者工具的使用\n\n1. yarn add redux-devtools-extension\n2. store中进行配置\n\n​ import {composeWithDevTools} from 'redux-devtools-extension'\n\n​ const store = createStoreallReducer,composeWithDevToolsapplyMiddlewarethunk\n\n### 8.求和案例_react-redux最终版\n\n1. 所有变量名字要规范，尽量触发对象的简写形式。\n2. reducers文件夹中，编写index.js专门用于汇总并暴露所有的reducer\n\n### 7.1.1. 学习文档\n\n1. 英文文档: https://redux.js.org/\n2. 中文文档: http://www.redux.org.cn/\n3. Github: https://github.com/reactjs/redux\n\n### 7.1.2. redux是什么\n\n1. redux是一个专门用于做**状态管理**的JS库(不是react插件库)。\n2. 它可以用在react, angular, vue等项目中, 但基本与react配合使用。\n3. 作用: 集中式管理react应用中多个组件**共享**的状态。\n\n### 7.1.3. 什么情况下需要使用redux\n\n1. 某个组件的状态，需要让其他组件可以随时拿到（共享）。\n2. 一个组件需要改变另一个组件的状态（通信）。\n3. 总体原则：能不用就不用, 如果不用比较吃力才考虑使用。\n\n### 7.1.4. redux工作流程\n\n![image-20210113121422231](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113121422.png)\n\n## 7.2. redux的三个核心概念\n\n### 7.2.1. action\n\n1. 动作的对象\n2. 包含2个属性\n\n- type：标识属性, 值为字符串, 唯一, 必要属性\n- data：数据属性, 值类型任意, 可选属性\n\n3. 例子：{ type: 'ADD_STUDENT',data:{name: 'tom',age:18} }\n\n### 7.2.2. reducer\n\n1. 用于初始化状态、加工状态。\n2. 加工时，根据旧的state和action， 产生新的state的**纯函数**.\n\n### 7.2.3. store\n\n1. 将state、action、reducer联系在一起的对象\n2. 如何得到此对象?\n\n   1) import {createStore} from 'redux'\n\n   2) import reducer from './reducers'\n\n   3) const store = createStore(reducer)\n\n3. 此对象的功能?\n\n   1) getState(): 得到state\n\n   2) dispatch(action): 分发action, 触发reducer调用, 产生新的state\n\n   3) subscribe(listener): 注册监听, 当产生了新的state时, 自动调用\n\n## 7.3. redux的核心API\n\n### 7.3.1. createstore()\n\n作用：创建包含指定reducer的store对象\n\n### 7.3.2. store对象\n\n1. 作用: redux库最核心的管理对象\n2. 它内部维护着:\n\n   1) state\n\n   2) reducer\n\n3. 核心方法:\n\n   1) getState()\n\n   2) dispatch(action)\n\n   3) subscribe(listener)\n\n4. 具体编码:\n\n   1) store.getState()\n\n   2) store.dispatch({type:'INCREMENT', number})\n\n   3) store.subscribe(render)\n\n### 7.3.3. applyMiddleware()\n\n作用：应用上基于redux的中间件(插件库)\n\n### 7.3.4. combineReducers()\n\n作用：合并多个reducer函数\n\n## 7.4. 使用redux编写应用\n\n## 7.5. redux异步编程\n\n### 7.5.1理解：\n\n1. redux默认是不能进行异步处理的,\n2. 某些时候应用中需要在**redux****中执行异步任务**(ajax, 定时器)\n\n### 7.5.2. 使用异步中间件\n\nnpm install --save redux-thunk\n\n## 7.6. react-redux\n\n### 7.6.1. 理解\n\n1. 一个react插件库\n2. 专门用来简化react应用中使用redux\n\n### 7.6.2. react-Redux将所有组件分成两大类\n\n1. UI组件\n\n   1) 只负责 UI 的呈现，不带有任何业务逻辑\n\n   2) 通过props接收数据(一般数据和函数)\n\n   3) 不使用任何 Redux 的 API\n\n   4) 一般保存在components文件夹下\n\n2. 容器组件\n\n   1) 负责管理数据和业务逻辑，不负责UI的呈现\n\n   2) 使用 Redux 的 API\n\n   3) 一般保存在containers文件夹下\n\n### 7.6.3. 相关API\n\n1. Provider：让所有组件都可以得到state数据\n\n![image-20210113121631846](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113121631.png)\n\n2. connect：用于包装 UI 组件生成容器组件\n\n![image-20210113121655155](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113121655.png)\n\n3. mapStateToprops：将外部的数据（即state对象）转换为UI组件的标签属性\n\n![image-20210113121704856](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113121705.png)\n\n4. mapDispatchToProps：将分发action的函数转换为UI组件的标签属性\n\n## 7.7. 使用上redux调试工具\n\n### 7.7.1. 安装chrome浏览器插件\n\n![image-20210113121721787](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/react20210113121721.png)\n\n### 7.7.2. 下载工具依赖包\n\n```\nnpm install --save-dev redux-devtools-extension\n```\n\n## 7.8. 纯函数和高阶函数\n\n### 7.8.1. 纯函数\n\n1. 一类特别的函数: 只要是同样的输入(实参)，必定得到同样的输出(返回)\n2. 必须遵守以下一些约束\n\n   1) 不得改写参数数据\n\n   2) 不会产生任何副作用，例如网络请求，输入和输出设备\n\n   3) 不能调用Date.now()或者Math.random()等不纯的方法\n\n3. redux的reducer函数必须是一个纯函数\n\n### 7.8.2. 高阶函数\n\n1. 理解: 一类特别的函数\n\n   1) 情况1: 参数是函数\n\n   2) 情况2: 返回是函数\n\n2. 常见的高阶函数:\n\n   1) 定时器设置函数\n\n   2) 数组的forEach()/map()/filter()/reduce()/find()/bind()\n\n   3) promise\n\n   4) react-redux中的connect函数\n\n3. 作用: 能实现更加动态, 更加可扩展的功能\n\n# 脚手架配置代理总结\n\n## 方法一\n\n在package.json中追加如下\n\n```json\n\"proxy\":\"http://localhost:5000\"\n```\n\n说明：\n\n1. 优点：配置简单，前端请求资源时可以不加任何前缀。\n2. 缺点：不能配置多个代理。\n3. 工作方式：上述方式配置代理，当请求了3000不存在的资源时，那么该请求会转发给5000 （优先匹配前端资源）\n\n## 方法二\n\n1. 第一步：创建代理配置文件\n\n在src下创建配置文件：src/setupProxy.js\n\n2. 编写setupProxy.js配置具体代理规则：\n\n```js\nconst proxy = require('http-proxy-middleware')\n   \n   module.exports = function(app) {\n     app.use(\n       proxy('/api1', {  //api1是需要转发的请求(所有带有/api1前缀的请求都会转发给5000)\n         target: 'http://localhost:5000', //配置转发目标地址(能返回数据的服务器地址)\n         changeOrigin: true, //控制服务器接收到的请求头中host字段的值\n         /*\n         \tchangeOrigin设置为true时，服务器收到的请求头中的host为：localhost:5000\n         \tchangeOrigin设置为false时，服务器收到的请求头中的host为：localhost:3000\n         \tchangeOrigin默认值为false，但我们一般将changeOrigin值设为true\n         */\n         pathRewrite: {'^/api1': ''} //去除请求前缀，保证交给后台服务器的是正常请求地址(必须配置)\n       }),\n       proxy('/api2', { \n         target: 'http://localhost:5001',\n         changeOrigin: true,\n         pathRewrite: {'^/api2': ''}\n       })\n     )\n   }\n```\n\n说明：\n\n1. 优点：可以配置多个代理，可以灵活的控制请求是否走代理。\n2. 缺点：配置繁琐，前端请求资源时必须加前缀。\n\n# 注意点\n\n## 一、todoList案例相关知识点\n\n  1.拆分组件、实现静态组件，注意：className、style的写法\n\n  2.动态初始化列表，如何确定将数据放在哪个组件的state中？\n\n​ ——某个组件使用：放在其自身的state中\n\n​ ——某些组件使用：放在他们共同的父组件state中（官方称此操作为：状态提升）\n\n  3.关于父子之间通信：\n\n​ 1.【父组件】给【子组件】传递数据：通过props传递\n\n​ 2.【子组件】给【父组件】传递数据：通过props传递，要求父提前给子传递一个函数\n\n  4.注意defaultChecked 和 checked的区别，类似的还有：defaultValue 和 value\n\n  5.状态在哪里，操作状态的方法就在哪里\n\n## 二、github搜索案例相关知识点\n\n  1.设计状态时要考虑全面，例如带有网络请求的组件，要考虑请求失败怎么办。\n\n  2.ES6小知识点：解构赋值+重命名\n\n​ let obj = {a:{b:1}}\n\n​ const {a} = obj; //传统解构赋值\n\n​ const {a:{b}} = obj; //连续解构赋值\n\n​ const {a:{b:value}} = obj; //连续解构赋值+重命名\n\n  3.消息订阅与发布机制\n\n​ 1.先订阅，再发布（理解：有一种隔空对话的感觉）\n\n​ 2.适用于任意组件间通信\n\n​ 3.要在组件的componentWillUnmount中取消订阅\n\n  4.fetch发送请求（关注分离的设计思想）\n\n​ try {\n\n​ const response= await fetch(`/api1/search/users2?q=${keyWord}`)\n\n​ const data = await response.json()\n\n​ console.log(data);\n\n​ } catch (error) {\n\n​ console.log('请求出错',error);\n\n​ }\n\n​ ## 三、路由的基本使用\n\n   1.明确好界面中的导航区、展示区\n\n   2.导航区的a标签改为Link标签\n\n​ `\u003cLink to=\"/xxxxx\"\u003eDemo\u003c/Link\u003e`\n\n   3.展示区写Route标签进行路径的匹配\n\n​ `` \u003cRoute path='/xxxx' component={Demo}/\u003e``\n\n   4.``\u003cApp\u003e``的最外侧包裹了一个``\u003cBrowserRouter\u003e``或``\u003cHashRouter\u003e``\n\n## 四、路由组件与一般组件\n\n   1.写法不同：\n\n​ 一般组件：``\u003cDemo/\u003e``\n\n​ 路由组件：``\u003cRoute path=\"/demo\" component={Demo}/\u003e``\n\n   2.存放位置不同：\n\n​ 一般组件：components\n\n​ 路由组件：pages\n\n   3.接收到的props不同：\n\n​ 一般组件：写组件标签时传递了什么，就能收到什么\n\n​ 路由组件：接收到三个固定的属性\n\n​ history:\n\n​ go: ƒ go(n)\n\n​ goBack: ƒ goBack()\n\n​ goForward: ƒ goForward()\n\n​ push: ƒ push(path, state)\n\n​ replace: ƒ replace(path, state)\n\n​ location:\n\n​ pathname: \"/about\"\n\n​ search: \"\"\n\n​ state: undefined\n\n​ match:\n\n​ params: {}\n\n​ path: \"/about\"\n\n​ url: \"/about\"\n\n## 五、NavLink与封装NavLink\n\n​ 1.NavLink可以实现路由链接的高亮，通过activeClassName指定样式名\n\n## 六、Switch的使用\n\n​ 1.通常情况下，path和component是一一对应的关系。\n\n​ 2.Switch可以提高路由匹配效率(单一匹配)。\n\n## 七、解决多级路径刷新页面样式丢失的问题\n\n​ 1.public/index.html 中 引入样式时不写 ./ 写 / （常用）\n\n​ 2.public/index.html 中 引入样式时不写 ./ 写 %PUBLIC_URL% （常用）\n\n​ 3.使用HashRouter\n\n## 八、路由的严格匹配与模糊匹配\n\n​ 1.默认使用的是模糊匹配（简单记：【输入的路径】必须包含要【匹配的路径】，且顺序要一致）\n\n​ 2.开启严格匹配：``\u003cRoute exact={true} path=\"/about\" component={About}/\u003e``\n\n​ 3.严格匹配不要随便开启，需要再开，有些时候开启会导致无法继续匹配二级路由\n\n## 九、Redirect的使用\n\n​ 1.一般写在所有路由注册的最下方，当所有路由都无法匹配时，跳转到Redirect指定的路由\n\n​ 2.具体编码：\n\n```jsx\n\u003cSwitch\u003e\n       \u003cRoute path=\"/about\" component={About}/\u003e\n       \u003cRoute path=\"/home\" component={Home}/\u003e\n       \u003cRedirect to=\"/about\"/\u003e\n\u003c/Switch\u003e\n```\n\n## 十、嵌套路由\n\n​ 1.注册子路由时要写上父路由的path值\n\n​ 2.路由的匹配是按照注册路由的顺序进行的\n\n## 十二、编程式路由导航\n\n​ 借助this.prosp.history对象上的API对操作路由跳转、前进、后退\n\n​ -this.prosp.history.push()\n\n​ -this.prosp.history.replace()\n\n​ -this.prosp.history.goBack()\n\n​ -this.prosp.history.goForward()\n\n​ -this.prosp.history.go()\n\n## 十三、BrowserRouter与HashRouter的区别\n\n   1.底层原理不一样：\n\n​ BrowserRouter使用的是H5的history API，不兼容IE9及以下版本。\n\n​ HashRouter使用的是URL的哈希值。\n\n   2.path表现形式不一样\n\n​ BrowserRouter的路径中没有#,例如：localhost:3000/demo/test\n\n​ HashRouter的路径包含#,例如：localhost:3000/#/demo/test\n\n   3.刷新后对路由state参数的影响\n\n​ (1).BrowserRouter没有任何影响，因为state保存在history对象中。\n\n​ (2).HashRouter刷新后会导致路由state参数的丢失！！！\n\n   4.备注：HashRouter可以用于解决一些路径错误相关的问题。\n\n## 十四、antd的按需引入+自定主题\n\n   1.安装依赖：yarn add react-app-rewired customize-cra babel-plugin-import less less-loader\n\n   2.修改package.json\n\n```json\n\"scripts\": {\n\t\"start\": \"react-app-rewired start\",\n\t\"build\": \"react-app-rewired build\",\n\t\"test\": \"react-app-rewired test\",\n\t\"eject\": \"react-scripts eject\"\n},\n```\n\n   3.根目录创建config-overrides.js\n\n```javascript\n//配置具体的修改规则\nconst { override, fixBabelImports,\naddLessLoader} = require('customize-cra');\nmodule.exports = override(\n\tfixBabelImports('import', {\n\t\tlibraryName: 'antd',\n\t\tlibraryDirectory: 'es',\n\t\tstyle: true,\n\t}),\n\taddLessLoader({\n\t\tlessOptions:{\n\t\t\tjavascriptEnabled: true,\n\t\t\tmodifyVars: { '@primary-color': \n'green' },\n\t\t}\n\t}),\n);\n```\n\n​ 4.备注：不用在组件里亲自引入样式了，即：import 'antd/dist/antd.css'**应该删掉**\n\n# 特别点\n\n## 1. setState\n\n### setState更新状态的2种写法\n\n```\n\t(1). setState(stateChange, [callback])------对象式的setState\n            1.stateChange为状态改变对象(该对象可以体现出状态的更改)\n            2.callback是可选的回调函数, 它在状态更新完毕、界面也更新后(render调用后)才被调用\n\t\t\t\t\t\n\t(2). setState(updater, [callback])------函数式的setState\n            1.updater为返回stateChange对象的函数。\n            2.updater可以接收到state和props。\n            4.callback是可选的回调函数, 它在状态更新、界面也更新后(render调用后)才被调用。\n总结:\n\t\t1.对象式的setState是函数式的setState的简写方式(语法糖)\n\t\t2.使用原则：\n\t\t\t\t(1).如果新状态不依赖于原状态 ===\u003e 使用对象方式\n\t\t\t\t(2).如果新状态依赖于原状态 ===\u003e 使用函数方式\n\t\t\t\t(3).如果需要在setState()执行后获取最新的状态数据, \n\t\t\t\t\t要在第二个callback函数中读取\n```\n\n------\n\n## 2. lazyLoad\n\n### 路由组件的lazyLoad\n\n```js\n\t//1.通过React的lazy函数配合import()函数动态加载路由组件 ===\u003e 路由组件代码会被分开打包\n\tconst Login = lazy(()=\u003eimport('@/pages/Login'))\n\t\n\t//2.通过\u003cSuspense\u003e指定在加载得到路由打包文件前显示一个自定义loading界面\n\t\u003cSuspense fallback={\u003ch1\u003eloading.....\u003c/h1\u003e}\u003e\n        \u003cSwitch\u003e\n            \u003cRoute path=\"/xxx\" component={Xxxx}/\u003e\n            \u003cRedirect to=\"/login\"/\u003e\n        \u003c/Switch\u003e\n    \u003c/Suspense\u003e\n```\n\n------\n\n## 4. Fragment\n\n### 使用\n\n\t\u003cFragment\u003e\u003cFragment\u003e\n\t\u003c\u003e\u003c/\u003e\n\n### 作用\n\n\u003e 可以不用必须有一个真实的DOM根标签了\n\n\u003chr/\u003e\n\n## 5. Context\n\n### 理解\n\n\u003e 一种组件间通信方式, 常用于【祖组件】与【后代组件】间通信\n\n### 使用\n\n```js\n1) 创建Context容器对象：\n\tconst XxxContext = React.createContext()  \n\t\n2) 渲染子组时，外面包裹xxxContext.Provider, 通过value属性给后代组件传递数据：\n\t\u003cxxxContext.Provider value={数据}\u003e\n\t\t子组件\n    \u003c/xxxContext.Provider\u003e\n    \n3) 后代组件读取数据：\n\n\t//第一种方式:仅适用于类组件 \n\t  static contextType = xxxContext  // 声明接收context\n\t  this.context // 读取context中的value数据\n\t  \n\t//第二种方式: 函数组件与类组件都可以\n\t  \u003cxxxContext.Consumer\u003e\n\t    {\n\t      value =\u003e ( // value就是context中的value数据\n\t        要显示的内容\n\t      )\n\t    }\n\t  \u003c/xxxContext.Consumer\u003e\n```\n\n### 注意\n\n\t在应用开发中一般不用context, 一般都用它的封装react插件\n\n\u003chr/\u003e\n\n## 6. 组件优化\n\n### Component的2个问题\n\n\u003e 1. 只要执行setState(),即使不改变状态数据, 组件也会重新render() ==\u003e 效率低\n\u003e\n\u003e 2. 只当前组件重新render(), 就会自动重新render子组件，纵使子组件没有用到父组件的任何数据 ==\u003e 效率低\n\n### 效率高的做法\n\n\u003e  只有当组件的state或props数据发生改变时才重新render()\n\n### 原因\n\n\u003e  Component中的shouldComponentUpdate()总是返回true\n\n### 解决\n\n\t办法1: \n\t\t重写shouldComponentUpdate()方法\n\t\t比较新旧state或props数据, 如果有变化才返回true, 如果没有返回false\n\t办法2:  \n\t\t使用PureComponent\n\t\tPureComponent重写了shouldComponentUpdate(), 只有state或props数据有变化才返回true\n\t\t注意: \n\t\t\t只是进行state和props数据的浅比较, 如果只是数据对象内部数据变了, 返回false  \n\t\t\t不要直接修改state数据, 而是要产生新数据\n\t项目中一般使用PureComponent来优化\n\n\u003chr/\u003e\n\n## 7. render props\n\n### 如何向组件内部动态传入带内容的结构(标签)?\n\n\tVue中: \n\t\t使用slot技术, 也就是通过组件标签体传入结构  \u003cA\u003e\u003cB/\u003e\u003c/A\u003e\n\tReact中:\n\t\t使用children props: 通过组件标签体传入结构\n\t\t使用render props: 通过组件标签属性传入结构,而且可以携带数据，一般用render函数属性\n\n### children props\n\n\t\u003cA\u003e\n\t  \u003cB\u003exxxx\u003c/B\u003e\n\t\u003c/A\u003e\n\t{this.props.children}\n\t问题: 如果B组件需要A组件内的数据, ==\u003e 做不到 \n\n### render props\n\n\t\u003cA render={(data) =\u003e \u003cC data={data}\u003e\u003c/C\u003e}\u003e\u003c/A\u003e\n\tA组件: {this.props.render(内部state数据)}\n\tC组件: 读取A组件传入的数据显示 {this.props.data} \n\n\u003chr/\u003e\n\n## 8. 错误边界\n\n#### 理解：\n\n错误边界(Error boundary)：用来捕获后代组件错误，渲染出备用页面\n\n#### 特点：\n\n只能捕获后代组件生命周期产生的错误，不能捕获自己组件产生的错误和其他组件在合成事件、定时器中产生的错误\n\n##### 使用方式：\n\ngetDerivedStateFromError配合componentDidCatch\n\n```js\n// 生命周期函数，一旦后台组件报错，就会触发\nstatic getDerivedStateFromError(error) {\n    console.log(error);\n    // 在render之前触发\n    // 返回新的state\n    return {\n        hasError: true,\n    };\n}\n\ncomponentDidCatch(error, info) {\n    // 统计页面的错误。发送请求发送到后台去\n    console.log(error, info);\n}\n```\n\n## 9. 组件通信方式总结\n\n#### 组件间的关系：\n\n- 父子组件\n- 兄弟组件（非嵌套组件）\n- 祖孙组件（跨级组件）\n\n#### 几种通信方式：\n\n\t\t1.props：\n\t\t\t(1).children props\n\t\t\t(2).render props\n\t\t2.消息订阅-发布：\n\t\t\tpubs-sub、event等等\n\t\t3.集中式管理：\n\t\t\tredux、dva等等\n\t\t4.conText:\n\t\t\t生产者-消费者模式\n\n#### 比较好的搭配方式：\n\n\t\t父子组件：props\n\t\t兄弟组件：消息订阅-发布、集中式管理\n\t\t祖孙组件(跨级组件)：消息订阅-发布、集中式管理、conText(开发用的少，封装插件用的多)\n\n# Hooks\n\n## React Hook/Hooks是什么?\n\n1. Hook是React 16.8.0版本增加的新特性/新语法\n2. 可以让你在函数组件中使用 state 以及其他的 React 特性\n\n## State Hook\n\n1. State Hook让函数组件也可以有state状态, 并进行状态数据的读写操作\n2. 语法: `const [xxx, setXxx] = React.useState(initValue)  `\n3. `useState()`说明:\n\n   - 参数: 第一次初始化指定的值在内部作缓存\n   - 返回值: 包含2个元素的数组, 第1个为内部当前状态值, 第2个为更新状态值的函数\n   - `setXxx()`2种写法:\n     - `setXxx(newValue)` 参数为非函数值, 直接指定新的状态值, 内部用其覆盖原来的状态值\n     - `setXxx(value =\u003e newValue)` 参数为函数, 接收原本的状态值, 返回新的状态值, 内部用其覆盖原来的状态值\n\n## Effect Hook\n\n1. Effect Hook 可以让你在函数组件中执行副作用操作(用于模拟类组件中的生命周期钩子)\n2. React中的副作用操作:\n\n   - 发ajax请求数据获取\n   - 设置订阅 / 启动定时器\n   - 手动更改真实DOM\n\n3. 语法和说明:\n\n   ```javascript\n   x f(() =\u003e { \n   \t// 在此可以执行任何带副作用操作\n   \treturn () =\u003e { // 在组件卸载前执行\n   \t// 在此做一些收尾工作, 比如清除定时器/取消订阅等\n   \t}\n   }, [stateValue]) // 如果指定的是[], 回调函数只会在第一次render()后执行\n   ```\n\n4. 可以把 `useEffect Hook`看做如下三个函数的组合\n\n   ```javascript\n   componentDidMount()\n   componentDidUpdate()\n   componentWillUnmount() \n   ```\n\n## Ref Hook\n\n1. Ref Hook可以在函数组件中存储/查找组件内的标签或任意其它数据\n2. 语法: `const refContainer = useRef()`\n3. 作用:保存标签对象,功能与`React.createRef()`一样\n\n# React Router 6\n\n## 1 简介\n\n1. React Router 以三个不同的包发布到 npm 上，它们分别为：\n   1. react-router: 路由的核心库，提供了很多的：组件、钩子。\n   2. \u003cstrong style=\"color:#dd4d40\"\u003e**react-router-dom:**\u003c/strong \u003e \u003cstrong style=\"color:#dd4d40\"\u003e包含react-router所有内容，并添加一些专门用于 DOM 的组件，例如 `\u003cBrowserRouter\u003e`等 \u003c/strong\u003e。\n   3. react-router-native: 包括react-router所有内容，并添加一些专门用于ReactNative的API，例如:`\u003cNativeRouter\u003e`等。\n2. 与React Router 5.x 版本相比，改变了什么？\n   1. 内置组件的变化：移除`\u003cSwitch/\u003e` ，新增 `\u003cRoutes/\u003e`等。\n   2. 语法的变化：`component={About}` 变为 `element={\u003cAbout/\u003e}`等。\n   5. 新增多个hook：`useParams`、`useNavigate`、`useMatch`等。\n   7. \u003cstrong style=\"color:#dd4d40\"\u003e官方明确推荐函数式组件了！！！\u003c/strong\u003e\n\n      ……\n\n## 2 Component\n\n### 1. `\u003cBrowserRouter\u003e`\n\n1. 说明：`\u003cBrowserRouter\u003e `用于包裹整个应用。\n2. 示例代码：\n\n   ```jsx\n   import React from \"react\";\n   import ReactDOM from \"react-dom\";\n   import { BrowserRouter } from \"react-router-dom\";\n   \n   ReactDOM.render(\n     \u003cBrowserRouter\u003e\n       {/* 整体结构（通常为App组件） */}\n     \u003c/BrowserRouter\u003e,root\n   );\n   ```\n\n### 2. `\u003cHashRouter\u003e`\n\n1. 说明：**作用与`\u003cBrowserRouter\u003e`一样，但`\u003cHashRouter\u003e`修改的是地址栏的hash值。**\n2. 备注：6.x版本中`\u003cHashRouter\u003e`、`\u003cBrowserRouter\u003e ` 的用法与 5.x 相同。\n\n### 3. `\u003cRoutes/\u003e 与 \u003cRoute/\u003e`\n\n1. v6版本中移出了先前的`\u003cSwitch\u003e`，引入了新的替代者：`\u003cRoutes\u003e`。\n2. `\u003cRoutes\u003e` 和 `\u003cRoute\u003e`要配合使用，且必须要用`\u003cRoutes\u003e`包裹`\u003cRoute\u003e`。\n3. `\u003cRoute\u003e` 相当于一个 if 语句，如果其路径与当前 URL 匹配，则呈现其对应的组件。\n4. `\u003cRoute caseSensitive\u003e` 属性用于指定：匹配时是否区分大小写（默认为 false）。\n5. 当URL发生变化时，`\u003cRoutes\u003e `都会查看其所有子` \u003cRoute\u003e` 元素以找到最佳匹配并呈现组件 。\n6. `\u003cRoute\u003e` 也可以嵌套使用，且可配合`useRoutes()`配置 “路由表” ，但需要通过 `\u003cOutlet\u003e` 组件来渲染其子路由。\n7. 示例代码：\n\n   ```jsx\n   \u003cRoutes\u003e\n       /*path属性用于定义路径，element属性用于定义当前路径所对应的组件*/\n       \u003cRoute path=\"/login\" element={\u003cLogin /\u003e}\u003e\u003c/Route\u003e\n   \n      /*用于定义嵌套路由，home是一级路由，对应的路径/home*/\n       \u003cRoute path=\"home\" element={\u003cHome /\u003e}\u003e\n          /*test1 和 test2 是二级路由,对应的路径是/home/test1 或 /home/test2*/\n         \u003cRoute path=\"test1\" element={\u003cTest/\u003e}\u003e\u003c/Route\u003e\n         \u003cRoute path=\"test2\" element={\u003cTest2/\u003e}\u003e\u003c/Route\u003e\n      \u003c/Route\u003e\n    \n      //Route也可以不写element属性, 这时就是用于展示嵌套的路由 .所对应的路径是/users/xxx\n       \u003cRoute path=\"users\"\u003e\n          \u003cRoute path=\"xxx\" element={\u003cDemo /\u003e} /\u003e\n       \u003c/Route\u003e\n   \u003c/Routes\u003e\n   ```\n\n### 4. `\u003cLink\u003e`\n\n1. 作用: 修改URL，且不发送网络请求（路由链接）。\n2. 注意: 外侧需要用`\u003cBrowserRouter\u003e`或`\u003cHashRouter\u003e`包裹。\n3. 示例代码：\n\n   ```jsx\n   import { Link } from \"react-router-dom\";\n   \n   function Test() {\n     return (\n       \u003cdiv\u003e\n        \u003cLink to=\"/路径\"\u003e按钮\u003c/Link\u003e\n       \u003c/div\u003e\n     );\n   }\n   ```\n\n### 5. `\u003cNavLink\u003e`\n\n1. 作用: 与`\u003cLink\u003e`组件类似，且可实现导航的“高亮”效果。\n2. 示例代码：\n\n   ```jsx\n   // 注意: NavLink默认类名是active，下面是指定自定义的class\n   \n   //自定义样式\n   \u003cNavLink\n       to=\"login\"\n       className={({ isActive }) =\u003e {\n           console.log('home', isActive)\n           return isActive ? 'base one' : 'base'\n       }}\n   \u003elogin\u003c/NavLink\u003e\n   \n   /*\n    默认情况下，当Home的子组件匹配成功，Home的导航也会高亮，\n    当NavLink上添加了end属性后，若Home的子组件匹配成功，则Home的导航没有高亮效果。\n   */\n   \u003cNavLink to=\"home\" end \u003ehome\u003c/NavLink\u003e\n   ```\n\n### 6. `\u003cNavigate\u003e`\n\n1. 作用：只要`\u003cNavigate\u003e`组件被渲染，就会修改路径，切换视图。\n2. `replace`属性用于控制跳转模式（push 或 replace，默认是push）。\n3. 示例代码：\n\n   ```jsx\n   import React,{useState} from 'react'\n   import {Navigate} from 'react-router-dom'\n   \n   export default function Home() {\n    const [sum,setSum] = useState(1)\n    return (\n      \u003cdiv\u003e\n        \u003ch3\u003e我是Home的内容\u003c/h3\u003e\n        {/* 根据sum的值决定是否切换视图 */}\n        {sum === 1 ? \u003ch4\u003esum的值为{sum}\u003c/h4\u003e : \u003cNavigate to=\"/about\" replace={true}/\u003e}\n        \u003cbutton onClick={()=\u003esetSum(2)}\u003e点我将sum变为2\u003c/button\u003e\n      \u003c/div\u003e\n    )\n   }\n   ```\n\n### 7. `\u003cOutlet\u003e`\n\n1. 当`\u003cRoute\u003e`产生嵌套时，渲染其对应的后续子路由。\n2. 示例代码：\n\n   ```jsx\n   //根据路由表生成对应的路由规则\n   const element = useRoutes([\n     {\n       path:'/about',\n       element:\u003cAbout/\u003e\n     },\n     {\n       path:'/home',\n       element:\u003cHome/\u003e,\n       children:[\n         {\n           path:'news',\n           element:\u003cNews/\u003e\n         },\n         {\n           path:'message',\n           element:\u003cMessage/\u003e,\n         }\n       ]\n     }\n   ])\n   \n   //Home.js\n   import React from 'react'\n   import {NavLink,Outlet} from 'react-router-dom'\n   \n   export default function Home() {\n    return (\n      \u003cdiv\u003e\n        \u003ch2\u003eHome组件内容\u003c/h2\u003e\n        \u003cdiv\u003e\n          \u003cul className=\"nav nav-tabs\"\u003e\n            \u003cli\u003e\n              \u003cNavLink className=\"list-group-item\" to=\"news\"\u003eNews\u003c/NavLink\u003e\n            \u003c/li\u003e\n            \u003cli\u003e\n              \u003cNavLink className=\"list-group-item\" to=\"message\"\u003eMessage\u003c/NavLink\u003e\n            \u003c/li\u003e\n          \u003c/ul\u003e\n          {/* 指定路由组件呈现的位置 */}\n          \u003cOutlet /\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    )\n   }\n   \n   ```\n\n## 3 Hooks\n\n### 1. useRoutes()\n\n1. 作用：根据路由表，动态创建`\u003cRoutes\u003e`和`\u003cRoute\u003e`。\n2. 示例代码：\n\n   ```jsx\n   //路由表配置：src/routes/index.js\n   import About from '../pages/About'\n   import Home from '../pages/Home'\n   import {Navigate} from 'react-router-dom'\n   \n   export default [\n    {\n      path:'/about',\n      element:\u003cAbout/\u003e\n    },\n    {\n      path:'/home',\n      element:\u003cHome/\u003e\n    },\n    {\n      path:'/',\n      element:\u003cNavigate to=\"/about\"/\u003e\n    }\n   ]\n   \n   //App.jsx\n   import React from 'react'\n   import {NavLink,useRoutes} from 'react-router-dom'\n   import routes from './routes'\n   \n   export default function App() {\n    //根据路由表生成对应的路由规则\n    const element = useRoutes(routes)\n    return (\n      \u003cdiv\u003e\n        ......\n         {/* 注册路由 */}\n         {element}\n        ......\n      \u003c/div\u003e\n    )\n   }\n   \n   ```\n\n### 2. useNavigate()\n\n1. 作用：返回一个函数用来实现编程式导航。\n2. 示例代码：\n\n   ```jsx\n   import React from 'react'\n   import {useNavigate} from 'react-router-dom'\n   \n   export default function Demo() {\n     const navigate = useNavigate()\n     const handle = () =\u003e {\n       //第一种使用方式：指定具体的路径\n       navigate('/login', {\n         replace: false,\n         state: {a:1, b:2}\n       }) \n       //第二种使用方式：传入数值进行前进或后退，类似于5.x中的 history.go()方法\n       navigate(-1)\n     }\n     \n     return (\n       \u003cdiv\u003e\n         \u003cbutton onClick={handle}\u003e按钮\u003c/button\u003e\n       \u003c/div\u003e\n     )\n   }\n   ```\n\n### 3. useParams()\n\n1. 作用：回当前匹配路由的`params`参数，类似于5.x中的`match.params`。\n2. 示例代码：\n\n   ```jsx\n   import React from 'react';\n   import { Routes, Route, useParams } from 'react-router-dom';\n   import User from './pages/User.jsx'\n   \n   function ProfilePage() {\n     // 获取URL中携带过来的params参数\n     let { id } = useParams();\n   }\n   \n   function App() {\n     return (\n       \u003cRoutes\u003e\n         \u003cRoute path=\"users/:id\" element={\u003cUser /\u003e}/\u003e\n       \u003c/Routes\u003e\n     );\n   }\n   ```\n\n### 4. useSearchParams()\n\n1. 作用：用于读取和修改当前位置的 URL 中的查询字符串。\n2. 返回一个包含两个值的数组，内容分别为：当前的seaech参数、更新search的函数。\n3. 示例代码：\n\n   ```jsx\n   import React from 'react'\n   import {useSearchParams} from 'react-router-dom'\n   \n   export default function Detail() {\n    const [search,setSearch] = useSearchParams()\n    const id = search.get('id')\n    const title = search.get('title')\n    const content = search.get('content')\n    return (\n      \u003cul\u003e\n        \u003cli\u003e\n          \u003cbutton onClick={()=\u003esetSearch('id=008\u0026title=哈哈\u0026content=嘻嘻')}\u003e点我更新一下收到的search参数\u003c/button\u003e\n        \u003c/li\u003e\n        \u003cli\u003e消息编号：{id}\u003c/li\u003e\n        \u003cli\u003e消息标题：{title}\u003c/li\u003e\n        \u003cli\u003e消息内容：{content}\u003c/li\u003e\n      \u003c/ul\u003e\n    )\n   }\n   \n   ```\n\n### 5. useLocation()\n\n1. 作用：获取当前 location 信息，对标5.x中的路由组件的`location`属性。\n2. 示例代码：\n\n   ```jsx\n   import React from 'react'\n   import {useLocation} from 'react-router-dom'\n   \n   export default function Detail() {\n    const x = useLocation()\n    console.log('@',x)\n     // x就是location对象: \n    /*\n      {\n         hash: \"\",\n         key: \"ah9nv6sz\",\n         pathname: \"/login\",\n         search: \"?name=zs\u0026age=18\",\n         state: {a: 1, b: 2}\n       }\n    */\n    return (\n      \u003cul\u003e\n        \u003cli\u003e消息编号：{id}\u003c/li\u003e\n        \u003cli\u003e消息标题：{title}\u003c/li\u003e\n        \u003cli\u003e消息内容：{content}\u003c/li\u003e\n      \u003c/ul\u003e\n    )\n   }\n   \n     \n   \n   \n   ```\n\n### 6. useMatch()\n\n1. 作用：返回当前匹配信息，对标5.x中的路由组件的`match`属性。\n2. 示例代码：\n\n   ```jsx\n   \u003cRoute path=\"/login/:page/:pageSize\" element={\u003cLogin /\u003e}/\u003e\n   \u003cNavLink to=\"/login/1/10\"\u003e登录\u003c/NavLink\u003e\n   \n   export default function Login() {\n     const match = useMatch('/login/:x/:y')\n     console.log(match) //输出match对象\n     //match对象内容如下：\n     /*\n      {\n         params: {x: '1', y: '10'}\n         pathname: \"/LoGin/1/10\"  \n         pathnameBase: \"/LoGin/1/10\"\n         pattern: {\n          path: '/login/:x/:y', \n          caseSensitive: false, \n          end: false\n         }\n       }\n     */\n     return (\n      \u003cdiv\u003e\n         \u003ch1\u003eLogin\u003c/h1\u003e\n       \u003c/div\u003e\n     )\n   }\n   ```\n\n### 7. useInRouterContext()\n\n​ 作用：如果组件在 `\u003cRouter\u003e` 的上下文中呈现，则 `useInRouterContext` 钩子返回 true，否则返回 false。\n\n### 8. useNavigationType()\n\n1. 作用：返回当前的导航类型（用户是如何来到当前页面的）。\n2. 返回值：`POP`、`PUSH`、`REPLACE`。\n3. 备注：`POP`是指在浏览器中直接打开了这个路由组件（刷新页面）。\n\n### 9. useOutlet()\n\n1. 作用：用来呈现当前组件中渲染的嵌套路由。\n2. 示例代码：\n\n   ```jsx\n   const result = useOutlet()\n   console.log(result)\n   // 如果嵌套路由没有挂载,则result为null\n   // 如果嵌套路由已经挂载,则展示嵌套的路由对象\n   ```\n\n### 10.useResolvedPath()\n\n1. 作用：给定一个 URL值，解析其中的：path、search、hash值。\n","lastmodified":"2023-05-09T16:33:58.295366497Z","tags":[]},"/redis":{"title":"redis","content":"\n## 1.Redis 持久化机制\n\nRedis主要提供了两种持久化机制：RDB和AOF;\n\n#### RDB\n\n默认开启，会按照配置的指定时间将内存中的数据快照到磁盛中，创建一个dump.rdb文件，Redis启动时再恢复到内存中。  \nRedis会单独创建fork一个子进程，将当前父进程的数据库数据复制到子进程的内存中，然后由子进程写入到临时文件中，持久化的过程结束了，再用这个临时文件替换上次的快照文件，然后子进程退出，内存释放。需要注意的是，**每次快照持久化都会将主进程的数据库数据复制一遍，导致内存开销加倍，若此时内存不足，则会阻塞服务器运行**，直到复制结束释放内存；都会将内存数据完整写入磁盛一次，所以如果数据量大的话，而且写操作频繁，必然会引起大量的  \n磁盘IO操作，严重影响性能，并且最后一次持久化后的数据可能会丟失。\n\n#### AOF\n\n以日志的形式记录每个**写操作（读操作不记录）**，只需追加文件但不可以改写文件，Redis启动时会根据日志从头到尾全部执行一遍以完成数据的恢复工作。包括flushDB也会执行。  \n主要有两种方式触发：有写操作就写、每秒定时写（也会丢数据）。  \n因为AOF采用追加的方式，所以文件会越来越大，针对这个问题，新增了重写机制，就是当日志文件大到一定程度的时候，会fork出一条新进程来遍历进程内存中的数据，每条记录对应一条set语句，写到临时文件中，然后再替换到旧的日志文件（类似rdb的操作方式）。默认触发是当aof文件大小是上次重写后大小的一倍且文件大于64M时触发。当两种方式同时开启时，数据恢复Redis会优先选择AOF恢复。一般情况下，只要使用默认开启的RDB即可，因为相对于AOF,RDB便于进行数据库备份，并且恢复数据集的速度也要快很多。  \n开启持久化缓存机制，对性能会有一定的影响，特别是当设置的内存满了的时候，更是下降到几百reqs/s。所以如果只是用来做缓存的话，可以关掉持久化。\n\n## 2.缓存雪崩、缓存穿透、缓存击穿\n\n![img](v2-70b01bc5d80e67981e8904e95860ae00_b.jpg)\n\n## 3.内存淘汰策略\n\n**Redis内存回收机制**  \nRedis的内存回收主要围绕以下两个方面：  \n**1.Redis过期策略**:删除过期时间的key值  \n**2.Redis淘汰策略**:内存使用到达maxmemory上限时触发内存淘汰数据  \nRedis的过期策略和内存淘汰策略不是一件事，实际研发中不要弄混淆了，下面会完整的介绍两者。\n\n### Redis过期策略\n\n##### 1.定时过期\n\n每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。\n\n##### 2.惰性过期\n\n只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。\n\n##### 3.定期过期\n\n每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果。  \n**Redis中同时使用了惰性过期和定期过期两种过期策略。**\n\n### Redis淘汰策略\n\n##### 1.简介\n\nRedis的内存淘汰策略，是指当内存使用达到maxmemory极限时，需要使用LAU淘汰算法来决定清理掉哪些数据，以保证新数据的存入。\n\n##### 2、LRU算法\n\nRedis默认情况下就是使用LRU策略算法。  \nLRU算法(least RecentlyUsed),最近最少使用算法,也就是说默认删除最近最少使用的键。  \n但是一定要注意一点！redis中并不会准确的删除所有键中最近最少使用的键，而是随机抽取3个键，删除这三个键中最近最少使用的键。  \n那么3这个数字也是可以可以设置采样的大小，如果设置为10，那么效果会更好，不过也会耗费更多的CPU资源。对应位置是配置文件中的maxmeory-samples。\n\n##### 3.缓存清理配置\n\nmaxmemory用来设置redis存放数据的最大的内存大小，一旦超出这个内存大小之后，就会立即使用LRU算法清理掉部分数据。  \n对于64 bit的机器，如果maxmemory设置为0，那么就默认不限制内存的使用，直到耗尽机器中所有的内存为止;，但是对于32 bit的机器，有一个隐式的闲置就是3GB\n\n##### 4.Redis数据淘汰策略\n\nmaxmemory-policy，可以设置内存达到最大闲置后，采取什么策略来处理。  \n对应的淘汰策略规则如下：  \n**1）noeviction**：当内存不足以容纳新写入数据时，新写入操作会报错。  \n**2）allkeys-lru**：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。  \n**3）allkeys-random**：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。  \n**4）volatile-lru**：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。  \n**5）volatile-random**：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。  \n**6）volatile-ttl**：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。\n\n##### 5.缓存清理的流程\n\n1）客户端执行数据写入操作  \n2）redis server接收到写入操作之后，检查maxmemory的限制，如果超过了限制，那么就根据对应的policy清理掉部分数据  \n3）写入操作完成执行。\n\n## 4.发布订阅模式\n\nRedis 中的发布与订阅分为两种类型，一种是`基于频道`来实现，一种是`基于模式`来实现\n\n### 基于频道\n\n- **subscribe channe1 channel2 channel3 … ：订阅一个或者多个频道**\n- **unsubscribe channe1 channel2 channel3 … ：退订订阅的指定频道(关闭客户端终端没用，需要命令退订)**\n- **publish channe1 message：对指定频道发送消息**\n- **pubsub numsub channel1 channel2：查看指定频道的订阅数**\n","lastmodified":"2023-05-09T16:33:58.295366497Z","tags":[]},"/typescript":{"title":"typescript","content":"\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/ts/20210410163210.png)\n\n### 一、TypeScript 是什么\n\nTypeScript 是一种由微软开发的自由和开源的编程语言。它是 JavaScript 的一个超集，而且本质上向这个语言添加了可选的静态类型和基于类的面向对象编程。\n\nTypeScript 提供最新的和不断发展的 JavaScript 特性，包括那些来自 2015 年的 ECMAScript 和未来的提案中的特性，比如异步功能和 Decorators，以帮助建立健壮的组件。下图显示了 TypeScript 与 ES5、ES2015 和 ES2016 之间的关系：\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/ts/20210410163211.webp)\n\n#### 1.1 TypeScript 与 JavaScript 的区别\n\n|                   TypeScript                   |                 JavaScript                 |\n| :--------------------------------------------: | :----------------------------------------: |\n| JavaScript 的超集用于解决大型项目的代码复杂性  |      一种脚本语言，用于创建动态网页。      |\n|          可以在编译期间发现并纠正错误          |  作为一种解释型语言，只能在运行时发现错误  |\n|           强类型，支持静态和动态类型           |          弱类型，没有静态类型选项          |\n| 最终被编译成 JavaScript 代码，使浏览器可以理解 |           可以直接在浏览器中使用           |\n|              支持模块、泛型和接口              |           不支持模块，泛型或接口           |\n|          支持 ES3，ES4，ES5 和 ES6 等          |  不支持编译其他 ES3，ES4，ES5 或 ES6 功能  |\n|       社区的支持仍在增长，而且还不是很大       | 大量的社区支持以及大量文档和解决问题的支持 |\n\n- JavaScript 是**动态类型**：意味着在程序开发阶段，开发者开发出的源代码没有经过类型检查就直接交给解释器执行。\n- JavaScript 是**弱类型**：意味着程序在运行阶段碰到开发者造成的类型问题时，程序会**自作主张**的尝试进行隐式类型转换，无法转换则报错，转换成功则返回运算结果（有时候不是开发者所期望的运行结果，即**运行不准确**）。\n- JavaScript **既是动态类型又是弱类型**，这使得 JavaScript 程序在运行期间**很容易发生类型错误**、**隐藏潜在错误**、以及**错误不被识别为错误导致程序运行不准确**。\n\n#### 1.2 获取 TypeScript\n\n命令行的 TypeScript 编译器可以使用 Node.js 包来安装。\n\n**1.安装 TypeScript**\n\n```\n$ npm install -g typescript\n$ yarn add typescript --dev\n```\n\n**2.编译 TypeScript 文件**\n\n```\n$ tsc helloworld.ts\n# helloworld.ts =\u003e helloworld.js\n```\n\n当然，对于刚入门 TypeScript 的小伙伴，也可以不用安装 `typescript`，而是直接使用线上的 TypeScript Playground 来学习新的语法或新特性。\n\n\u003e TypeScript Playground：https://www.typescriptlang.org/play/\n\n### 二、TypeScript 类型\n\n这里简单提一提 Typescript 的类型系统，Typescript 官网文档对它的类型划分为以下几类：\n\n| 类型                          | 含义           | 示例                                            |\n| :---------------------------- | :------------- | :---------------------------------------------- |\n| basic Types                   | 基本类型       | number、Tuple、Void…                          |\n| Interfaces                    | 接口类型       | {x : 'xx'} / interface xxx { x 'xx'}            |\n| Unions and Intersection Types | 并集、交集类型 | object \\| null                                  |\n| Literal Types                 | 字面量类型     | 1 \\| 2 \\| 3                                     |\n| Enums                         | 枚举类型       | Enum x {x '1'}                                  |\n| Functions                     | 函数类型       | function (m: number): Void { // xxx,no return } |\n| Classes                       | 类类型         | 完整的 java 类，访问控制，单继承多实现          |\n| Generics                      | 泛型           | 值泛型：Array\\\u003cnumber\u003e，函数泛型、类泛型        |\n\n#### 2.1 Boolean 类型\n\n```typescript\nlet isDone: boolean = false;\n// ES5：var isDone = false;\n```\n\n#### 2.2 Number 类型\n\n```typescript\nlet count: number = 10;\n// ES5：var count = 10;\n```\n\n#### 2.3 String 类型\n\n```typescript\nlet name: string = \"Semliker\";\n// ES5：var name = 'Semlinker';\n```\n\n#### 2.4 Array 类型\n\n```typescript\nlet list: number[] = [1, 2, 3];\n// ES5：var list = [1,2,3];\n\nlet list: Array\u003cnumber\u003e = [1, 2, 3]; // Array\u003cnumber\u003e泛型语法\n// ES5：var list = [1,2,3];\n```\n\n#### 2.5 Enum 类型\n\n使用枚举我们可以定义一些带名字的常量。使用枚举可以清晰地表达意图或创建一组有区别的用例。TypeScript 支持数字的和基于字符串的枚举。\n\n**1.数字枚举**\n\n```typescript\nenum Direction {\n  NORTH,\n  SOUTH,\n  EAST,\n  WEST,\n}\n\nlet dir: Direction = Direction.NORTH;\n```\n\n默认情况下，NORTH 的初始值为 0，其余的成员会从 1 开始自动增长。换句话说，Direction.SOUTH 的值为 1，Direction.EAST 的值为 2，Direction.WEST 的值为 3。上面的枚举示例代码经过编译后会生成以下代码：\n\n```typescript\n\"use strict\";\nvar Direction;\n(function (Direction) {\n  Direction[(Direction[\"NORTH\"] = 0)] = \"NORTH\";\n  Direction[(Direction[\"SOUTH\"] = 1)] = \"SOUTH\";\n  Direction[(Direction[\"EAST\"] = 2)] = \"EAST\";\n  Direction[(Direction[\"WEST\"] = 3)] = \"WEST\";\n})(Direction || (Direction = {}));\nvar dir = Direction.NORTH;\n```\n\n当然我们也可以设置 NORTH 的初始值，比如：\n\n```typescript\nenum Direction {\n  NORTH = 3,\n  SOUTH,\n  EAST,\n  WEST,\n}\n```\n\n**2.字符串枚举**\n\n在 TypeScript 2.4 版本，允许我们使用字符串枚举。在一个字符串枚举里，每个成员都必须用字符串字面量，或另外一个字符串枚举成员进行初始化。\n\n```typescript\nenum Direction {\n  NORTH = \"NORTH\",\n  SOUTH = \"SOUTH\",\n  EAST = \"EAST\",\n  WEST = \"WEST\",\n}\n```\n\n**3.异构枚举**\n\n异构枚举的成员值是数字和字符串的混合：\n\n```typescript\nenum Enum {\n  A,\n  B,\n  C = \"C\",\n  D = \"D\",\n  E = 8,\n  F,\n}\n```\n\n#### 2.6 Any 类型\n\n在 TypeScript 中，任何类型都可以被归为 any 类型。这让 any 类型成为了类型系统的顶级类型（也被称作全局超级类型）。\n\n```typescript\nlet notSure: any = 666;\nnotSure = \"Semlinker\";\nnotSure = false;\n```\n\n`any` 类型本质上是类型系统的一个逃逸舱。作为开发者，这给了我们很大的自由：TypeScript 允许我们对 `any` 类型的值执行任何操作，而无需事先执行任何形式的检查。比如：\n\n```typescript\nlet value: any;\n\nvalue.foo.bar; // OK\nvalue.trim(); // OK\nvalue(); // OK\nnew value(); // OK\nvalue[0][1]; // OK\n```\n\n在许多场景下，这太宽松了。使用 `any` 类型，可以很容易地编写类型正确但在运行时有问题的代码。如果我们使用 `any` 类型，就无法使用 TypeScript 提供的大量的保护机制。为了解决 `any` 带来的问题，TypeScript 3.0 引入了 `unknown` 类型。\n\n#### 2.7 Unknown 类型\n\n就像所有类型都可以赋值给 `any`，所有类型也都可以赋值给 `unknown`。这使得 `unknown` 成为 TypeScript 类型系统的另一种顶级类型（另一种是 `any`）。下面我们来看一下 `unknown` 类型的使用示例：\n\n```typescript\nlet value: unknown;\n\nvalue = true; // OK\nvalue = 42; // OK\nvalue = \"Hello World\"; // OK\nvalue = []; // OK\nvalue = {}; // OK\nvalue = Math.random; // OK\nvalue = null; // OK\nvalue = undefined; // OK\nvalue = new TypeError(); // OK\nvalue = Symbol(\"type\"); // OK\n```\n\n对 `value` 变量的所有赋值都被认为是类型正确的。但是，当我们尝试将类型为 `unknown` 的值赋值给其他类型的变量时会发生什么？\n\n```typescript\nlet value: unknown;\n\nlet value1: unknown = value; // OK\nlet value2: any = value; // OK\nlet value3: boolean = value; // Error\nlet value4: number = value; // Error\nlet value5: string = value; // Error\nlet value6: object = value; // Error\nlet value7: any[] = value; // Error\nlet value8: Function = value; // Error\n```\n\n`unknown` 类型只能被赋值给 `any` 类型和 `unknown` 类型本身。直观地说，这是有道理的：只有能够保存任意类型值的容器才能保存 `unknown` 类型的值。毕竟我们不知道变量 `value` 中存储了什么类型的值。\n\n现在让我们看看当我们尝试对类型为 `unknown` 的值执行操作时会发生什么。以下是我们在之前 `any` 章节看过的相同操作：\n\n```typescript\nlet value: unknown;\n\nvalue.foo.bar; // Error\nvalue.trim(); // Error\nvalue(); // Error\nnew value(); // Error\nvalue[0][1]; // Error\n```\n\n将 `value` 变量类型设置为 `unknown` 后，这些操作都不再被认为是类型正确的。通过将 `any` 类型改变为 `unknown` 类型，我们已将允许所有更改的默认设置，更改为禁止任何更改。\n\n#### 2.8 Tuple 类型\n\n众所周知，数组一般由同种类型的值组成，但有时我们需要在单个变量中存储不同类型的值，这时候我们就可以使用元组。在 JavaScript 中是没有元组的，**元组是 TypeScript 中特有的类型**，其工作方式类似于数组。\n\n元组可用于定义具有有限数量的未命名属性的类型。每个属性都有一个关联的类型。使用元组时，必须提供每个属性的值。为了更直观地理解元组的概念，我们来看一个具体的例子：\n\n```typescript\nlet tupleType: [string, boolean];\ntupleType = [\"Semlinker\", true];\n```\n\n在上面代码中，我们定义了一个名为 `tupleType` 的变量，它的类型是一个类型数组 `[string, boolean]`，然后我们按照正确的类型依次初始化 tupleType 变量。与数组一样，我们可以通过下标来访问元组中的元素：\n\n```typescript\nconsole.log(tupleType[0]); // Semlinker\nconsole.log(tupleType[1]); // true\n```\n\n在元组初始化的时候，如果出现类型不匹配的话，比如：\n\n```typescript\ntupleType = [true, \"Semlinker\"];\n```\n\n此时，TypeScript 编译器会提示以下错误信息：\n\n```typescript\n[0]: Type 'true' is not assignable to type 'string'.\n[1]: Type 'string' is not assignable to type 'boolean'.\n```\n\n很明显是因为类型不匹配导致的。在元组初始化的时候，我们还必须提供每个属性的值，不然也会出现错误，比如：\n\n```typescript\ntupleType = [\"Semlinker\"];\n```\n\n此时，TypeScript 编译器会提示以下错误信息：\n\n```typescript\nProperty '1' is missing in type '[string]' but required in type '[string, boolean]'.\n```\n\n#### 2.9 Void 类型\n\n某种程度上来说，void 类型像是与 any 类型相反，它表示没有任何类型。当一个函数没有返回值时，你通常会见到其返回值类型是 void：\n\n```typescript\n// 声明函数返回值为void\nfunction warnUser(): void {\n  console.log(\"This is my warning message\");\n}\n```\n\n以上代码编译生成的 ES5 代码如下：\n\n```typescript\n\"use strict\";\nfunction warnUser() {\n  console.log(\"This is my warning message\");\n}\n```\n\n需要注意的是，声明一个 void 类型的变量没有什么作用，因为它的值只能为 `undefined` 或 `null`：\n\n```typescript\nlet unusable: void = undefined;\n```\n\n#### 2.10 Null 和 Undefined 类型\n\nTypeScript 里，`undefined` 和 `null` 两者有各自的类型分别为 `undefined` 和 `null`。\n\n```typescript\nlet u: undefined = undefined;\nlet n: null = null;\n```\n\n默认情况下 `null` 和 `undefined` 是所有类型的子类型。就是说你可以把 `null` 和 `undefined` 赋值给 `number` 类型的变量。**然而，如果你指定了`--strictNullChecks` 标记，`null` 和 `undefined` 只能赋值给 `void` 和它们各自的类型。**\n\n#### 2.11 Never 类型\n\n`never` 类型表示的是那些永不存在的值的类型。例如，`never` 类型是那些总是会抛出异常或根本就不会有返回值的函数表达式或箭头函数表达式的返回值类型。\n\n```\n// 返回never的函数必须存在无法达到的终点\nfunction error(message: string): never {\n  throw new Error(message);\n}\n\nfunction infiniteLoop(): never {\n  while (true) {}\n}\n```\n\n在 TypeScript 中，可以利用 never 类型的特性来实现全面性检查，具体示例如下：\n\n```typescript\ntype Foo = string | number;\n\nfunction controlFlowAnalysisWithNever(foo: Foo) {\n  if (typeof foo === \"string\") {\n    // 这里 foo 被收窄为 string 类型\n  } else if (typeof foo === \"number\") {\n    // 这里 foo 被收窄为 number 类型\n  } else {\n    // foo 在这里是 never\n    const check: never = foo;\n  }\n}\n```\n\n### 三、TypeScript 断言\n\n有时候你会遇到这样的情况，你会比 TypeScript 更了解某个值的详细信息。通常这会发生在你清楚地知道一个实体具有比它现有类型更确切的类型。\n\n通过类型断言这种方式可以告诉编译器，“相信我，我知道自己在干什么”。类型断言好比其他语言里的类型转换，但是不进行特殊的数据检查和解构。它没有运行时的影响，只是在编译阶段起作用。\n\n类型断言有两种形式：\n\n#### 3.1 \\\u003cxx\u003e语法\n\n```typescript\nlet someValue: any = \"this is a string\";\nlet strLength: number = (\u003cstring\u003esomeValue).length;\n```\n\n#### 3.2 as 语法\n\n```typescript\nlet someValue: any = \"this is a string\";\nlet strLength: number = (someValue as string).length;\n```\n\n### 四、类型守卫\n\n\u003e A type guard is some expression that performs a runtime check that guarantees the type in some scope. —— TypeScript 官方文档\n\n类型保护是可执行运行时检查的一种表达式，用于确保该类型在一定的范围内。换句话说，类型保护可以保证一个字符串是一个字符串，尽管它的值也可以是一个数值。类型保护与特性检测并不是完全不同，其主要思想是尝试检测属性、方法或原型，以确定如何处理值。目前主要有四种的方式来实现类型保护：\n\n#### 4.1 in 关键字\n\n```typescript\ninterface Admin {\n  name: string;\n  privileges: string[];\n}\n\ninterface Employee {\n  name: string;\n  startDate: Date;\n}\n\ntype UnknownEmployee = Employee | Admin;\n\nfunction printEmployeeInformation(emp: UnknownEmployee) {\n  console.log(\"Name: \" + emp.name);\n  if (\"privileges\" in emp) {\n    console.log(\"Privileges: \" + emp.privileges);\n  }\n  if (\"startDate\" in emp) {\n    console.log(\"Start Date: \" + emp.startDate);\n  }\n}\n```\n\n#### 4.2 typeof 关键字\n\n```typescript\nfunction padLeft(value: string, padding: string | number) {\n  if (typeof padding === \"number\") {\n      return Array(padding + 1).join(\" \") + value;\n  }\n  if (typeof padding === \"string\") {\n      return padding + value;\n  }\n  throw new Error(`Expected string or number, got '${padding}'.`);\n}\n```\n\n`typeof` 类型保护只支持两种形式：`typeof v === \"typename\"` 和 `typeof v !== typename`，`\"typename\"` 必须是 `\"number\"`， `\"string\"`， `\"boolean\"` 或 `\"symbol\"`。但是 TypeScript 并不会阻止你与其它字符串比较，语言不会把那些表达式识别为类型保护。\n\n#### 4.3 instanceof 关键字\n\n```typescript\ninterface Padder {\n  getPaddingString(): string;\n}\n\nclass SpaceRepeatingPadder implements Padder {\n  constructor(private numSpaces: number) {}\n  getPaddingString() {\n    return Array(this.numSpaces + 1).join(\" \");\n  }\n}\n\nclass StringPadder implements Padder {\n  constructor(private value: string) {}\n  getPaddingString() {\n    return this.value;\n  }\n}\n\nlet padder: Padder = new SpaceRepeatingPadder(6);\n\nif (padder instanceof SpaceRepeatingPadder) {\n  // padder的类型收窄为 'SpaceRepeatingPadder'\n}\n```\n\n#### 4.4 自定义类型保护的类型谓词\n\n```typescript\nfunction isNumber(x: any): x is number {\n  return typeof x === \"number\";\n}\n\nfunction isString(x: any): x is string {\n  return typeof x === \"string\";\n}\n```\n\n### 五、联合类型和类型别名\n\n#### 5.1 联合类型\n\n联合类型通常与 `null` 或 `undefined` 一起使用：\n\n```typescript\nconst sayHello = (name: string | undefined) =\u003e {\n  /* ... */\n};\n```\n\n例如，这里 `name` 的类型是 `string | undefined` 意味着可以将 `string` 或 `undefined` 的值传递给`sayHello` 函数。\n\n```typescript\nsayHello(\"Semlinker\");\nsayHello(undefined);\n```\n\n通过这个示例，你可以凭直觉知道类型 A 和类型 B 联合后的类型是同时接受 A 和 B 值的类型。\n\n#### 5.2 可辨识联合\n\nTypeScript 可辨识联合（Discriminated Unions）类型，也称为代数数据类型或标签联合类型。**它包含 3 个要点：可辨识、联合类型和类型守卫。**\n\n这种类型的本质是结合联合类型和字面量类型的一种类型保护方法。**如果一个类型是多个类型的联合类型，且多个类型含有一个公共属性，那么就可以利用这个公共属性，来创建不同的类型保护区块。**\n\n**1.可辨识**\n\n可辨识要求联合类型中的每个元素都含有一个单例类型属性，比如：\n\n```typescript\nenum CarTransmission {\n  Automatic = 200,\n  Manual = 300\n}\n\ninterface Motorcycle {\n  vType: \"motorcycle\"; // discriminant\n  make: number; // year\n}\n\ninterface Car {\n  vType: \"car\"; // discriminant\n  transmission: CarTransmission\n}\n\ninterface Truck {\n  vType: \"truck\"; // discriminant\n  capacity: number; // in tons\n}\n```\n\n在上述代码中，我们分别定义了 `Motorcycle`、 `Car` 和 `Truck` 三个接口，在这些接口中都包含一个 `vType` 属性，该属性被称为可辨识的属性，而其它的属性只跟特性的接口相关。\n\n**2.联合类型**\n\n基于前面定义了三个接口，我们可以创建一个 `Vehicle` 联合类型：\n\n```typescript\ntype Vehicle = Motorcycle | Car | Truck;\n```\n\n现在我们就可以开始使用 `Vehicle` 联合类型，对于 `Vehicle` 类型的变量，它可以表示不同类型的车辆。\n\n**3.类型守卫**\n\n下面我们来定义一个 `evaluatePrice` 方法，该方法用于根据车辆的类型、容量和评估因子来计算价格，具体实现如下：\n\n```typescript\nconst EVALUATION_FACTOR = Math.PI; \nfunction evaluatePrice(vehicle: Vehicle) {\n  return vehicle.capacity * EVALUATION_FACTOR;\n}\n\nconst myTruck: Truck = { vType: \"truck\", capacity: 9.5 };\nevaluatePrice(myTruck);\n```\n\n对于以上代码，TypeScript 编译器将会提示以下错误信息：\n\n```typescript\nProperty 'capacity' does not exist on type 'Vehicle'.\nProperty 'capacity' does not exist on type 'Motorcycle'.\n```\n\n原因是在 Motorcycle 接口中，并不存在 `capacity` 属性，而对于 Car 接口来说，它也不存在 `capacity` 属性。那么，现在我们应该如何解决以上问题呢？这时，我们可以使用类型守卫。下面我们来重构一下前面定义的 `evaluatePrice` 方法，重构后的代码如下：\n\n```typescript\nfunction evaluatePrice(vehicle: Vehicle) {\n  switch(vehicle.vType) {\n    case \"car\":\n      return vehicle.transmission * EVALUATION_FACTOR;\n    case \"truck\":\n      return vehicle.capacity * EVALUATION_FACTOR;\n    case \"motorcycle\":\n      return vehicle.make * EVALUATION_FACTOR;\n  }\n}\n```\n\n在以上代码中，我们使用 `switch` 和 `case` 运算符来实现类型守卫，从而确保在 `evaluatePrice` 方法中，我们可以安全地访问 `vehicle` 对象中的所包含的属性，来正确的计算该车辆类型所对应的价格。\n\n#### 5.3 类型别名Type Aliases\n\n一个类型会被使用多次，此时我们更希望通过一个单独的名字来引用它，这就是**类型别名**（type alias）。所谓类型别名，顾名思义，一个可以指代任意类型的名字。\n\n```typescript\ntype Point = {\n  x: number;\n  y: number;\n};\n \n// Exactly the same as the earlier example\nfunction printCoord(pt: Point) {\n  console.log(\"The coordinate's x value is \" + pt.x);\n  console.log(\"The coordinate's y value is \" + pt.y);\n}\n \nprintCoord({ x: 100, y: 100 });\n```\n\n你可以使用类型别名给任意类型一个名字，举个例子，命名一个联合类型：\n\n```typescript\ntype ID = number | string;\n```\n\n注意别名是唯一的别名，你不能使用类型别名创建同一个类型的不同版本。当你使用类型别名的时候，它就跟你编写的类型是一样的。换句话说，代码看起来可能不合法，但对 TypeScript 依然是合法的，因为两个类型都是同一个类型的别名:\n\n```javascript\ntype UserInputSanitizedString = string;\n \nfunction sanitizeInput(str: string): UserInputSanitizedString {\n  return sanitize(str);\n}\n \n// Create a sanitized input\nlet userInput = sanitizeInput(getInput());\n \n// Can still be re-assigned with a string though\nuserInput = \"new input\";\n```\n\n### 六、交叉类型\n\nTypeScript 交叉类型是将多个类型合并为一个类型。这让我们可以把现有的多种类型叠加到一起成为一种类型，它包含了所需的所有类型的特性。\n\n```typescript\ninterface IPerson {\n  id: string;\n  age: number;\n}\n\ninterface IWorker {\n  companyId: string;\n}\n\ntype IStaff = IPerson \u0026 IWorker;\n\nconst staff: IStaff = {\n  id: 'E1006',\n  age: 33,\n  companyId: 'EFT'\n};\n\nconsole.dir(staff)\n```\n\n在上面示例中，我们首先为 IPerson 和 IWorker 类型定义了不同的成员，然后通过 `\u0026` 运算符定义了 IStaff 交叉类型，所以该类型同时拥有 IPerson 和 IWorker 这两种类型的成员。\n\n### 七、TypeScript 函数\n\n#### 7.1 TypeScript 函数与 JavaScript 函数的区别\n\n| TypeScript     | JavaScript         |\n| :------------- | :----------------- |\n| 含有类型       | 无类型             |\n| 箭头函数       | 箭头函数（ES2015） |\n| 函数类型       | 无函数类型         |\n| 必填和可选参数 | 所有参数都是可选的 |\n| 默认参数       | 默认参数           |\n| 剩余参数       | 剩余参数           |\n| 函数重载       | 无函数重载         |\n\n#### 7.2 箭头函数\n\n**1.常见语法**\n\n```typescript\nmyBooks.forEach(() =\u003e console.log('reading'));\n\nmyBooks.forEach(title =\u003e console.log(title));\n\nmyBooks.forEach((title, idx, arr) =\u003e\n  console.log(idx + '-' + title);\n);\n\nmyBooks.forEach((title, idx, arr) =\u003e {\n  console.log(idx + '-' + title);\n});\n```\n\n**2.使用示例**\n\n```typescript\n// 未使用箭头函数\nfunction Book() {\n  let self = this;\n  self.publishDate = 2016;\n  setInterval(function () {\n    console.log(self.publishDate);\n  }, 1000);\n}\n\n// 使用箭头函数\nfunction Book() {\n  this.publishDate = 2016;\n  setInterval(() =\u003e {\n    console.log(this.publishDate);\n  }, 1000);\n}\n```\n\n#### 7.3 参数类型和返回类型\n\n```typescript\nfunction createUserId(name: string, id: number): string {\n  return name + id;\n}\n```\n\n#### 7.4 函数类型\n\n```typescript\nlet IdGenerator: (chars: string, nums: number) =\u003e string;\n\nfunction createUserId(name: string, id: number): string {\n  return name + id;\n}\n\nIdGenerator = createUserId;\n```\n\n#### 7.5 可选参数及默认参数\n\n```typescript\n// 可选参数\nfunction createUserId(name: string, age?: number, id: number): string {\n  return name + id;\n}\n\n// 默认参数\nfunction createUserId(\n  name: string = \"Semlinker\",\n  age?: number,\n  id: number\n): string {\n  return name + id;\n}\n```\n\n#### 7.6 剩余参数\n\n```typescript\nfunction push(array, ...items) {\n  items.forEach(function (item) {\n    array.push(item);\n  });\n}\n\nlet a = [];\npush(a, 1, 2, 3);\n```\n\n#### 7.7 函数重载\n\n函数重载或方法重载是使用相同名称和不同参数数量或类型创建多个方法的一种能力。要解决前面遇到的问题，方法就是为同一个函数提供多个函数类型定义来进行函数重载，编译器会根据这个列表去处理函数的调用。\n\n```typescript\nfunction add(a: number, b: number): number;\nfunction add(a: string, b: string): string;\nfunction add(a: string, b: number): string;\nfunction add(a: number, b: string): string;\nfunction add(a: Combinable, b: Combinable) {\n  if (typeof a === \"string\" || typeof b === \"string\") {\n    return a.toString() + b.toString();\n  }\n  return a + b;\n}\n```\n\n在以上代码中，我们为 add 函数提供了多个函数类型定义，从而实现函数的重载。之后，可恶的错误消息又消失了，因为这时 result 变量的类型是 `string` 类型。在 TypeScript 中除了可以重载普通函数之外，我们还可以重载类中的成员方法。\n\n方法重载是指在同一个类中方法同名，参数不同（参数类型不同、参数个数不同或参数个数相同时参数的先后顺序不同），调用时根据实参的形式，选择与它匹配的方法执行操作的一种技术。所以类中成员方法满足重载的条件是：在同一个类中，方法名相同且参数列表不同。下面我们来举一个成员方法重载的例子：\n\n```typescript\nclass Calculator {\n  add(a: number, b: number): number;\n  add(a: string, b: string): string;\n  add(a: string, b: number): string;\n  add(a: number, b: string): string;\n  add(a: Combinable, b: Combinable) {\n    if (typeof a === \"string\" || typeof b === \"string\") {\n      return a.toString() + b.toString();\n    }\n    return a + b;\n  }\n}\n\nconst calculator = new Calculator();\nconst result = calculator.add(\"Semlinker\", \" Kakuqo\");\n```\n\n这里需要注意的是，当 TypeScript 编译器处理函数重载时，它会查找重载列表，尝试使用第一个重载定义。如果匹配的话就使用这个。因此，在定义重载的时候，一定要把最精确的定义放在最前面。另外在 Calculator 类中，`add(a: Combinable, b: Combinable){ }` 并不是重载列表的一部分，因此对于 add 成员方法来说，我们只定义了四个重载方法。\n\n### 八、TypeScript 数组\n\n#### 8.1 数组解构\n\n```typescript\nlet x: number; let y: number ;let z: number;\nlet five_array = [0,1,2,3,4];\n[x,y,z] = five_array;\n```\n\n#### 8.2 数组展开运算符\n\n```typescript\nlet two_array = [0, 1];\nlet five_array = [...two_array, 2, 3, 4];\n```\n\n#### 8.3 数组遍历\n\n```typescript\nlet colors: string[] = [\"red\", \"green\", \"blue\"];\nfor (let i of colors) {\n  console.log(i);\n}\n```\n\n### 九、TypeScript 对象\n\n#### 9.1 对象解构\n\n```typescript\nlet person = {\n  name: \"Semlinker\",\n  gender: \"Male\",\n};\n\nlet { name, gender } = person;\n```\n\n#### 9.2 对象展开运算符\n\n```typescript\nlet person = {\n  name: \"Semlinker\",\n  gender: \"Male\",\n  address: \"Xiamen\",\n};\n\n// 组装对象\nlet personWithAge = { ...person, age: 33 };\n\n// 获取除了某些项外的其它项\nlet { name, ...rest } = person;\n```\n\n### 十、TypeScript 接口\n\n在面向对象语言中，接口是一个很重要的概念，它是对行为的抽象，而具体如何行动需要由类去实现。\n\nTypeScript 中的接口是一个非常灵活的概念，除了可用于对类的一部分行为进行抽象以外，也常用于对「对象的形状（Shape）」进行描述。\n\n#### 10.1 对象的形状\n\n```typescript\ninterface Person {\n  name: string;\n  age: number;\n}\n\nlet Semlinker: Person = {\n  name: \"Semlinker\",\n  age: 33,\n};\n```\n\n#### 10.2 可选 | 只读属性\n\n```typescript\ninterface Person {\n  readonly name: string;\n  age?: number;\n}\n```\n\n只读属性用于**限制只能在对象刚刚创建的时候修改其值**。此外 TypeScript 还提供了 `ReadonlyArray` 类型，它与 `Array` 相似，只是把所有可变方法去掉了，因此可以确保数组创建后再也不能被修改。\n\n```typescript\nlet a: number[] = [1, 2, 3, 4];\nlet ro: ReadonlyArray\u003cnumber\u003e = a;\nro[0] = 12; // error!\nro.push(5); // error!\nro.length = 100; // error!\na = ro; // error!\n```\n\n### 十一、TypeScript 类\n\n#### 11.1 类的属性与方法\n\n在面向对象语言中，类是一种面向对象计算机编程语言的构造，是创建对象的蓝图，描述了所创建的对象共同的属性和方法。\n\n在 TypeScript 中，我们可以通过 `Class` 关键字来定义一个类：\n\n```typescript\nclass Greeter {\n  // 静态属性\n  static cname: string = \"Greeter\";\n  // 成员属性\n  greeting: string;\n\n  // 构造函数 - 执行初始化操作\n  constructor(message: string) {\n    this.greeting = message;\n  }\n\n  // 静态方法\n  static getClassName() {\n    return \"Class name is Greeter\";\n  }\n\n  // 成员方法\n  greet() {\n    return \"Hello, \" + this.greeting;\n  }\n}\n\nlet greeter = new Greeter(\"world\");\n```\n\n那么成员属性与静态属性，成员方法与静态方法有什么区别呢？这里无需过多解释，我们直接看一下以下编译生成的 ES5 代码：\n\n```typescript\n\"use strict\";\nvar Greeter = /** @class */ (function () {\n    // 构造函数 - 执行初始化操作\n    function Greeter(message) {\n        this.greeting = message;\n    }\n    // 静态方法\n    Greeter.getClassName = function () {\n        return \"Class name is Greeter\";\n    };\n    // 成员方法\n    Greeter.prototype.greet = function () {\n        return \"Hello, \" + this.greeting;\n    };\n    // 静态属性\n    Greeter.cname = \"Greeter\";\n    return Greeter;\n}());\nvar greeter = new Greeter(\"world\");\n```\n\n#### 11.2 访问器\n\n在 TypeScript 中，我们可以通过 `getter` 和 `setter` 方法来实现数据的封装和有效性校验，防止出现异常数据。\n\n```typescript\nlet passcode = \"Hello TypeScript\";\n\nclass Employee {\n  private _fullName: string;\n\n  get fullName(): string {\n    return this._fullName;\n  }\n\n  set fullName(newName: string) {\n    if (passcode \u0026\u0026 passcode == \"Hello TypeScript\") {\n      this._fullName = newName;\n    } else {\n      console.log(\"Error: Unauthorized update of employee!\");\n    }\n  }\n}\n\nlet employee = new Employee();\nemployee.fullName = \"Semlinker\";\nif (employee.fullName) {\n  console.log(employee.fullName);\n}\n```\n\n#### 11.3 类的继承\n\n继承 (Inheritance) 是一种联结类与类的层次模型。指的是一个类（称为子类、子接口）继承另外的一个类（称为父类、父接口）的功能，并可以增加它自己的新功能的能力，继承是类与类或者接口与接口之间最常见的关系。\n\n继承是一种 is-a 关系：\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/ts/20210410163212.webp)\n\n在 TypeScript 中，我们可以通过 `extends` 关键字来实现继承：\n\n```typescript\nclass Animal {\n  name: string;\n  constructor(theName: string) {\n    this.name = theName;\n  }\n  move(distanceInMeters: number = 0) {\n    console.log(`${this.name} moved ${distanceInMeters}m.`);\n  }\n}\n\nclass Snake extends Animal {\n  constructor(name: string) {\n    super(name);\n  }\n  move(distanceInMeters = 5) {\n    console.log(\"Slithering...\");\n    super.move(distanceInMeters);\n  }\n}\n\nlet sam = new Snake(\"Sammy the Python\");\nsam.move();\n```\n\n#### 11.4 ECMAScript 私有字段\n\n在 TypeScript 3.8 版本就开始支持**ECMAScript 私有字段**，使用方式如下：\n\n```typescript\nclass Person {\n  #name: string;\n\n  constructor(name: string) {\n    this.#name = name;\n  }\n\n  greet() {\n    console.log(`Hello, my name is ${this.#name}!`);\n  }\n}\n\nlet semlinker = new Person(\"Semlinker\");\n\nsemlinker.#name;\n//     ~~~~~\n// Property '#name' is not accessible outside class 'Person'\n// because it has a private identifier.\n```\n\n与常规属性（甚至使用 `private` 修饰符声明的属性）不同，私有字段要牢记以下规则：\n\n- 私有字段以 `#` 字符开头，有时我们称之为私有名称；\n- 每个私有字段名称都唯一地限定于其包含的类；\n- 不能在私有字段上使用 TypeScript 可访问性修饰符（如 public 或 private）；\n- 私有字段不能在包含的类之外访问，甚至不能被检测到。\n\n### 十二、TypeScript 泛型\n\n软件工程中，我们不仅要创建一致的定义良好的 API，同时也要考虑可重用性。组件不仅能够支持当前的数据类型，同时也能支持未来的数据类型，这在创建大型系统时为你提供了十分灵活的功能。\n\n**在像 C# 和 Java 这样的语言中，可以使用泛型来创建可重用的组件，一个组件可以支持多种类型的数据。这样用户就可以以自己的数据类型来使用组件。**\n\n设计泛型的关键目的是在成员之间提供有意义的约束，这些成员可以是：类的实例成员、类的方法、函数参数和函数返回值。\n\n泛型（Generics）是允许同一个函数接受不同类型参数的一种模板。相比于使用 any 类型，使用泛型来创建可复用的组件要更好，因为泛型会保留参数类型。\n\n#### 12.1 泛型接口\n\n```typescript\ninterface GenericIdentityFn\u003cT\u003e {\n  (arg: T): T;\n}\n```\n\n#### 12.2 泛型类\n\n```typescript\nclass GenericNumber\u003cT\u003e {\n  zeroValue: T;\n  add: (x: T, y: T) =\u003e T;\n}\n\nlet myGenericNumber = new GenericNumber\u003cnumber\u003e();\nmyGenericNumber.zeroValue = 0;\nmyGenericNumber.add = function (x, y) {\n  return x + y;\n};\n```\n\n#### 12.3 泛型变量\n\n对刚接触 TypeScript 泛型的小伙伴来说，看到 T 和 E，还有 K 和 V 这些泛型变量时，估计会一脸懵逼。其实这些大写字母并没有什么本质的区别，只不过是一个约定好的规范而已。也就是说使用大写字母 A-Z 定义的类型变量都属于泛型，把 T 换成 A，也是一样的。下面我们介绍一下一些常见泛型变量代表的意思：\n\n- T（Type）：表示一个 TypeScript 类型\n- K（Key）：表示对象中的键类型\n- V（Value）：表示对象中的值类型\n- E（Element）：表示元素类型\n\n#### 12.4 泛型工具类型\n\n为了方便开发者 TypeScript 内置了一些常用的工具类型，比如 Partial、Required、Readonly、Record 和 ReturnType 等。出于篇幅考虑，这里我们只简单介绍 Partial 工具类型。不过在具体介绍之前，我们得先介绍一些相关的基础知识，方便读者自行学习其它的工具类型。\n\n**1.typeof**\n\n在 TypeScript 中，`typeof` 操作符可以用来获取一个变量声明或对象的类型。\n\n```typescript\ninterface Person {\n  name: string;\n  age: number;\n}\n\nconst sem: Person = { name: 'semlinker', age: 30 };\ntype Sem= typeof sem; // -\u003e Person\n\nfunction toArray(x: number): Array\u003cnumber\u003e {\n  return [x];\n}\n\ntype Func = typeof toArray; // -\u003e (x: number) =\u003e number[]\n```\n\n**2.keyof**\n\n`keyof` 操作符可以用来一个对象中的所有 key 值：\n\n```typescript\ninterface Person {\n    name: string;\n    age: number;\n}\n\ntype K1 = keyof Person; // \"name\" | \"age\"\ntype K2 = keyof Person[]; // \"length\" | \"toString\" | \"pop\" | \"push\" | \"concat\" | \"join\" \ntype K3 = keyof { [x: string]: Person };  // string | number\n```\n\n**3.in**\n\n`in` 用来遍历枚举类型：\n\n```typescript\ntype Keys = \"a\" | \"b\" | \"c\"\n\ntype Obj =  {\n  [p in Keys]: any\n} // -\u003e { a: any, b: any, c: any }\n```\n\n**4.infer**\n\n在条件类型语句中，可以用 `infer` 声明一个类型变量并且对它进行使用。\n\n```typescript\ntype ReturnType\u003cT\u003e = T extends (\n  ...args: any[]\n) =\u003e infer R ? R : any;\n```\n\n以上代码中 `infer R` 就是声明一个变量来承载传入函数签名的返回值类型，简单说就是用它取到函数返回值的类型方便之后使用。\n\n**5.extends**\n\n有时候我们定义的泛型不想过于灵活或者说想继承某些类等，可以通过 extends 关键字添加泛型约束。\n\n```typescript\ninterface ILengthwise {\n  length: number;\n}\n\nfunction loggingIdentity\u003cT extends ILengthwise\u003e(arg: T): T {\n  console.log(arg.length);\n  return arg;\n}\n```\n\n现在这个泛型函数被定义了约束，因此它不再是适用于任意类型：\n\n```typescript\nloggingIdentity(3);  // Error, number doesn't have a .length property\n```\n\n这时我们需要传入符合约束类型的值，必须包含必须的属性：\n\n```typescript\nloggingIdentity({length: 10, value: 3});\n```\n\n**6.Partial**\n\n`Partial` 的作用就是将某个类型里的属性全部变为可选项 `?`。\n\n**定义：**\n\n```typescript\n/**\n * node_modules/typescript/lib/lib.es5.d.ts\n * Make all properties in T optional\n */\ntype Partial\u003cT\u003e = {\n  [P in keyof T]?: T[P];\n};\n```\n\n在以上代码中，首先通过 `keyof T` 拿到 `T` 的所有属性名，然后使用 `in` 进行遍历，将值赋给 `P`，最后通过 `T[P]` 取得相应的属性值。中间的 `?` 号，用于将所有属性变为可选。\n\n**示例：**\n\n```typescript\ninterface Todo {\n  title: \"string;\"\n  description: string;\n}\n\nfunction updateTodo(todo: Todo, fieldsToUpdate: Partial\u003cTodo\u003e) {\n  return { ...todo, ...fieldsToUpdate };\n}\n\nconst todo1 = {\n  title: \"\"organize desk\",\"\n  description: \"clear clutter\",\n};\n\nconst todo2 = updateTodo(todo1, {\n  description: \"throw out trash\",\n});\n```\n\n在上面的 `updateTodo` 方法中，我们利用 `Partial` 工具类型，定义 `fieldsToUpdate` 的类型为 `Partial`，即：\n\n```typescript\n{\n   title?: string | undefined;\n   description?: string | undefined;\n}\n```\n\n### 十三、TypeScript 装饰器\n\n#### 13.1 装饰器是什么\n\n- 它是一个表达式\n- 该表达式被执行后，返回一个函数\n- 函数的入参分别为 target、name 和 descriptor\n- 执行该函数后，可能返回 descriptor 对象，用于配置 target 对象\n\n#### 13.2 装饰器的分类\n\n- 类装饰器（Class decorators）\n- 属性装饰器（Property decorators）\n- 方法装饰器（Method decorators）\n- 参数装饰器（Parameter decorators）\n\n#### 13.3 类装饰器\n\n类装饰器声明：\n\n```typescript\ndeclare type ClassDecorator = \u003cTFunction extends Function\u003e(\n  target: TFunction\n) =\u003e TFunction | void;\n```\n\n类装饰器顾名思义，就是用来装饰类的。它接收一个参数：\n\n- target: TFunction - 被装饰的类\n\n看完第一眼后，是不是感觉都不好了。没事，我们马上来个例子：\n\n```typescript\nfunction Greeter(target: Function): void {\n  target.prototype.greet = function (): void {\n    console.log(\"Hello Semlinker!\");\n  };\n}\n\n@Greeter\nclass Greeting {\n  constructor() {\n    // 内部实现\n  }\n}\n\nlet myGreeting = new Greeting();\nmyGreeting.greet(); // console output: 'Hello Semlinker!';\n```\n\n上面的例子中，我们定义了 `Greeter` 类装饰器，同时我们使用了 `@Greeter` 语法糖，来使用装饰器。\n\n\u003e 友情提示：读者可以直接复制上面的代码，在 TypeScript Playground 中运行查看结果。\n\n有的读者可能想问，例子中总是输出 `Hello Semlinker!` ，能自定义输出的问候语么 ？这个问题很好，答案是可以的。\n\n具体实现如下：\n\n```typescript\nfunction Greeter(greeting: string) {\n  return function (target: Function) {\n    target.prototype.greet = function (): void {\n      console.log(greeting);\n    };\n  };\n}\n\n@Greeter(\"Hello TS!\")\nclass Greeting {\n  constructor() {\n    // 内部实现\n  }\n}\n\nlet myGreeting = new Greeting();\nmyGreeting.greet(); // console output: 'Hello TS!';\n```\n\n#### 13.4 属性装饰器\n\n属性装饰器声明：\n\n```typescript\ndeclare type PropertyDecorator = (target:Object, \n  propertyKey: string | symbol ) =\u003e void;\n```\n\n属性装饰器顾名思义，用来装饰类的属性。它接收两个参数：\n\n- target: Object - 被装饰的类\n- propertyKey: string | symbol - 被装饰类的属性名\n\n趁热打铁，马上来个例子热热身：\n\n```typescript\nfunction logProperty(target: any, key: string) {\n  delete target[key];\n\n  const backingField = \"_\" + key;\n\n  Object.defineProperty(target, backingField, {\n    writable: true,\n    enumerable: true,\n    configurable: true\n  });\n\n  // property getter\n  const getter = function (this: any) {\n    const currVal = this[backingField];\n    console.log(`Get: ${key} =\u003e ${currVal}`);\n    return currVal;\n  };\n\n  // property setter\n  const setter = function (this: any, newVal: any) {\n    console.log(`Set: ${key} =\u003e ${newVal}`);\n    this[backingField] = newVal;\n  };\n\n  // Create new property with getter and setter\n  Object.defineProperty(target, key, {\n    get: getter,\n    set: setter,\n    enumerable: true,\n    configurable: true\n  });\n}\n\nclass Person { \n  @logProperty\n  public name: string;\n\n  constructor(name : string) { \n    this.name = name;\n  }\n}\n\nconst p1 = new Person(\"semlinker\");\np1.name = \"kakuqo\";\n```\n\n以上代码我们定义了一个 `logProperty` 函数，来跟踪用户对属性的操作，当代码成功运行后，在控制台会输出以下结果：\n\n```\nSet: name =\u003e semlinker\nSet: name =\u003e kakuqo\n```\n\n#### 13.5 方法装饰器\n\n方法装饰器声明：\n\n```\ndeclare type MethodDecorator = \u003cT\u003e(target:Object, propertyKey: string | symbol,    \n  descriptor: TypePropertyDescript\u003cT\u003e) =\u003e TypedPropertyDescriptor\u003cT\u003e | void;\n```\n\n方法装饰器顾名思义，用来装饰类的方法。它接收三个参数：\n\n- target: Object - 被装饰的类\n- propertyKey: string | symbol - 方法名\n- descriptor: TypePropertyDescript - 属性描述符\n\n废话不多说，直接上例子：\n\n```\nfunction LogOutput(tarage: Function, key: string, descriptor: any) {\n  let originalMethod = descriptor.value;\n  let newMethod = function(...args: any[]): any {\n    let result: any = originalMethod.apply(this, args);\n    if(!this.loggedOutput) {\n      this.loggedOutput = new Array\u003cany\u003e();\n    }\n    this.loggedOutput.push({\n      method: key,\n      parameters: args,\n      output: result,\n      timestamp: new Date()\n    });\n    return result;\n  };\n  descriptor.value = newMethod;\n}\n\nclass Calculator {\n  @LogOutput\n  double (num: number): number {\n    return num * 2;\n  }\n}\n\nlet calc = new Calculator();\ncalc.double(11);\n// console ouput: [{method: \"double\", output: 22, ...}]\nconsole.log(calc.loggedOutput); \n```\n\n下面我们来介绍一下参数装饰器。\n\n#### 13.6 参数装饰器\n\n参数装饰器声明：\n\n```\ndeclare type ParameterDecorator = (target: Object, propertyKey: string | symbol, \n  parameterIndex: number ) =\u003e void\n```\n\n参数装饰器顾名思义，是用来装饰函数参数，它接收三个参数：\n\n- target: Object - 被装饰的类\n- propertyKey: string | symbol - 方法名\n- parameterIndex: number - 方法中参数的索引值\n\n```\nfunction Log(target: Function, key: string, parameterIndex: number) {\n  let functionLogged = key || target.prototype.constructor.name;\n  console.log(`The parameter in position ${parameterIndex} at ${functionLogged} has\n been decorated`);\n}\n\nclass Greeter {\n  greeting: string;\n  constructor(@Log phrase: string) {\n this.greeting = phrase; \n  }\n}\n\n// console output: The parameter in position 0 \n// at Greeter has been decorated\n```\n\n介绍完 TypeScript 入门相关的基础知识，猜测很多刚入门的小伙伴已有 **“从入门到放弃”** 的想法，最后我们来简单介绍一下编译上下文。\n\n### 十四、编译上下文\n\n#### 14.1 tsconfig.json 的作用\n\n- 用于标识 TypeScript 项目的根路径；\n- 用于配置 TypeScript 编译器；\n- 用于指定编译的文件。\n\n#### 14.2 tsconfig.json 重要字段\n\n- files - 设置要编译的文件的名称；\n- include - 设置需要进行编译的文件，支持路径模式匹配；\n- exclude - 设置无需进行编译的文件，支持路径模式匹配；\n- compilerOptions - 设置与编译流程相关的选项。\n\n#### 14.3 compilerOptions 选项\n\ncompilerOptions 支持很多选项，常见的有 `baseUrl`、 `target`、`baseUrl`、 `moduleResolution` 和 `lib` 等。\n\ncompilerOptions 每个选项的详细说明如下：\n\n```typescript\n{\n  \"compilerOptions\": {\n\n    /* 基本选项 */\n    \"target\": \"es5\",                       // 指定 ECMAScript 目标版本: 'ES3' (default), 'ES5', 'ES6'/'ES2015', 'ES2016', 'ES2017', or 'ESNEXT'\n    \"module\": \"commonjs\",                  // 指定使用模块: 'commonjs', 'amd', 'system', 'umd' or 'es2015'\n    \"lib\": [],                             // 指定要包含在编译中的库文件\n    \"allowJs\": true,                       // 允许编译 javascript 文件\n    \"checkJs\": true,                       // 报告 javascript 文件中的错误\n    \"jsx\": \"preserve\",                     // 指定 jsx 代码的生成: 'preserve', 'react-native', or 'react'\n    \"declaration\": true,                   // 生成相应的 '.d.ts' 文件\n    \"sourceMap\": true,                     // 生成相应的 '.map' 文件\n    \"outFile\": \"./\",                       // 将输出文件合并为一个文件\n    \"outDir\": \"./\",                        // 指定输出目录\n    \"rootDir\": \"./\",                       // 用来控制输出目录结构 --outDir.\n    \"removeComments\": true,                // 删除编译后的所有的注释\n    \"noEmit\": true,                        // 不生成输出文件\n    \"importHelpers\": true,                 // 从 tslib 导入辅助工具函数\n    \"isolatedModules\": true,               // 将每个文件做为单独的模块 （与 'ts.transpileModule' 类似）.\n\n    /* 严格的类型检查选项 */\n    \"strict\": true,                        // 启用所有严格类型检查选项\n    \"noImplicitAny\": true,                 // 在表达式和声明上有隐含的 any类型时报错\n    \"strictNullChecks\": true,              // 启用严格的 null 检查\n    \"noImplicitThis\": true,                // 当 this 表达式值为 any 类型的时候，生成一个错误\n    \"alwaysStrict\": true,                  // 以严格模式检查每个模块，并在每个文件里加入 'use strict'\n\n    /* 额外的检查 */\n    \"noUnusedLocals\": true,                // 有未使用的变量时，抛出错误\n    \"noUnusedParameters\": true,            // 有未使用的参数时，抛出错误\n    \"noImplicitReturns\": true,             // 并不是所有函数里的代码都有返回值时，抛出错误\n    \"noFallthroughCasesInSwitch\": true,    // 报告 switch 语句的 fallthrough 错误。（即，不允许 switch 的 case 语句贯穿）\n\n    /* 模块解析选项 */\n    \"moduleResolution\": \"node\",            // 选择模块解析策略： 'node' (Node.js) or 'classic' (TypeScript pre-1.6)\n    \"baseUrl\": \"./\",                       // 用于解析非相对模块名称的基目录\n    \"paths\": {},                           // 模块名到基于 baseUrl 的路径映射的列表\n    \"rootDirs\": [],                        // 根文件夹列表，其组合内容表示项目运行时的结构内容\n    \"typeRoots\": [],                       // 包含类型声明的文件列表\n    \"types\": [],                           // 需要包含的类型声明文件名列表\n    \"allowSyntheticDefaultImports\": true,  // 允许从没有设置默认导出的模块中默认导入。\n\n    /* Source Map Options */\n    \"sourceRoot\": \"./\",                    // 指定调试器应该找到 TypeScript 文件而不是源文件的位置\n    \"mapRoot\": \"./\",                       // 指定调试器应该找到映射文件而不是生成文件的位置\n    \"inlineSourceMap\": true,               // 生成单个 soucemaps 文件，而不是将 sourcemaps 生成不同的文件\n    \"inlineSources\": true,                 // 将代码与 sourcemaps 生成到一个文件中，要求同时设置了 --inlineSourceMap 或 --sourceMap 属性\n\n    /* 其他选项 */\n    \"experimentalDecorators\": true,        // 启用装饰器\n    \"emitDecoratorMetadata\": true          // 为装饰器提供元数据的支持\n  }\n}\n```\n","lastmodified":"2023-05-09T16:33:58.295366497Z","tags":[]},"/vuereact":{"title":"vue\u0026react","content":"\n## Vue和React区别\n\n### 监听数据变化的实现原理不同\n\n- Vue 通过 getter/setter 以及一些函数的劫持，能精确知道数据变化，不需要特别的优化就能达到很好的性能\n- React 默认是通过比较引用的方式进行的，如果不优化（PureComponent/shouldComponentUpdate）可能导致大量不必要的VDOM的重新渲染\n\n为什么 React 不精确监听数据变化呢？这是因为 Vue 和 React 设计理念上的区别，Vue 使用的是可变数据，而React更强调数据的不可变。所以应该说没有好坏之分，Vue更加简单，而React构建大型应用的时候更加鲁棒。\n\n因为一般都会用一个数据层的框架比如 Vuex 和 Redux，所以这部分不作过多解释，在最后的 vuex 和 redux的区别 中也会讲到。\n\n### 数据流的不同\n\n![image-20200607142534002](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/面试/Vue\u0026React/20210405162043.png)\n\n大家都知道Vue中默认是支持双向绑定的。在Vue1.0中我们可以实现两种双向绑定：\n\n1. 父子组件之间，props 可以双向绑定\n2. 组件与DOM之间可以通过 v-model 双向绑定\n\n在 Vue2.x 中去掉了第一种，也就是父子组件之间不能双向绑定了（但是提供了一个语法糖自动帮你通过事件的方式修改），并且 Vue2.x 已经不鼓励组件对自己的 props 进行任何修改了。  \n所以现在我们只有 组件 \u003c--\u003e DOM 之间的双向绑定这一种。\n\n然而 React 从诞生之初就不支持双向绑定，React一直提倡的是单向数据流，他称之为 onChange/setState()模式。\n\n不过由于我们一般都会用 Vuex 以及 Redux 等单向数据流的状态管理框架，因此很多时候我们感受不到这一点的区别了。\n\n### HoC 和 mixins\n\n在 Vue 中我们组合不同功能的方式是通过 mixin，而在React中我们通过 HoC (高阶组件）。\n\nReact 最早也是使用 mixins 的，不过后来他们觉得这种方式对组件侵入太强会导致很多问题，就弃用了 mixinx 转而使用 HoC，关于mixin究竟哪里不好，可以参考React官方的这篇文章 Mixins Considered Harmful\n\n而 Vue 一直是使用 mixin 来实现的。\n\n为什么 Vue 不采用 HoC 的方式来实现呢？\n\n高阶组件本质就是高阶函数，React 的组件是一个纯粹的函数，所以高阶函数对React来说非常简单。\n\n但是Vue就不行了，Vue中组件是一个被包装的函数，并不简单的就是我们定义组件的时候传入的对象或者函数。比如我们定义的模板怎么被编译的？比如声明的props怎么接收到的？这些都是vue创建组件实例的时候隐式干的事。由于vue默默帮我们做了这么多事，所以我们自己如果直接把组件的声明包装一下，返回一个高阶组件，那么这个被包装的组件就无法正常工作了。\n\n### 组件通信的区别\n\n![image-20200607142712786](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/面试/Vue\u0026React/20210405162044.png)\n\n其实这部分两个比较相似。\n\n在Vue 中有三种方式可以实现组件通信：\n\n- 父组件通过 props 向子组件传递数据或者回调，虽然可以传递回调，但是我们一般只传数据，而通过 事件的机制来处理子组件向父组件的通信\n- 子组件通过 事件 向父组件发送消息\n- 通过 V2.2.0 中新增的 provide/inject 来实现父组件向子组件注入数据，可以跨越多个层级。\n\n另外有一些比如访问 $parent/$children等比较dirty的方式这里就不讲了。\n\n在 React 中，也有对应的三种方式：\n\n- 父组件通过 props 可以向子组件传递数据或者回调\n- 可以通过 context 进行跨层级的通信，这其实和 provide/inject 起到的作用差不多。\n\n可以看到，React 本身并不支持自定义事件，Vue中子组件向父组件传递消息有两种方式：事件和回调函数，而且Vue更倾向于使用事件。但是在 React 中我们都是使用回调函数的，这可能是他们二者最大的区别。\n\n### 模板渲染方式的不同\n\n在表层上， 模板的语法不同\n\n- React 是通过JSX渲染模板\n- 而Vue是通过一种拓展的HTML语法进行渲染\n\n但其实这只是表面现象，毕竟React并不必须依赖JSX。  \n在深层上，模板的原理不同，这才是他们的本质区别：\n\n- React是在组件JS代码中，通过原生JS实现模板中的常见语法，比如插值，条件，循环等，都是通过JS语法实现的\n- Vue是在和组件JS代码分离的单独的模板中，通过指令来实现的，比如条件语句就需要 v-if 来实现\n\n对这一点，我个人比较喜欢React的做法，因为他更加纯粹更加原生，而Vue的做法显得有些独特，会把HTML弄得很乱。举个例子，说明React的好处：\n\nreact中render函数是支持闭包特性的，所以我们import的组件在render中可以直接调用。但是在Vue中，由于模板中使用的数据都必须挂在 this 上进行一次中转，所以我们import 一个组件完了之后，还需要在 components 中再声明下，这样显然是很奇怪但又不得不这样的做法。\n\n### Vuex 和 Redux 的区别\n\n从表面上来说，store 注入和使用方式有一些区别。\n\n在 Vuex 中，$store 被直接注入到了组件实例中，因此可以比较灵活的使用：\n\n- 使用 dispatch 和 commit 提交更新\n- 通过 mapState 或者直接通过 this.$store 来读取数据\n\n在 Redux 中，我们每一个组件都需要显示的用 connect 把需要的 props 和 dispatch 连接起来。\n\n另外 Vuex 更加灵活一些，组件中既可以 dispatch action 也可以 commit updates，而 Redux 中只能进行 dispatch，并不能直接调用 reducer 进行修改。\n\n从实现原理上来说，最大的区别是两点：\n\n- Redux 使用的是不可变数据，而Vuex的数据是可变的。Redux每次都是用新的state替换旧的state，而Vuex是直接修改\n- Redux 在检测数据变化的时候，是通过 diff 的方式比较差异的，而Vuex其实和Vue的原理一样，是通过 getter/setter来比较的（如果看Vuex源码会知道，其实他内部直接创建一个Vue实例用来跟踪数据变化）\n\n而这两点的区别，其实也是因为 React 和 Vue的设计理念上的区别。React更偏向于构建稳定大型的应用，非常的科班化。相比之下，Vue更偏向于简单迅速的解决问题，更灵活，不那么严格遵循条条框框。因此也会给人一种大型项目用React，小型项目用 Vue 的感觉。\n","lastmodified":"2023-05-09T16:33:58.295366497Z","tags":[]},"/web-test":{"title":"web-test","content":"\n## 单元测试\n\n首先声明一点，长期以来，前端开发的单元测试并不是在前端的开发过程中所必须的，也不是每个前端开发工程师所注意和重视的，甚至扩大到软件开发过程中单元测试这一环也不是在章程上有书面规定所要求的。但是随着每个工程的复杂化、代码的高复用性要求和前端代码模块之间的高内聚低耦合的需求，前端工程中的单元测试流程就显得很有其必要。\n\n### 1.前端单元测试是什么\n\n   首先我们要明确测试是什么：\n\n\u003e ​ 为检测特定的目标是否符合标准而采用专用的工具或者方法进行验证，并最终得出特定的结果。\n\n​ 对于前端开发过程来说，这里的特定目标就是指我们写的代码，而工具就是我们需要用到的测试框架(库)、测试用例等。检测处的结果就是展示测试是否通过或者给出测试报告，这样才能方便问题的排查和后期的修正。\n\n​ 基于测试“是什么”的说法，为便于刚从事前端开发的同行的进阶理解，那我们就列出单元测试它“不是什么”：\n\n\u003e 需要访问数据库的测试不是单元测试\n\u003e\n\u003e 需要访问网络的测试不是单元测试\n\u003e\n\u003e 需要访问文件系统的测试不是单元测试\n\u003e\n\u003e --- 修改代码的艺术\n\n对于单元测试“不是什么”的引用解释，至此点到为止。鉴于篇幅限制，对于引用内容，我想前端开发的同行们看到后会初步有一个属于自己的理解。\n\n### 2.单元测试的意义以及为什么需要单元测试\n\n**2.1 单元测试的意义**\n\n   对于现在的前端工程，一个标准完整的项目，测试是非常有必要的。很多时候我们只是完成了项目而忽略了项目测试的部分，测试的意义主要在于下面几点：\n\n1. TDD（测试驱动开发） 被证明是有效的软件编写原则，它能覆盖更多的功能接口。\n2. 快速反馈你的功能输出，验证你的想法。\n3. 保证代码重构的安全性，没有一成不变的代码，测试用例能给你多变的代码结构一个定心丸。\n4. 易于测试的代码，说明是一个好的设计。做单元测试之前，肯定要实例化一个东西，假如这个东西有很多依赖的话，这个测试构7. 造过程将会非常耗时，会影响你的测试效率，怎么办呢？要依赖分离，一个类尽量保证功能单一，比如视图与功能分离，这样的话，你的代码也便于维护和理解。\n\n**2.2 为什么需要单元测试**\n\n1. 首先是一个前端单元测试的根本性原由：JavaScript 是动态语言，缺少类型检查，编译期间无法定位到错误; JavaScript 宿主的兼容性问题。比如 DOM 操作在不同浏览器上的表现。\n2. 正确性：测试可以验证代码的正确性，在上线前做到心里有底。\n3. 自动化：当然手工也可以测试，通过console可以打印出内部信息，但是这是一次性的事情，下次测试还需要从头来过，效率不能得到保证。通过编写测试用例，可以做到一次编写，多次运行。\n4. 解释性：测试用例用于测试接口、模块的重要性，那么在测试用例中就会涉及如何使用这些API。其他开发人员如果要使用这些API，那阅读测试用例是一种很好地途径，有时比文档说明更清晰。\n5. 驱动开发，指导设计：代码被测试的前提是代码本身的可测试性，那么要保证代码的可测试性，就需要在开发中注意API的设计，TDD将测试前移就是起到这么一个作用。\n6. 保证重构：互联网行业产品迭代速度很快，迭代后必然存在代码重构的过程，那怎么才能保证重构后代码的质量呢？有测试用例做后盾，就可以大胆的进行重构。\n\n### 3.如何写单元测试用例\n\n**3.1** 原则\n\n- 测试代码时，只考虑测试，不考虑内部实现\n- 数据尽量模拟现实，越靠近现实越好\n- 充分考虑数据的边界条件\n- 对重点、复杂、核心代码，重点测试\n- 利用AOP(beforeEach、afterEach),减少测试代码数量，避免无用功能\n- 测试、功能开发相结合，有利于设计和代码重构\n\n**3.2** 两个常用的单元测试方法论\n\n  在单元测试中，常用的方法论有两个：TDD（测试驱动开发）\u0026BDD（行为驱动开发）\n\n  对于之前没听说过前端测试这两个模式的同行可以[***在此了解一下\\***](https://www.cnblogs.com/Leo_wl/p/4780678.html)，篇幅限制此处不再敖述。\n\n**3.3** 相信你看完之后也有一个自己对TDD和BDD的个人观点，在此我先谈谈我对TDD和BDD的 理解：\n\n### **TDD（Test-driven development）**：\n\n其基本思路是通过测试来推动整个开发的进行。\n\n- **单元测试的首要目的不是为了能够编写出大覆盖率的全部通过的测试代码，而是需要从使用者(调用者)的角度出发，尝试函数逻辑的各种可能性，进而辅助性增强代码质量**\n- 测试是手段而不是目的。测试的主要目的不是证明代码正确，而是帮助发现错误，包括低级的错误\n- 测试要快。快速运行、快速编写\n- 测试代码保持简洁\n- 不会忽略失败的测试。一旦团队开始接受1个测试的构建失败，那么他们渐渐地适应2、3、4或者更多的失败。在这种情况下，测试集就不再起作用\n\n**需要注意的是**：\n\n- 一定不能误解了TDD的核心目的！\n- 测试不是为了覆盖率和正确率\n- 而是作为实例，告诉开发人员要编写什么代码\n- 红灯（代码还不完善，测试挂）-\u003e 绿灯（编写代码，测试通过）-\u003e 重构（优化代码并保证测试通过）\n\n**TDD的过程是**：\n\n1. 需求分析，思考实现。考虑如何“使用”产品代码，是一个实例方法还是一个类方法，是从构造函数传参还是从方法调用传参，方法的命名，返回值等。这时其实就是在做设计，而且设计以代码来体现。此时测试为红\n2. 实现代码让测试为”绿灯“\n3. 重构，然后重复测试\n4. 最终符合所有要求即：\n   - 每个概念都被清晰的表达\n   - 代码中无自我重复\n   - 没有多余的东西\n   - 通过测试\n\n### BDD(Behavior-driven development)：\n\n行为驱动开发（BDD），重点是通过与利益相关者（简单说就是客户）的讨论，取得对预期的软件行为的认识，其**重点在于沟通**\n\n**BDD过程是：**\n\n1. 从业务的角度定义具体的，以及可衡量的目标\n2. 找到一种可以达到设定目标的、对业务最重要的那些功能的方法\n3. 然后像故事一样描述出一个个具体可执行的行为。其描述方法基于一些通用词汇，这些词汇具有准确无误的表达能力和一致的含义。例如，`expect`, `should`, `assert`\n4. 寻找合适语言及方法，对行为进行实现\n5. 测试人员检验产品运行结果是否符合预期行为。最大程度的交付出符合用户期望的产品，避免表达不一致带来的问题\n\n### 4.Mocha/Karma+Travis.CI的前端测试工作流\n\n   以上内容从什么是单元测试谈到单元测试的方法论。那么怎样用常用框架进行单元测试？单元测试的工具环境是什么？单元测试的实际示例是怎样的？\n\n   **首先应该简单介绍一下Mocha、Karma和Travis.CI**\n\n**Mocha：**mocha 是一个功能丰富的前端测试框架。所谓\"测试框架\"，就是运行测试的工具。通过它，可以为JavaScript应用添加测试，从而保证代码的质量。mocha 既可以基于 Node.js 环境运行 也可以在浏览器环境运行。欲了解更多可去[**官方网站**](https://mochajs.org/)进行学习。其官方介绍为：\n\n\u003e Mocha is a feature-rich JavaScript test framework running on Node.js and in the browser, making asynchronous testing simple and fun. Mocha tests run serially, allowing for flexible and accurate reporting, while mapping uncaught exceptions to the correct test cases. [**Hosted on GitHub.**](https://github.com/mochajs/mocha)\n\n**Karma：**一个基于Node.js的JavaScript测试执行过程管理工具（Test Runner）。该工具可用于测试所有主流Web浏览器，也可集成到CI（Continuous integration）工具，也可和其他代码编辑器一起使用。这个测试工具的一个强大特性就是，它可以监控文件的变化，然后自行执行，通过console.log显示测试结果。Karma的一个强大特性就是，它可以监控一套文件的变换，并立即开始测试已保存的文件，用户无需离开文本编辑器。测试结果通常显示在命令行中，而非代码编辑器。这也就让 Karma 基本可以和任何 JS 编辑器一起使用。\n\n**Travis.CI:** 提供的是持续集成服务（Continuous Integration，简称 CI）。它绑定 Github 上面的项目，只要有新的代码，就会自动抓取。然后，提供一个运行环境，执行测试，完成构建，还能部署到服务器。\n\n持续集成指的是只要代码有变更，就自动运行构建和测试，反馈运行结果。确保符合预期以后，再将新代码\"集成\"到主干。\n\n持续集成的好处在于，每次代码的小幅变更，就能看到运行结果，从而不断累积小的变更，而不是在开发周期结束时，一下子合并一大块代码。\n\n对于Travis.CI，建议移步到[**阮大大**](http://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html)和[**廖大大**](https://www.liaoxuefeng.com/article/0014631488240837e3633d3d180476cb684ba7c10fda6f6000)的个人网站上学习，两位老师讲的要比我在这儿写的更清晰。\n\n**断言库**\n\n基本工具框架介绍完毕后，相信稍微了解点测试的同行都知道，做单元测试是需要写测试脚本的，那么测试脚本就需要用到断言库。”断言“，个人理解即为”用彼代码断定测试此代码的正确性，检验并暴露此代码的错误。“那么对于前端单元测试来说，有以下常用断言库：\n\n看一段代码示例：\n\nexpect(add(1, 1)).to.be.equal(2);\n\n这是一句断言代码。\n\n所谓\"断言\"，就是判断源码的实际执行结果与预期结果是否一致，如果不一致就抛出一个错误。上面这句断言的意思是，调用 add(1, 1)，结果应该等于 2。所有的测试用例（it 块）都应该含有一句或多句的断言。它是编写测试用例的关键。断言功能由断言库来实现，Mocha 本身不带断言库，所以必须先引入断言库。\n\n引入断言库代码示例：\n\nvar expect = require('chai').expect;\n\n断言库有很多种，Mocha 并不限制使用哪一种，它允许你使用你想要的任何断言库。上面代码引入的断言库是 chai，并且指定使用它的 expect 断言风格。下面这些常见的断言库:\n\n- [**assert**](http://nodejs.cn/api/assert.html) 这个是 Node.js 中的断言模块。\n- [**should.js**](https://github.com/shouldjs/should.js)\n- [**expect.js**](https://github.com/Automattic/expect.js)\n- [**chai.js**](http://www.chaijs.com/)\n\n### node assert\n\n- assert(value[, message])\n- assert.ok(value[, message])\n- assert.equal(actual, expect[, message])\n- assert.notEqual(actual, expected[, message])\n- assert.strictEqual(actual, expect[, message])\n- assert.notStrictEqual(actial, expected[, message])\n- assert.deepEqual(actual, expect[, message])\n- assert.notDeepEqual(actual, expected[, message])\n- assert.deepStrictEqual(actual, expect[, message])\n- assert.notDeepStrictEqual(actual, expected[, message])\n- assert.throws(block[, error][, message])\n- assert.doesNotThrow(block[, error][, message])\n\n#### assert(value[, message])\n\n断言 value 的值是否为true，这里的等于判断使用的是 == 而不是 ===。message 是断言描述，为可选参数。\n\n```\nconst assert = require('assert');\nassert(true);\n复制代码\n```\n\n#### assert.ok(value[, message])\n\n使用方法同 `assert(value[, message])`。\n\n#### assert.equal(actual, expect[, message])\n\n预期 actual 与 expect值相等。equal用于比较的 actual 和 expect 是基础类型(string, number, boolearn, null, undefined)的数据。其中的比较使用的是 == 而不是 ===。\n\n```\nit('assert.equal', () =\u003e {\n  assert.equal(null, false, 'null compare with false');  // 报错\n  assert.equal(null, true, 'null compare with true');  // 报错\n  assert.equal(undefined, false, 'undefined compare with false'); // 报错\n  assert.equal(undefined, true, 'undefined compare with true'); // 报错\n  assert.equal('', false, '\"\" compare with false');  // 正常\n})\n复制代码\n```\n\n#### notEqual(actual, expected[, message])\n\n用法同 `assert.equal(actual, expect[, message])` 只是对预期结果取反（即不等于）。\n\n#### assert.strictEqual(actual, expect[, message])\n\n用法同 `assert.equal(actual, expect[, message])` 但是内部比较是使用的是 === 而不是 ==。\n\n#### assert.notStrictEqual(actial, expected[, message])\n\n用法同 `assert.strictEqual(actual, expect[, message])` 只是对预期结果取反（即不严格等于）。\n\n```\nit('assert.strictEqual', () =\u003e {\n  assert.strictEqual('', false); // 报错\n})\n复制代码\n```\n\n#### assert.deepEqual(actual, expect[, message])\n\ndeepEqual 方法用于比较两个对象。比较的过程是比较两个对象的 key 和 value 值是否相同, 比较时用的是 == 而不是 ===。\n\n```\nit('assert.deepEqual', () =\u003e {\n  const a = { v: 'value' };\n  const b = { v: 'value' };\n  assert.deepEqual(a, b);\n})\n复制代码\n```\n\n#### assert.notDeepEqual(actual, expected[, message])\n\n用法同 `assert.deepEqual(actual, expect[, message])` 只是对预期结果取反（即不严格深等于）。\n\n#### assert.deepStrictEqual(actual, expect[, message])\n\n用法同 `assert.deepEqual(actual, expect[, message])` 但是内部比较是使用的是 === 而不是 ==。\n\n#### assert.notDeepStrictEqual(actual, expected[, message])\n\n用法同 `assert.deepStrictEqual(actual, expect[, message])` 只是对结果取反（即不严格深等于）。\n\n#### assert.throws(block[, error][, message])\n\n错误断言与捕获, 断言指定代码块运行一定会报错或抛出错误。若代码运行未出现错误则会断言失败，断言异常。\n\n```\nit('throws', () =\u003e {\n  var fun = function() {\n    xxx\n  };\n  assert.throws(fun, 'fun error');\n})\n复制代码\n```\n\n#### assert.doesNotThrow(block[, error][, message])\n\n错误断言与捕获, 用法同 throws 类似，只是和 throws 预期结果相反。断言指定代码块运行一定不会报错或抛出错误。若代码运行出现错误则会断言失败，断言异常。\n\n```\nit('throws', () =\u003e {\n  var fun = function() {\n    xxx\n  };\n  assert.doesNotThrow(fun, 'fun error');\n})\n```\n\n### **Mocha**\n\n- 安装mocha\n\n```\nnpm install mocha -g\n复制代码\n```\n\n当然也可以在不在全局安装，只安局部安装在项目中\n\n```\nnpm install mocha --save\n```\n\n- 创建一个测试文件 `test.js`\n\n```javascript\nvar assert = require('assert')\n\ndescribe('Array', function() {\n  describe('#indexOf()', function() {\n    it('should return -1 when the value is not present', function() {\n      assert.equal(-1, [1, 2, 3].indexOf(-1))\n    })\n  })\n})\n```\n\n这段文件和简单就是测试 `Array` 的一个 `indexOf()` 方法。这里我是用的断言库是 Node 所提供的 [`Assert`](https://nodejs.org/api/assert.html) 模块里的API。这里断言 -1 等于 数组 `[1, 2, 3]` 执行 `indexOf(-1)`后返回的值，如果测试通过则不会报错，如果有误就会报出错误。\n\n下面我们使用全局安装的 `mocha` 来运行一下这个文件 `mocha test.js`。  \n下面是返回结果\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/webtest/20210410161204.png)\n\n基础测试用例实例\n\n```javascript\nconst assert = require('assert');\n\ndescribe('测试套件描述', function() {\n  it('测试用例描述: 1 + 2 = 3', function() {\n    // 测试代码\n    const result = 1 + 2;\n    // 测试断言\n    assert.equal(result, 3);\n  });\n});\n复制代码\n```\n\nMocha 测试用例主要包含下面几部分:\n\n1. describe 定义的测试套件(test suite)\n2. it 定义的测试用例(test case)\n3. 测试代码\n4. 断言部分\n\n说明：每个测试文件中可以有多个测试套件和测试用例。mocha不仅可以在node环境运行, 也可以在浏览器环境运行；在node中运行也可以通过`npm i mocha -g`全局安装mocha然后以命令行的方式运行测试用例也是可行的。\n\n**这里略微详细介绍下测试脚本写法**\n\nMocha 的作用是运行测试脚本，首先必须学会写测试脚本。所谓\"测试脚本\"，就是用来测试源码的脚本。下面是一个加法模块 add.js 的代码。\n\n```javascript\n// add.js\nfunction add(x, y) {\n  return x + y;\n}\n\nmodule.exports = add;\n```\n\n要测试这个加法模块是否正确，就要写测试脚本。通常，测试脚本与所要测试的源码脚本同名，但是后缀名为.test.js（表示测试）或者.spec.js（表示规格）。比如，add.js 的测试脚本名字就是 add.test.js。\n\n```javascript\n// add.test.js\nvar add = require('./add.js');\nvar expect = require('chai').expect;\n\ndescribe('加法函数的测试', function() {\n  it('1 加 1 应该等于 2', function() {\n    expect(add(1, 1)).to.be.equal(2);\n  });\n});\n```\n\n上面这段代码，就是测试脚本，它可以独立执行。测试脚本里面应该包括一个或多个 describe 块，每个 describe 块应该包括一个或多个 it 块。\n\ndescribe 块称为\"测试套件\"（test suite），表示一组相关的测试。它是一个函数，第一个参数是测试套件的名称（\"加法函数的测试\"），第二个参数是一个实际执行的函数。\n\nit 块称为\"测试用例\"（test case），表示一个单独的测试，是测试的最小单位。它也是一个函数，第一个参数是测试用例的名称（\"1 加 1 应该等于 2\"），第二个参数是一个实际执行的函数。\n\n**expect 断言的优点是很接近自然语言，下面是一些例子。**\n\n```javascript\n// 相等或不相等\nexpect(4 + 5).to.be.equal(9);\nexpect(4 + 5).to.be.not.equal(10);\nexpect(foo).to.be.deep.equal({ bar: 'baz' });\n\n// 布尔值为true\nexpect('everthing').to.be.ok;\nexpect(false).to.not.be.ok;\n\n// typeof\nexpect('test').to.be.a('string');\nexpect({ foo: 'bar' }).to.be.an('object');\nexpect(foo).to.be.an.instanceof(Foo);\n\n// include\nexpect([1, 2, 3]).to.include(2);\nexpect('foobar').to.contain('foo');\nexpect({ foo: 'bar', hello: 'universe' }).to.include.keys('foo');\n\n// empty\nexpect([]).to.be.empty;\nexpect('').to.be.empty;\nexpect({}).to.be.empty;\n\n// match\nexpect('foobar').to.match(/^foo/);\n```\n\n基本上，expect 断言的写法都是一样的。头部是 expect 方法，尾部是断言方法，比如 equal、a/an、ok、match 等。两者之间使用 to 或 to.be 连接。如果 expect 断言不成立，就会抛出一个错误。事实上，只要不抛出错误，测试用例就算通过。\n\n```\nit('1 加 1 应该等于 2', function() {});\n```\n\n上面的这个测试用例，内部没有任何代码，由于没有抛出了错误，所以还是会通过。\n\n### Karma\n\n基于 karma 测试常用的一些模块\n\n#### 模块安装\n\n```\n# 基础测试库\nnpm install karma-cli -g\nnpm install karma mocha karma-mocha --save-dev\n\n# 断言库\nnpm install should --save-dev\nnpm install karma-chai --save-dev\n\n# 浏览器相关\nnpm install karma-firefox-launcher --save-dev\nnpm install karma-chrome-launcher --save-dev\n```\n\n#### 配置\n\n这里的配置主要关注的是`karma.conf.js`的相关配置。如果要使用 karma 和 mocha 最好通过`npm install karma-cli -g`全局安装`karma-cli`。[**具体配置配置说明**](http://karma-runner.github.io/2.0/config/configuration-file.html)\n\n需要注意的两个字段:\n\n- singleRun: 如果值为 true, 则在浏览器运行完测试后会自动退出关闭浏览器窗口。singleRun的值我们可以更具运行环境来动态赋值, 可以启动命令中添加`NODE_ENV`变量。\n- browsers: 浏览器配置(可以配置多个浏览器); 如果浏览器无法启动需要进行相关浏览器的配置。设置自启动浏览器时候如果浏览器启动失败可能需要设置为`--no-sandbox`模式。\n\n```\n{\n  \"browsers\": [\"Chrome\", \"ChromeHeadless\", \"ChromeHeadlessNoSandbox\"],\n  \"customLaunchers\": {\n    \"ChromeHeadlessNoSandbox\": {\n      \"base\": \"ChromeHeadless\",\n      \"flags\": [\"--no-sandbox\"]\n    }\n  }\n}\n复制代码\n```\n\n或者\n\n```\n{\n  \"browsers\": [\"Chrome_travis_ci\"],\n  \"customLaunchers\": {\n    \"Chrome_travis_ci\": {\n      \"base\": \"Chrome\",\n      \"flags\": [\"--no-sandbox\"]\n    }\n  }\n}复制代码\n```\n\n### Travis.CI集成自动化测试\n\n[阮一峰](http://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html)\n\n在github创建并完成一个可以待测试的项目。这里的完成是指需要完成基本的项目功能，和测试用例代码。\n\n配置travis-ci能识别读取的配置文件，这样travis-ci接入的时候才能够知道测试时的一些配置。\n\ngithub 和 travis-ci 是个站点，换句话说就是两个东西如果能打通呢。需要用户登录 travis-ci 并授权访问到你的 github 项目并进行相关的项目设置。\n\n接入完成后就可以根据自己的需要来运行写好的测试代码，也可以设置定期任务去跑测试。\n\n#### 项目创建、完善项目功能和测试代码。\n\n- 项目需求： 实现一个求和方法\n- 测试: 通过 `mocha` 来测试完成的求和方法。\n\n下面是项目结构，项目创建完成后通过 `npm i mocha -D` 安装 `mocha` 模块。然后在本地运行 `npm test` 看是否能够测试通过。如果能够测试通过则说明我们的可以继续下一步了。\n\n![img](Web%E6%B5%8B%E8%AF%95.assets/1-20200706204633514.png)\n\n#### 创建 travis-ci 测试配置文件\n\n创建 [travis-ci](https://www.travis-ci.org/)配置文件 `.travis.yml`, 文件内容。更多关于配置文件的说明在travis官网可查询\n\n```\nlanguage: node_js\nnode_js:\n  - \"node\"\n  - \"8.9.4\"\n```\n\n至此基本完成了项目开发和测试代码编写的过程，下一步就可以接入 [travis-ci](https://www.travis-ci.org/)测试了。\n\n#### 接入 travis-ci\n\n通过GitHub登录 travis-ci 的官网 [www.travis-ci.org/](https://www.travis-ci.org/)\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/webtest20210410161117.png)\n\n找到GitHub上刚才创建的需要测试的项目，并开启测试\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/webtest20210410161111.png)\n\n查看测试过程，及时发现问题。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/webtest20210410161127.png)\n\n查看测试状态是否通过测试，如果未通过及时排查问题反复修改；如果通过可以在项目文档中添加一个测试通过的标识。\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/webtest/20210410161142.png)\n\n### 总结\n\n但是同时，在我个人观点范畴内，至少目前我还是坚持开发为主测试为辅的流程，对于像TDD这种单元测试指导开发流程，目前并不推崇。个人认为，这是一个很有创新性的方法论，也并不是现在完全不可行，个人认为只是可行的范畴还不够宽，可行的条件要求还很严苛。所以相对于TDD，测试主导开发，对于目前准备进阶的前端开发者，个人更建议，了解某种以后会使用的新趋势和技术是有必要的，但作为技术人应该在学习新的、前卫的技术的同时不可迷失自我一味追求新技术，更重要的是要磨练当下的主流技能。相比于未来的单元测试主导开发流程，倒不如在目前这个时间节点精进基础开发流程，比如让自己的JS代码更专注于模块化和功能化的实现，这样的同时也会让单元测试更有效率，真正发挥目前单元测试对前端工程化的作用。\n\n## A/B 测试\n\n所谓 A/B 测试，简单来说，就是为同一个目标制定两个方案（比如两个页面），让一部分用户使用 A 方案，另一部分用户使用 B 方案，记录下用户的使用情况，看哪个方案更符合设计目标。\n\n### 基本概念\n\n网站设计中，我们经常会面临多个设计方案的选择，比如某个按钮是用红色还是用蓝色，是放左边还是放右边。传统的解决方法通常是集体讨论表决，或者由某位专家或领导来拍板。虽然传统解决办法多数情况下也是有效的，但A/B 测试（A/B Testing）可能是解决这类问题的一个更好的方法。\n\n所谓 A/B 测试，简单来说，就是为同一个目标制定两个方案（比如两个页面），让一部分用户使用 A 方案，另一部分用户使用 B 方案，记录下用户的使用情况，看哪个方案更符合设计目标。当然，在实际操作过程之中还有许多需要注意的细节。\n\nA/B 测试最核心的思想，即：\n\n1. **多个方案并行测试；**\n2. **每个方案只有一个变量不同；**\n3. **以某种规则优胜劣汰。**\n\n需要特别留意的是第 2 点，它暗示了 A/B 测试的应用范围，——必须是单变量。\n\n另外，虽然 A/B 测试名字中只包含 A、B ，但并不是说它只能用于比较两个方案的好坏，**事实上，你完全可以设计多个方案进行测试，比如ＡＢＣ测试，“A/B 测试”这个名字只是一个习惯的叫法。**\n\n回到网站设计，**一般来说，每个设计方案应该大体上是相同的，只是某一个地方有所不同，比如某处排版、文案、图片、颜色等。然后对不同的用户展示不同的方案。**\n\n要注意，不同的用户在他的一次浏览过程中，看到的应该一直是同一个方案。比如他一开始看到的是 A 方案，则在此次会话中应该一直向他展示 A 方案，而不能一会儿让他看 A 方案，一会儿让他看 B 方案。\n\n同时，还需要注意控制访问各个版本的人数，大多数情况下我们会希望将访问者平均分配到各个不同的版本上。要做到这些很简单，根据 cookie （比如 cookie 会话ID的最后一位数字）决定展示哪个版本就是一个不错的方法。\n\n**要实现 A/B 测试，我们需要做以下几个工作：**\n\n**1、开发两个（或多个）不同的版本并部署；**\n\n**2、收集数据；**\n\n**3、分析数据，得出结果。**\n\n### 实践方法\n\n![img](https://cdn.jsdelivr.net/gh/radoapx/rax-picbed/PicGo/blogs/bigfront/webtest/20210410161149.jpg)\n\n从左到右，３条较粗的竖线代表了 A/B 测试中的３个关键角色：客户端（Client）、服务器（Server）、数据层（Data）。从上到下代表了３种访问形式：\n\n1. **无 A/B 测试的普通访问流程**（Non AB test）\n2. **基于后端分流的 A/B 测试访问流程**（Back-end AB test）\n3. **基于前端分流的 A/B 测试访问流程**（Front-end AB test）。\n\nA/B 测试需要将多个不同的版本展现给不同的用户，即需要一个“分流”的环节。从上图中我们可以看到，分流可以在客户端做，也可以在服务器端做。\n\n**传统的 A/B 测试一般是在服务端分流的**，即基于后端的 A/B 测试（Back-end AB test），当用户的请求到达服务器时，服务器根据一定的规则，给不同的用户返回不同的版本，同时记录数据的工作也在服务端完成。\n\n**基于后端的 A/B 测试技术实现上稍微简单一些**，**不过缺点是收集到的数据通常是比较宏观的PV（Page View）信息。**虽然可以进行比较复杂的宏观行为分析，但要想知道用户在某个版本的页面上的具体行为往往就无能为力了。\n\n**基于前端的 A/B 测试则可以比较精确地记录下用户在页面上的每一个行为。**它的特点是，利用前端 Java 方法，在客户端进行分流，同时，可以用 Java 记录下用户的鼠标行为（甚至键盘行为，如果需要的话），直接发送到服务器记录。\n\n下面，我将重点介绍一下我们在基于前端的 A/B 测试上的一些实践。\n\n首先遇到的问题是如何分流的问题。对于大部分需求来说，我们希望各个版本的访问人数平均分配。可以根据某一个 Cookie ID 来划分用户，**比如“123.180.140.\\*.1267882109577.3”，可以根据这个 Cookie ID 的最后一位（在本例中是“3”）来划分人群，比如单数的显示 A 版本，偶数的显示 B 版本。**\n\n正确展示对应的版本后，就要开始采集需要的数据了。当前版本有多少 PV （Page Views，访问量），如果需要记录这个数据的话，在正确版本加载完成之时就要发送一个打点信息。不过很多需求中，具体版本的 PV 的精确数值可能不是很重要，而且要收集这个信息需要多一次打点操作，所以一般情况下这个数据是可选的。\n\n必须的数据是测试区域内用户的点击信息。当用户在测试区域点击了鼠标左键（无论这个点击是点击在链接、文字、图片还是空白处），我们就需要发送一条对应的打点信息到打点服务器。一般来说，这个打点信息至少需要包含以下数据：\n\n- 当前 A/B 测试以及版本标识\n- 点击事件的位置\n- 点击时间戳（客户端时间）\n- 当前点中的URL（如果点在非超链接区域，此项为空）\n- 用户标识（比如 Cookie ID）\n- 用户浏览器信息\n\n### 应用例子\n\n用ＡＢ测试的核心思想分析下：\n\n- **两个方案并行测试（符合）；**\n- **每个方案只有一个变量不同（按钮的文案不同，符合）；**\n- **以某种规则优胜劣汰（最终转化率的高低，符合）**\n\n**注意：有人认为按钮的点击率是最终的衡量指标，其实不是，有时候点击率提高了，但转化率反而会降低，所以这里最终转化率是衡量指标。**\n\n举个例子：你的按钮文案是“点击领一百元红包”，点击率自然会提高，但用户点击后发现并没有一百元，就走了，导致转化率下降。\n\n言归正传，结果数据显示，A版本的转化率为5.8%，B版本的转化率为８.2%。在没做这个测试之前，你知道“加入学习”好还是“立即参加”好吗？\n\n这几乎是一个完美的 A/B 测试案例：有明确的测试目标，清晰的衡量标准（订单转化率），以及完美的结果数字。\n","lastmodified":"2023-05-09T16:33:58.295366497Z","tags":[]},"/webpack4":{"title":"webpack4","content":"\n### Webpack是什么\n\nwebpack 是一种前端资源构建工具，一个静态模块打包器(module bundler)。  \n在 webpack 看来, 前端的所有资源文件(js/json/css/img/less/…)都会作为模块处理。  \n它将根据模块的依赖关系进行静态分析，打包生成对应的静态资源(bundle)。\n\n![image-20200331004145708](image-20200331004145708.png)\n\n### webpack 五个核心概念\n\n#### Entry\n\n入口(Entry)指示 webpack 以哪个文件为入口起点开始打包，分析构建内部依赖图。\n\n#### Output\n\n输出(Output)指示 webpack 打包后的资源 bundles 输出到哪里去，以及如何命名。\n\n#### Loader\n\nLoader 让 webpack 能 够 去 处 理 那 些 非 JavaScript 文 件 (webpack 自 身 只 理 解 JavaScript)\n\n#### Plugins\n\n插件(Plugins)可以用于执行范围更广的任务。插件的范围包括，从打包优化和压缩， 一直到重新定义环境中的变量等。\n\n#### Mode\n\n模式(Mode)指示 webpack 使用相应模式的配置。\n\n| 选项        | 描述                                                         | 特点                        |\n| ----------- | ------------------------------------------------------------ | --------------------------- |\n| development | 会将 DefinePlugin 中 process.env.NODE_ENV 的值设置 为 development。启用 NamedChunksPlugin 和 NamedModulesPlugin。 | 能让代码本地调试 运行的环境 |\n| production  | 会将 DefinePlugin 中 process.env.NODE_ENV 的值设置 为 production。启用 FlagDependencyUsagePlugin, FlagIncludedChunksPlugin, ModuleConcatenationPlugin, NoEmitOnErrorsPlugin, OccurrenceOrderPlugin, SideEffectsFlagPlugin 和 TerserPlugin。 | 能让代码优化上线 运行的环境 |\n\n### 基本编译打包命令\n\n1. 创建文件\n2. 运行指令\n\n   - 开发环境指令：\n\n     `webpack src/js/index.js -o build/js/built.js --mode=development `\n\n     功能：webpack 能够编译打包 js 和 json 文件，并且能将 es6 的模块化语法转换成 浏览器能识别的语法。\n\n   - 生产环境指令：`webpack src/js/index.js -o build/js/built.js --mode=production`\n\n     功能：在开发配置功能上多一个功能，压缩代码。\n\n3. 结论:webpack 能够编译打包 js 和 json 文件。 能将 es6 的模块化语法转换成浏览器能识别的语法。能压缩代码。\n4. 问题 不能编译打包 css、img 等文件。 不能将 js 的 es6 基本语法转化为 es5 以下语法。\n\n这就引出了webpack的自定义配置。\n\n### webpack 开发环境的基本配置\n\n#### 创建配置文件\n\n1. 创建文件 webpack.config.js\n2. 配置内容如下\n\n   ```javascript\n   const { resolve } = require('path'); // node 内置核心模块，用来处理路径问题。\n   module.exports = {\n   entry: './src/js/index.js', // 入口文件\n   output: { // 输出配置\n   filename: './built.js', // 输出文件名\n   path: resolve(__dirname, 'build/js') // 输出文件路径配置\n   },\n   mode: 'development' //开发环境\n   };\n   ```\n\n3. 运行指令: `webpack`(与上节相比，少了入口出口文件以及参数)\n4. 结论: 此时功能与上节一致。\n\n#### 打包样式资源\n\n1. 下载安装 相关样式的loader 包\n\n   `npm i css-loader style-loader less-loader less -D`\n\n2. 修改配置文件（主要新增module对象，制定rules，每一个rule会用正则匹配到对应的一类文件，然后用指定的loader加载）\n\n   ```javascript\n   // resolve 用来拼接绝对路径的方法\n   const {resolve} = require('path');\n   module.exports = {\n   // webpack 配置\n   // 入口起点\n   \tentry: './src/index.js',\n   // 输出\n   \toutput: {\n   // 输出文件名\n   \t\tfilename: 'built.js',\n   // 输出路径\n   // __dirname nodejs 的变量，代表当前文件的目录绝对路径\n   \t\tpath: resolve(__dirname, 'build')\n   \t},\n   // loader 的配置\n   \tmodule: {\n   \t\trules: [\n   // 详细 loader 配置\n   // 不同文件必须配置不同 loader 处理\n   \t\t\t{\n   // 匹配哪些文件\n   \t\t\t\ttest: /\\.css$/,\n   // 使用哪些 loader 进行处理\n   \t\t\t\tuse: [\n   // use 数组中 loader 执行顺序：从右到左，从下到上 依次执行\n   // 创建 style 标签，将 js 中的样式资源插入进行，添加到 head 中生效\n   \t\t\t\t\t'style-loader',\n   // 将 css 文件变成 commonjs 模块加载 js 中，里面内容是样式字符串\n   \t\t\t\t\t'css-loader'\n   \t\t\t\t]\n   \t\t\t},\n   \t\t\t{\n   \t\t\t\ttest: /\\.less$/,\n   \t\t\t\tuse: [\n   \t\t\t\t\t'style-loader',\n   \t\t\t\t\t'css-loader',\n   // 将 less 文件编译成 css 文件\n   // 需要下载 less-loader 和 less\n   \t\t\t\t\t'less-loader'\n   \t\t\t\t]\n   \t\t\t}\n   \t\t]\n   \t},\n   // plugins 的配置\n   \tplugins: [\n   // 详细 plugins 的配置\n   \t],\n   // 模式\n   \tmode: 'development', // 开发模式\n   // mode: 'production'\n   }\n   ```\n\n3. 运行指令: webpack\n\n如果遇到其他样式文件如sass，可以参考自行配置。\n\n#### 打包 HTML 资源\n\n1. 创建相关html文件\n2. 下载安装 plugin 包\n\n   `npm install --save-dev html-webpack-plugin`\n\n3. 修改配置文件（html比较特殊，使用plugins 配置）\n\n   ```javascript\n   const { resolve } = require('path');\n   const HtmlWebpackPlugin = require('html-webpack-plugin');\n   module.exports = {\n   entry: './src/index.js',\n   output: {\n   filename: 'built.js',\n   path: resolve(__dirname, 'build')\n   },\n   module: {\n   rules: [\n   // loader 的配置\n   ]\n   },\n   plugins: [\n   // plugins 的配置\n   // html-webpack-plugin\n   // 功能：默认会创建一个空的 HTML，自动引入打包输出的所有资源（JS/CSS）\n   // 需求：需要有结构的 HTML 文件\n   new HtmlWebpackPlugin({\n   // 复制 './src/index.html' 文件，并自动引入打包输出的所有资源（JS/CSS）\n   template: './src/index.html'\n   })\n   ],\n   mode: 'development'\n   };\n   ```\n\n4. 运行指令: webpack\n\n#### 打包图片资源\n\n1. 下载安装 loader 包 `npm install --save-dev html-loader url-loader file-loader`\n2. 配置对应文件（同上增加rule）\n\n   ```javascript\n   const {resolve} = require('path');\n   const HtmlWebpackPlugin = require('html-webpack-plugin');\n   module.exports = {\n   \tentry: './src/index.js',\n   \toutput: {\n   \t\tfilename: 'built.js',\n   \t\tpath: resolve(__dirname, 'build')\n   \t},\n   \tmodule: {\n   \t\trules: [\n   \t\t\t{\n   // 问题：默认处理不了 html 中 img 图片\n   // 处理图片资源\n   \t\t\t\ttest: /\\.(jpg|png|gif)$/,\n   // 使用一个 loader\n   // 下载 url-loader file-loader\n   \t\t\t\tloader: 'url-loader',\n   \t\t\t\toptions: {\n   // 图片大小小于 8kb，就会被 base64 处理\n   // 优点: 减少请求数量（减轻服务器压力）\n   // 缺点：图片体积会更大（文件请求速度更慢）\n   \t\t\t\t\tlimit: 8 * 1024,\n   // 问题：因为 url-loader 默认使用 es6 模块化解析，而 html-loader 引入图片是 commonjs\n   // 解析时会出问题：[object Module]\n   // 解决：关闭 url-loader 的 es6 模块化，使用 commonjs 解析\n   \t\t\t\t\tesModule: false,\n   // 给图片进行重命名\n   // [hash:10]取图片的 hash 的前 10 位\n   // [ext]取文件原来扩展名\n   \t\t\t\t\tname: '[hash:10].[ext]'\n   \t\t\t\t}\n   \t\t\t},\n   \t\t\t{\n   \t\t\t\ttest: /\\.html$/,\n   // 处理 html 文件的 img 图片（负责引入 img，从而能被 url-loader 进行处理）\n   \t\t\t\tloader: 'html-loader'\n   \t\t\t}\n   \t\t]\n   \t},\n   \tplugins: [\n   \t\tnew HtmlWebpackPlugin({\n   \t\t\ttemplate: './src/index.html'\n   \t\t})\n   \t],\n   \tmode: 'development'\n   };\n   ```\n\n3. 运行指令: webpack\n\n#### 打包其他资源\n\n对于除了以上的文件外，还有其他各种资源文件需要处理，如.ttf .svg .eot等，这里直接使用`file-loader`处理。\n\n增加对应rule即可:\n\n```javascript\n// 打包其他资源(除了 html/js/css 资源以外的资源)\n{\n// 排除 css/js/html 资源\n\texclude: /\\.(css|js|html|less)$/,\n\tloader: 'file-loader',\n\toptions: {\n\t\tname: '[hash:10].[ext]'\n\t}\n}\n```\n\n#### devserver 搭建\n\n对于前端工程化开发，一定离不开一个能够热更新的开发服务器。\n\n可以在webpack.config.js中如下配置：\n\n```javascript\nmodule.exports = {\n\t...\n    mode: 'development',\n\tdevServer: {\n\t\t// 项目构建后路径\n\t\tcontentBase: resolve(__dirname, 'build'),\n\t\t// 启动 gzip 压缩\n\t\tcompress: true,\n\t\t// 端口号\n\t\tport: 3000,\n\t\t// 自动打开浏览器\n\t\topen: true\n\t}\n    ...\n};\n```\n\n下载`webpack-dev-server`后直接运行即可。\n\n#### 打包后对应文件放在对应文件夹\n\n以img为例，在对应的rule下设置`outputPath`\n\n```javascript\n{\n\t// 处理图片资源\n\ttest: /\\.(jpg|png|gif)$/,\n\tloader: 'url-loader',\n\toptions: {\n\t\tlimit: 8 * 1024,\n\t\tname: '[hash:10].[ext]',\n\t\t// 关闭 es6 模块化\n\t\tesModule: false,\n\t\toutputPath: 'imgs'\n\t}\n}\n```\n\n#### 开发config模板\n\n```javascript\n\nmodule.exports = {\n  entry: './src/js/index.js',\n  output: {\n    filename: 'js/built.js',\n    path: resolve(__dirname, 'build'),\n  },\n  module: {\n    rules: [\n      // loader 的配置\n      {\n        // 处理 less 资源\n        test: /\\.less$/,\n        use: ['style-loader', 'css-loader', 'less-loader'],\n      },\n      {\n        // 处理 css 资源\n        test: /\\.css$/,\n        use: ['style-loader', 'css-loader'],\n      },\n      {\n        // 处理图片资源\n        test: /\\.(jpg|png|gif)$/,\n        loader: 'url-loader',\n        options: {\n          limit: 8 * 1024,\n          name: '[hash:10].[ext]',\n          // 关闭 es6 模块化\n          esModule: false,\n          outputPath: 'imgs',\n        },\n      },\n      {\n        // 处理 html 中 img 资源\n        test: /\\.html$/,\n        loader: 'html-loader',\n      },\n      {\n        // 处理其他资源\n        exclude: /\\.(html|js|css|less|jpg|png|gif)/,\n        loader: 'file-loader',\n        options: {\n          name: '[hash:10].[ext]',\n          outputPath: 'media',\n        },\n      },\n    ],\n  },\n  plugins: [\n    // plugins 的配置\n    new HtmlWebpackPlugin({\n      template: './src/index.html',\n    }),\n  ],\n  mode: 'development',\n  \n}\n```\n\n### webpack 生产环境的基本配置\n\n#### 提取 css 成单独文件\n\n假设如下文件目录\n\n![image-20200408154347927](image-20200408154347927.png)\n\n下载插件 `npm install --save-dev mini-css-extract-plugin`\n\n处理css时:\n\n```javascript\nconst MiniCssExtractPlugin = require('mini-css-extract-plugin');\n....\nrules: [\n  {\n    test: /\\.css$/,\n    use: [\n      // 创建 style 标签，将样式放入\n      // 'style-loader',\n      // 这个 loader 取代 style-loader。作用：提取 js 中的 css 成单独文件\n      MiniCssExtractPlugin.loader,\n      // 将 css 文件整合到 js 文件中\n      'css-loader',\n    ],\n  },\n],\nplugins: [\n\tnew MiniCssExtractPlugin({\n\t\t// 对输出的 css 文件进行重命名\n\t\tfilename: 'css/built.css'\n\t})\n]\n```\n\n#### css 兼容性处理\n\n![image-20200408154656853](image-20200408154656853.png)\n\n`npm install --save-dev postcss-loader postcss-preset-env`\n\n如下，配置`postcss-loader`后，再在`package.json`中配置`browserslist`(详细配置可以谷歌)，再配置`process.env.NODE_ENV`后即可，执行`webpack`后，可以发现CSS自动添加了不同浏览器的适配代码。\n\n```javascript\nconst { resolve } = require('path')\nconst HtmlWebpackPlugin = require('html-webpack-plugin')\nconst MiniCssExtractPlugin = require('mini-css-extract-plugin')\n\n// 设置nodejs环境变量\n// process.env.NODE_ENV = 'development';\n\nmodule.exports = {\n  entry: './src/js/index.js',\n  output: {\n    filename: 'js/built.js',\n    path: resolve(__dirname, 'build'),\n  },\n  module: {\n    rules: [\n      {\n        test: /\\.css$/,\n        use: [\n          MiniCssExtractPlugin.loader,\n          'css-loader',\n          /*\n            css兼容性处理：postcss --\u003e postcss-loader postcss-preset-env\n\n            帮postcss找到package.json中browserslist里面的配置，通过配置加载指定的css兼容性样式\n\n            \"browserslist\": {\n              // 开发环境 --\u003e 设置node环境变量：process.env.NODE_ENV = development\n              \"development\": [\n                \"last 1 chrome version\",\n                \"last 1 firefox version\",\n                \"last 1 safari version\"\n              ],\n              // 生产环境：默认是看生产环境\n              \"production\": [\n                \"\u003e0.2%\",\n                \"not dead\",\n                \"not op_mini all\"\n              ]\n            }\n          */\n          // 使用loader的默认配置\n          // 'postcss-loader',\n          // 修改loader的配置\n          {\n            loader: 'postcss-loader',\n            options: {\n              ident: 'postcss',\n              plugins: () =\u003e [\n                // postcss的插件\n                require('postcss-preset-env')(),\n              ],\n            },\n          },\n        ],\n      },\n    ],\n  },\n  plugins: [\n    new HtmlWebpackPlugin({\n      template: './src/index.html',\n    }),\n    new MiniCssExtractPlugin({\n      filename: 'css/built.css',\n    }),\n  ],\n  mode: 'development',\n}\n```\n\n#### 压缩 css\n\n使用`npm install --save-dev optimize-css-assets-webpack-plugin`\n\n```javascript\nconst OptimizeCssAssetsWebpackPlugin = require('optimize-css-assets-webpack-plugin')\nplugins: [\n\t// 压缩 css\n\tnew OptimizeCssAssetsWebpackPlugin()\n]\n```\n\n只需加入以上配置即可！\n\n#### js 语法检查(eslint)\n\n使用`npm install --save-dev eslint-loader eslint eslint-config-airbnb-base eslint-plugin-import`\n\n语法检查： eslint-loader eslint  \n注意：只检查自己写的源代码，第三方的库是不用检查的  \n设置检查规则：  \n\tpackage.json中eslintConfig中设置：\n\n```json\n\"eslintConfig\": {\n\t\"extends\": \"airbnb-base\"\n}\n```\n\n依赖airbnb --\u003e eslint-config-airbnb-base eslint-plugin-import eslint\n\n配置rule:\n\n```javascript\n{\n  test: /\\.js$/,\n  exclude: /node_modules/,//记得排除\n  loader: 'eslint-loader',\n  options: {\n    // 自动修复eslint的错误\n    fix: true\n  }\n}\n```\n\n如果不想检查某一句：\n\n```javascript\n// 下一行eslint所有规则都失效（下一行不进行eslint检查）\n// eslint-disable-next-line\nconsole.log(add(2, 5));\n```\n\n#### js 兼容性处理(babel)\n\n安装`npm install --save-dev babel-loader @babel/core @babel/preset-env @babel/polyfill core-js`\n\njs 兼容性处理：babel-loader @babel/core\n\n  1. 基本js兼容性处理 --\u003e @babel/preset-env\n\n    问题：只能转换基本语法，如promise高级语法不能转换\n  2. 全部js兼容性处理 --\u003e @babel/polyfill  \n\n    问题：我只要解决部分兼容性问题，但是将所有兼容性代码全部引入，体积太大了~\n\n  3. 需要做兼容性处理的就做：按需加载 --\u003e core-js\n\n配置\n\n```javascript\n{\n  test: /\\.js$/,\n  exclude: /node_modules/,\n  loader: 'babel-loader',\n  options: {\n    // 预设：指示babel做怎么样的兼容性处理\n    presets: [\n      [\n        '@babel/preset-env',\n        {\n          // 按需加载\n          useBuiltIns: 'usage',\n          // 指定core-js版本\n          corejs: {\n            version: 3\n          },\n          // 指定兼容性做到哪个版本浏览器\n          targets: {\n            chrome: '60',\n            firefox: '60',\n            ie: '9',\n            safari: '10',\n            edge: '17'\n          }\n        }\n      ]\n    ]\n  }\n}\n```\n\n#### js 压缩\n\n生产环境下会自动压缩js代码\n\n```javascript\nconst { resolve } = require('path');\nconst HtmlWebpackPlugin = require('html-webpack-plugin');\n\nmodule.exports = {\n  entry: './src/js/index.js',\n  output: {\n    filename: 'js/built.js',\n    path: resolve(__dirname, 'build')\n  },\n  plugins: [\n    new HtmlWebpackPlugin({\n      template: './src/index.html'\n    })\n  ],\n  // 生产环境下会自动压缩js代码\n  mode: 'production'\n};\n```\n\n#### HTML 压缩\n\n直接使用之前安装的`HtmlWebpackPlugin`\n\n```javascript\nplugins: [\n  new HtmlWebpackPlugin({\n    template: './src/index.html',\n    // 压缩html代码\n    minify: {\n      // 移除空格\n      collapseWhitespace: true,\n      // 移除注释\n      removeComments: true\n    }\n  })\n]\n```\n\n### webpack 优化配置\n\n1. webpack性能优化\n\n   - 开发环境性能优化\n   - 生产环境性能优化\n2. 开发环境性能优化\n   - 优化打包构建速度\n   - HMR\n   - 优化代码调试\n   - source-map\n3. 生产环境性能优化\n  - 优化打包构建速度\n    - oneOf\n    - babel缓存\n    - 多进程打包\n    - externals\n    - dll\n  - 优化代码运行的性能\n    - 缓存(hash-chunkhash-contenthash)\n    - tree shaking\n    - code split\n    - 懒加载/预加载\n    - pwa\n\n#### HMR\n\n![image-20200408201733744](image-20200408201733744.png)\n\nHMR: hot module replacement 热模块替换 / 模块热替换  \n作用：一个模块发生变化，只会重新打包这一个模块（而不是打包所有模块） ，极大提升构建速度。\n\n样式文件：可以使用HMR功能：因为style-loader内部实现了~\n\n js文件：默认不能使用HMR功能 --\u003e 需要修改js代码，添加支持HMR功能的代码。\n\n\u003e 注意：HMR功能对js的处理，只能处理非入口js文件的其他文件。\n\u003e\n\u003e ```javascript\n\u003e if (module.hot) {\n\u003e   // 一旦 module.hot 为true，说明开启了HMR功能。 --\u003e 让HMR功能代码生效\n\u003e   module.hot.accept('./print.js', function() {\n\u003e     // 方法会监听 print.js 文件的变化，一旦发生变化，其他模块不会重新打包构建。\n\u003e     // 会执行后面的回调函数\n\u003e     print();\n\u003e   });\n\u003e }\n\u003e ```\n\nhtml文件: 默认不能使用HMR功能.同时会导致问题：html文件不能热更新了~ （不用做HMR功能）\n\n\u003e 解决：修改entry入口，将html文件引入-\u003eentry: ['./src/js/index.js', './src/index.html'],\n\ndevserver中加入hot即可：\n\n```javascript\ndevServer: {\n  contentBase: resolve(__dirname, 'build'),\n  compress: true,\n  port: 3000,\n  open: true,\n  // 开启HMR功能\n  // 当修改了webpack配置，新配置要想生效，必须重新webpack服务\n  hot: true\n}\n```\n\n#### source-map\n\nsource-map: 一种 提供源代码到构建后代码映射 技术 （如果构建后代码出错了，通过映射可以追踪源代码错误）\n\n在module.exports中加入`devtool: source-map`\n\n上为最基本写法，可以增加功能`[inline-|hidden-|eval-][nosources-][cheap-[module-]]source-map`\n\n！ 内联 和 外部的区别：1. 外部生成了文件，内联没有 2. 内联构建速度更快\n\n- source-map：外部  \n    错误代码准确信息 和 源代码的错误位置\n- inline-source-map：内联  \n    只生成一个内联source-map  \n    错误代码准确信息 和 源代码的错误位置\n- hidden-source-map：外部  \n    错误代码错误原因，但是没有错误位置  \n    不能追踪源代码错误，只能提示到构建后代码的错误位置\n- eval-source-map：内联  \n    每一个文件都生成对应的source-map，都在eval  \n    错误代码准确信息 和 源代码的错误位置\n- nosources-source-map：外部  \n    错误代码准确信息, 但是没有任何源代码信息\n- cheap-source-map：外部  \n    错误代码准确信息 和 源代码的错误位置  \n    只能精确的行\n- cheap-module-source-map：外部  \n    错误代码准确信息 和 源代码的错误位置  \n    module会将loader的source map加入\n\n种类这么多，至于如何使用：\n\n- 开发环境：速度快，调试更友好  \n    速度快(eval\u003einline\u003echeap\u003e…)  \n      eval-cheap-souce-map  \n      eval-source-map  \n    调试更友好  \n      souce-map  \n      cheap-module-souce-map  \n      cheap-souce-map  \n    --\u003e eval-source-map / eval-cheap-module-souce-map\n- 生产环境：源代码要不要隐藏? 调试要不要更友好  \n    内联会让代码体积变大，所以在生产环境不用内联  \n    nosources-source-map 全部隐藏  \n    hidden-source-map 只隐藏源代码，会提示构建后代码错误信息  \n    --\u003e source-map / cheap-module-souce-map\n\n#### oneOf\n\nrules中的loader在执行时所有文件都会被匹配一次，使用`oneof`来使一个文件只被匹配一次：\n\n注意：不能 两个配置处理同一种类型文件，所以可以把一些一定执行的放在oneof外部。\n\n```javascript\nrules: [\n  {\n    // 在package.json中eslintConfig --\u003e airbnb\n    test: /\\.js$/,\n    exclude: /node_modules/,\n    // 优先执行\n    enforce: 'pre',\n    loader: 'eslint-loader',\n    options: {\n      fix: true\n    }\n  },\n  {\n    // 以下loader只会匹配一个\n    // 注意：不能 两个配置处理同一种类型文件\n    oneOf: [\n      {\n        test: /\\.css$/,\n        use: [...commonCssLoader]\n      },\n      {\n        test: /\\.less$/,\n        use: [...commonCssLoader, 'less-loader']\n      },\n      /*\n        正常来讲，一个文件只能被一个loader处理。\n        当一个文件要被多个loader处理，那么一定要指定loader执行的先后顺序：\n          先执行eslint 在执行babel\n      */\n      {\n        test: /\\.js$/,\n        exclude: /node_modules/,\n        loader: 'babel-loader',\n        options: {\n          presets: [\n            [\n              '@babel/preset-env',\n              {\n                useBuiltIns: 'usage',\n                corejs: {version: 3},\n                targets: {\n                  chrome: '60',\n                  firefox: '50'\n                }\n              }\n            ]\n          ]\n        }\n      },\n      {\n        test: /\\.(jpg|png|gif)/,\n        loader: 'url-loader',\n        options: {\n          limit: 8 * 1024,\n          name: '[hash:10].[ext]',\n          outputPath: 'imgs',\n          esModule: false\n        }\n      },\n      {\n        test: /\\.html$/,\n        loader: 'html-loader'\n      },\n      {\n        exclude: /\\.(js|css|less|html|jpg|png|gif)/,\n        loader: 'file-loader',\n        options: {\n          outputPath: 'media'\n        }\n      }\n    ]\n  }\n]\n```\n\n#### 缓存\n\n1. ##### babel缓存\n  \n\n     cacheDirectory: true  \n     --\u003e 让第二次打包构建速度更快\n\n```javascript\n{\n  test: /\\.js$/,\n  exclude: /node_modules/,\n  loader: 'babel-loader',\n  options: {\n    presets: [\n      [\n        '@babel/preset-env',\n        {\n          useBuiltIns: 'usage',\n          corejs: { version: 3 },\n          targets: {\n            chrome: '60',\n            firefox: '50'\n          }\n        }\n      ]\n    ],\n    // 开启babel缓存\n    // 第二次构建时，会读取之前的缓存\n    cacheDirectory: true\n  }\n}\n```\n\n2. ##### 文件资源缓存\n\n- hash: 每次wepack构建时会生成一个唯一的hash值。  \n  问题: 因为js和css同时使用一个hash值。  \n  如果重新打包，会导致所有缓存失效。（可能我却只改动一个文件）\n\n  ```javascript\n  {\n    test: /\\.(jpg|png|gif)/,\n    loader: 'url-loader',\n    options: {\n      limit: 8 * 1024,\n      name: '[hash:10].[ext]',\n      outputPath: 'imgs',\n      esModule: false\n    }\n  }\n  new MiniCssExtractPlugin({\n    filename: 'css/built.[hash:10].css'\n  })\n  ```\n\n- chunkhash：根据chunk生成的hash值。如果打包来源于同一个chunk，那么hash值就一样。  \n  问题: js和css的hash值还是一样的  \n  因为css是在js中被引入的，所以同属于一个chunk。\n\n  chunk是指所有跟着index引入的文件最后会变成一个文件，一个chunk。\n\n- contenthash: 根据文件的内容生成hash值。不同文件hash值一定不一样--\u003e 让代码上线运行缓存更好使用\n\n  ```javascript\n  new MiniCssExtractPlugin({\n    filename: 'css/built.[contenthash:10].css'\n  })\n  ```\n\n![image-20200408205255192](image-20200408205255192.png)\n\n可以发现，如果修改css不修改js，则css缓存失效，js不失效。\n\n#### tree shaking\n\n**根本目的：去除无用代码**\n\n**基础：ES6模块的出现，ES6模块依赖关系是确定的，`和运行时的状态无关`，可以进行可靠的静态分析。**\n\n**前提**：\n\n1. 必须使用ES6模块化  \n2. 开启production环境\n\n**步骤**\n\n- 从ES6顶层模块开始分析，可以清除未使用的模块\n- 对多层调用的模块进行重构，提取其中的代码，简化函数的调用结构\n- 不会清除IIFE(立即调用函数表达式)\n- 对于IIFE的返回函数，如果未使用会被清除\n\n**配置** ：\n\n在package.json中：\n\n`\"sideEffects\": false` 所有代码都没有副作用（都可以进行tree shaking)\n\n问题：可能会把css / @babel/polyfill （副作用）文件干掉,可以`\"sideEffects\": [\"\\*.css\", \"\\*.less\"]`解决。\n\n**小结**\n\n如果要`更好`的使用Webpack Tree shaking,请满足：\n\n- 使用ES2015(ES6)的模块\n- 避免使用IIFE\n- 如果使用第三方的模块，可以尝试直接从文件路径引用的方式使用（这并不是最佳的方式）\n\n```javascript\nimport { fn } from 'module'; \n=\u003e \nimport fn from 'module/XX';\n```\n\n#### code split\n\n##### 方案一\n\n假如我有两个如下名字的文件，直接在`index.js`中引入`test.js`的话，最后两个文件被打包成一个，可以使用如下方法:\n\n```javascript\n// 单入口\n// entry: './src/js/index.js',\nentry: {\n  // 多入口：有一个入口，最终输出就有一个bundle\n  index: './src/js/index.js',\n  test: './src/js/test.js'\n},\noutput: {\n  // [name]：取文件名\n  filename: 'js/[name].[contenthash:10].js',\n  path: resolve(__dirname, 'build')\n}\n```\n\n##### 方案二\n\n```javascript\nmodule.exports = {\n  /*\n    1. 可以将node_modules中代码单独打包一个chunk最终输出\n    2. 自动分析多入口chunk中，有没有公共的文件。如果有会打包成单独一个chunk\n  */\n  optimization: {\n    splitChunks: {\n      chunks: 'all'\n    }\n  },\n  mode: 'production'\n};\n\n```\n\n##### 方案三\n\n```javascript\n/*\n  通过js代码，让某个文件被单独打包成一个chunk\n  import动态导入语法：能将某个文件单独打包\n  默认使用chunkid命名，webpackChunkName指定打包后名字\n*/\nimport(/* webpackChunkName: 'test' */'./test')\n  .then(({ mul, count }) =\u003e {\n    // 文件加载成功~\n    // eslint-disable-next-line\n    console.log(mul(2, 5));\n  })\n  .catch(() =\u003e {\n    // eslint-disable-next-line\n    console.log('文件加载失败~');\n  });\n```\n\n#### lazy loading\n\n如下，不使用第一行引入方式而是第八行（也会像上面一样单独打包成一个chunk）\n\n```javascript\n// import { mul } from './test';\n\ndocument.getElementById('btn').onclick = function() {\n  // 懒加载~：当文件需要使用时才加载~\n  // 预加载 prefetch：会在使用之前，提前加载js文件 \n  // 正常加载可以认为是并行加载（同一时间加载多个文件）  \n  // 预加载 prefetch：等其他资源加载完毕，浏览器空闲了，再偷偷加载资源\n  import(/* webpackChunkName: 'test', webpackPrefetch: true */'./test').then(({ mul }) =\u003e {\n    console.log(mul(4, 5));\n  });\n};\n```\n\nPWA\n\nPWA: 渐进式网络开发应用程序(离线可访问)  \nworkbox --\u003e workbox-webpack-plugin\n\n安装:`npm install --save-dev workbox-webpack-plugin`\n\n配置：\n\n```javascript\nconst WorkboxWebpackPlugin = require('workbox-webpack-plugin');\nnew WorkboxWebpackPlugin.GenerateSW({\n  /*\n    1. 帮助serviceworker快速启动\n    2. 删除旧的 serviceworker\n    生成一个 serviceworker 配置文件~\n  */\n  clientsClaim: true,\n  skipWaiting: true\n})\n```\n\n在index.js注册\n\n```javascript\n// 注册serviceWorker\n// 处理兼容性问题\nif ('serviceWorker' in navigator) {\n  window.addEventListener('load', () =\u003e {\n    navigator.serviceWorker\n      .register('/service-worker.js')\n      .then(() =\u003e {\n        console.log('sw注册成功了~');\n      })\n      .catch(() =\u003e {\n        console.log('sw注册失败了~');\n      });\n  });\n}\n```\n\n问题：\n\n1. eslint不认识 window、navigator全局变量  \n    解决：需要修改package.json中eslintConfig配置  \n    \"env\": {  \n      \"browser\": true // 支持浏览器端全局变量  \n    }\n    \n 2. sw代码必须运行在服务器上  \n    --\u003e nodejs  \n    --\u003e  \n      `npm i serve -g`\n    \n\n    serve -s build 启动服务器，将build目录下所有资源作为静态资源暴露出去\n\n#### 多进程打包\n\n`npm install --save-dev thread-loader`\n\n```javascript\n{\n  test: /\\.js$/,\n  exclude: /node_modules/,\n  use: [\n    /* \n      开启多进程打包。 \n      进程启动大概为600ms，进程通信也有开销。\n      只有工作消耗时间比较长，才需要多进程打包\n    */\n    {\n      loader: 'thread-loader',\n      options: {\n        workers: 2 // 进程2个\n      }\n    },\n    {\n      loader: 'babel-loader',\n      options: {\n        presets: [\n          [\n            '@babel/preset-env',\n            {\n              useBuiltIns: 'usage',\n              corejs: { version: 3 },\n              targets: {\n                chrome: '60',\n                firefox: '50'\n              }\n            }\n          ]\n        ],\n        // 开启babel缓存\n        // 第二次构建时，会读取之前的缓存\n        cacheDirectory: true\n      }\n    }\n  ]\n}\n```\n\n#### externals\n\n忽略需要打包的内容，此时需要cdn引入。\n\n```javascript\nconst { resolve } = require('path');\nconst HtmlWebpackPlugin = require('html-webpack-plugin');\n\nmodule.exports = {\n  entry: './src/js/index.js',\n  output: {\n    filename: 'js/built.js',\n    path: resolve(__dirname, 'build')\n  },\n  plugins: [\n    new HtmlWebpackPlugin({\n      template: './src/index.html'\n    })\n  ],\n  mode: 'production',\n  externals: {\n    // 拒绝jQuery被打包进来\n    jquery: 'jQuery'\n  }\n};\n```\n\n#### dll\n\nwebpack.dll.js：\n\n```javascript\n/*\n  使用dll技术，对某些库（第三方库：jquery、react、vue...）进行单独打包\n    当你运行 webpack 时，默认查找 webpack.config.js 配置文件\n    需求：需要运行 webpack.dll.js 文件\n      --\u003e webpack --config webpack.dll.js\n*/\n\nconst { resolve } = require('path');\nconst webpack = require('webpack');\n\nmodule.exports = {\n  entry: {\n    // 最终打包生成的[name] --\u003e jquery\n    // ['jquery'] --\u003e 要打包的库是jquery\n    jquery: ['jquery'],\n  },\n  output: {\n    filename: '[name].js',\n    path: resolve(__dirname, 'dll'),\n    library: '[name]_[hash]' // 打包的库里面向外暴露出去的内容叫什么名字\n  },\n  plugins: [\n    // 打包生成一个 manifest.json --\u003e 提供和jquery映射\n    new webpack.DllPlugin({\n      name: '[name]_[hash]', // 映射库的暴露的内容名称\n      path: resolve(__dirname, 'dll/manifest.json') // 输出文件路径\n    })\n  ],\n  mode: 'production'\n};\n```\n\n单独打包后，上一节可以看出，其他文件正常打包后只需引入打包好的ddl即可，则`webpack.config.js`中配置：\n\n```javascript\nconst webpack = require('webpack');\n// 告诉webpack哪些库不参与打包，同时使用时的名称也得变~\nconst AddAssetHtmlWebpackPlugin = require('add-asset-html-webpack-plugin');\nplugins: [\n  new HtmlWebpackPlugin({\n    template: './src/index.html'\n  }),\n  // 告诉webpack哪些库不参与打包，同时使用时的名称也得变~\n  new webpack.DllReferencePlugin({\n    manifest: resolve(__dirname, 'dll/manifest.json')\n  }),\n  // 将某个文件打包输出去，并在html中自动引入该资源\n  new AddAssetHtmlWebpackPlugin({\n    filepath: resolve(__dirname, 'dll/jquery.js')\n  })\n]\n```\n\n### webpack 配置详情\n\n#### entry\n\nentry: 入口起点\n\n  1. string --\u003e './src/index.js'\n\n     单入口，打包形成一个chunk。 输出一个bundle文件。  \n     此时chunk的名称默认是 main。\n\n  2. array --\u003e ['./src/index.js', './src/add.js']\n\n     多入口，所有入口文件最终只会形成一个chunk, 输出出去只有一个bundle文件。\n\n     - 只有在HMR功能中让html热更新生效~\n\n  3. object\n\n     多入口，有几个入口文件就形成几个chunk，输出几个bundle文件  \n     此时chunk的名称是 key(即下面的index、add)。\n\n     - 特殊用法\n\n       ```javascript\n       entry:{\n         // 所有入口文件最终只会形成一个chunk, 输出出去只有一个bundle文件。\n         index: ['./src/index.js', './src/count.js'], \n         // 形成一个chunk，输出一个bundle文件。\n         add: './src/add.js'\n       }\n       ```\n\n#### output\n\n```javascript\noutput: {\n  // 文件名称（指定名称+目录）\n  filename: 'js/[name].js',\n  // 输出文件目录（将来所有资源输出的公共目录）\n  path: resolve(__dirname, 'build'),\n  // 所有资源引入公共路径前缀 --\u003e 'imgs/a.jpg' --\u003e '/imgs/a.jpg'\n  publicPath: '/',\n  chunkFilename: 'js/[name]_chunk.js', // 非入口chunk的名称\n  // library: '[name]', // 整个库向外暴露的变量名\n  // libraryTarget: 'window' // 变量名添加到哪个上 browser\n  // libraryTarget: 'global' // 变量名添加到哪个上 node\n  // libraryTarget: 'commonjs'\n}\n```\n\n#### module\n\n```javascript\nmodule: {\n  rules: [\n    // loader的配置\n    {\n      test: /\\.css$/,\n      // 多个loader用use\n      use: ['style-loader', 'css-loader']\n    },\n    {\n      test: /\\.js$/,\n      // 排除node_modules下的js文件\n      exclude: /node_modules/,\n      // 只检查 src 下的js文件\n      include: resolve(__dirname, 'src'),\n      // 优先执行\n      enforce: 'pre',\n      // 延后执行\n      // enforce: 'post',\n      // 单个loader用loader\n      loader: 'eslint-loader',\n      options: {}\n    },\n    {\n      // 以下配置只会生效一个\n      oneOf: []\n    }\n  ]\n},\n```\n\n#### resolve\n\n```javascript\n// 解析模块的规则\nresolve: {\n  // 配置解析模块路径别名: 优点简写路径 缺点路径没有提示\n  alias: {\n    $css: resolve(__dirname, 'src/css')\n  },\n  // 配置省略文件路径的后缀名\n  extensions: ['.js', '.json', '.jsx', '.css'],\n  // 告诉 webpack 解析模块是去找哪个目录\n  modules: [resolve(__dirname, '../../node_modules'), 'node_modules']\n}\n```\n\n#### devserve\n\n```javascript\ndevServer: {\n  // 运行代码的目录\n  contentBase: resolve(__dirname, 'build'),\n  // 监视 contentBase 目录下的所有文件，一旦文件变化就会 reload\n  watchContentBase: true,\n  watchOptions: {\n    // 忽略文件\n    ignored: /node_modules/\n  },\n  // 启动gzip压缩\n  compress: true,\n  // 端口号\n  port: 5000,\n  // 域名\n  host: 'localhost',\n  // 自动打开浏览器\n  open: true,\n  // 开启HMR功能\n  hot: true,\n  // 不要显示启动服务器日志信息\n  clientLogLevel: 'none',\n  // 除了一些基本启动信息以外，其他内容都不要显示\n  quiet: true,\n  // 如果出错了，不要全屏提示~\n  overlay: false,\n  // 服务器代理 --\u003e 解决开发环境跨域问题\n  proxy: {\n    // 一旦devServer(5000)服务器接受到 /api/xxx 的请求，就会把请求转发到另外一个服务器(3000)\n    '/api': {\n      target: 'http://localhost:3000',\n      // 发送请求时，请求路径重写：将 /api/xxx --\u003e /xxx （去掉/api）\n      pathRewrite: {\n        '^/api': ''\n      }\n    }\n  }\n}\n```\n\n#### optimization\n\n生产环境下才有意义。\n\n```javascript\noptimization: {\n  splitChunks: {\n    chunks: 'all'\n    // 默认值，可以不写~\n    /* minSize: 30 * 1024, // 分割的chunk最小为30kb\n    maxSiza: 0, // 最大没有限制\n    minChunks: 1, // 要提取的chunk最少被引用1次\n    maxAsyncRequests: 5, // 按需加载时并行加载的文件的最大数量\n    maxInitialRequests: 3, // 入口js文件最大并行请求数量\n    automaticNameDelimiter: '~', // 名称连接符\n    name: true, // 可以使用命名规则\n    cacheGroups: {\n      // 分割chunk的组\n      // node_modules文件会被打包到 vendors 组的chunk中。--\u003e vendors~xxx.j\n      // 满足上面的公共规则，如：大小超过30kb，至少被引用一次。\n      vendors: {\n        test: /[\\\\/]node_modules[\\\\/]/,\n        // 优先级\n        priority: -10\n      },\n      default: {\n        // 要提取的chunk最少被引用2次\n        minChunks: 2,\n        // 优先级\n        priority: -20,\n        // 如果当前要打包的模块，和之前已经被提取的模块是同一个，就会复用\n        reuseExistingChunk: true\n      } \n    }*/\n  },\n  // 将当前模块的记录其他模块的hash单独打包为一个文件 runtime\n  // 解决：修改a文件导致b文件的contenthash变化\n  runtimeChunk: {\n    name: entrypoint =\u003e `runtime-${entrypoint.name}`\n  },\n  minimizer: [\n    // 配置生产环境的压缩方案：js和css\n      //默认使用Terser，一般不需要配置\n    new TerserWebpackPlugin({\n      // 开启缓存\n      cache: true,\n      // 开启多进程打包\n      parallel: true,\n      // 启动source-map\n      sourceMap: true\n    })\n  ]\n}\n```\n\n### Webpack 5\n\n**本片写于2020，现新文章已经发布，在这里只做归档，建议参考最新文章**\n\n此版本重点关注以下内容:\n\n- 通过持久缓存提高构建性能.\n- 使用更好的算法和默认值来改善长期缓存.\n- 通过更好的树摇和代码生成来改善捆绑包大小.\n- 清除处于怪异状态的内部结构，同时在 v4 中实现功能而不引入任何重大更改.\n- 通过引入重大更改来为将来的功能做准备，以使我们能够尽可能长时间地使用 v5.\n\n下载:\n\n`npm i webpack@next webpack-cli -D`\n\n#### 自动删除 Node.js Polyfills\n\n早期，webpack 的目标是允许在浏览器中运行大多数 node.js 模块，但是模块格局发生了变化，许多模块用途现在主要是为前端目的而编写的。webpack \u003c= 4 附带了许多 node.js 核心模块的 polyfill，一旦模块使用任何核心模块（即 crypto 模块），这些模块就会自动应用。\n\n尽管这使使用为 node.js 编写的模块变得容易，但它会将这些巨大的 polyfill 添加到包中。在许多情况下，这些 polyfill 是不必要的。\n\nwebpack 5 会自动停止填充这些核心模块，并专注于与前端兼容的模块。\n\n迁移：\n\n- 尽可能尝试使用与前端兼容的模块。\n- 可以为 node.js 核心模块手动添加一个 polyfill。错误消息将提示如何实现该目标。\n\n#### Chunk 和模块 ID\n\n添加了用于长期缓存的新算法。在生产模式下默认情况下启用这些功能。\n\n`chunkIds: \"deterministic\", moduleIds: \"deterministic\"`\n\n#### Chunk ID\n\n你可以不用使用 `import(/* webpackChunkName: \"name\" */ \"module\")` 在开发环境来为 chunk 命名，生产环境还是有必要的\n\nwebpack 内部有 chunk 命名规则，不再是以 id(0, 1, 2)命名了\n\n#### Tree Shaking\n\n1. webpack 现在能够处理对嵌套模块的 tree shaking\n\n```js\n// inner.js\nexport const a = 1;\nexport const b = 2;\n\n// module.js\nimport * as inner from './inner';\nexport { inner };\n\n// user.js\nimport * as module from './module';\nconsole.log(module.inner.a);\n```\n\n在生产环境中, inner 模块暴露的 `b` 会被删除\n\n2. webpack 现在能够多个模块之前的关系\n\n```js\nimport { something } from './something';\n\nfunction usingSomething() {\n  return something;\n}\n\nexport function test() {\n  return usingSomething();\n}\n```\n\n当设置了`\"sideEffects\": false`时，一旦发现`test`方法没有使用，不但删除`test`，还会删除`\"./something\"`\n\n3. webpack 现在能处理对 Commonjs 的 tree shaking\n\n#### Output\n\nwebpack 4 默认只能输出 ES5 代码\n\nwebpack 5 开始新增一个属性 output.ecmaVersion, 可以生成 ES5 和 ES6 / ES2015 代码.\n\n如：`output.ecmaVersion: 2015`\n\n#### SplitChunk\n\n```js\n// webpack4\nminSize: 30000;\n```\n\n```js\n// webpack5\nminSize: {\n  javascript: 30000,\n  style: 50000,\n}\n```\n\n#### Caching\n\n```js\n// 配置缓存\ncache: {\n  // 磁盘存储\n  type: \"filesystem\",\n  buildDependencies: {\n    // 当配置修改时，缓存失效\n    config: [__filename]\n  }\n}\n```\n\n缓存将存储到 `node_modules/.cache/webpack`\n\n#### 监视输出文件\n\n之前 webpack 总是在第一次构建时输出全部文件，但是监视重新构建时会只更新修改的文件。\n\n此次更新在第一次构建时会找到输出文件看是否有变化，从而决定要不要输出全部文件。\n\n#### 默认值\n\n- `entry: \"./src/index.js`\n- `output.path: path.resolve(__dirname, \"dist\")`\n- `output.filename: \"[name].js\"`\n\n#### 更多内容\n\nhttps://github.com/webpack/changelog-v5\n\n### 模板\n\n加上了vue，使用时注意设置process.env.NODE_ENV。\n\n```javascript\nconst { resolve } = require('path');\nconst MiniCssExtractPlugin = require('mini-css-extract-plugin');\nconst OptimizeCssAssetsWebpackPlugin = require('optimize-css-assets-webpack-plugin');\nconst HtmlWebpackPlugin = require('html-webpack-plugin');\nconst VueLoaderPlugin = require('vue-loader/lib/plugin')\n\n// 定义nodejs环境变量：决定使用browserslist的哪个环境\n// process.env.NODE_ENV = 'production';\n\n// 复用loader\nconst commonCssLoader = [\n  MiniCssExtractPlugin.loader,\n  'css-loader',\n  {\n    // 还需要在package.json中定义browserslist\n    loader: 'postcss-loader',\n    options: {\n      ident: 'postcss',\n      plugins: () =\u003e [require('postcss-preset-env')()]\n    }\n  }\n];\n\nmodule.exports = {\n  entry: './src/js/index.js',\n  output: {\n    filename: 'js/bundle.js',\n    path: resolve(__dirname, 'dist')\n  },\n  module: {\n    rules: [\n        {\n                test: /\\.vue$/,\n                loader: 'vue-loader'\n            },\n      {\n        test: /\\.css$/,\n        use: [...commonCssLoader]\n      },\n      {\n        test: /\\.less$/,\n        use: [...commonCssLoader, 'less-loader']\n      },\n      /*\n        正常来讲，一个文件只能被一个loader处理。\n        当一个文件要被多个loader处理，那么一定要指定loader执行的先后顺序：\n          先执行eslint 在执行babel\n      */\n      {\n        // 在package.json中eslintConfig --\u003e airbnb\n        test: /\\.js$/,\n        exclude: /node_modules/,\n        // 优先执行\n        enforce: 'pre',\n        loader: 'eslint-loader',\n        options: {\n          fix: true\n        }\n      },\n      {\n        test: /\\.js$/,\n        exclude: /node_modules/,\n        loader: 'babel-loader',\n        options: {\n          presets: [\n            [\n              '@babel/preset-env',\n              {\n                useBuiltIns: 'usage',\n                corejs: {version: 3},\n                targets: {\n                  chrome: '60',\n                  firefox: '50'\n                }\n              }\n            ]\n          ]\n        }\n      },\n      {\n        test: /\\.(jpg|png|gif)/,\n        loader: 'url-loader',\n        options: {\n          limit: 8 * 1024,\n          name: '[hash:10].[ext]',\n          outputPath: 'imgs',\n          esModule: false\n        }\n      },\n      {\n        test: /\\.html$/,\n        loader: 'html-loader'\n      },\n      {\n        exclude: /\\.(js|css|less|html|jpg|png|gif)/,\n        loader: 'file-loader',\n        options: {\n          outputPath: 'media'\n        }\n      }\n    ]\n  },\n  plugins: [\n    new VueLoaderPlugin(),\n    new MiniCssExtractPlugin({\n      filename: 'css/built.css'\n    }),\n    new OptimizeCssAssetsWebpackPlugin(),\n    new HtmlWebpackPlugin({\n      template: './src/index.html',\n      minify: {\n        collapseWhitespace: true,\n        removeComments: true\n      }\n    })\n  ],\n  mode: 'development',\n  devServer: {\n  \tcontentBase: resolve(__dirname, 'build'),\n  \tcompress: true,\n  \tport: 3000,\n  \topen: true,\n  }\n};\n```\n","lastmodified":"2023-05-09T16:33:58.295366497Z","tags":[]}}