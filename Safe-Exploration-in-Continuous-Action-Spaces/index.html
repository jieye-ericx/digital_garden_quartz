<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Abstract 我们解决了在物理系统（如数据中心冷却装置或机器人）上部署强化学习（RL）代理的问题，在这些系统中，决不能违反关键约束。
我们展示了如何利用这些系统典型的平滑动力学，并使RL算法在学习过程中从不违反约束。**我们的技术是直接在策略中添加一个安全层，该安全层通过分析解决每个状态下的动作校正公式。**获得一个优雅的闭式解的新颖性是由于一个线性化模型得到的，该模型是根据过去由任意动作组成的轨迹得到的。这是为了模拟真实世界中的情况，在这种情况下，数据日志是使用无法用数学方法描述的行为策略生成的； 这种情况使得已知的安全意识的off-policy方法不适用。我们在新的基于物理的典型环境中证明了我们的方法的有效性，并在奖励塑造失败时，通过保持零约束违反而获胜。
1. Introduction 在过去二十年中，RL主要在玩具环境（Sutton&Barto，1998）和视频游戏（Mnih等人，2015）中进行探索，其中现实世界的应用仅限于一些典型的用例，如推荐系统（Shani等人，2005）。然而，RL最近也在物理世界中找到了工业应用的道路；e、 g.、数据中心冷却（Evans&Gao，2016）、机器人技术（Gu等人，2016）和自动驾驶车辆（Sallab等人，2017）。在所有这些使用案例中，安全性是一个关键因素 ：除非从部署的第一刻起就彻底解决并确保安全操作，否则RL被视为与它们不兼容。
在上述实际应用中，约束是问题描述不可或缺的一部分永远不违反这些原则通常是绝对必要的，因此，在这项工作中，我们的目标是在整个学习过程中保持零约束违反。请注意，对于离散动作空间，实现这一目标比对于连续动作空间更简单。例如，可以在脱机数据上预先训练约束冲突分类器，以修剪不安全的操作。然而，在我们的环境中，由于候选动作的数量是无限的，这一目标变得更具挑战性。尽管如此，我们确实能够在连续的行动空间中实现这一目标，并且在整个学习过程中从不违反约束。
具体来说，我们解决了物理系统中的安全控制问题，其中某些可观测量将受到约束。举例说明，在数据中心冷却的情况下，温度和压力应始终保持在各自阈值以下；机器人不得超过角度和扭矩限制；而且，自动驾驶车辆必须始终保持与障碍物的距离在一定范围内。在这项工作中，我们将这些量表示为安全信号。由于这些是物理量，我们利用它们的平滑性来避免意外的、不安全的操作。此外，我们通过可用的历史数据处理些常见情况；因此，它可以用于预训练模型，以帮助从初始RL部署时刻开始的安全性。
如上所述，安全勘探传统上要求访问使用某些已知行为策略生成的数据，并在此基础上进行逐步安全更新；参见（Thomas，2015）对此类off-policy方法的全面研究。这些数据是必要的，因为除非另有假设，否则一个state下的action可能会在未来产生灾难性后果。因此，在部署之前，应提前推断长期行为。相反，在这项工作中，我们不需要行为策略知识，因为我们关注的是物理系统，其行为具有相对短期的后果。避免行为政策知识是我们工作中的一个关键优势，因为缺乏此类数据是一个具有挑战性但又熟悉的现实情况。复杂系统中过去的轨迹很少是使用一致的能数学描述的行为策略生成的。？？这类系统传统上由人工或复杂软件控制，其逻辑很难理解。因此，在这种情况下， off-policy RL方法被视为不适用。相反，我们展示了如何有效利用单步过渡数据来确保安全性。为了证明我们的方法与行为策略的独立性，在我们的实验中，我们用纯粹随机的动作生成了预训练数据。
我们的方法依赖于一个模型的一次性初始预训练，该模型预测在单个时间步长内安全信号的变化。该模型的优点在于其简单性：它是关于action的一阶近似值，其系数是状态反馈神经网络（NN）的输出。然后，我们在安全层中使用该模型，该安全层在agent的策略之上，以便在需要时纠正action；也就是在每次策略查询之后，它解决了一个找到对action的最小更改以满足安全约束的优化问题，由于与动作有关的线性关系，可以解析地导出闭合形式的解，并相当于基本的算术运算。因此，我们的安全层是可微的，并且有一个简单的三行软件实现。请注意，与我们的安全机制相关的“安全层”纯粹是语义上的选择；它只是一个简单的计算，不局限于当今流行的深度策略网络，并且可以应用于任何连续控制算法（不一定基于RL）。
2. Related Work 由于这项工作侧重于具有连续状态和动作空间的控制问题，我们将比较局限于在策略优化的背景下安全RL的性质，该策略优化试图在学习过程中保持安全。例如（Achiam et al."><meta property="og:title" content><meta property="og:description" content="Abstract 我们解决了在物理系统（如数据中心冷却装置或机器人）上部署强化学习（RL）代理的问题，在这些系统中，决不能违反关键约束。
我们展示了如何利用这些系统典型的平滑动力学，并使RL算法在学习过程中从不违反约束。**我们的技术是直接在策略中添加一个安全层，该安全层通过分析解决每个状态下的动作校正公式。**获得一个优雅的闭式解的新颖性是由于一个线性化模型得到的，该模型是根据过去由任意动作组成的轨迹得到的。这是为了模拟真实世界中的情况，在这种情况下，数据日志是使用无法用数学方法描述的行为策略生成的； 这种情况使得已知的安全意识的off-policy方法不适用。我们在新的基于物理的典型环境中证明了我们的方法的有效性，并在奖励塑造失败时，通过保持零约束违反而获胜。
1. Introduction 在过去二十年中，RL主要在玩具环境（Sutton&Barto，1998）和视频游戏（Mnih等人，2015）中进行探索，其中现实世界的应用仅限于一些典型的用例，如推荐系统（Shani等人，2005）。然而，RL最近也在物理世界中找到了工业应用的道路；e、 g.、数据中心冷却（Evans&Gao，2016）、机器人技术（Gu等人，2016）和自动驾驶车辆（Sallab等人，2017）。在所有这些使用案例中，安全性是一个关键因素 ：除非从部署的第一刻起就彻底解决并确保安全操作，否则RL被视为与它们不兼容。
在上述实际应用中，约束是问题描述不可或缺的一部分永远不违反这些原则通常是绝对必要的，因此，在这项工作中，我们的目标是在整个学习过程中保持零约束违反。请注意，对于离散动作空间，实现这一目标比对于连续动作空间更简单。例如，可以在脱机数据上预先训练约束冲突分类器，以修剪不安全的操作。然而，在我们的环境中，由于候选动作的数量是无限的，这一目标变得更具挑战性。尽管如此，我们确实能够在连续的行动空间中实现这一目标，并且在整个学习过程中从不违反约束。
具体来说，我们解决了物理系统中的安全控制问题，其中某些可观测量将受到约束。举例说明，在数据中心冷却的情况下，温度和压力应始终保持在各自阈值以下；机器人不得超过角度和扭矩限制；而且，自动驾驶车辆必须始终保持与障碍物的距离在一定范围内。在这项工作中，我们将这些量表示为安全信号。由于这些是物理量，我们利用它们的平滑性来避免意外的、不安全的操作。此外，我们通过可用的历史数据处理些常见情况；因此，它可以用于预训练模型，以帮助从初始RL部署时刻开始的安全性。
如上所述，安全勘探传统上要求访问使用某些已知行为策略生成的数据，并在此基础上进行逐步安全更新；参见（Thomas，2015）对此类off-policy方法的全面研究。这些数据是必要的，因为除非另有假设，否则一个state下的action可能会在未来产生灾难性后果。因此，在部署之前，应提前推断长期行为。相反，在这项工作中，我们不需要行为策略知识，因为我们关注的是物理系统，其行为具有相对短期的后果。避免行为政策知识是我们工作中的一个关键优势，因为缺乏此类数据是一个具有挑战性但又熟悉的现实情况。复杂系统中过去的轨迹很少是使用一致的能数学描述的行为策略生成的。？？这类系统传统上由人工或复杂软件控制，其逻辑很难理解。因此，在这种情况下， off-policy RL方法被视为不适用。相反，我们展示了如何有效利用单步过渡数据来确保安全性。为了证明我们的方法与行为策略的独立性，在我们的实验中，我们用纯粹随机的动作生成了预训练数据。
我们的方法依赖于一个模型的一次性初始预训练，该模型预测在单个时间步长内安全信号的变化。该模型的优点在于其简单性：它是关于action的一阶近似值，其系数是状态反馈神经网络（NN）的输出。然后，我们在安全层中使用该模型，该安全层在agent的策略之上，以便在需要时纠正action；也就是在每次策略查询之后，它解决了一个找到对action的最小更改以满足安全约束的优化问题，由于与动作有关的线性关系，可以解析地导出闭合形式的解，并相当于基本的算术运算。因此，我们的安全层是可微的，并且有一个简单的三行软件实现。请注意，与我们的安全机制相关的“安全层”纯粹是语义上的选择；它只是一个简单的计算，不局限于当今流行的深度策略网络，并且可以应用于任何连续控制算法（不一定基于RL）。
2. Related Work 由于这项工作侧重于具有连续状态和动作空间的控制问题，我们将比较局限于在策略优化的背景下安全RL的性质，该策略优化试图在学习过程中保持安全。例如（Achiam et al."><meta property="og:type" content="website"><meta property="og:image" content="https://jieye-ericx.github.io/icon.png"><meta property="og:url" content="https://jieye-ericx.github.io/Safe-Exploration-in-Continuous-Action-Spaces/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="Abstract 我们解决了在物理系统（如数据中心冷却装置或机器人）上部署强化学习（RL）代理的问题，在这些系统中，决不能违反关键约束。
我们展示了如何利用这些系统典型的平滑动力学，并使RL算法在学习过程中从不违反约束。**我们的技术是直接在策略中添加一个安全层，该安全层通过分析解决每个状态下的动作校正公式。**获得一个优雅的闭式解的新颖性是由于一个线性化模型得到的，该模型是根据过去由任意动作组成的轨迹得到的。这是为了模拟真实世界中的情况，在这种情况下，数据日志是使用无法用数学方法描述的行为策略生成的； 这种情况使得已知的安全意识的off-policy方法不适用。我们在新的基于物理的典型环境中证明了我们的方法的有效性，并在奖励塑造失败时，通过保持零约束违反而获胜。
1. Introduction 在过去二十年中，RL主要在玩具环境（Sutton&Barto，1998）和视频游戏（Mnih等人，2015）中进行探索，其中现实世界的应用仅限于一些典型的用例，如推荐系统（Shani等人，2005）。然而，RL最近也在物理世界中找到了工业应用的道路；e、 g.、数据中心冷却（Evans&Gao，2016）、机器人技术（Gu等人，2016）和自动驾驶车辆（Sallab等人，2017）。在所有这些使用案例中，安全性是一个关键因素 ：除非从部署的第一刻起就彻底解决并确保安全操作，否则RL被视为与它们不兼容。
在上述实际应用中，约束是问题描述不可或缺的一部分永远不违反这些原则通常是绝对必要的，因此，在这项工作中，我们的目标是在整个学习过程中保持零约束违反。请注意，对于离散动作空间，实现这一目标比对于连续动作空间更简单。例如，可以在脱机数据上预先训练约束冲突分类器，以修剪不安全的操作。然而，在我们的环境中，由于候选动作的数量是无限的，这一目标变得更具挑战性。尽管如此，我们确实能够在连续的行动空间中实现这一目标，并且在整个学习过程中从不违反约束。
具体来说，我们解决了物理系统中的安全控制问题，其中某些可观测量将受到约束。举例说明，在数据中心冷却的情况下，温度和压力应始终保持在各自阈值以下；机器人不得超过角度和扭矩限制；而且，自动驾驶车辆必须始终保持与障碍物的距离在一定范围内。在这项工作中，我们将这些量表示为安全信号。由于这些是物理量，我们利用它们的平滑性来避免意外的、不安全的操作。此外，我们通过可用的历史数据处理些常见情况；因此，它可以用于预训练模型，以帮助从初始RL部署时刻开始的安全性。
如上所述，安全勘探传统上要求访问使用某些已知行为策略生成的数据，并在此基础上进行逐步安全更新；参见（Thomas，2015）对此类off-policy方法的全面研究。这些数据是必要的，因为除非另有假设，否则一个state下的action可能会在未来产生灾难性后果。因此，在部署之前，应提前推断长期行为。相反，在这项工作中，我们不需要行为策略知识，因为我们关注的是物理系统，其行为具有相对短期的后果。避免行为政策知识是我们工作中的一个关键优势，因为缺乏此类数据是一个具有挑战性但又熟悉的现实情况。复杂系统中过去的轨迹很少是使用一致的能数学描述的行为策略生成的。？？这类系统传统上由人工或复杂软件控制，其逻辑很难理解。因此，在这种情况下， off-policy RL方法被视为不适用。相反，我们展示了如何有效利用单步过渡数据来确保安全性。为了证明我们的方法与行为策略的独立性，在我们的实验中，我们用纯粹随机的动作生成了预训练数据。
我们的方法依赖于一个模型的一次性初始预训练，该模型预测在单个时间步长内安全信号的变化。该模型的优点在于其简单性：它是关于action的一阶近似值，其系数是状态反馈神经网络（NN）的输出。然后，我们在安全层中使用该模型，该安全层在agent的策略之上，以便在需要时纠正action；也就是在每次策略查询之后，它解决了一个找到对action的最小更改以满足安全约束的优化问题，由于与动作有关的线性关系，可以解析地导出闭合形式的解，并相当于基本的算术运算。因此，我们的安全层是可微的，并且有一个简单的三行软件实现。请注意，与我们的安全机制相关的“安全层”纯粹是语义上的选择；它只是一个简单的计算，不局限于当今流行的深度策略网络，并且可以应用于任何连续控制算法（不一定基于RL）。
2. Related Work 由于这项工作侧重于具有连续状态和动作空间的控制问题，我们将比较局限于在策略优化的背景下安全RL的性质，该策略优化试图在学习过程中保持安全。例如（Achiam et al."><meta name=twitter:image content="https://jieye-ericx.github.io/icon.png"><title>ericx 's 数字花园</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://jieye-ericx.github.io//icon.png><link href=https://jieye-ericx.github.io/styles.80333fa2099c0bee674efa435fde378c.min.css rel=stylesheet><link href=https://jieye-ericx.github.io/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://jieye-ericx.github.io/js/darkmode.48459b7116d092b4e98d2cab704cad80.min.js></script>
<script src=https://jieye-ericx.github.io/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://jieye-ericx.github.io/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://jieye-ericx.github.io/",fetchData=Promise.all([fetch("https://jieye-ericx.github.io/indices/linkIndex.d1ffeb07c444307ac70f84984f3aa0d9.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://jieye-ericx.github.io/indices/contentIndex.2fd0bb3e22e4db8398171e719cba2382.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://jieye-ericx.github.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://jieye-ericx.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/jieye-ericx.github.io\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=jieye-ericx.github.io src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://jieye-ericx.github.io/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://jieye-ericx.github.io/>ericx 's 数字花园</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated
Unknown
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/Safe%20Exploration%20in%20Continuous%20Action%20Spaces.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#abstract>Abstract</a></li></ol><ol><li><a href=#61-an-alternative-additional-loss-term>6.1. An Alternative: Additional Loss Term</a></li></ol><ol><li><a href=#71-ball-domain>7.1 Ball Domain</a></li><li><a href=#72-spaceship-domain>7.2. Spaceship Domain</a></li><li><a href=#74-safety-layer-versus-reward-shaping>7.4. Safety Layer versus Reward Shaping</a></li></ol></nav></details></aside><a href=#abstract><h2 id=abstract><span class=hanchor arialabel=Anchor># </span>Abstract</h2></a><p>我们<strong>解决了在物理系统（如数据中心冷却装置或机器人）上部署强化学习（RL）代理的问题，在这些系统中，决不能违反关键约束。</strong></p><p>我们展示了如何利用这些系统典型的平滑动力学，并使RL算法在学习过程中从不违反约束。**我们的技术是直接在策略中添加一个安全层，该安全层通过分析解决每个状态下的动作校正公式。**获得一个优雅的闭式解的新颖性是由于一个线性化模型得到的，该模型是根据过去由任意动作组成的轨迹得到的。这是为了模拟真实世界中的情况，在这种情况下，数据日志是使用无法用数学方法描述的行为策略生成的；
这种情况使得已知的安全意识的off-policy方法不适用。我们在新的基于物理的典型环境中证明了我们的方法的有效性，并在奖励塑造失败时，通过保持零约束违反而获胜。</p><a href=#1-introduction><h1 id=1-introduction><span class=hanchor arialabel=Anchor># </span>1. Introduction</h1></a><p>在过去二十年中，RL主要在玩具环境（Sutton&Barto，1998）和视频游戏（Mnih等人，2015）中进行探索，其中现实世界的应用仅限于一些典型的用例，如推荐系统（Shani等人，2005）。然而，RL最近也在物理世界中找到了工业应用的道路；e、 g.、数据中心冷却（Evans&Gao，2016）、机器人技术（Gu等人，2016）和自动驾驶车辆（Sallab等人，2017）。在所有这些使用案例中，安全性是一个关键因素 ：<strong>除非从部署的第一刻起就彻底解决并确保安全操作，否则RL被视为与它们不兼容。</strong></p><p>在上述实际应用中，约束是问题描述不可或缺的一部分永远不违反这些原则通常是绝对必要的，因此，<strong>在这项工作中，我们的目标是在整个学习过程中保持零约束违反</strong>。请注意，对于离散动作空间，实现这一目标比对于连续动作空间更简单。例如，可以在脱机数据上预先训练约束冲突分类器，以修剪不安全的操作。然而，在我们的环境中，由于候选动作的数量是无限的，这一目标变得更具挑战性。尽管如此，我们确实能够在连续的行动空间中实现这一目标，并且在整个学习过程中从不违反约束。</p><p>具体来说，我们解决了物理系统中的安全控制问题，其中某些可观测量将受到约束。举例说明，在数据中心冷却的情况下，温度和压力应始终保持在各自阈值以下；机器人不得超过角度和扭矩限制；而且，自动驾驶车辆必须始终保持与障碍物的距离在一定范围内。在这项工作中，我们<strong>将这些量表示为安全信号</strong>。由于这些是物理量，我们利用它们的平滑性来避免意外的、不安全的操作。此外，<strong>我们通过可用的历史数据处理些常见情况；因此，它可以用于预训练模型，以帮助从初始RL部署时刻开始的安全性。</strong></p><p>如上所述，安全勘探传统上要求访问使用某些已知行为策略生成的数据，并在此基础上进行逐步安全更新；参见（Thomas，2015）对此类off-policy方法的全面研究。这些数据是必要的，因为除非另有假设，否则一个state下的action可能会在未来产生灾难性后果。因此，在部署之前，应提前推断长期行为。相反，<strong>在这项工作中，我们不需要行为策略知识，因为我们关注的是物理系统，其行为具有相对短期的后果</strong>。<strong>避免行为政策知识是我们工作中的一个关键优势，因为缺乏此类数据是一个具有挑战性但又熟悉的现实情况</strong>。复杂系统中过去的轨迹很少是使用一致的能数学描述的行为策略生成的。？？这类系统传统上由人工或复杂软件控制，其逻辑很难理解。因此，在这种情况下， off-policy RL方法被视为不适用。相反，我们展示了如何有效利用单步过渡数据来确保安全性。<strong>为了证明我们的方法与行为策略的独立性，在我们的实验中，我们用纯粹随机的动作生成了预训练数据。</strong></p><p><strong>我们的方法依赖于一个模型的一次性初始预训练，该模型预测在单个时间步长内安全信号的变化</strong>。该模型的优点在于其简单性：它是关于action的一阶近似值，其系数是状态反馈神经网络（NN）的输出。然后，我们在安全层中使用该模型，该安全层在agent的策略之上，以便在需要时纠正action；也就是在每次策略查询之后，它解决了一个找到对action的最小更改以满足安全约束的优化问题，由于与动作有关的线性关系，可以解析地导出闭合形式的解，并相当于基本的算术运算。因此，我们的安全层是可微的，并且有一个简单的三行软件实现。<strong>请注意，与我们的安全机制相关的“安全层”纯粹是语义上的选择；它只是一个简单的计算，不局限于当今流行的深度策略网络，并且可以应用于任何连续控制算法（不一定基于RL）。</strong></p><a href=#2-related-work><h1 id=2-related-work><span class=hanchor arialabel=Anchor># </span>2. Related Work</h1></a><p>由于这项工作侧重于具有连续状态和动作空间的控制问题，我们将比较局限于在策略优化的背景下安全RL的性质，该策略优化试图在学习过程中保持安全。例如（Achiam et al.，2017），其中约束策略优化是通过改进的信赖域策略梯度来解决的。在这里，算法的更新规则在每次迭代中将策略投影到一个安全的可行性集。在某些政策规律性假设下，结果表明，在预期情况下，该政策保持在约束范围内。因此，它不适用于我们的用例，在这些用例中，必须确保所有访问state的安全。另一项最新工作（Berkenkamp et al.，2017）描述了离散化确定性控制框架可保证安全运行的控制理论条件。如果某些Lipschitz连续性条件成立，则为政策牵引区域确定了适当的Lyapunov函数。虽然在适当的条件下，安全爆炸是有保证的，但需要了解具体的系统。此外，NN可能不是具有合理系数的Lipschitz连续。最后，最近的工作（Pham等人，2017年）使用了首次引入的图内QP解算器（Amos&Kolter，2017年）。</p><p>它展示了一种与我们采取的方法性质相似的方法：在 policy层面解决操作优化问题，以确保state的安全。然而，有两个主要区别需要注意。首先，该解决方案依赖于完整QP解算器的图内实现（Amos&Kolter，2017），该解算器在每次正向传播时运行迭代内点算法。这不仅是一个实现上的挑战（目前只有pytorch版本可用），而且成本高昂。与我们工作的另一个不同之处是（Pham等人，2017年）需要专家知识来明确手动设计机械臂的物理约束。。相反，<strong>在我们的方法中，不需要手动操作；这些动态直接从数据中学习，同时也与行为策略无关。</strong></p><p>总而言之，据我们所知，这项工作是第一次在policy层面直接解决在海量状态下的安全问题，同时也以数据驱动的方式使用任意数据日志。此外，它可以应用于任何连续控制算法；它不局限于特定的RL算法或任何算法。</p><a href=#3-defifinitions><h1 id=3-defifinitions><span class=hanchor arialabel=Anchor># </span>3. Defifinitions</h1></a><p>我们考虑一个特殊情况的约束马尔可夫决策过程（CMDP）（奥特曼，1999），其中观察到的安全信号应该保持边界。</p><blockquote><p>约束马尔可夫决策过程（Constrained MDP, CMDP）是对智能体施加了额外限制的MDP，在CMDP中，智能体不仅要实施策略和获得回报，还要确保环境状态的一些指标不超出限制。</p><p>相比于MDP，CMDP中智能体的每个动作都对应多个（而非一个）奖励。此外，由于约束的引入，CMDP不满足贝尔曼
<a href=https://baike.baidu.com/item/%e6%9c%80%e4%bc%98%e5%8c%96%e5%8e%9f%e7%90%86/797797 rel=noopener>最优化原理</a>，其最优策略是对初始状态敏感的，因此CMDP无法使用动态规划求解。离散CMDP的常见解法是
<a href=https://baike.baidu.com/item/%e7%ba%bf%e6%80%a7%e8%a7%84%e5%88%92/1661606 rel=noopener>线性规划</a>（linear programming）。</p></blockquote><p>$$
[K]:{1,&mldr;,K}\\ [x]^+:max{x,0},x\in\mathbb{R}
$$</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211117133604330.png width=auto alt=image-20211117133604330></p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211117133702359.png width=auto alt=image-20211117133702359></p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211117133729087.png width=auto alt=image-20211117133729087>
$$
C = {c_i|S × A →\mathbb{R} | i ∈ [K]}\\ C是一个直接约束函数的集合,基于此，作者定义了safety\\ signals\\ \bar{C} = {\bar{c_i} : S → \mathbb{R} | i ∈ [K]}
$$
这些是即时约束值的逐状态观察，我们在后面介绍这些值是为了便于记法。</p><p><strong>举例说明，如果$c_1(s,a)$是在状态s中选择动作a后检测到的数据中心的温度，则$\bar{c_1}(s&rsquo;)$是与转移到$s&rsquo;$后检测出的相同温度。</strong></p><p>在这项工作所涉及的系统类型中，P是确定性的，它决定$f\\ s.t. \\ s&rsquo;= f(s,a)$,因此,有$\bar{c_i}(s&rsquo;)\triangleq c_i(s,a)$，对于一般的非确定性转换核，$\bar{c_i}(s&rsquo;)$可以定义为$s&rsquo;\sim P(\cdot|s,a) $的期望。</p><p>最后，令policy $\mu : S \rightarrow A $是从状态到动作的静态映射。</p><a href=#4-state-wise-constrained-policy-optimization><h1 id=4-state-wise-constrained-policy-optimization><span class=hanchor arialabel=Anchor># </span>4. State-wise Constrained Policy Optimization</h1></a><p><img src=https://jieye-ericx.github.io//../../pics/image-20211117142310394.png width=auto alt=image-20211117142310394></p><p>每一个 agent 要最大化他的个人利益，</p><p>我们强调，我们的目标是确保不仅对上式的解，而且对其优化过程都有状态约束。这一目标通常可能难以实现，因为对于任意MDP，某些操作可能会对可能的状态路径产生长期影响。然而，对于物理系统的类型，我们认为通过调整单个（或很少）时间步长中的动作可以确保安全约束是合理的。在现实世界的使用案例中，冷却系统动力学受一阶传热微分方程（Goodwine，2010）和二阶牛顿微分方程（控制水传质）等因素的影响。后者还控制机械臂或施加力的车辆的运动。<strong>在这些类型的控制问题中，即使存在惯性，只要在选择$C_i$时有合理的松弛，满足状态约束也是可行的。我们将对此进行进一步扩展，并在第7节中提供证据。</strong></p><a href=#5-linear-safety-signal-model><h1 id=5-linear-safety-signal-model><span class=hanchor arialabel=Anchor># </span>5. Linear Safety-Signal Model</h1></a><p>解决（1）是一项困难的任务，即使对于上面列出的系统类型也是如此。困难一个主要原因是RL agent需要探索新的和更好的的action。在事先不了解环境的情况下，使用随机策略初始化的RL agent无法确保在初始训练阶段满足每状态约束。当奖赏被仔细地塑造以惩罚不希望出现的状态时，这种说法也成立：对于RL代理来说，为了学会避免不希望出现的行为，在我们的动态规划方案中它需要违反足够多次约束，以使负面影响得以传播。</p><p>因此，在这项工作中，我们结合了一些基于单步动力学的基本形式的先验知识。日志中的单步转换数据非常常见，如前所述，与了解行为策略（如需要专家知识来明确手动设计机械臂的物理约束）相比更为现实。我们不试图学习完整的转换模型，而只是学习直接约束函数$c_i(s,a)$。虽然简单地用以$(s,a)$为输入的NNs来近似它们是有吸引力的，但我们选择了一种更优雅的方法，该方法具有第6.1小节中列出的显著优势。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211117145728192.png width=auto alt=image-20211117145728192></p><ul><li>$w_i$ weights of a NN</li><li>$g(s;w_i)$ takes <em>s</em> as input and outputs a vector of the same dimension as <em>a</em></li></ul><p>因此我们结合了一些基于单步动力学的基本形式的先验知识。采用日志数据，与了解行为策略（如需要专家知识来明确手动设计机械臂的物理约束）相比更为现实。我们不试图学习完整的转换模型，而只是学习直接约束函数$c_i(s,a)$。虽然简单地用以$(s,a)$为输入的NNs来近似它们是有吸引力的，但我们选择了一种更优雅的方法。</p><p>该模型是关于a的$c_i(s,a)$的一阶近似；也就是使用状态特征明确表示安全信号变化对动作的敏感性。有关可视化，请参见图1。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211117150645473.png width=auto alt=image-20211117150645473></p><p>Remark 1 Linear approximations of non-linear physical systems prove accurate and are well accepted in many fields, e.g. aircraft design (Liang & Sun, 2013). For a comprehensive study see (Enqvist,2005).</p><p>非线性物理系统的线性近似证明是准确的，并在许多领域得到广泛接受，例如飞机设计（Liang&Sun，2013）。有关综合性研究，请参见（Enqvist，2005）。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211117151430348.png width=auto alt=image-20211117151430348></p><p>在我们的实验中，为了生成D，我们只需在一个均匀随机的位置初始化代理，并让它对多个情节执行单一形式的随机动作。当达到时间限制或违反约束时，事件终止。后者对应于管理生产系统的现实世界机制：高性能、高效的控制通常由低效的保守政策支持；当出现不安全操作标志时，将触发抢占机制，并引入保守策略以确保满足约束。</p><p>D上的$g(s;w_i)$ 培训在RL培训之前的预培训阶段，每个任务执行一次。但是，在RL培训期间，$g(s;w_i)$ 的额外持续培训也是可选的。因为在我们的实验中，持续训练与单独的预训练相比没有任何益处，所以我们只展示了后者的结果。</p><a href=#6-safety-layer-via-analytical-optimization><h1 id=6-safety-layer-via-analytical-optimization><span class=hanchor arialabel=Anchor># </span>6. Safety Layer via Analytical Optimization</h1></a><p>我们现在展示如何通过对策略本身的简单添加，使用PG策略梯度算法（Baxter&Bartlett，2001）来解决问题（1）。我们使用DDPG（Lillicrap et al.，2015）进行实验，其政策网络直接输出行动，而不是其概率。然而，我们的方法不仅限于此，而且可以添加到概率策略梯度或任何其他连续控制算法中。</p><p>用$\mu_\theta(s)$表示深度策略网络选择的确定性动作,然后，在策略网络之上我们组成一个附加的最后一层，其作用是解决</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211117153303914.png width=auto alt=image-20211117153303914></p><p>这一层，我们称之为安全层，在欧几里德范数中尽可能少地扰动原始动作，以满足必要的约束。图2显示了其与策略网络的关系。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211117154517518.png width=auto alt=image-20211117154517518></p><p>为了求解（3），我们现在用第5节中介绍的$c_i(s,a)$替换线性模型，并得到二次规划
<img src=https://jieye-ericx.github.io//../../pics/image-20211117154734999.png width=auto alt=image-20211117154734999></p><p>由于正定二次目标和线性约束，我们现在可以找到这个凸问题的全局解。通常，为了解决这一问题，可以实现图内迭代QP解算器，如in（Amos&Kolter，2017）。这将产生一种与中类似的方法（Pham等人，2017年），<strong>但其优点是所有物理约束模型都直接从数据中学习，而不是手工设计</strong>。或者，如果已知活动约束的数量以某个m为界≤ K、 可以对所有（K m）进行彻底迭代组合可能的活动约束，并选择最佳可行约束；这对于m的值较小来说是合理的。</p><p>然而，在这项工作中，以一个简化的假设为代价，我们获得了（4）的封闭形式解析解的好处，该解析解具有三行代码的软件实现。假设一次激活的约束不超过一个。正如在我们的实验中所证明，当一个代理在物理域中导航并避开障碍物时，这是合理的假设。由于与每个障碍物的距离被建模为一个单独的约束，因此一次只有一个障碍物是距离最近的一个。从第7节的情节和视频中可以看出，靠近角落不会造成任何问题。</p><p>此外，对于具有多个交叉约束的其他系统，可以学习联合模型。例如，与其将两墙之间的距离视为两个约束，不如将两墙之间的最小距离视为一个约束。在初步实验中，当我们在第7节的第一个也是最简单的任务中将两个约束组合为一个时，这种方法产生了与不使用它类似的结果。然而，使用单个$g(\cdot ; \cdot)$网络联合建模多个安全信号的动力学是一个需要仔细关注的主题，我们将其留给未来的工作。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211117161909644.png width=auto alt=image-20211117161909644></p><p>解（6）本质上是原始作用$\mu_\theta(s)$到斜率为$g(s;w_i*)$的“安全”超平面的线性投影∗ ) 并截获$\bar{c_{i*}}(s)-C_{i*}$.</p><p>就实现而言，它包括一些基本的算术运算：向量积后跟“max”运算。它的简单性有三个好处：</p><p>i）它有一个简单、几乎毫不费力的软件实现；</p><p>ii）其计算成本可忽略不计；</p><p>iii）它是可微的（几乎所有地方都是可微的，就像ReLu一样）。</p><a href=#61-an-alternative-additional-loss-term><h2 id=61-an-alternative-additional-loss-term><span class=hanchor arialabel=Anchor># </span>6.1. An Alternative: Additional Loss Term</h2></a><p>为了强调我们的线性模型在解决（3）中的突出作用，我们现在简要描述我们最初尝试的另一种可能选择的缺点：用于ci（s，a）的直进（s，a）反馈神经网络模型。在这种情况下，可以通过惩罚违反约束的目标并求解不受约束的替代项来获得（3）的近似解</p><a href=#7-experiments><h1 id=7-experiments><span class=hanchor arialabel=Anchor># </span>7. Experiments</h1></a><p>根据接下来介绍的每个任务，我们将运行第5节中描述的初始预培训阶段。我们构建了每个任务有1000个随机动作集的D。然后，我们将预先培训的安全层添加到策略网络中。如前所述，我们在实验中选择的RL算法是DDPG（Lillicrap等人，2015）。在本节中，我们展示了在训练过程中，DDPG从未违反约束，并且与没有我们添加的情况相比，收敛速度更快。</p><p>为了模拟本工作中描述的基于物理的用例，在这些用例中，连续的安全信号是对状态的观察，应该受到约束，我们在Mujoco中设置了适当的模拟域（Todorov等人，2012）。在这些域中，对象位于某个可行有界区域。因此，每个约束都会降低对象到每个少数边界的距离。尽管任务定义的距离下限为零，在实践中，我们将其设置为一些较小的正值，以便在出现惯性的情况下，允许对避免行为进行松弛。在所有模拟中，<strong>如果违反约束，事件立即终止</strong>。这些条件与我们实际示例中的条件一致：数据中心冷却系统的公式和算法的最大温度设置不必是最初由操作者设置的值；可以设置一个更低、更保守的值，以允许一些松弛。事件终止对应于抢占，然后在每次达到保守温度上限时交换到某个备份启发式。</p><p>现在我们介绍两个新的Mujoco域：Ball和Spaceship，每个域由两个任务组成。球体和宇宙飞船的动力学分别由一阶和二阶微分方程控制。因此，它们是本文中感兴趣的系统的代表性领域。</p><a href=#71-ball-domain><h2 id=71-ball-domain><span class=hanchor arialabel=Anchor># </span>7.1 Ball Domain</h2></a><p>在球域中，目标是通过每4个时间步直接设置球的速度，使球尽可能靠近变化的目标位置（通常，典型扭矩控制器的工作频率低于环境的频率）。每episode最多持续30秒，在此期间，目标每隔2秒出现在一个新的、均匀随机的位置。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211117171728260.png width=auto alt=image-20211117171728260></p><p>这是为了安全层在球离开后一个立方体后开始纠正动作。让球位置、球速度和目标位置分别为$x_B、v_B、x_T \in \mathbb{R^d} $,</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211117172641072.png width=auto alt=image-20211117172641072></p><p>由于球的速度是直接控制的，其动力学由一阶微分方程控制。因此，代表了热转换等现象和一些已知的控制任务，如保持旋转发动机的速度，无论是经典蒸汽发动机还是现代车辆中的巡航控制（Sotomayor等人，2006）。</p><a href=#72-spaceship-domain><h2 id=72-spaceship-domain><span class=hanchor arialabel=Anchor># </span>7.2. Spaceship Domain</h2></a><p>在宇宙飞船领域，目标是通过控制飞船的推力引擎将飞船带到固定的目标位置。因此，与在球域中设置速度相反，我们在这里设置力。我们在这一领域的第一个任务是太空船走廊，在这里，安全区域被限定在两个无限平行的墙壁之间。我们的第二项任务，宇宙飞船竞技场，在安全区域的形状上与第一项不同；它以菱形的四面墙为界。图4给出了这两个任务的图像。</p><p>当三个事件中的一个发生时，情节就结束了：到达目标，宇宙飞船的船头碰到墙壁，或者达到时间限制。走廊的时间限制为15秒，竞技场的时间限制为45秒。与图4中的屏幕截图相关，飞船的初始化位置在走廊屏幕的最低三分之一部分和竞技场屏幕的最右三分之一部分均匀随机。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211117201615140.png width=auto alt=image-20211117201615140></p><p>在这个领域，state是宇宙飞船的位置和速度；action $a∈ [-1,1]^2$在前进/后退和右/左方向上驱动两个发动机；过渡过程受施加阻尼的物理规则控制；reward很少;仅在到达目标后才可获得1000分，否则为0；$\gamma=0.99$.与Ball一样，在（3）中的$C_i$选择中，与每面墙的距离较小。这是为了安全层在飞船用船头实际到达墙壁之前的几个时间步开始纠正动作。对于这两个任务，此间距均为0.05，其中，为了进行比较，Corridor task中墙之间的距离为1。</p><p>由于控制是通过力来实现的，宇宙飞船的动力学由一个二阶微分方程控制。因此，该域表示诸如泵送大量水进行冷却以及驱动诸如机械臂之类的对象的情况。</p><p>//<strong>7.3. Implementation Details</strong></p><a href=#74-safety-layer-versus-reward-shaping><h2 id=74-safety-layer-versus-reward-shaping><span class=hanchor arialabel=Anchor># </span>7.4. Safety Layer versus Reward Shaping</h2></a><p>在展示我们的安全层的性能之前，我们首先介绍一种确保安全的自然替代方法：通过人为地塑造奖励来操纵代理以避免不需要的区域。这可以通过将奖励设置为状态空间子集中的大负值来实现。在我们的例子中，这些区域是封闭边界的邻域。因此，<strong>为了进行比较，我们首先在没有安全层的DDPG上进行了一系列这样的奖励成形实验</strong>。我们将惩罚定为与最初奖励相同的级别：-1在Ball中，-1000在Spaceship。形成边界的边距是一个参数M，我们在每个任务中交叉验证该参数。我们试验了M的值∈ {0.08, 0.11, 0.14, 0.17, 0.2, 0.23}. 对于每一个M，我们运行DDPG，其中每一个训练集都有一个评估集，我们计算它是否因违反约束而终止。图5给出了每M中10个DDPG运行种子的ac累积约束违反的上下分位数的中值。
每个任务，我们用红色标记M的最佳选择。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211117203717637.png width=auto alt=image-20211117203717637></p><p>图5描述了为确保安全而进行奖励成形的缺点。第一个错误是未能实现我们的零约束违反目标；所有参数选择都导致episode因触发约束而终止。第二个缺点是难以选择正确的参数。从图中可以看出，对M的依赖没有明显的趋势；每项任务都有一个不同的“最佳值”，并且情节没有结构。</p><p>接下来，我们将安全层的性能与最佳奖励塑造选择的性能进行比较，并与完全没有奖励塑造的性能进行比较。也就是说，我们比较了以下备选方案：DDPG、DDPG+奖励塑造和DDPG+安全层。对于奖励成形，我们使用与最佳M相同的模拟结果（图5中的红色）。图6总结了比较结果。第一层给出了每个评估事件的折扣奖励总和，第二层提供了所有评估事件累积的违反约束的数量。plots显示了10个种子的中位数以及上下分位数。</p><p>从图6可以看出，安全层从未违反约束。这适用于四项任务中每项任务的所有10个种子。其次，安全层大大加快了融合。事实上，对于宇宙飞船来说，它是唯一能够实现收敛的算法。相反，如果没有安全层，大量的事件以约束违反而结束，并且往往无法达到收敛。这是由于我们任务的性质：频繁的跨边界事件终止会阻碍我们在稀疏奖励环境中的学习过程。然而，有了安全层，这些终止永远不会发生，允许代理像没有边界一样操纵。接下来，将进行以下特定领域的讨论。</p><p>在球域中，由于在1D任务中，目标始终位于两个可能方向之一，因此与3D任务相比，其奖励较少。这种简单的设置允许DDPG收敛到一个合理的返回值，即使没有奖励或安全层；在3D中，这种情况仅适用于种子的上分位数。在这两项任务中，通过奖励塑造获得了改善。
尽管如此， DDPG+安全层以更快的收敛速度获得最高的贴现收益。至于累积约束违规，由于DDPG收敛速度慢于DDPG+奖励成形，它也会在较高的值上稳定下来。尽管如此，DDPG+安全层满足了我们的安全目标，并保持了0个累积的约束违规。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211117214837839.png width=auto alt=image-20211117214837839></p><p>在宇宙飞船领域，奖励只在目标处获得一次，每次违反约束，宇宙飞船都会在远离目标的地方重新初始化。这种设置对于DDPG来说是致命的，因为DDPG无法在这两项任务中收敛到任何合理的策略。令人惊讶的是，奖励形成并没有带来任何改善，反而产生了负面影响：它导致了高度负的情景回报。另一方面，DDPG+安全层以极快的速度收敛到高性能安全策略。这种行为源于封闭区域类型的任务；在远离墙壁的同时进行探索，使飞船能够快速到达目标，然后学习如何到达目标。随着对安全的重新考虑，当DDPG和DDPG+奖励成形失败时，DDPG+安全层再次占上风，并保持0个累积约束违规。</p><a href=#8-discussion><h1 id=8-discussion><span class=hanchor arialabel=Anchor># </span>8. Discussion</h1></a><p>**在这项工作中，我们提出了一种基于状态的动作纠正机制，该机制实现了在智能体被约束到受限区域的任务中零违反约束的目标。**这与标准的奖励形成方案形成对比，后者未能实现上述目标。由此产生的收益不仅在于维护安全，还在于提高奖励方面的绩效。这表明我们的方法促进了更有效的探索——它指导了可行政策方向上的探索行动。由于我们的解决方案是独立的，并且直接应用于策略级别，因此它独立于所使用的RL算法，并且可以插入任何其他连续控制算法。</p><p>在这项工作中，我们涉及到一种在现实世界的关键系统中实施的抢占机制，该机制在临界情况下停止RL代理，并用安全操作启发式替换它。后者一直运行，直到系统回到远离运行限制的状态。这种启发式方法预计会比RL代理保守且效率较低；<strong>因此，我们工作的贡献也可以解释为通过最小化此类接管发生的次数来降低操作成本</strong>。</p><p>最后，与业界经常考虑的off-policy方法相比，我们的方法的一个优势是，不需要知道用于生成现有数据日志的行为策略。这要归功于我们的单步模型，我们根据随机交流产生的轨迹进行训练。这些轨迹始终在操作限制范围内（由于跨越限制时事件终止），并且与任何长期行为可以提升的特定政策无关。然而，由于其探索性，它们携带了丰富的信息。因此，在未来的工作中研究其他类型的训练前数据是非常有趣的，这些数据是特定现实领域的典型数据。</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://jieye-ericx.github.io/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Jieye ericx using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://jieye-ericx.github.io/>Home</a></li><li><a href=https://github.com/jieye-ericx>Github</a></li></ul></footer></div></div></body></html>