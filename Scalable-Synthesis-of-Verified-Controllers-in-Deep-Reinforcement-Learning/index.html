<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Scalable Synthesis of Verified Controllers in Deep Reinforcement Learning 深度强化学习中验证控制器的可伸缩综合
Abstract 最近，人们对设计核查有着极大的兴趣管理关键安全的学习型控制器（LEC）技术系统。鉴于控制此类控制器行为的神经策略不透明且缺乏可解释性，许多现有方法通过使用 shields（一种动态监测和维修机制）加强安全性能，确保LEC不会发出将违反预期的安全条件。然而，随着问题维度和客观复杂性的增加，这些方法已显示出明显的可扩展性限制 。
在本文中，我们提出了一种新的自动验证pipeline，它能够合成高质量的安全shield，即使问题域涉及数百个维度，或者所需目标涉及随机扰动、liveness因素和其他复杂的非功能特性。
我们的主要见解涉及将安全验证与神经控制器培训分离，并使用预先计算的验证安全屏蔽来约束培训过程。在一系列高维深RL基准上的实验结果证明了我们方法的有效性。
1 Introduction 深度强化学习（DRL）已被证明是为各种网络物理系统（CPS）实施自主控制器的强大工具[20,13,24]。由于这些学习型控制器（LEC）旨在在安全关键环境中运行，因此最近人们对开发验证方法产生了极大兴趣，以确保其行为符合预期的安全特性[14,31,2,34,3,19]。虽然这些不同的方法都为控制器的安全性提供了强有力的保证，但在问题维度和客观复杂性方面，扩展它们的技术已被证明是具有挑战性的。试图静态验证神经控制器始终保持所需安全保证的方法可能会拒绝偶尔违反安全性的高质量控制器[14,31]。 或者，动态监控控制器动作的技术，当这些动作可能导致不安全状态时触发安全防护罩，要求防护罩的行为与神经控制器的行为紧密一致，神经控制器的行为通常除了安全性外，还接受各种性能目标的培训[3,34]。平衡这些相互竞争的目标，一方面确保安全，另一方面最大化目标回报，这本身就带来了一系列挑战，可能会损害可验证性、性能和安全性。"><meta property="og:title" content><meta property="og:description" content="Scalable Synthesis of Verified Controllers in Deep Reinforcement Learning 深度强化学习中验证控制器的可伸缩综合
Abstract 最近，人们对设计核查有着极大的兴趣管理关键安全的学习型控制器（LEC）技术系统。鉴于控制此类控制器行为的神经策略不透明且缺乏可解释性，许多现有方法通过使用 shields（一种动态监测和维修机制）加强安全性能，确保LEC不会发出将违反预期的安全条件。然而，随着问题维度和客观复杂性的增加，这些方法已显示出明显的可扩展性限制 。
在本文中，我们提出了一种新的自动验证pipeline，它能够合成高质量的安全shield，即使问题域涉及数百个维度，或者所需目标涉及随机扰动、liveness因素和其他复杂的非功能特性。
我们的主要见解涉及将安全验证与神经控制器培训分离，并使用预先计算的验证安全屏蔽来约束培训过程。在一系列高维深RL基准上的实验结果证明了我们方法的有效性。
1 Introduction 深度强化学习（DRL）已被证明是为各种网络物理系统（CPS）实施自主控制器的强大工具[20,13,24]。由于这些学习型控制器（LEC）旨在在安全关键环境中运行，因此最近人们对开发验证方法产生了极大兴趣，以确保其行为符合预期的安全特性[14,31,2,34,3,19]。虽然这些不同的方法都为控制器的安全性提供了强有力的保证，但在问题维度和客观复杂性方面，扩展它们的技术已被证明是具有挑战性的。试图静态验证神经控制器始终保持所需安全保证的方法可能会拒绝偶尔违反安全性的高质量控制器[14,31]。 或者，动态监控控制器动作的技术，当这些动作可能导致不安全状态时触发安全防护罩，要求防护罩的行为与神经控制器的行为紧密一致，神经控制器的行为通常除了安全性外，还接受各种性能目标的培训[3,34]。平衡这些相互竞争的目标，一方面确保安全，另一方面最大化目标回报，这本身就带来了一系列挑战，可能会损害可验证性、性能和安全性。"><meta property="og:type" content="website"><meta property="og:image" content="https://jieye-ericx.github.io/icon.png"><meta property="og:url" content="https://jieye-ericx.github.io/Scalable-Synthesis-of-Verified-Controllers-in-Deep-Reinforcement-Learning/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="Scalable Synthesis of Verified Controllers in Deep Reinforcement Learning 深度强化学习中验证控制器的可伸缩综合
Abstract 最近，人们对设计核查有着极大的兴趣管理关键安全的学习型控制器（LEC）技术系统。鉴于控制此类控制器行为的神经策略不透明且缺乏可解释性，许多现有方法通过使用 shields（一种动态监测和维修机制）加强安全性能，确保LEC不会发出将违反预期的安全条件。然而，随着问题维度和客观复杂性的增加，这些方法已显示出明显的可扩展性限制 。
在本文中，我们提出了一种新的自动验证pipeline，它能够合成高质量的安全shield，即使问题域涉及数百个维度，或者所需目标涉及随机扰动、liveness因素和其他复杂的非功能特性。
我们的主要见解涉及将安全验证与神经控制器培训分离，并使用预先计算的验证安全屏蔽来约束培训过程。在一系列高维深RL基准上的实验结果证明了我们方法的有效性。
1 Introduction 深度强化学习（DRL）已被证明是为各种网络物理系统（CPS）实施自主控制器的强大工具[20,13,24]。由于这些学习型控制器（LEC）旨在在安全关键环境中运行，因此最近人们对开发验证方法产生了极大兴趣，以确保其行为符合预期的安全特性[14,31,2,34,3,19]。虽然这些不同的方法都为控制器的安全性提供了强有力的保证，但在问题维度和客观复杂性方面，扩展它们的技术已被证明是具有挑战性的。试图静态验证神经控制器始终保持所需安全保证的方法可能会拒绝偶尔违反安全性的高质量控制器[14,31]。 或者，动态监控控制器动作的技术，当这些动作可能导致不安全状态时触发安全防护罩，要求防护罩的行为与神经控制器的行为紧密一致，神经控制器的行为通常除了安全性外，还接受各种性能目标的培训[3,34]。平衡这些相互竞争的目标，一方面确保安全，另一方面最大化目标回报，这本身就带来了一系列挑战，可能会损害可验证性、性能和安全性。"><meta name=twitter:image content="https://jieye-ericx.github.io/icon.png"><title>ericx 's 数字花园</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://jieye-ericx.github.io//icon.png><link href=https://jieye-ericx.github.io/styles.80333fa2099c0bee674efa435fde378c.min.css rel=stylesheet><link href=https://jieye-ericx.github.io/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://jieye-ericx.github.io/js/darkmode.48459b7116d092b4e98d2cab704cad80.min.js></script>
<script src=https://jieye-ericx.github.io/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://jieye-ericx.github.io/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://jieye-ericx.github.io/",fetchData=Promise.all([fetch("https://jieye-ericx.github.io/indices/linkIndex.d1ffeb07c444307ac70f84984f3aa0d9.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://jieye-ericx.github.io/indices/contentIndex.2fd0bb3e22e4db8398171e719cba2382.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://jieye-ericx.github.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://jieye-ericx.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/jieye-ericx.github.io\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=jieye-ericx.github.io src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://jieye-ericx.github.io/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://jieye-ericx.github.io/>ericx 's 数字花园</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated
Unknown
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/Scalable%20Synthesis%20of%20Verified%20Controllers%20in%20Deep%20Reinforcement%20Learning.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#abstract>Abstract</a></li><li><a href=#1-introduction>1 Introduction</a></li><li><a href=#2-motivating-example-and-overview>2 Motivating Example and Overview</a><ol><li><a href=#21-倒立摆>2.1 倒立摆</a></li><li><a href=#22-提取已验证策略的挑战>2.2 提取已验证策略的挑战</a></li><li><a href=#23-verifying-a-linear-controller-family><strong>2.3 Verifying a Linear Controller Family</strong></a></li><li><a href=#24-safety-planner-as-shield>2.4 Safety Planner as Shield</a></li></ol></li><li><a href=#3-preliminaries>3 Preliminaries</a><ol><li><a href=#31-markov-decision-process-and-reinforcement-learning>3.1 Markov Decision Process and Reinforcement Learning</a></li><li><a href=#32-properties-and-rewards>3.2 Properties and Rewards</a></li><li><a href=#33-controller-types>3.3 Controller Types</a></li><li><a href=#34-随机转移系统>3.4 随机转移系统</a></li></ol></li><li><a href=#4-approach>4 Approach</a><ol><li><a href=#41-safety-probability-of-reachable-set>4.1 Safety Probability of Reachable Set</a></li><li><a href=#42-验证线性控制器族>4.2 验证线性控制器族</a></li><li><a href=#43-shield>4.3 shield</a></li></ol></li><li><a href=#5-试验>5 试验</a></li><li><a href=#6-相关工作>6 相关工作</a></li><li><a href=#7-结论>7 结论</a></li></ol></nav></details></aside><a href=#scalable-synthesis-of-verified-controllers-in-deep-reinforcement-learning><h1 id=scalable-synthesis-of-verified-controllers-in-deep-reinforcement-learning><span class=hanchor arialabel=Anchor># </span>Scalable Synthesis of Verified Controllers in Deep Reinforcement Learning</h1></a><p>深度强化学习中验证控制器的可伸缩综合</p><a href=#abstract><h2 id=abstract><span class=hanchor arialabel=Anchor># </span>Abstract</h2></a><p>最近，人们对设计核查有着极大的兴趣管理关键安全的学习型控制器（LEC）技术系统。鉴于控制此类控制器行为的神经策略不透明且缺乏可解释性，许多现有方法通过使用 shields（一种动态监测和维修机制）加强安全性能，确保LEC不会发出将违反预期的安全条件。然而，随着问题维度和客观复杂性的增加，这些方法已显示出明显的可扩展性限制 。</p><p><strong>在本文中，我们提出了一种新的自动验证pipeline，它能够合成高质量的安全shield，即使问题域涉及数百个维度，或者所需目标涉及随机扰动、liveness因素和其他复杂的非功能特性。</strong></p><p>我们的主要见解涉及<strong>将安全验证与神经控制器培训分离，并使用预先计算的验证安全屏蔽来约束培训过程</strong>。在一系列高维深RL基准上的实验结果证明了我们方法的有效性。</p><a href=#1-introduction><h2 id=1-introduction><span class=hanchor arialabel=Anchor># </span>1 Introduction</h2></a><p>深度强化学习（DRL）已被证明是为各种网络物理系统（CPS）实施自主控制器的强大工具[20,13,24]。由于这些学习型控制器（LEC）旨在在安全关键环境中运行，因此最近人们对开发验证方法产生了极大兴趣，以确保其行为符合预期的安全特性[14,31,2,34,3,19]。<strong>虽然这些不同的方法都为控制器的安全性提供了强有力的保证，但在问题维度和客观复杂性方面，扩展它们的技术已被证明是具有挑战性的</strong>。试图静态验证神经控制器始终保持所需安全保证的方法可能会拒绝偶尔违反安全性的高质量控制器[14,31]。
或者，<strong>动态监控控制器动作的技术，当这些动作可能导致不安全状态时触发安全防护罩，要求防护罩的行为与神经控制器的行为紧密一致，神经控制器的行为通常除了安全性外，还接受各种性能目标的培训[3,34]。平衡这些相互竞争的目标，一方面确保安全，另一方面最大化目标回报，这本身就带来了一系列挑战，可能会损害可验证性、性能和安全性。</strong></p><p>本文提出了一种新的学习和验证管道来解决这些缺点。与其他基于shield的方法类似[2,3,34]，我们的工作没有直接静态验证神经控制器。相反，<strong>它根据已知的系统动力学和问题规范生成一系列线性控制器</strong>；这个系列共同充当一个盾牌，可以动态地强制执行系统的安全操作。控制器使用线性二次调节器（LQR）进行合成，该调节器基于以与状态和控制输入相关的不同参数值表示的成本函数产生不同的实例化。我们的验证策略搜索满足总体期望安全属性的不同控制器实例的组合；此搜索由为此目的定制的概率可达性分析驱动。</p><p>此聚合中的不同控制器针对可能的系统轨迹中的不同时间步进行了优化。然而，并非所有不同控制器的组合都能被验证为安全的。尽管如此，即使是可能不安全的组合也可用于为其安全效力提供强有力的概率保证，验证者可将其作为后续搜索的一部分加以利用。**值得注意的是，我们的技术独立于神经控制器行为和神经网络内部结构来考虑安全验证，因此能够在客观复杂性和问题维度方面实现可伸缩性。**例如，在我们的实验中，我们成功地验证了超过800维的CPS基准，这一规模远远高于现有方法的报告。</p><blockquote><p>CPS</p></blockquote><p>由于这些线性控制器仅基于安全考虑而生成，因此它们不打算作为控制系统实际运行的主要机制，这还必须考虑其他性能相关目标。为了确保神经控制器的安全性，考虑到最佳执行，采用以安全为唯一目标开发的综合线性控制器系列，我们的管道还将控制器系列作为DRL培训过程的一部分进行集成。这种集成使神经控制器的训练过程偏向于已验证的安全特性。然而，由于神经控制器目标不会对训练过程施加安全要求，我们的技术允许学习具有复杂目标的神经策略（例如，在特定时间顺序中实现不同的目标，确保各种活性属性等）。**一个训练有素的神经控制器通常表现良好，但不能保证安全。然而，可以通过将神经控制器与经验证的线性控制器系列结合使用来加强安全性。**因此，我们的工作为基于DRL的学习型控制器演示了一个完全自动化的安全验证管道，该控制器以复杂的目标进行培训，具有非常积极的可扩展性特征。</p><p>论文的其余部分组织如下。在第2节中，我们提供了额外的动机和我们的技术的非正式概述。3提供了我们方法中使用的必要背景材料。4形式化了我们的方法，并给出了我们的验证算法。5介绍了一系列复杂CPS基准的详细实验研究；这些基准具有高维性，具有导致随机非确定性行为的非平凡噪声项，并且使用反映重要非功能特性（如liveness）的目标函数进行培训。相关工作和结论见第节。第6节和第7节。</p><a href=#2-motivating-example-and-overview><h2 id=2-motivating-example-and-overview><span class=hanchor arialabel=Anchor># </span>2 Motivating Example and Overview</h2></a><p>我们的整体方法，如图1所示，</p><img src=../../pics/image-20211220155028969.png alt=image-20211220155028969 style=zoom:50%><p>通过在控制神经控制器与其环境之间交互的循环中插入安全规划器，确保强化学习系统的安全。规划器<strong>以神经网络控制器和线性控制器族作为输入</strong>。选择器φ在模拟轨迹的每k步从线性控制器族中拾取一个线性控制器。给定由φ选取的线性控制器、初始状态集的分布和（有界）噪声的特征，安全规划器计算控制器在轨迹的下一个k步中可到达的所有状态，以及这些状态下着陆概率密度的下限。这些状态包括选定控制器的有界可达集。
我们的验证算法旨在生成一个最优选择器φopt，以最大化所选线性策略的可达集与安全状态空间之间的重叠。<strong>安全屏蔽以神经控制器、选定的线性控制器和动力学的k步可达集作为输入</strong>。它监控神经网络的行为，并确保提议的行为在线性策略的可到达集内产生状态，在不可到达时应用安全线性控制器提议的行为。由于我们将所选线性控制器的可达集与安全状态空间的重叠部分最大化，因此我们可以提供高概率且通常精确的安全保证。为了说明我们的方法，我们认为我们的技术应用在一个简单的但不平凡的，问题自主控制的倒立摆摆，安全要求是摆仍然直立。</p><a href=#21-倒立摆><h3 id=21-倒立摆><span class=hanchor arialabel=Anchor># </span>2.1 倒立摆</h3></a><p>我们考虑的倒立摆模型被广泛用于评估强化学习和连续控制任务。该系统由图2所示的ODE控制。我们假设倒立摆从初始状态S0的均匀分布开始，</p><img src=../../pics/image-20211220235943222.png alt=image-20211220235943222 style=zoom:50%><p>摆锤的质量为m，长度为l。系统状态为s=[θ，ω]，其中θ为角度，ω为角速度。一个连续的控制动作试图将摆锤保持在直立位置。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211221000313979.png width=auto alt=image-20211221000313979></p><p>我们希望强制执行的全局安全特性是，钟摆永远不会下降，并且钟摆的速度不应太大，以预先确定的间隔存在。我们将摆Su的一组不安全状态定义为</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211221000603991.png width=auto alt=image-20211221000603991></p><p>然而，控制器的性能目标是保持摆锤摆动的简单活性特性。我们可以将所需的 liveness和安全性编码为一个用于训练神经控制器的奖励函数：</p><blockquote><p>liveness到底是啥</p></blockquote><p><img src=https://jieye-ericx.github.io//../../pics/image-20211221002415648.png width=auto alt=image-20211221002415648></p><p>t是一个时间步长。|˙θt |定义摆锤的位置变化。因为我们希望pendulum保持摆动，所以我们寻求最大化这一项的累积总量。
(π2 −|θt |）和（π2）−|ωt |）是到不安全区域Su的L1距离。当系统状态为安全时，minn（π2−|θt |）和minn（π2）−|ωt |）将为0；如果违反了安全属性，则在神经网络训练期间，将以负奖励惩罚该trajec  tory。深度强化学习算法旨在使最终控制器πnn的回报最大化。</p><p>该摆模型的动力学如图2所示。将ml2处ω˙t=gl sin（θt）+中的sin（θ）替换为θ，得到线性化（近似）摆模型。为简单起见，线性化的˙s可以写成˙st=Ast+Bat，其中</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211221131139398.png width=auto alt=image-20211221131139398></p><p>每∆t时间间隔，我们更新系统状态一次，给出转换函数$s_{t+1}=s_t+∆t·\dot{s}<em>t$。我们还将随机性与有界噪声项w结合起来$w\in[\epsilon_l,\epsilon_u]$.然后，转换函数变为$s</em>{t+1}=s_t+\triangle t \cdot \dot{s_t}+w $事实上，虽然我们考虑这里的线性化模型，只要W足够大，也可以容纳由相应的非线性模型产生的所有可能的转变。</p><a href=#22-提取已验证策略的挑战><h3 id=22-提取已验证策略的挑战><span class=hanchor arialabel=Anchor># </span>2.2 提取已验证策略的挑战</h3></a><p><strong>先前在基于 shield的DRL验证中的工作[34,3,33]通过模仿神经网络提取线性控制器，安全性是学习控制器的主要目标。然而，在实践中，强化学习任务除了安全性外，还需要考虑不同的非功能特性。</strong>
在我们的示例中，我们希望同时最大化摆锤的速度，同时保持安全性。在表1中，我们表明，从使用不同奖励训练的神经网络中提取的安全控制器会影响其可验证性。在这个实验中，我们为摆锤任务设置了两个不同的奖励函数。在第一个设置中，我们只考虑方程1的安全部分；另一种是既考虑活力又考虑安全性。对于每个奖励函数，**我们提取50个不同的线性控制器，并使用[34]中提供的验证工具对其进行验证。**验证列显示了50个被认为是安全的蒸馏控制器中的控制器数量。这些结果表明，当考虑这两个属性时，安全验证变得更具挑战性，即使对于像我们的运行示例这样简单的问题也是如此。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211221132512843.png width=auto alt=image-20211221132512843></p><a href=#23-verifying-a-linear-controller-family><h3 id=23-verifying-a-linear-controller-family><span class=hanchor arialabel=Anchor># </span><strong>2.3 Verifying a Linear Controller Family</strong></h3></a><p>首先，我们用LQR(线性二次型调节器)建立了一个线性控制器族。线性控制器系列的目标是为摆锤模型提供一个安全的控制器。帮助确保安全的自然方法是定义具有强稳定性约束的控制器。我们使用LQR生成一批具有不同Q和R值的线性策略，以惩罚状态变量和控制信号的变化。对于<strong>摆锤示例</strong>，我们将线性控制器的数量∏kl |=3。Q和R是对角矩阵。Q的对角线元素的值从（0,20）中随机抽样，R的对角线元素的值从（0,10）中抽样。<img src=../../pics/image-20211221132901787.png alt=image-20211221132901787 style=zoom:50%></p><p>给定一个线性控制器族∏kl，我们希望找到这些政策的组合，共同为钟摆提供安全保障。我们合成一个选择器φ来构建这样的组合。首先，我们将一个线性控制器的选择间隔固定为<em>k</em>步。</p><p>也就是说，选择器φ每k步识别一个线性控制器。我们认为执行的长度是m。因此，我们需要选择线性策略M/k的次数。如果在本例中设置M=200和k=100，则需要为给定的模拟进行2次选择。给出一个$∏^k_l=（K1，K2，K3）$，$\phi (\Pi^k_l,t)$的例子返回基于步骤t的线性控制器，例如，当t=99时，$\phi (\Pi^k_l,t)$=K1，当t=100时，$\phi (\Pi^k_l,t)$=K2。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211221134229700.png width=auto alt=image-20211221134229700></p><p>图三，说明不同线性控制器如何捕获不同可达状态的图示。在步骤100，控制器K2和K3生成的可达集违反安全属性；在步骤200，使用K1为前100个步骤生成的可达集，我们发现K2在步骤200生成的可达集是K1和K3生成的可达集的子集，因此是要使用的首选控制器。</p><p><strong>选择器φ的任务是提供高概率的安全控制器。为了综合使安全概率最大化的最优选择器φopt，我们需要分析不同策略组合的可达性。目标是在步骤t为可达集$R^s_t$中的安全状态百分比$\hat{p_t}$提供下限。</strong>
假设可达集$R^s_t$与$s_u$没有重叠；在这种情况下，$\hat{p_t}$是1。$R^s_t$中的s上标表示随机性下的可达集。然后将最佳选择器定义为：</p><img src=../../pics/image-20211221141036244.png alt=image-20211221141036244 style=zoom:50%><p>参数化$\hat{p_t}(\Pi^k_l,\phi)$定义当使用φ从线性控制器系列$\Pi^k_l$中选择的控制器生成可达集Rst时，步骤t处安全过渡概率的下限. 因为可能的选择器的数量是${|\Pi^k_l|^{\frac{M}{k}}}$,**当$|\Pi^k_l|$和$\frac{M}{k}$增加时，很难检查φ的所有可行情况。因此，我们还提供了几种策略来减少可能的搜索空间。**例如，在图3中，包含K2和K3的所有实例都从搜索空间中剪除Ω 在步骤100，因为它们的可达集与Su重叠，而K1的可达集与Su不重叠，K1K1、K1K3是从Ω 在步骤200，因为K2的可达集是K1和K3的可达集的子集。</p><blockquote><p>对于上述$\Pi^k_l$的实例，φopt=（K1，K2），<img src=../../pics/image-20211221142502170.png alt=image-20211221142502170 style=zoom:50%>
φopt的实例化确保了系统在M=200步时的安全性，并且我们说选择器及其相应的线性控制器族已得到验证。</p></blockquote><a href=#24-safety-planner-as-shield><h3 id=24-safety-planner-as-shield><span class=hanchor arialabel=Anchor># </span>2.4 Safety Planner as Shield</h3></a><p>最佳选择器ωopt确保了钟摆相对于安全性能的稳定性，但不考虑自治控制器应该表现的其它期望特性，例如活性。为了将最优安全控制器与启用DRL的神经控制器相结合，我们的验证方法还包括一个集成这些不同目标的安全规划器。</p><p>考虑两个具有不同噪声项的系统。第一个系统E为应用程序建模。它有噪声项$\hat{w}∈[\epsilon^1_l ,\epsilon^1_u]$。第二个系统$E&rsquo;$用于验证,它有噪声项$\hat{w&rsquo;}∈[\epsilon^2_l ,\epsilon^2_u]$。验证环境E&rsquo;考虑的噪声应大于E，即$\epsilon^1_l > \epsilon^2_l$,$\epsilon^1_u &lt; \epsilon^2_u$。</p><p>假设在步骤t的状态转换处，系统状态为st，神经网络提供的动作为at，经验证的线性控制器系列提供的动作为a&rsquo;t.在不考虑噪声项的情况下，施加at和a&rsquo;t得到$\hat{s}<em>{t+1}$和$\hat{s}&rsquo;</em>{t+1}$。为了考虑噪声，将E和E&rsquo;下的所有可达状态分别表示为绿色区域$ Region(\hat{s}<em>{t+1},\hat{w})$和蓝色区域$ Region(\hat{s}&rsquo;</em>{t+1},\hat{w}&rsquo;)$。如图4(a)所示，当区域$ Region(\hat{s}<em>{t+1},\hat{w})$⊆$ Region(\hat{s}&rsquo;</em>{t+1},\hat{w}&rsquo;)$，神经网络采取的行动在安全控制器提供的安全保证下运行。也就是说，<strong>如果我们在步骤t+1之后使用安全控制器接管神经网络，我们可以保证系统不会违反安全控制器维护的安全属性。</strong></p><p>给定一个安全控制器，该控制器经验证可安全运行M步,保证初始状态空间R中的任何状态在至少<em>M</em>步中是安全的,因此，在图4的（b）中，尽管神经控制器的可到达集合位于区域$ Region(\hat{s}&rsquo;<em>{t+1},\hat{w}&rsquo;)$之外，但是当区域$Region(\hat{s}</em>{t+1},\hat{w})\nsubseteq Region(\hat{s}&rsquo;_{t+1},\hat{w}&rsquo; )\cup R_0$ 时不需要干预,st位于确定的安全区域之外。在这种情况下，我们需要采取行动a&rsquo;t，以确保st保持安全。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211221182831975.png width=auto alt=image-20211221182831975></p><p>图四，三种可能的过渡情况。绿色阴影部分表示步骤t+1处$\pi_{nn}$的可达集，蓝色阴影部分表示已验证的可达集。R0是系统的初始状态空间。s0∈R0。ab不需要任何干预，而c需要。</p><a href=#3-preliminaries><h2 id=3-preliminaries><span class=hanchor arialabel=Anchor># </span>3 Preliminaries</h2></a><a href=#31-markov-decision-process-and-reinforcement-learning><h3 id=31-markov-decision-process-and-reinforcement-learning><span class=hanchor arialabel=Anchor># </span>3.1 Markov Decision Process and Reinforcement Learning</h3></a><a href=#32-properties-and-rewards><h3 id=32-properties-and-rewards><span class=hanchor arialabel=Anchor># </span>3.2 Properties and Rewards</h3></a><p>应用程序的安全属性要求控制器操作产生的可能状态的可到达集位于安全区域。对于任何状态S，设n为S中的维数，n=| S |。让我们$s\in \mathbb{R}^n$，L&lt;s&lt;U，其中L∈Rn和U∈Rn.
我们将安全属性编码为$R_{safe}$。现在，假设系统到达状态$x\in \mathbb{R}^n$,</p><img src=../../pics/image-20211221192027384.png alt=image-20211221192027384 style=zoom:50%><p>我们期望的活性属性要求系统状态保持变化。状态的某些维度描述了这些状态变化，例如，摆锤状态的第二维度是其角速度，其定义为摆锤角旋转的变化率（状态的第一维度）。我们识别那些描述状态变化的维度 $Dim_{live}$，并将它们编码为$R_{live}$。基于</p><img src=../../pics/image-20211221192325684.png alt=image-20211221192325684 style=zoom:50%>
<img src=../../pics/image-20211221192345530.png alt=image-20211221192345530 style=zoom:50%><p>$T\in \mathbb{R}^n$是阈值向量。如果一个状态维度的绝对值大于这个值，我们给予正奖励。最大化这个奖励意味着我们希望尽可能多的维度达到阈值。</p><a href=#33-controller-types><h3 id=33-controller-types><span class=hanchor arialabel=Anchor># </span>3.3 Controller Types</h3></a><p>我们考虑两种控制器。我们期望性能控制器$\pi_{nn}$是一个通过强化学习算法训练的神经网络控制器。根据使用的算法，πnn可以是确定性的，也可以是随机的。给定系统的状态s，$\pi_{nn}（s_t）$在状态st处输出一个动作$a_t$。</p><p>确定性线性控制器族$∏^k_l$是一组线性策略$K_i\in \mathbb{R}^{n×n}$.$∏^k_l$上标k是一个时间单位。在每个k单位时间间隔
<img src=https://jieye-ericx.github.io//../../pics/image-20211221193702757.png width=auto alt=image-20211221193702757>中，一个选择器$\phi (\Pi^k_l,t)$将选择$\Pi^k_l$中的线性控制器来预测该时间间隔内的动作。</p><a href=#34-随机转移系统><h3 id=34-随机转移系统><span class=hanchor arialabel=Anchor># </span>3.4 随机转移系统</h3></a><p><strong>随机线性转移系统</strong>过渡概率函数P建模为：</p><img src=../../pics/image-20211221195748724.png alt=image-20211221195748724 style=zoom:50%><p>其中st是状态向量，at是动作。矩阵A和B是线性化动力学的两个矩阵，它计算$\dot{s}_t$。随机性是通过在每个变换中添加噪声（或误差）项w引入的。在本文中，我们考虑有界噪声，即$\exist \epsilon_l,\epsilon_u\in \mathbb{R}^n,w\in[\epsilon_l,\epsilon_u]$.</p><p><strong>随机线性时变（Time-Variant）转移系统</strong></p><p>我们还研究了随机线性时变转移系统的验证算法。我们将随机线性时变过渡系统描述为:</p><img src=../../pics/image-20211221200757725.png alt=image-20211221200757725 style=zoom:50%><p>这里，At和Bt是随时间变化的矩阵。随机线性转移系统是At和Bt为常数时的一个特例。尽管我们的验证算法不直接支持非线性动力学，但我们注意到，存在一系列工作[29]，展示了如何使用（时变）线性系统近似此类动力学。将这些方法应用到我们的环境中，可以在实践中推广我们的技术。我们还注意到，近似误差可被视为随机噪声的一部分，因此即使在近似后也能确保可靠性。</p><p><strong>可达集</strong></p><p>系统的初始状态分布由下界$\mathcal{L}<em>{s0}$和上界$\mathcal{U}</em>{s0}$限定，其中s0为初始状态。如果我们现在有一个线性控制器由$\phi (\Pi^k_l,0)$选择,步骤1的无噪声状态$\hat{s}_1$为</p><img src=../../pics/image-20211221201901114.png alt=image-20211221201901114 style=zoom:50%>
<img src=../../pics/image-20211221202917317.png alt=image-20211221202917317 style=zoom:50%>
<a href=#4-approach><h2 id=4-approach><span class=hanchor arialabel=Anchor># </span>4 Approach</h2></a><a href=#41-safety-probability-of-reachable-set><h3 id=41-safety-probability-of-reachable-set><span class=hanchor arialabel=Anchor># </span>4.1 Safety Probability of Reachable Set</h3></a><p>我们希望计算连续空间中可达集的安全概率，因此需要找到可达集$R^s_t$的概率密度函数（PDF）。假设$f_t(s)$,$D_{s0}$是均匀分布，w服从$D_{s0}$。根据等式2，在任何给定步骤的可达状态分布是$D_{s0}$和$D_{w}$的线性组合。</p><p>给定一个可达集$R^s_t$,我们希望描述其安全概率pt的分布，这是一种表示$R^s_t$“表面”安全部分的度量：<img src=../../pics/image-20211221205107786.png alt=image-20211221205107786 style=zoom:50%></p><img src=../../pics/image-20211221205134008.png alt=image-20211221205134008 style=zoom:50%><p>在附录E.2中得到了证明，对于任何有界噪声，上界由定理3给出。当噪声均匀时，我们提供了推论1中所示的更紧的界。</p><img src=../../pics/image-20211221205810620.png alt=image-20211221205810620 style=zoom:50%>
<a href=#42-验证线性控制器族><h3 id=42-验证线性控制器族><span class=hanchor arialabel=Anchor># </span>4.2 验证线性控制器族</h3></a><p>我们为算法1中的线性控制器系列和附录a中的完整算法提供了验证算法的草图。我们算法的输入包括最大执行步骤数M、线性控制器系列∏kl和选择器的搜索空间Ω. 该算法的主体在搜索空间中迭代选择器Ω 而在运行时减少搜索空间。在第2-3行中，我们计算了第3.4节中描述的随机可达集Rsm和第4.1节中描述的安全概率下界。</p><p>验证算法的关键组件如图5所示。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211222143050389.png width=auto alt=image-20211222143050389></p><p>搜索空间Ω 是一棵树，因为我们每k步在$\Pi_l^k$中选择不同的控制器。树中的每个节点表示从step(d-1)×k到step d×k-1中选择的一个控制器，其中d是对应树节点的深度。我们定义了一个函数prefixName（φ，m)，它表示一组与φ具有相同祖先的选择器，直到深度m。此函数非常有用，因为我们通常可以从中剪切一组具有相同祖先的选择器Ω.</p><p>在控制器选择过程中，我们使用了<strong>三种策略</strong>来减少搜索空间。</p><p>策略1基于记录的最佳累积安全概率$\mathcal{L}<em>{opt}$,我们使用$m-\mathcal{L}$表示累积不安全概率上限，其中m是当前运行的时间步长，L是直到步骤m的累积安全概率下限。当我们发现$m-\mathcal{L}$大于最佳累积不安全概率上界$M-\mathcal{L}</em>{opt}$，随着阶跃的增加，我们不会发现更好的累积安全下限。因此，我们可以剪切选择器$ PrefixSame(\phi,m)$。</p><p>策略2通过比较两个选择器之间的可达集来减少搜索空间。例如，在图5的策略2中，黄色和绿色节点的可到达集合分别为黄色和绿色。黄色可达集是绿色可达集的子集。因此，选择黄色节点比选择绿色节点更安全。在这种情况下，可以删除将绿色节点作为祖先共享的所有选择器。</p><p>策略3计算单个选择器的不变量。例如，在图5的策略3中，黄色节点和紫色节点属于同一选择器。黄色节点的可达集是紫色节点的可达集的子集。因此，该选择器的可达集将随着时间的推移而缩小，因此用作不变量。如果在我们发现这个不变量之前没有安全冲突，那么可达集将永远不会与不安全区域相交，因此我们可以直接返回一个经过验证的选择器。</p><a href=#43-shield><h3 id=43-shield><span class=hanchor arialabel=Anchor># </span>4.3 shield</h3></a><p>我们使用由算法1计算的控制器族上的验证选择器$\phi_{opt}$定义的shield来监控和确保神经网络控制器$\pi_{nn}$的安全性。算法2描述了屏蔽的操作。给定神经网络控制器$\pi_{nn}$，线性控制器族$\Pi_l^k$，合成选择器$\phi_{opt}$，用于验证的噪声$\hat{w}&rsquo;$,系统的真实噪声$\hat{w}$，以及步骤t的状态$s_t$，算法2返回屏蔽动作$\hat{a}_t$。</p><p><strong>Region</strong>是计算给定状态s和噪声w允许的所有可能状态的函数，其中$w\in [\epsilon_l,\epsilon_u]$,$Region(s,w)={s&rsquo;|s&rsquo;-s\in [\epsilon_l,\epsilon_u]}
$,</p><p>DRL训练过程中，神经控制器预测动作并在系统上执行这些动作。神经控制器从训练期间与不同动作相关的奖励中学习。为了在训练期间应用屏蔽，在执行操作之前，我们应用算法2来强制执行所选操作是安全的。这种方法的一个特别重要的例子是当$\sum_{i=1}^M \hat{p}_t=M$时，如定理1所形式化。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211222150202741.png width=auto alt=image-20211222150202741></p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211222154922821.png width=auto alt=image-20211222154922821></p><a href=#5-试验><h2 id=5-试验><span class=hanchor arialabel=Anchor># </span>5 试验</h2></a><p>我们已经将验证算法应用于各种随机过渡系统，其<strong>维数范围为2到896</strong>。我们将安全约束与每个基准相关联，合成一个经过验证的线性控制器系列，以确保这些特性保持不变，并使用该系列来训练具有额外性能（也称为活性）目标的神经网络。由此产生的系统由一个性能敏感的神经网络组成，该神经网络经过安全约束意识的训练，再加上一个由线性控制器家族表示的安全屏蔽。我们使用最近策略优化（PPO）[24]来训练神经网络控制器，这是一种用于强化学习的标准训练技术。</p><p><strong>基准</strong>我们在24个基准上评估我们的算法。有6个基准Pendulum Cartpole, Cartpole, Carplatoon, and Helicopter, DroneInWind
DroneInWind环境是时变的，因为我们允许环境中的风的角度和强度随时间变化。我们还考虑堆栈环境变量的这些基准命名为N-B，以评估我们的方法的有效性随着维数的增加；这里，n是叠加深度，B是六个基准之一。为了使实验不只是利用为基本程序发现的安全特性，每个堆叠层都定义了一个随机注入的偏移量，使每个堆叠元素都不同于其他元素。附录B中提供了这些基准的详细信息。</p><p><strong>safe training</strong></p><p>我们使用经验证的线性控制器系列获得的安全保证来培训神经控制器。表2显示了我们的方法的有效性。</p><p>首先，尽管LQR控制器被证实是安全的，但考虑到性能目标，它的性能可能会很差。
然而，将其用作高性能神经控制器的shield可以实现性能和安全效益。表2总结了不同控制器的奖励比较。该表显示了屏蔽控制器相对于基本PPO算法的标准化性能特征，无需增加安全规划器和LQR系列（无性能目标）。因此，</p><p>在标有Shield/PPO的列中，大于1的数字表明，与安全计划员一起培训的控制器优于仅接受PPO培训的算法。类似的解释适用于标有Shield/LQR的列。虽然使用安全和性能（Shield）培训的控制器的性能优于仅了解安全（LQR）的控制器并不令人惊讶，但值得注意的是，在24个基准中的19个基准上，屏蔽控制器的性能奖励高于PPO培训的控制器。最后两列表示遇到的安全违规数量——PPO培训的网络在每个基准上显示出不可忽略的安全违规数量；由于我们的验证算法能够为每个基准生成一个可证明安全的屏蔽，安全增强控制器在任何基准中都没有表现出违反。</p><p><strong>合成时间和比较</strong></p><p>我们还将我们的工作与其他基于屏蔽的方法进行了比较，如[34,33]，其中作者还验证了基于屏障证书的方法和反例引导归纳合成（CEGIS）回路的线性控制器系列。基于屏障证书的方法广泛用于多项式动力学。然而，在我们所分析的随机线性动态系统中，其可扩展性是有限的。</p><p>我们将我们的验证算法与[34]中提供的工具在20个时不变基准上进行了比较。结果如图6所示。我们的算法比基于屏障证书的方法要快得多。在20个基准测试中的13个测试中，他们的工具无法在一小时内找到经过验证的控制器。[32]支持随机和潜在的时变系统。然而，它只能从这个基准测试集中验证一个控制器。正如[34,33,3]所指出的，在一个启用学习的系统中，单个经过验证的控制器通常不足以构建一个保护整个状态空间安全的屏蔽。关于这些实验的更多细节见附录C。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20211222160809131.png width=auto alt=image-20211222160809131></p><a href=#6-相关工作><h2 id=6-相关工作><span class=hanchor arialabel=Anchor># </span>6 相关工作</h2></a><p>最近，人们对探索增强学习型系统安全性的技术产生了浓厚的兴趣。[22,25,17,18]中描述的技术定义了能够为开环LEC提供更强保证的合适验证方法。对于我们工作中讨论的闭环系统，[1,5,12]重点关注指定控制器安全性作为额外奖励。通过改变奖励，这些方法寻求增加学习控制器的安全特性。这些方法与我们的方法不同，因为我们认为可证明的可验证的方法独立于基于训练的奖励框架而应用。<strong>[31,14,15,10,28,21]直接验证神经网络。然而，网络的复杂性、所需的计算量以及验证过程中引入的近似值使得这些方法难以扩展到高维问题。</strong></p><p>另一项工作是通过在主题网络上应用模仿学习技术来探索可验证性[34,3,33]。这些方法也考虑了组成控制器族来合成shield。与我们的工作相比，[34,3,33]的一个显著差异是，<strong>他们根据系统的空间状态选择不同的控制器。然而，在我们的方法中，我们为轨迹中的每k步选择一个新的控制器</strong>。因此，我们的控制器选择过程基于系统的时间状态。此外，由于[34,3,33]必须将严重偏向安全考虑的模拟简单控制器与同样考虑性能目标的神经控制器对齐，因此可伸缩性验证具有挑战性，特别是在考虑复杂性能目标系统时。</p><p>也存在合成安全控制器的工具，而不将其视为强化学习的屏障[32,26]。与[32]类似，我们的验证算法支持受随机干扰的线性、时变、离散时间系统，但我们的算法显然更具可**扩展性。具体地说，我们验证了由LQR生成的线性控制器族的安全性，并使用标准强化学习算法（补充了验证的线性控制器族）学习了其他非功能性特性。**强化学习算法通常可以支持各种目标定义的复杂属性。例如，[6,30]将LTL规范编码为奖励，并使用强化学习训练神经控制器。然而，仅仅将规范编码为奖励并不能保证关键安全特性得到保护。相比之下，我们的方法提供了预期的可验证结果，利用了使用标准强化学习技术学习其他复杂属性的能力。
还存在一些方法，认为伪造方法[8，4，7，9，23 ]的目的是寻找潜在的不安全保险箱在CPS系统中。它们可以处理复杂的规格和高维系统。但是，它们不提供可证明的保证。另一项相关工作是使用收缩度量来共同学习控制器和证书[27]，并将它们与李亚普诺夫证书相结合。从事这项工作是未来研究的主题。</p><a href=#7-结论><h2 id=7-结论><span class=hanchor arialabel=Anchor># </span>7 结论</h2></a><p>在本文中，我们提出了一种新的管道，它合成了一个具有明确安全保证的神经网络控制器。首先，我们提出一个线性控制器族，用于稳定系统。然后，我们就这些安全属性验证该族。
这种经过验证的线性控制器系列用于网络培训，并确保部署的控制器不会违反安全约束。由于安全验证与培训过程是解耦的，因此我们的方法具有令人愉快的可伸缩性特征，对性能目标非常敏感。此外，由于我们在学习过程中注入了safety planner，因此所产生的控制器在培训时考虑了安全因素，从而产生了高质量的经验证的学习控制器，其性能通常优于未经验证的控制器。我们工作的关键洞察是，我们可以将与学习相关的属性与验证所需的属性解耦，从而在不牺牲正确性保证的情况下产生显著的可伸缩性好处。</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://jieye-ericx.github.io/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Jieye ericx using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://jieye-ericx.github.io/>Home</a></li><li><a href=https://github.com/jieye-ericx>Github</a></li></ul></footer></div></div></body></html>