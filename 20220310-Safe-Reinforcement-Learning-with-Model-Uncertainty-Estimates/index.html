<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Safe Reinforcement Learning with Model Uncertainty Estimates
Abstract 目前许多自主系统的设计强烈依赖于深度神经网络（DNN）的黑盒预测。然而，DNN往往对看不见的数据的预测过于自信，并可能对远离分布的测试数据给出不可预测的结果。对于安全关键应用，例如行人周围的碰撞避免，这种分布变化具有鲁棒性的预测的重要性是显而易见的。模型不确定性的度量可以用来识别看不见的数据，但是像贝叶斯神经网络这样的最新提取方法大多难以计算。本文使用MCDropout和Bootstrapping给出了可计算的、可并行化的不确定性估计。这些方法嵌入到一个安全的强化学习框架中，在行人周围形成感知不确定性的导航。其结果是一种避碰策略，它知道自己不知道什么，并谨慎地避免行人表现出看不见的行为。 仿真结果表明，与不确定基线相比，该策略对新的观测结果更具鲁棒性，并采取更安全的措施。
1 INTRODUCTION 强化学习（RL）用于在操作、运动规划和行为预测方面产生最先进的结果。然而，底层神经网络通常缺乏产生定性预测不确定性估计的能力，并且往往对分布外的测试数据过于自信[1]–[3]。在安全关键任务中，如汽车或行人的避碰，对看不见数据的错误但自信的预测可能会导致致命故障[4]。我们研究了安全RL的方法，这些方法对看不见的观察具有鲁棒性，并且知道他们不知道什么可以在不可预测的测试用例中发出警报；最终导致更安全的行动。
一项特别具有挑战性的安全关键任务是在校园环境中使用自动穿梭巴士或漫游车避免行人[5]，[6]。人类通过理解其他行人和车辆的隐藏意图并与之互动，实现了基本上无碰撞的导航[7]，[8]。此外，大多数情况下，这种互动是在没有语言交流的情况下完成的。我们之前的工作使用RL捕捉隐藏的意图，并在行人周围实现协作导航[9]–[11]。然而，RL方法总是面临着从模拟到现实世界的通用性问题，并且无法保证在远离训练测试数据的情况下的性能。如第一节所示，仅针对协作行人的示例策略可能无法推广到现实世界中的非协作行人。
经过训练的策略将输出一个最佳猜测策略，该策略可能会假设协作行为，并且在不标记新观察的情况下，会失败。为了避免此类故障情况，本文开发了一个安全的RLdynamic避碰框架，以模型不确定性的形式表达新的观察结果。如图6所示，该框架进一步解释了不确定性，并谨慎地避免了高不确定性区域。
现有的安全RL研究大多集中在使用外部新奇探测器或内部修改来识别环境或模型不确定性[12]。请注意，我们的工作以模型不确定性估计为目标，因为它们可能会揭示训练数据稀疏的测试数据部分，并且模型可能无法概括[13]。风险敏感RL（RSRL）的工作通常关注环境的不确定性，以检测和避免高风险事件，这些事件的发生概率低，但成本高[14]–[18]。RSRL中的其他工作针对MDP中的模型不确定性，但不容易应用于神经网络[15]，[19]。我们的工作主要与风险敏感的RL方法正交，并且可以组合成一个RL策略，该策略对看不见的数据具有鲁棒性，对高风险事件敏感。
 这里上面作者并没有把他们的东西和现有的RL的区别讲清晰？
 总结提取模型不确定性的困难，阐述自己方法的优势 由于给定观测值的模型结果是确定性的，因此从经过区分训练的神经网络中提取模型不确定性是复杂的。大多数情况下，贝叶斯神经网络用于提取模型不确定性，但需要对网络结构进行重大重组[20]。此外，即使是近似形式，如马尔可夫链蒙特卡罗[20]或变分方法[21]–[23]，也会带来大量计算成本，并具有与样本相关的精度[2]、[20]、[24]。我们的工作使用Monte Carlo Dropout（MC Dropout）[25]和bootstrapping[26]给出了神经网络的可并行和计算上可行的不确定性估计，而无需显著重构网络结构[27]，[28]。"><meta property="og:title" content><meta property="og:description" content="Safe Reinforcement Learning with Model Uncertainty Estimates
Abstract 目前许多自主系统的设计强烈依赖于深度神经网络（DNN）的黑盒预测。然而，DNN往往对看不见的数据的预测过于自信，并可能对远离分布的测试数据给出不可预测的结果。对于安全关键应用，例如行人周围的碰撞避免，这种分布变化具有鲁棒性的预测的重要性是显而易见的。模型不确定性的度量可以用来识别看不见的数据，但是像贝叶斯神经网络这样的最新提取方法大多难以计算。本文使用MCDropout和Bootstrapping给出了可计算的、可并行化的不确定性估计。这些方法嵌入到一个安全的强化学习框架中，在行人周围形成感知不确定性的导航。其结果是一种避碰策略，它知道自己不知道什么，并谨慎地避免行人表现出看不见的行为。 仿真结果表明，与不确定基线相比，该策略对新的观测结果更具鲁棒性，并采取更安全的措施。
1 INTRODUCTION 强化学习（RL）用于在操作、运动规划和行为预测方面产生最先进的结果。然而，底层神经网络通常缺乏产生定性预测不确定性估计的能力，并且往往对分布外的测试数据过于自信[1]–[3]。在安全关键任务中，如汽车或行人的避碰，对看不见数据的错误但自信的预测可能会导致致命故障[4]。我们研究了安全RL的方法，这些方法对看不见的观察具有鲁棒性，并且知道他们不知道什么可以在不可预测的测试用例中发出警报；最终导致更安全的行动。
一项特别具有挑战性的安全关键任务是在校园环境中使用自动穿梭巴士或漫游车避免行人[5]，[6]。人类通过理解其他行人和车辆的隐藏意图并与之互动，实现了基本上无碰撞的导航[7]，[8]。此外，大多数情况下，这种互动是在没有语言交流的情况下完成的。我们之前的工作使用RL捕捉隐藏的意图，并在行人周围实现协作导航[9]–[11]。然而，RL方法总是面临着从模拟到现实世界的通用性问题，并且无法保证在远离训练测试数据的情况下的性能。如第一节所示，仅针对协作行人的示例策略可能无法推广到现实世界中的非协作行人。
经过训练的策略将输出一个最佳猜测策略，该策略可能会假设协作行为，并且在不标记新观察的情况下，会失败。为了避免此类故障情况，本文开发了一个安全的RLdynamic避碰框架，以模型不确定性的形式表达新的观察结果。如图6所示，该框架进一步解释了不确定性，并谨慎地避免了高不确定性区域。
现有的安全RL研究大多集中在使用外部新奇探测器或内部修改来识别环境或模型不确定性[12]。请注意，我们的工作以模型不确定性估计为目标，因为它们可能会揭示训练数据稀疏的测试数据部分，并且模型可能无法概括[13]。风险敏感RL（RSRL）的工作通常关注环境的不确定性，以检测和避免高风险事件，这些事件的发生概率低，但成本高[14]–[18]。RSRL中的其他工作针对MDP中的模型不确定性，但不容易应用于神经网络[15]，[19]。我们的工作主要与风险敏感的RL方法正交，并且可以组合成一个RL策略，该策略对看不见的数据具有鲁棒性，对高风险事件敏感。
 这里上面作者并没有把他们的东西和现有的RL的区别讲清晰？
 总结提取模型不确定性的困难，阐述自己方法的优势 由于给定观测值的模型结果是确定性的，因此从经过区分训练的神经网络中提取模型不确定性是复杂的。大多数情况下，贝叶斯神经网络用于提取模型不确定性，但需要对网络结构进行重大重组[20]。此外，即使是近似形式，如马尔可夫链蒙特卡罗[20]或变分方法[21]–[23]，也会带来大量计算成本，并具有与样本相关的精度[2]、[20]、[24]。我们的工作使用Monte Carlo Dropout（MC Dropout）[25]和bootstrapping[26]给出了神经网络的可并行和计算上可行的不确定性估计，而无需显著重构网络结构[27]，[28]。"><meta property="og:type" content="website"><meta property="og:image" content="https://jieye-ericx.github.io/icon.png"><meta property="og:url" content="https://jieye-ericx.github.io/20220310-Safe-Reinforcement-Learning-with-Model-Uncertainty-Estimates/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="Safe Reinforcement Learning with Model Uncertainty Estimates
Abstract 目前许多自主系统的设计强烈依赖于深度神经网络（DNN）的黑盒预测。然而，DNN往往对看不见的数据的预测过于自信，并可能对远离分布的测试数据给出不可预测的结果。对于安全关键应用，例如行人周围的碰撞避免，这种分布变化具有鲁棒性的预测的重要性是显而易见的。模型不确定性的度量可以用来识别看不见的数据，但是像贝叶斯神经网络这样的最新提取方法大多难以计算。本文使用MCDropout和Bootstrapping给出了可计算的、可并行化的不确定性估计。这些方法嵌入到一个安全的强化学习框架中，在行人周围形成感知不确定性的导航。其结果是一种避碰策略，它知道自己不知道什么，并谨慎地避免行人表现出看不见的行为。 仿真结果表明，与不确定基线相比，该策略对新的观测结果更具鲁棒性，并采取更安全的措施。
1 INTRODUCTION 强化学习（RL）用于在操作、运动规划和行为预测方面产生最先进的结果。然而，底层神经网络通常缺乏产生定性预测不确定性估计的能力，并且往往对分布外的测试数据过于自信[1]–[3]。在安全关键任务中，如汽车或行人的避碰，对看不见数据的错误但自信的预测可能会导致致命故障[4]。我们研究了安全RL的方法，这些方法对看不见的观察具有鲁棒性，并且知道他们不知道什么可以在不可预测的测试用例中发出警报；最终导致更安全的行动。
一项特别具有挑战性的安全关键任务是在校园环境中使用自动穿梭巴士或漫游车避免行人[5]，[6]。人类通过理解其他行人和车辆的隐藏意图并与之互动，实现了基本上无碰撞的导航[7]，[8]。此外，大多数情况下，这种互动是在没有语言交流的情况下完成的。我们之前的工作使用RL捕捉隐藏的意图，并在行人周围实现协作导航[9]–[11]。然而，RL方法总是面临着从模拟到现实世界的通用性问题，并且无法保证在远离训练测试数据的情况下的性能。如第一节所示，仅针对协作行人的示例策略可能无法推广到现实世界中的非协作行人。
经过训练的策略将输出一个最佳猜测策略，该策略可能会假设协作行为，并且在不标记新观察的情况下，会失败。为了避免此类故障情况，本文开发了一个安全的RLdynamic避碰框架，以模型不确定性的形式表达新的观察结果。如图6所示，该框架进一步解释了不确定性，并谨慎地避免了高不确定性区域。
现有的安全RL研究大多集中在使用外部新奇探测器或内部修改来识别环境或模型不确定性[12]。请注意，我们的工作以模型不确定性估计为目标，因为它们可能会揭示训练数据稀疏的测试数据部分，并且模型可能无法概括[13]。风险敏感RL（RSRL）的工作通常关注环境的不确定性，以检测和避免高风险事件，这些事件的发生概率低，但成本高[14]–[18]。RSRL中的其他工作针对MDP中的模型不确定性，但不容易应用于神经网络[15]，[19]。我们的工作主要与风险敏感的RL方法正交，并且可以组合成一个RL策略，该策略对看不见的数据具有鲁棒性，对高风险事件敏感。
 这里上面作者并没有把他们的东西和现有的RL的区别讲清晰？
 总结提取模型不确定性的困难，阐述自己方法的优势 由于给定观测值的模型结果是确定性的，因此从经过区分训练的神经网络中提取模型不确定性是复杂的。大多数情况下，贝叶斯神经网络用于提取模型不确定性，但需要对网络结构进行重大重组[20]。此外，即使是近似形式，如马尔可夫链蒙特卡罗[20]或变分方法[21]–[23]，也会带来大量计算成本，并具有与样本相关的精度[2]、[20]、[24]。我们的工作使用Monte Carlo Dropout（MC Dropout）[25]和bootstrapping[26]给出了神经网络的可并行和计算上可行的不确定性估计，而无需显著重构网络结构[27]，[28]。"><meta name=twitter:image content="https://jieye-ericx.github.io/icon.png"><title>ericx 's 数字花园</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://jieye-ericx.github.io//icon.png><link href=https://jieye-ericx.github.io/styles.80333fa2099c0bee674efa435fde378c.min.css rel=stylesheet><link href=https://jieye-ericx.github.io/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://jieye-ericx.github.io/js/darkmode.48459b7116d092b4e98d2cab704cad80.min.js></script>
<script src=https://jieye-ericx.github.io/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://jieye-ericx.github.io/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://jieye-ericx.github.io/",fetchData=Promise.all([fetch("https://jieye-ericx.github.io/indices/linkIndex.498f8802e86f71aa2af4d45d6c4096d6.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://jieye-ericx.github.io/indices/contentIndex.5368a39ad15318a851f2d088b43f3a36.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://jieye-ericx.github.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://jieye-ericx.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/jieye-ericx.github.io\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=jieye-ericx.github.io src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://jieye-ericx.github.io/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://jieye-ericx.github.io/>ericx 's 数字花园</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated
Unknown
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/20220310%20Safe%20Reinforcement%20Learning%20with%20Model%20Uncertainty%20Estimates.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#abstract>Abstract</a></li><li><a href=#1-introduction>1 INTRODUCTION</a><ol><li></li></ol></li><li><a href=#2-related-work>2 RELATED WORK</a><ol><li><a href=#a外部验证和新颖性检测>A.外部验证和新颖性检测</a></li><li><a href=#b环境和模型不确定性>B.环境和模型不确定性</a></li><li><a href=#c模型不确定性的测量>C.模型不确定性的测量</a></li><li><a href=#d模型不确定性在rl中的应用>D.模型不确定性在RL中的应用</a></li></ol></li><li><a href=#3-approach>3 APPROACH</a><ol><li><a href=#a-collision-prediction-network>A. Collision Prediction Network</a></li><li><a href=#b-mc-dropout和自举的不确定性估计>B. MC-Dropout和自举的不确定性估计</a></li><li><a href=#c选择行动>C.选择行动</a></li><li><a href=#d自适应方差>D.自适应方差</a></li><li><a href=#e收集数据集>E.收集数据集</a></li></ol></li><li><a href=#4-results>4 RESULTS</a><ol><li><a href=#a1d中的区域新颖性检测>A.1D中的区域新颖性检测</a></li><li><a href=#b多维观测中的新颖性检测>B.多维观测中的新颖性检测</a></li><li><a href=#c使用不确定性来逃避局部极小值>C.使用不确定性来逃避局部极小值</a></li></ol></li><li><a href=#5-discussion-and-future-work>5 DISCUSSION AND FUTURE WORK</a><ol><li><a href=#a精确校准的模型不确定性估计>A.精确校准的模型不确定性估计</a></li></ol></li><li><a href=#6-conclusion>6 CONCLUSION</a></li></ol></nav></details></aside><p><strong>Safe Reinforcement Learning with Model Uncertainty Estimates</strong></p><a href=#abstract><h2 id=abstract><span class=hanchor arialabel=Anchor># </span>Abstract</h2></a><p>目前许多自主系统的设计强烈依赖于深度神经网络（DNN）的黑盒预测。然而，DNN往往对看不见的数据的预测<strong>过于自信</strong>，并可能对远离分布的测试数据<strong>给出不可预测的结果</strong>。对于安全关键应用，例如行人周围的碰撞避免，这种<strong>分布变化具有鲁棒性的预测的重要性是显而易见的</strong>。模型不确定性的度量可以用来识别看不见的数据，但是像贝叶斯神经网络这样的最新提取方法大多难以计算。本文使用MCDropout和Bootstrapping给出了可计算的、可并行化的不确定性估计。这些方法嵌入到一个安全的强化学习框架中，在行人周围形成感知不确定性的导航。其<strong>结果是一种避碰策略，它知道自己不知道什么，并谨慎地避免行人表现出看不见的行为。</strong>
仿真结果表明，与不确定基线相比，该策略对新的观测结果更具鲁棒性，并采取更安全的措施。</p><a href=#1-introduction><h2 id=1-introduction><span class=hanchor arialabel=Anchor># </span>1 INTRODUCTION</h2></a><p>强化学习（RL）用于在操作、运动规划和行为预测方面产生最先进的结果。然而，<strong>底层神经网络通常缺乏产生定性预测不确定性估计的能力，并且往往对分布外的测试数据过于自信</strong>[1]–[3]。<strong>在安全关键任务中，如汽车或行人的避碰，对看不见数据的错误但自信的预测可能会导致致命故障</strong>[4]。我们研究了安全RL的方法，这些方法对看不见的观察具有鲁棒性，并且知道他们不知道什么可以在不可预测的测试用例中发出警报；最终导致更安全的行动。</p><p>一项特别具有挑战性的安全关键任务是在校园环境中使用自动穿梭巴士或漫游车避免行人[5]，[6]。人类通过理解其他行人和车辆的隐藏意图并与之互动，实现了基本上无碰撞的导航[7]，[8]。此外，大多数情况下，这种互动是在没有语言交流的情况下完成的。<strong>我们之前的工作使用RL捕捉隐藏的意图，并在行人周围实现协作导航[9]–[11]。然而，RL方法总是面临着从模拟到现实世界的通用性问题，并且无法保证在远离训练测试数据的情况下的性能。如第一节所示，仅针对协作行人的示例策略可能无法推广到现实世界中的非协作行人。</strong></p><p>经过训练的策略将输出一个最佳猜测策略，该策略可能会假设协作行为，并且在不标记新观察的情况下，会失败。为了避免此类故障情况，本文开发了一个安全的RLdynamic避碰框架，以模型不确定性的形式表达新的观察结果。如图6所示，该框架进一步解释了不确定性，并谨慎地避免了高不确定性区域。</p><p>现有的安全RL研究大多集中在使用外部新奇探测器或内部修改来识别环境或模型不确定性[12]。请注意，我们的工作以<strong>模型不确定性估计为目标</strong>，因为它们可能会揭示训练数据稀疏的测试数据部分，并且模型可能无法概括[13]。风险敏感RL（RSRL）的工作通常关注环境的不确定性，以检测和避免高风险事件，这些事件的发生概率低，但成本高[14]–[18]。RSRL中的其他工作针对MDP中的模型不确定性，但不容易应用于神经网络[15]，[19]。我们的工作主要与风险敏感的RL方法正交，并且可以组合成一个RL策略，该策略对看不见的数据具有鲁棒性，对高风险事件敏感。</p><blockquote><p>这里上面作者并没有把他们的东西和现有的RL的区别讲清晰？</p></blockquote><a href=#总结提取模型不确定性的困难阐述自己方法的优势><h4 id=总结提取模型不确定性的困难阐述自己方法的优势><span class=hanchor arialabel=Anchor># </span>总结提取模型不确定性的困难，阐述自己方法的优势</h4></a><p>由于给定观测值的模型结果是确定性的，因此从经过区分训练的神经网络中提取模型不确定性是复杂的。大多数情况下，贝叶斯神经网络用于提取模型不确定性，但需要对网络结构进行重大重组[20]。此外，即使是近似形式，如马尔可夫链蒙特卡罗[20]或变分方法[21]–[23]，也会带来大量计算成本，并具有与样本相关的精度[2]、[20]、[24]。<strong>我们的工作使用Monte Carlo Dropout（MC Dropout）[25]和bootstrapping[26]给出了神经网络的可并行和计算上可行的不确定性估计，而无需显著重构网络结构[27]，[28]。</strong></p><a href=#总结自己的工作><h4 id=总结自己的工作><span class=hanchor arialabel=Anchor># </span>总结自己的工作</h4></a><p>这项工作的主要贡献是<strong>i）一种识别新行人观察的算法，ii）比不确定基线更谨慎、更安全地避免这些观察，iii）将现有的不确定强化学习框架[29]扩展到更复杂的动态环境，并采用探索辅助方法，iv）模拟环境中的演示</strong>。</p><a href=#2-related-work><h2 id=2-related-work><span class=hanchor arialabel=Anchor># </span>2 RELATED WORK</h2></a><p>本节研究安全强化学习中的相关工作，以开发对数据外观测具有鲁棒性的动态避碰策略。</p><a href=#a外部验证和新颖性检测><h3 id=a外部验证和新颖性检测><span class=hanchor arialabel=Anchor># </span>A.外部验证和新颖性检测</h3></a><p>许多相关工作使用off-policy评估或外部新颖性检测来验证学习到的RL策略[12]、[30]、[31]。</p><p>可达性分析可以通过提供区域安全界限来验证政策，但在协作行人环境中，界限过于保守[32]-[35].</p><p>新颖性检测方法在探测器的新颖性输出上设置一个阈值，如果超过该阈值，则切换到安全控制器[30]。然而，切换到安全控制器通常是突然的，可能会产生不舒服和不可预测的驾驶行为。</p><p><strong>在我们的框架中，车辆远离不确定区域，如图3所示，以预测性地避免潜在安全控制器的干预。</strong></p><p>![image-20220312020452276](../../../../../Application Support/typora-user-images/image-20220312020452276.png)</p><p>图三：区域新颖性检测。一个简单的网络预测碰撞（红色）和无碰撞（绿色）标签，给出代理的（橙色）heading（左图：x轴）和障碍物（蓝色）标题的一维观察。当在训练数据集（a）上进行测试时，该网络准确预测不确定性较低的标签。当在一个新的观察集（b）上测试时，网络无法预测准确的决策标签，但识别它们时具有很高的区域不确定性（左下：带高值的绿点，右下：浅绿线）。图4描述了一个代理如何采取肯定安全的行动（深绿色）谨慎地避开新的障碍物，而不是相信假阳性碰撞预测。</p><p>![image-20220312155034116](../../../../../Application Support/typora-user-images/image-20220312155034116.png)</p><p>图四：在区域新颖性检测后谨慎回避。图4a中的代理（橙色）使用图3a中的不确定性估计值，以自信地避开已知障碍物（蓝色）。在图4b中，如图3b所示，代理识别新的障碍物外观，并谨慎地避开障碍物。</p><a href=#b环境和模型不确定性><h3 id=b环境和模型不确定性><span class=hanchor arialabel=Anchor># </span>B.环境和模型不确定性</h3></a><p>本文的<strong>重点是通过模型不确定性（也称为参数或认知不确定性[36]）来检测新的观测结果</strong>。环境不确定性的正交概念抓住了由于部分观测的不完美性质而产生的不确定性[13]。例如，<strong>即使在现实世界中进行无限训练，对行人轨迹的观察也不会完全捕捉到行人的决策过程</strong>，因此有时会模棱两可；她会左转还是右转？RL框架通过学习平均结果来解释不可观测的决策模糊性[13]。相比之下，模型的不确定性反映了一个模型对环境中所有可能观测结果的拟合程度。它可以用无限的观察值来解释，在训练数据有限的应用中，或者在测试数据远离训练数据的应用中，它通常很高[13]。因此，模型不确定性捕捉到了模型无法推广到新测试数据的情况，以及不应信任网络预测的提示[13]。</p><a href=#c模型不确定性的测量><h3 id=c模型不确定性的测量><span class=hanchor arialabel=Anchor># </span>C.模型不确定性的测量</h3></a><p>一个新的研究课题采用神经网络来表达其模型的不确定性[21]，[25]，[26]。已经探索了自举法，以生成近似的不确定性度量来指导探索[26]。</p><p>通过在部分重叠的数据集样本上训练一组网络，它们在公共数据区域一致，在不常见数据区域不一致，且样本方差较大[2]，[26]</p><p>如果Dropout在测试期间被激活，并且已被证明在高斯过程中近似于贝叶斯推断，则可以对其进行类似的解释[25]，[27]。另一种方法是使用超网，即学习另一个网络的权重以直接给出参数不确定性值的网络，但其计算成本非常高[37]。一种创新但有争议的方法通过批量标准化检索贝叶斯不确定性估计[38]。这项工作使用MC-Dropout和自举来给出计算上易于处理的不确定性估计。</p><a href=#d模型不确定性在rl中的应用><h3 id=d模型不确定性在rl中的应用><span class=hanchor arialabel=Anchor># </span>D.模型不确定性在RL中的应用</h3></a><p>最近在RL中使用了模型不确定性度量，通过指导探索高不确定性区域来加快培训[26]、[39]、[40]。Kahn等人在基于模型的RL中使用不确定性估计来避免静态障碍物碰撞[29]。与基于模型的RL方法不同，我们可以主张使用无模型RL，并得出最优策略输出π的不确定性∗ = argmaxπ（Q）。
然而，不确定性估计将包含来自多个目标的不确定性的混合，并且不会关注碰撞的不确定性区域。我们的工作通过[29]将基于模型的框架扩展到高度复杂的行人碰撞避免领域。[29]通过使用指导勘探的不确定性估计来逃避局部最优政策，分析新动态场景中区域不确定性的增加，使用LSTM，以及行动目标指导，进一步扩展。</p><a href=#3-approach><h2 id=3-approach><span class=hanchor arialabel=Anchor># </span>3 APPROACH</h2></a><p>这项工作提出了一种算法，<strong>利用不确定性信息，谨慎地避免在新的场景中的动态障碍</strong>。如图2的系统架构所示，代理观察模拟障碍物的位置和速度，以及目标。一组长短期记忆（LSTM）网络预测一组运动原语u的碰撞概率。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20220312164712214.png width=auto alt=image-20220312164712214></p><p>图2：系统架构。代理观察环境并选择成本最低的运动原语u*在避免碰撞的同时达到目标。在每个时间步长上，使用不同的丢失掩码对LSTM网络集合进行多次采样，以获得每个运动原语u的样本均值和方差碰撞概率。</p><p>MC-dropout和自举用于要求预测的分布。根据预测，为每个运动原语绘制样本平均值E(Pcoll)和方差Var(Pcoll)。同时，一个简单的模型估计在每个评估的运动原语结束时到达目标tgoal的时间。在下一阶段，最小成本运动原语u在环境中选择并执行一个步骤。**环境返回下一个观察结果，并在一集结束时返回碰撞标签。**在一组事件之后，网络权重W被调整，训练过程继续。算法的每个部分将在下面详细解释。</p><a href=#a-collision-prediction-network><h3 id=a-collision-prediction-network><span class=hanchor arialabel=Anchor># </span>A. Collision Prediction Network</h3></a><p>一组LSTM网络（集合）预测运动原语的碰撞概率。<strong>网络的每个前向传递i返回已评估运动原语的碰撞概率</strong>,</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20220312171207740.png width=auto alt=image-20220312171207740></p><p>1coll:碰撞标签;Ot-1:t-1:过去l个时间步的观测历史;Ot:当前的观察;Ut-1:t-1:过去行动的拼接;Ut:t+h:长度为h的计算运动原语.</p><p>RL代理在部分可观察的环境中运行，在该环境中，它只能观察行人的位置、速度和半径。
观察结果还包含RL代理的相对目标位置。运动原语Ut:t+h是一组预先计算的运动原语U的元素集合。在这项工作中，U包含11个长度h=1的离散运动原语，它们由航向角α描述∈ [[π6 , π6 ]. 无论长度如何，在再次查询网络之前，将采用一个时间步长的最佳运动原语。</p><p>选择LSTM网络进行动态避障，因为它们是通过了解行人隐藏的时间意图来预测行人路径的最先进模型[42]，[43]。基于这一成功，本研究首次将LSTM应用于RL环境下的行人避让。为了安全避免，LSTM预测需要从机器人视野中观察到行人的第一步开始精确。为了处理可变长度的观察输入，在训练和测试期间使用 masking[44]来停用超过观察历史长度的LSTM单元。</p><a href=#b-mc-dropout和自举的不确定性估计><h3 id=b-mc-dropout和自举的不确定性估计><span class=hanchor arialabel=Anchor># </span>B. MC-Dropout和自举的不确定性估计</h3></a><p>MC Dropout[25]和bootstrapping[2]，[26]用于计算模型不确定性Var（Pcoll）的随机估计。对于自举，多个网络被训练并存储在一个集合中。每个网络都是随机初始化的，并在样本数据集上进行训练，样本数据集是从更大的经验数据集中抽取的[26]。</p><p>通过在观测空间的不同但重叠的部分上进行训练，网络预测对于不常见的观测是不同的，对于常见的观测是相似的。由于每个网络都可以并行地进行训练和测试，因此引导不会带来巨大的计算成本，可以在真正的机器人上运行。</p><p>Dropout[27]传统上用于规范网络。它通过将单位权重乘以辍学掩码，在每次向前传递中随机停用网络单元。辍学掩码是一组伯努利随机变量，其值为[0,1]，每个变量的保持概率为p。传统上，辍学在测试期间被停用，每个单元乘以p。然而，[25]表明，测试期间辍学的激活，称为MC辍学，通过近似深高斯过程中的贝叶斯推理给出模型不确定性估计。为了检索带有漏失的模型不确定性，我们的工作在具有不同漏失掩码（p=0.7）的自举集成中对每个网络执行多个前向传递，并获得预测的分布。对于nb网络中的nd丢失样本，总共对N=ndnb前向传递进行采样。虽然辍学者被认为对新的观察结果过于自信[26]，但表一显示，引导和辍学的结合可靠地检测到了新的场景。</p><p>从每个网络和每个退出掩码的可并行化碰撞预测中，得出样本均值和方差。</p><a href=#c选择行动><h3 id=c选择行动><span class=hanchor arialabel=Anchor># </span>C.选择行动</h3></a><p>模型预测控制器（MPC）以最小的联合成本选择最安全的运动原语：</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20220312181504817.png width=auto alt=image-20220312181504817></p><p>所选的MPC考虑了<strong>二阶概率矩</strong>[29]、[45]、[46]，能够选择更安全的行动。一阶和二阶矩（E（·）和Var（·））是在每个运动原语的N个正向过程中计算的。MPC通过测量直线距离来估计从每个运动原语结束到目标tgoal的时间。每个成本项由其自身的系数λ加权。注意，<strong>避碰的软约束要求选择λg和λc，以便预测的碰撞成本λcEN（Pi coll）(≤ λc）大于目标成本λgtgoal</strong>。与[29]相比，这项工作不会将方差项与选定的速度相乘。例如，在高速公路上停车或减速是不安全的。相反，拟议的工作侧重于识别和避免地平面区域内的不确定性观测。</p><a href=#d自适应方差><h3 id=d自适应方差><span class=hanchor arialabel=Anchor># </span>D.自适应方差</h3></a><p>请注意，在培训过程中，过度厌恶不确定性的模型会阻碍探索，并且很少找到最佳策略。此外，在预测过程中对多个前向过程进行平均会降低集合的多样性，这还会阻碍探索行动。**所提出的方法随着时间的推移增加了对高度不确定行为λv的惩罚，以克服这种影响。**因此，该政策在早期培训阶段有效地探索了高模型不确定性的方向</p><p>λv收敛，在执行过程中起到不确定性规避的作用。这项工作使λv在[50000，200]中线性增加，λg=2，λc=25。</p><a href=#e收集数据集><h3 id=e收集数据集><span class=hanchor arialabel=Anchor># </span>E.收集数据集</h3></a><p>所选操作在学习环境中执行。在每集结束时(Tend)，环境返回一个<strong>碰撞标签1coll。如果在事件发生期间发生碰撞，则碰撞标签为一，否则为零</strong>。从一个事件的开始到结束的观察记录otstart:tend和操作utstart:tend与冲突标签相关联，并<strong>存储在经验数据集</strong>中。在运行几集之后，从完整的体验集中随机抽取子集，为下一个观察、表演训练周期训练网络集合。策略推出周期对于了解动态障碍物将如何对代理学习到的策略做出反应是必要的。[30]中针对静态避障采取的监督学习方法不会学习环境因素对经过训练的策略的反应。</p><a href=#4-results><h2 id=4-results><span class=hanchor arialabel=Anchor># </span>4 RESULTS</h2></a><p>我们表明，我们的算法使用不确定性信息来区域性地检测新的障碍物观测，并且比不确定基线造成的碰撞更少。首先，一个简单的1D案例说明了该模型如何区域性地识别新的障碍物观测。在具有新的多维观测的放大环境中，所提出的模型继续显示出区域性增加的不确定性值。
在各种新的场景中，将该模型与不确定基线进行比较；<strong>该模型对新数据具有更强的鲁棒性，并减少了碰撞</strong>。</p><a href=#a1d中的区域新颖性检测><h3 id=a1d中的区域新颖性检测><span class=hanchor arialabel=Anchor># </span>A.1D中的区域新颖性检测</h3></a><p>首先，我们表明，模型不确定性估计能够在区域范围内检测到新的一维观测值，如图3所示。对于1维测试用例，训练<strong>一个具有MC辍学和自举功能的两层完全连接网络来预测碰撞标签</strong>。为了生成数据集，代理随机选择标题动作，独立于障碍物观察，环境报告碰撞标签。网络输入是代理航向角和障碍物航向。重要的是，训练集只包含位于代理右侧的障碍物（上图：x>0）。</p><p>训练后，网络准确地预测碰撞和无碰撞标签，且训练分布中障碍物观测的不确定性较低，如图3a所示。
对于代理左侧（底图：x&lt;0）的训练外障碍物观察，神经网络无法概括和预测碰撞（红色）和非碰撞（绿色）标签，用于可能与障碍物（蓝色）碰撞的动作（直线）。但是，该代理会识别模型高度不确定性的区域（左：y轴，右：浅色），以便在看不见的障碍物方向上采取行动。**高不确定性值表明，网络预测是误报，不应被信任。**基于不确定性估计的左右差异，MPC倾向于采取肯定安全的保守行动（右下角：深绿色线），而不是预测安全但不确定的假阳性行动（右下角：浅绿色线）图4说明了MPC如何选择保守的行动来避免新的障碍，以及如何选择自信的行动来避免已知的障碍。</p><a href=#b多维观测中的新颖性检测><h3 id=b多维观测中的新颖性检测><span class=hanchor arialabel=Anchor># </span>B.多维观测中的新颖性检测</h3></a><p>以下实验表明，我们的模型能够在区域范围内识别多维观测中的不确定性，并选择更安全的措施。</p><ol><li><p>实验设置：在一个基于GYM[47]的模拟环境中，使用一个代理和一个动态障碍物训练了一个单层16单元LSTM模型。环境中的动态障碍物能够遵循合作RVO[48]、GA3C-CADRL[11]或非合作或静态策略。对于分析的场景，agent接受了遵循RVO政策的障碍物培训，并按照第三节所述进行了观察。培训过程花了20分钟，计算量很低。大型英特尔至强白金8124M，配备2VCPU和4GiB内存。集合中使用的五个网络中的每一个都通过随机MC辍学前向传递进行20次采样。每一步总共抽取100个样本，平均耗时32ms。通过在GPU上并行计算，可以进一步减少训练和执行时间。</p><p>在测试设置中，对障碍物的观察进行操纵，以创建具有新颖观察的场景，这些观察可能会打破经过训练的模型。在一种情况下，通过添加高斯噪声来模拟传感器噪声∼N（µ=0，σ=0.5）以m为单位观察位置，以m/s为单位观察速度。在另一种情况下，观察值以20%的概率随机丢弃。在模拟传感器故障的第三个和第四个场景中，障碍物位置和速度分别被掩盖。在训练时没有对障碍物进行任何操作。</p></li><li><p>区域新颖性检测</p><p>图5显示，提出的模型继续在高维观测空间中区域性地识别新的障碍物观测。在展示的实验中，一个不确定性感知代理（橙色）观察一个动态障碍物（蓝色）和新添加的噪声，并评估避免它的行动。在障碍物方向（浅绿线）的碰撞预测比进入自由空间（深绿线）的碰撞预测具有更高的不确定性。预测不确定性从左到右的差异虽然是随机的，也不是完全平滑的，但MPC使用它来引导代理远离嘈杂的障碍物，并在不发生碰撞的情况下谨慎地避开它（橙色/黄色线）。图6b显示了不确定性感知代理的完整轨迹，并说明了图6a中具有相同速度和半径的不确定性感知代理如何无法推广到新噪声，并在五个时间步后与障碍物碰撞。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20220312184707540.png width=auto alt=image-20220312184707540></p><p>图5：不确定性的区域识别。不确定性感知代理（橙色）可以避免在噪声中观察到的动态障碍物（蓝色）。在一个时间步中，障碍物方向（浅绿线）上的碰撞预测比自由空间（深绿线）上的碰撞预测具有更高的不确定性。代理人选择一个不确定度较低的动作，谨慎地避开障碍物。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20220312185520824.png width=auto alt=image-20220312185520824></p><p>图6：新场景中的谨慎回避。在无噪音的情况下，agent（橙色）被训练为避免动态RVO agent（蓝色）。在测试中，高斯噪声被添加到观测值中，图6a中的不确定性不知道模型无法概括，并导致碰撞。图6b中提出的不确定性感知代理对新的观察结果更加谨慎，并成功地避免了障碍。</p></li><li><p>一种新的不确定性情景识别方法</p><p>表一显示，在所有测试的新场景中，包括添加噪声的示例情况，整体模型不确定性都很高。测量的不确定度是一个时间步内每个动作的碰撞预测方差之和。不确定性值在20个疗程中进行了平均，随机初始化，50集和所有时间步，直到每集结束。如表一所示，训练分布测试集的不确定性相对较低。所有其他场景都会导致更高的不确定性值，不确定性值的相对大小可以解释为与训练案例相比，模型的观测值集有多新颖。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20220312185821423.png width=auto alt=image-20220312185821423></p><p>表一：新情景中增加的不确定性。在四种新的测试场景中，碰撞预测V ar（Pcoll）的不确定性都高于SEED训练分布样本的不确定性。</p></li><li><p>在新场景中减少碰撞</p><p>该模型利用不确定性信息更加谨慎地采取行动，并对新的场景更具鲁棒性。图7显示，与不确定基线相比，这种行为在新场景中导致的碰撞更少。建议的模型（红色）和基线（蓝色）在来自训练分布的样本上表现相似。在附加噪声、掩蔽位置和掩蔽速度信息的测试场景中，所提出的模型引起的碰撞更少，<strong>并且对新的观测类别更具鲁棒性</strong>。在观测数据丢失的情况下，两种模型在碰撞方面表现相似，但不确定模型被认为需要更长的时间才能达到目标。除了方差惩罚λv设置为零之外，基线模型已使用相同的超参数和环境进行训练。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20220312191354649.png width=auto alt=image-20220312191354649></p><p>图7：新情况下的碰撞更少。在新的情况下，提出的不确定性感知模型（红色）比不确定性感知基线（蓝色）引起的碰撞更少。由于障碍物方向的区域不确定性增加，该模型倾向于采取比基线更谨慎地避开障碍物的行动。</p></li><li><p>推广到其他新场景</p><p>在所有被演示的情况下，我们都可以找到一个可以推广到噪声、隐蔽位置观测等的模型。然而，<strong>我们无法设计一个能够捕捉现实生活中可能发生的所有新场景的模拟</strong>。一个显著新颖的事件应具有高度的模型不确定性。<strong>在行人回避任务中，新的观察结果可能是不常见的行人行为，例如个人车辆上的非合作行人</strong>。但实际上，对于部署的模型来说，所有形式的观察都是新奇的，应该通过更加谨慎的驾驶来识别和应对。显示的结果表明，模型不确定性能够识别此类观察结果，MPC选择具有额外缓冲空间的行动，以谨慎避免这些行人。</p></li></ol><a href=#c使用不确定性来逃避局部极小值><h3 id=c使用不确定性来逃避局部极小值><span class=hanchor arialabel=Anchor># </span>C.使用不确定性来逃避局部极小值</h3></a><p>这项工作增加了方差惩罚λv，以避免在训练过程中陷入MPC优化的局部极小。图8显示，随着λv的增加，所提出的算法可以通过在训练的早期阶段鼓励探索性行动来避开局部极小值。在实验中，一名特工（橙色）被训练通过连续选择一个动作（左图）来达到被静态障碍物（蓝色）阻挡的目标（星形）。在容易避开的情况下，障碍物放置在离代理起始位置更远的位置（深橙色）；在一个更接近特工的具有挑战性的案件中。接近障碍物是一个挑战，因为特工最初是朝着障碍物的方向前进，需要探索避免行动。在早期训练阶段，随机初始化网络的碰撞估计是不具信息性的，并且目标成本驱使代理进入障碍。早期阶段的负方差惩罚λv迫使代理探索远离目标的行动，避免陷入局部极小值。</p><p>图8显示，在具有挑战性的训练案例中，具有常数λv的代理无法探索，算法陷入了糟糕的局部极小值（右下角的图：蓝色），其中80%的运行以冲突结束。λv增大且超参数相同（右下角图：红色）的策略在早期阶段更具探索性，并在平均五个会话中收敛到较低的最小值。在这个简单的测试用例中，两种算法的性能相似，都收敛到一个接近零冲突的策略（右上图）。</p><p><img src=https://jieye-ericx.github.io//../../pics/image-20220312191719028.png width=auto alt=image-20220312191719028></p><p>图8：逃离局部极小值。比较了对不确定行为λv（蓝色）和增加λv（红色）惩罚不变的两种策略的训练过程。在一个容易避免的情况下（右上），两种策略都找到了一个好的策略，可以导致接近零的碰撞（y轴）。在一个更具挑战性的回避案例（右下）中，建议的增加λv策略在早期阶段进行了探索，找到了一个比常数λv更好的最小值。</p><a href=#5-discussion-and-future-work><h2 id=5-discussion-and-future-work><span class=hanchor arialabel=Anchor># </span>5 DISCUSSION AND FUTURE WORK</h2></a><a href=#a精确校准的模型不确定性估计><h3 id=a精确校准的模型不确定性估计><span class=hanchor arialabel=Anchor># </span>A.精确校准的模型不确定性估计</h3></a><p>在另一个新的场景中，一名特工接受了避免合作RVO特工的培训，并在非合作特工上进行了测试。不确定度值没有显著增加，这可以由两个原因解释。首先，对于该模型而言，非合作代理并不新颖；可能是因为远离RVO的特工也在直线上行动。对于一个只接受过协作代理培训的模型，人类认为非协作代理可能是新颖的，这一事实并没有改变一个事实，即该模型可能具有足够的普遍性，以至于不认为它是新颖的。另一种解释是，观察到的辍学者过度自信是一种不确定性估计。未来的工作将为神经网络找到模型不确定性的未公开估计，从而为真实的模型不确定性提供更有力的保证。</p><a href=#6-conclusion><h2 id=6-conclusion><span class=hanchor arialabel=Anchor># </span>6 CONCLUSION</h2></a><p>这项工作开发了一个带有模型不确定性估计的安全RL框架，以谨慎避免新场景中的动态障碍。一组LSTM网络通过辍学和自举训练来估计碰撞概率并获得预测不确定性估计。结果表明，不确定性估计的大小揭示了各种场景中的新奇之处，表明模型知道它不知道的东西。MPC利用新障碍物观测方向的区域不确定性增加，在新场景中采取更谨慎的行动。
这种谨慎的行为使得不确定性感知框架比不确定性感知基线更具鲁棒性和安全性。这项工作是向开发深层神经网络在安全关键任务中应用的巨大能力迈出的又一步。</p><p>Monte-Carlo Dropout（蒙特卡罗 dropout），简称 MC dropout，是一种从贝叶斯理论出发的 Dropout 理解方式，将 Dropout 解释为
<a href="https://www.zhihu.com/search?q=%e9%ab%98%e6%96%af%e8%bf%87%e7%a8%8b&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7b%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A868558657%7d" rel=noopener>高斯过程</a>的贝叶斯近似。</p><p>云里雾里的，理论证明看起来挺复杂，有兴趣可以参考论文：
<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1506.02142" rel=noopener>Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.</a> 以及这篇论文的
<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1506.02157" rel=noopener>Appendix</a>。</p><p>但其实，MC dropout 用起来就简单了，不需要修改现有的
<a href="https://www.zhihu.com/search?q=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%9e%8b&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7b%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A868558657%7d" rel=noopener>神经网络模型</a>，只需要神经网络模型中带 dropout 层，无论是标准的 dropout 还是其变种，如 drop-connect，都是可以的。</p><p>在训练的时候，MC dropout 表现形式和 dropout 没有什么区别，按照正常模型训练方式训练即可。</p><p>在测试的时候，在前向传播过程，神经网络的 dropout 是不能关闭的。这就是和平常使用的唯一的区别。</p><p>MC dropout 的 MC 体现在我们需要对同一个输入进行多次前向传播过程，这样在 dropout 的加持下可以得到“不同网络结构”的输出，将这些输出进行平均和统计方差，即可得到模型的预测结果及 uncertainty。而且，这个过程是可以并行的，所以在时间上可以等于进行一次前向传播。</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://jieye-ericx.github.io/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Jieye ericx using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://jieye-ericx.github.io/>Home</a></li><li><a href=https://github.com/jieye-ericx>Github</a></li></ul></footer></div></div></body></html>