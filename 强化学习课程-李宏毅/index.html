<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Reinforcement Learning 0 从监督学习到强化学习 那什么是 Reinforcement Learning 呢,到目前为止，我们讲的几乎都是 Supervised Learning,假设你要做一个 Image 的 Classifier,你不只要告诉机器,它的 Input 是什么,你还要告诉机器,它应该输出什么样的 Output,然后接下来呢,你就可以 Train 一个 Image 的 Classifier"><meta property="og:title" content="Reinforcement Learning"><meta property="og:description" content="Reinforcement Learning 0 从监督学习到强化学习 那什么是 Reinforcement Learning 呢,到目前为止，我们讲的几乎都是 Supervised Learning,假设你要做一个 Image 的 Classifier,你不只要告诉机器,它的 Input 是什么,你还要告诉机器,它应该输出什么样的 Output,然后接下来呢,你就可以 Train 一个 Image 的 Classifier"><meta property="og:type" content="website"><meta property="og:image" content="https://jieye-ericx.github.io/icon.png"><meta property="og:url" content="https://jieye-ericx.github.io/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E6%9D%8E%E5%AE%8F%E6%AF%85/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reinforcement Learning"><meta name=twitter:description content="Reinforcement Learning 0 从监督学习到强化学习 那什么是 Reinforcement Learning 呢,到目前为止，我们讲的几乎都是 Supervised Learning,假设你要做一个 Image 的 Classifier,你不只要告诉机器,它的 Input 是什么,你还要告诉机器,它应该输出什么样的 Output,然后接下来呢,你就可以 Train 一个 Image 的 Classifier"><meta name=twitter:image content="https://jieye-ericx.github.io/icon.png"><title>Reinforcement Learning</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://jieye-ericx.github.io//icon.png><link href=https://jieye-ericx.github.io/styles.80333fa2099c0bee674efa435fde378c.min.css rel=stylesheet><link href=https://jieye-ericx.github.io/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://jieye-ericx.github.io/js/darkmode.48459b7116d092b4e98d2cab704cad80.min.js></script>
<script src=https://jieye-ericx.github.io/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://jieye-ericx.github.io/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://jieye-ericx.github.io/",fetchData=Promise.all([fetch("https://jieye-ericx.github.io/indices/linkIndex.00d2be006410ecaf55044e2f8ad173cb.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://jieye-ericx.github.io/indices/contentIndex.b930bf1a27f41767a0fa851124895c72.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://jieye-ericx.github.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://jieye-ericx.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/jieye-ericx.github.io\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=jieye-ericx.github.io src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://jieye-ericx.github.io/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://jieye-ericx.github.io/>jieye の 数字花园</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Reinforcement Learning</h1><p class=meta>Last updated
Dec 12, 2021</p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#0-从监督学习到强化学习>0 从监督学习到强化学习</a></li><li><a href=#1-what-is-rlthree-steps-in-ml>1 What is RL?(Three steps in ML)</a><ol><li><a href=#11-machine-learning--looking-for-a-function>1.1 Machine Learning ≈ Looking for a Function</a></li><li><a href=#12-example-playing-video-game>1.2 Example: Playing Video Game</a></li><li><a href=#13-example-learning-to-play-go>1.3 Example: Learning to play Go</a></li><li><a href=#14-三步走>1.4 三步走</a></li></ol></li><li><a href=#2-policy-gradient>2 Policy Gradient</a><ol><li><a href=#21-how-to-control-your-actor>2.1 How to control your actor</a></li><li><a href=#22-version-0>2.2 Version 0</a></li><li><a href=#23-version-1>2.3 Version 1</a></li><li><a href=#24-version-2>2.4 Version 2</a></li><li><a href=#qa>Q&A</a></li><li><a href=#25-version-3>2.5 Version 3</a></li><li><a href=#26-policy-gradient>2.6 Policy Gradient</a></li></ol></li><li><a href=#3-critic>3 Critic</a><ol><li><a href=#31-how-to-estimate-vθs>3.1 How to estimate Vθ(s)</a></li><li><a href=#32-version-35>3.2 Version 3.5</a></li><li><a href=#33-version-4>3.3 Version 4</a></li><li><a href=#34-tip-of-actor-critic>3.4 Tip of Actor-Critic</a></li><li><a href=#35-outlook-deep-q-network-dqn>3.5 Outlook: Deep Q Network (DQN)</a></li><li><a href=#qa-1>Q&A</a></li></ol></li></ol></nav></details></aside><a href=#reinforcement-learning><h1 id=reinforcement-learning><span class=hanchor arialabel=Anchor># </span>Reinforcement Learning</h1></a><a href=#0-从监督学习到强化学习><h2 id=0-从监督学习到强化学习><span class=hanchor arialabel=Anchor># </span>0 从监督学习到强化学习</h2></a><p>那什么是 Reinforcement Learning 呢,到目前为止，我们讲的几乎都是 Supervised Learning,假设你要做一个 Image 的 Classifier,你不只要告诉机器,它的 Input 是什么,你还要告诉机器,它应该输出什么样的 Output,然后接下来呢,你就可以 Train 一个 Image 的 Classifier</p><p><img src=https://jieye-ericx.github.io//image-20210911230950586.png width=auto alt=image-20210911230950586></p><p>那在多数这门课讲到目前为止的技术,基本上都是基于 Supervised Learning 的方法,就算是我们在讲 Self Supervised Learning 的时候,我们其实也是,很类似 Supervised Learning 的方法,只是我们的 Label,不需要特别僱用人力去标记,它可以自动产生，或者是我们在讲 Auto-encoder 的时候,我们虽然说它是一个 Unsupervised 的方法,我们没有用到人类的标记,但事实上,我们还是有一个 Label,只是这个 Label,不需要耗费人类的力量来产生而已</p><p>但是 RL 就是另外一个面向的问题了,在 RL 裡面,我们遇到的问题是这样子的,我们,<strong>机器当我们给它一个输入的时候,我们不知道最佳的输出应该是什么</strong></p><p>举例来说,假设你要叫机器学习下围棋</p><p><img src=https://jieye-ericx.github.io//image-20210911231032976.png width=auto alt=image-20210911231032976></p><p>用 Supervised Learning 的方法,好像也可以做,你就是告诉机器说,看到现在的盘势长这个样子的时候,下一步应该落子的位置在哪裡,但是问题是,下一步应该落子的位置到底应该在哪裡呢,哪一个是最好的下一步呢,哪一步是神之一手呢,可能人类根本就不知道</p><p>当然你可以说,让机器阅读很多职业棋士的棋谱,让机器阅读很多高段棋士的棋谱,也许这些棋谱裡面的答案,也许这些棋谱裡面给某一个盘势,人类下的下一步,就是一个很好的答案,但它是不是最好的答案呢,我们不知道,在这个<strong>你不知道正确答案是什么的情况下,往往就是 RL 可以派上用场的时候</strong>,所以当你今天,你发现你要收集有标注的资料很困难的时候,正确答案人类也不知道是什么的时候,也许就是你可以考虑使用 RL 的时候</p><p>但是 <strong>RL 在学习的时候,机器其实也不是一无所知的</strong>,我们虽然不知道正确的答案是什么,但是机器会知道什么是好,什么是不好,机器会跟环境去做互动,得到一个叫做 <strong>Reward</strong> 的东西,这我们等一下都还会再细讲。</p><p>所以机器会知道,它现在的输出是好的还是不好的,来藉由跟环境的互动,藉由知道什么样的输出是好的,什么样的输出是不好的,机器还是可以学出一个模型</p><a href=#1-what-is-rlthree-steps-in-ml><h2 id=1-what-is-rlthree-steps-in-ml><span class=hanchor arialabel=Anchor># </span>1 What is RL?(Three steps in ML)</h2></a><p><img src=https://jieye-ericx.github.io//image-20210911231230408.png width=auto alt=image-20210911231230408></p><p>首先呢,我们会从最基本的 RL 的概念开始,那在介绍这个 RL 概念的时候,有很多不同的切入点啦,也许你比较常听过的切入点是这样,比如说从 Markov Decision Process 开始讲起</p><p>那我们这边选择了一个比较不一样的切入点,我要告诉你说,虽然如果你自己读 RL 的文献的话,你会觉得,哇 RL 很複杂哦,跟一般的 Machine Learning 好像不太一样哦,但是我这边要告诉你说,<strong>RL 它跟我们这一门课学的 Machine Learning,是一样的框架</strong></p><p>我们在今天这个这学期,一开始的第一堂课就告诉你说,Machine Learning 就是三个步骤,那 RL 呢,RL 也是一模一样的三个步骤,等一下会再跟大家说明</p><a href=#11-machine-learning--looking-for-a-function><h3 id=11-machine-learning--looking-for-a-function><span class=hanchor arialabel=Anchor># </span>1.1 Machine Learning ≈ Looking for a Function</h3></a><p>在今天,在这个本学期这一门课的第一开始,就告诉你说,什么是机器学习,<strong>机器学习就是找一个 Function</strong>,Reinforcement Learning,RL 也是机器学习的一种,那它也在找一个 Function,它在找什么样的 Function 呢</p><p>那 Reinforcement Learning 裡面呢,我们会有一个 Actor,还有一个 Environment,那这个 <strong>Actor 跟 Environment,会进行互动</strong></p><p><img src=https://jieye-ericx.github.io//image-20210911232111496.png width=auto alt=image-20210911232111496></p><ul><li>你的这个 Environment,你的这个环境啊,会给 Actor 一个 Observation,会给,那这个 Observation 呢,就是 Actor 的输入</li><li>那 Actor 呢,看到这个 Observation 以后呢,它会有一个输出,这个输出呢,叫做 Action,那这个 Action 呢,会去影响 Environment</li><li>这个 Actor 採取 Action 以后呢,Environment 就会给予新的 Observation,然后 Actor 呢,会给予新的 Action,那这个 Observation 是 Actor 的输入,那这个 Action 呢,是 Actor 的输出</li></ul><p>所以 <strong>Actor 本身啊,它就是一个 Function</strong>,其实 Actor,它就是我们要找的 Function,这个 Function 它的轮入,就是环境给它的 Observation,输出就是这个 Actor 要採取的 Action,而今天在这个互动的过程中呢,<strong>这个 Environment,会不断地给这个 Actor 一些 Reward</strong>,告诉它说,你现在採取的这个 Action,它是好的还是不好的</p><p>而我们今天要找的这个 Actor,我们今天要找的这个 Function,可以拿 Observation 当作 Input,Actor 当作 Output 的 Function,这个 Function 的目标,是要去 Maximizing,我们可以从 Environment,获得到的 Reward 的总和,我们希望呢,找一个 Function,那用这个 Function 去跟环境做互动,<strong>用 Observation 当作 Input,输出 Action,最终得到的 Reward 的总和,可以是最大的,这个就是 RL 要找的 Function</strong></p><a href=#12-example-playing-video-game><h3 id=12-example-playing-video-game><span class=hanchor arialabel=Anchor># </span>1.2 Example: Playing Video Game</h3></a><p>那我知道这样讲,你可能还是觉得有些抽象,所以我们举更具体的例子,那等一下举的例子呢,都是用 Space Invader 当作例子啦,那 Space Invader 就是一个非常简单的小游戏,那 RL 呢,最早的几篇论文,也都是玩,让那个机器呢,去玩这个 Space Invader 这个游戏</p><p>在 Space Invader 裡面呢</p><p><img src=https://jieye-ericx.github.io//image-20210912211107238.png width=auto alt=image-20210912211107238></p><ul><li>你要<strong>操控的是下面这个绿色的东西</strong>,这个下面这个绿色的东西呢,是你的太空梭,你可以採取的行为,也就是 Action 呢 有三个,<strong>左移 右移跟开火</strong>,就这三个行为,然后你现在要做的事情啊,就是杀掉画面上的这些外星人</li><li>画面上这些黄色的东西,也就是<strong>外星人</strong>啦,然后你开火,击中那些外星人的话,那外星人就死掉了</li><li>那前面这些东西是什么呢,那个是你的<strong>防护罩</strong>,如果你不小心打到自己的防护罩的话,你的防护罩呢,也是会被打掉的,那你可以躲在防护罩后面,你就可以挡住外星人的攻击</li><li>然后接下来呢 会有分数,那在萤幕画面上会有分数,当你杀死外星人的时候,你会得到<strong>分数</strong>,或者是在有些版本的 Space Invader 裡面,会有一个补给包,从上面横过去 飞过去,那你打到补给包的话,会被加一个很高的分数,那这个 Score 呢,就是 Reward,就是环境给我们的 Reward</li></ul><p>那这个游戏呢,它是会终止的,那什么时候终止呢,当所有的外星人都被杀光的时候就终止,或者是呢,外星人其实也会对你的母舰开火啦,外星人击中你的母舰 你也是会,这个你就被摧毁了,那这个游戏呢,也就终止了,好 那这个是介绍一下,Space Invader 这一个游戏,</p><p>那如果你今天呢,要用 Actor 去玩 Space Invader,大概会像是什么样子呢</p><p>现在你的 Actor 啊,<strong>Actor 虽然是一个机器,但是它是坐在人的这一个位置,它是站在人这一个角度,去操控摇杆</strong>,去控制那个母舰,去跟外星人对抗,而你的环境是什么,<strong>你的环境呢,是游戏的主机</strong>,游戏的主机这边去操控那些外星人,外星人去攻击你的母舰,所以 Observation 是游戏的画面</p><p><img src=https://jieye-ericx.github.io//image-20210912211546018.png width=auto alt=image-20210912211546018></p><p>所以对 Actor 来说,它看到的,其实就跟人类在玩游戏的时候,看到的东西是一样的,就看到一个游戏的画面</p><p>那输出呢,就是 Actor 可以採取的行为,那可以採取哪些行为,通常是事先定义好的,在这个游戏裡面,就只有向左 向右跟开火,三种可能的行为而已,好 那当你的 Actor 採取向右这个行为的时候,那它会得到 Reward</p><p>那因为在这个游戏裡面,只有<strong>杀掉外星人会得到分数,而我们就是把分数定义成我们的 Reward</strong>,那向左 向右其实并不会,不可能杀掉任何的外星人,所以你得到的 Reward 呢,就是 0 分,好 那你採取一个 Action 以后呢,游戏的画面就变了</p><ul><li>游戏的画面变的时候,就代表了有了新的 Observation 进来</li><li>有了新的 Observation 进来,你的 Actor 就会决定採取新的 Action</li></ul><p><img src=https://jieye-ericx.github.io//image-20210912211850357.png width=auto alt=image-20210912211850357></p><p>你的 Actor 是一个 Function,这个 Function 会根据输入的 Observation,输出对应的 Action,那新的画面进来,假设你的 Actor,现在它採取的行为是开火,而开火这个行为正好杀掉一隻外星人的时候,你就会得到分数,那这边假设得到的分数是 5 分,杀那个外星人,得到的分数是 5 分,那你就得到 Reward 等于 5</p><p>那这个呢,就是拿 Actor 去玩,玩这个 Space Invader 这个游戏的状况,好 那这个 Actor 呢,它想要学习的是什么呢,我们在玩游戏的过程中,会不断地得到 Reward,那在刚才例子裡面,做第一个行为的时候,向右的时候得到的是 0 分,做第二个行为,开火的时候得到的是 5 分,那接下来你採取了一连串行为,都有可能给你分数</p><p>而 Actor 要做的事情,我<strong>们要学习的目标,我们要找的这个 Actor</strong> 就是,我们想要 Learn 出一个 Actor,这个 Actor,这个 Function,我们使用它在这个游戏裡面的时候,<strong>可以让我们得到的 Reward 的总和会是最大的</strong>,那这个就是拿 Actor 去,这个就是 RL 用在玩这个小游戏裡面的时候,做的事情</p><a href=#13-example-learning-to-play-go><h3 id=13-example-learning-to-play-go><span class=hanchor arialabel=Anchor># </span>1.3 Example: Learning to play Go</h3></a><p>那其实如果把 RL 拿来玩围棋,拿来下围棋,其实做的事情跟小游戏,其实也没有那麽大的差别,只是规模跟问题的複杂度不太一样而已</p><p>那如果今天你要让机器来下围棋,那你的 Actor 就是就是 AlphaGo,那你的环境是什么,你的环境就是 AlphaGo 的人类对手</p><p>你的 Actor 的输入就是棋盘,棋盘上黑子跟白子的位置,那如果是在游戏的一开始,棋盘上就空空的,空空如也,上面什么都没有,没有任何黑子跟白子</p><p>那这个 Actor 呢,看到这个棋盘呢,它就要产生输出,它就要决定它下一步,应该落子在哪裡,那如果是围棋的话,你的输出的可能性就是有 19×19 个可能性,那这 19×19 个可能性,每一个可能性,就对应到棋盘上的一个位置</p><p>那假设现在你的 Actor,决定要落子在这个地方</p><p><img src=https://jieye-ericx.github.io//image-20210912212426858.png width=auto alt=image-20210912212426858></p><p>那这一个结果,就会输入给你的环境,那其实就是一个棋士,然后呢 这个环境呢,就会再产生新的 Observation,因为这个李世石这个棋士呢,也会再落一子,那现在看到的环境又不一样了,那你的 Actor 看到这个新的 Observation,它就会产生新的 Action,然后就这样反覆继续下去</p><p><img src=https://jieye-ericx.github.io//image-20210912212456020.png width=auto alt=image-20210912212456020></p><p>你就可以让机器做下围棋这件事情,好 那在这个,在这个下围棋这件事情裡面的 Reward,是怎麽计算的呢</p><p><strong>在下围棋裡面,你所採取的行为,几乎都没有办法得到任何 Reward</strong>,在下围棋这个游戏裡,在下围棋这件事情裡面呢,你会定义说,如果赢了,就得到 1 分,如果输了就得到 -1 分</p><p><img src=https://jieye-ericx.github.io//image-20210912212536103.png width=auto alt=image-20210912212536103></p><p>也就是说在下围棋这整个,这个你的 Actor 跟环境互动的过程中,其实<strong>只有游戏结束,只有整场围棋结束的最后一子,你才能够拿到 Reward</strong>,就你最后,最后 Actor 下一子下去,赢了,就得到 1 分,那最后它落了那一子以后,游戏结束了,它输了,那就得到 -1 分,那在中间整个互动的过程中的 Reward,就都算是 0 分,没有任何的 Reward,那这个 Actor 学习的目标啊,就是要去最大化,它可能可以得到的 Reward。</p><a href=#14-三步走><h3 id=14-三步走><span class=hanchor arialabel=Anchor># </span>1.4 三步走</h3></a><p>刚才讲的也许你都已经听过了,那这个是 RL 最常见的一种解说方式,那接下来要告诉你说,RL 跟机器学习的 Framework,它们之间的关係是什么</p><p><img src=https://jieye-ericx.github.io//image-20210912212810370.png width=auto alt=image-20210912212810370></p><p>开学第一堂课就告诉你说,Machine Learning 就是三个步骤</p><ol><li>第一个步骤,你有一个 Function,那个 Function 裡面有一些未知数,Unknown 的 Variable,这些未知数是要被找出来的</li><li>第二步,订一个 Loss Function,第三步,想办法找出未知数去最小化你的 Loss</li><li>第三步就是 Optimization</li></ol><p>而 RL 其实也是一模一样的三个步骤</p><a href=#step-1-function-with-unknown><h4 id=step-1-function-with-unknown><span class=hanchor arialabel=Anchor># </span>Step 1: Function with Unknown</h4></a><p>第一个步骤,我们现在有未知数的这个 Function,到底是什么呢,这个有未知数的 Function,就是我们的 Actor,那在 RL 裡面,<strong>你的 Actor 呢,就是一个 Network</strong>,那我们<strong>现在通常叫它 Policy 的 Network。</strong></p><p><img src=https://jieye-ericx.github.io//image-20210912213733742.png width=auto alt=image-20210912213733742></p><p>那在过去啊,<strong>在还没有把 Deep Learning 用到 RL 的时候,通常你的 Actor 是比较简单的,它不是 Network,它可能只是一个 Look-Up-Table</strong>,告诉你说看到什么样的输入,就产生什么样的输出,那今天我们都知道要用 Network,来当做这个 Actor,那这个 Network,其实就是一个很複杂的 Function</p><p><img src=https://jieye-ericx.github.io//image-20210912213816733.png width=auto alt=image-20210912213816733></p><p>这个複杂的 Function 它的输入是什么呢,它的<strong>输入就是游戏的画面</strong>,就是游戏的画面,这个游戏画面上的 Pixel,像素,就是这一个 Actor 的输入</p><p>那它的输出是什么呢,它的<strong>输出就是,每一个可以採取的行为</strong>,它的分数,每一个可以採取的 Action 它的分数,举例来说 输入这样的画面,给你的 Actor,你的 Actor 其实就是一个 Network,它的输出可能就是给,向左 0.7 分,向右 0.2 分,开火 0.1 分</p><p>那事实上啊,这件事情<strong>跟分类是没有什么两样</strong>的,你知道分类就是输入一张图片,输出就是决定这张图片是哪一个类别,那你的 Network 会给每一个类别,一个分数,你可能会通过一个 Softmax Layer,然后每一个类别都有个分数,而且这些分数的总和是 1</p><p>那其实在 RL 裡面,你的 Actor 你的 Policy Network,跟分类的那个 Network,其实是一模一样的,你就是输入一张图片,输出其实最后你也会有个 Softmax Layer,然后呢,你就会 Left、Right 跟 Fire,三个 Action 各给一个分数,那这些分数的总和,你也会让它是 1</p><p><img src=https://jieye-ericx.github.io//image-20210912213951581.png width=auto alt=image-20210912213951581></p><p>那至于这个 <strong>Network 的架构呢,那你就可以自己设计了</strong>,要设计怎麽样都行,比如说如果输入是一张图片,欸 也许你就会想要用 CNN 来处理</p><p>不过在助教的程式裡面,其实不是用 CNN 来处理啦,因为在我们的作业裡面,其实在玩游戏的时候,不是直接让我们的 Machine 去看游戏的画面,让它直接去看游戏的,让它直接去看游戏的画面比较难做啦,所以我们是让,看这个跟现在游戏的状况有关的一些参数而已,所以在这个助教的,在这个作业的这个 Sample Code 裡面呢,还没有用到 CNN 那麽複杂,就是一个简单的 Fully Connected Network,但是假设你要让你的 Actor,它的输入真的是游戏画面,欸 那你可能就会採取这个 CNN,你可能就用 CNN 当作你的 Network 的架构</p><p><strong>甚至你可能说</strong>,我不要只看现在这一个时间点的游戏画面,我要看整场游戏到目前为止发生的所有事情,可不可以呢？可以,那过去你可能会用 RNN 考虑,现在的画面跟过去所有的画面,那现在你可能会想要<strong>用 Transformer,考虑所有发生过的事情</strong>,所以 Network 的架构是你可以自己设计的,只要能够输入游戏的画面,输出类似像类别这样的 Action 就可以了,那最后机器会决定採取哪一个 Action,取决于每一个 Action 取得的分数</p><p>常见的做法啊,是直接把这个分数,就当做一个机率,然后按照这个机率,去 Sample,去随机决定要採取哪一个 Action</p><p><img src=https://jieye-ericx.github.io//image-20210912213951581.png width=auto alt=image-20210912213951581></p><p>举例来说 在这个例子裡面,向左得到 0.7 分,那就是有 70% 的机率会採取向左,20% 的机率会採取向右,10% 的机率会採取开火</p><p>那你可能会问说,为什么不是用argmax呢,为什么不是看 Left 的分数最高,就直接向左呢,你也可以这麽做,但是在助教的程式裡面,还有多数 RL 应用的时候 你会发现,我们都是採取 Sample,</p><p>採取 <strong>Sample 有一个好处是说,今天就算是看到同样的游戏画面,你的机器每一次採取的行为,也会略有不同</strong>,那在很多的游戏裡面这种随机性,也许是重要的,比如说你在做剪刀石头布的时候,如果你总是会出石头,就跟小叮噹一样,那你就很容易被打爆,如果你有一些随机性,就比较不容易被打爆</p><p>那其实之所以今天的输出,是用随机 Sample 的,还有另外一个很重要的理由,那这个我们等一下会再讲到,好 所以这是第一步,我们有一个 Function,这个 Function 有 Unknown 的 Variable,我们有一个 Network,那裡面有参数,这个参数就是 Unknown 的 Variable,就是要被学出来的东西,这是第一步</p><a href=#step-2-define-loss><h4 id=step-2-define-loss><span class=hanchor arialabel=Anchor># </span>Step 2: Define “Loss”</h4></a><p>然后接下来第二步,我们要定义 Loss,在 RL 裡面,我们的 Loss 长得是什么样子呢,我们再重新来看一下,我们的机器跟环境互动的过程,那只是现在用不一样的方法,来表示刚才说过的事情</p><p><img src=https://jieye-ericx.github.io//image-20210912214755516.png width=auto alt=image-20210912214755516></p><p>首先有一个初始的游戏画面,这个初始的游戏画面,被作为你的 Actor 的输入</p><p>你的 Actor 那就输出了一个 Action,比如说向右,输入的游戏画面呢,我们叫它 s1,然后输出的 Action 呢,就叫它 a1</p><p>那现在会得到一个 Reward,这边因为向右没有做任何事情,没有杀死任何的外星人,所以得到的 Reward 可能就是 0 分</p><p><img src=https://jieye-ericx.github.io//image-20210912214950357.png width=auto alt=image-20210912214950357></p><p>採取向右以后,会看到新的游戏画面,这个叫做 s2,根据新的游戏画面 s2,你的 Actor 会採取新的行为,比如说开火,这边用 a2,来表示看到游戏画面 s2 的时候,所採取的行为</p><p>那假设开火恰好杀死一隻外星人,和你的 Actor 就得到 Reward,这个 Reward 的分数呢,是 5 分,然后採取开火这个行为以后</p><p><img src=https://jieye-ericx.github.io//image-20210912215036248.png width=auto alt=image-20210912215036248></p><p>接下来你会看到新的游戏画面,那机器又会採取新的行为,那这个互动的过程呢,就会反覆持续下去,直到机器在採取某一个行为以后,游戏结束了,那什么时候游戏结束呢,就看你游戏结束的条件是什么嘛</p><p>举例来说,採取最后一个行为以后,比如说向右移,正好被外星人的子弹打中,那你的飞船就毁了,那游戏就结束了,或者是最后一个行为是开火,把最后一隻外星人杀掉,那游戏也就结束了,就你执行某一个行为,满足游戏结束的条件以后,游戏就结束了</p><p><img src=https://jieye-ericx.github.io//image-20210912215129396.png width=auto alt=image-20210912215129396></p><p>那<strong>从游戏开始到结束的这整个过程啊,被称之为一个 Episode</strong>,那在整个游戏的过程中,机器会採取非常多的行为,每一个行为都可能得到 Reward,把所有的 Reward 通通集合起来,我们就得到一个东西,叫做整场游戏的 Total Reward,</p><p><img src=https://jieye-ericx.github.io//image-20210912215203605.png width=auto alt=image-20210912215203605></p><p>那这个 Total Reward 呢,就是从游戏一开始得到的 r1,一直得,一直加,累加到游戏最后结束的时候,得到的 rt,假设这个游戏裡面会互动,T 次,那麽就得到一个 Total Reward,我们这边用 R,来表示 Total Reward,其实这个 Total Reward 又有另外一个名字啊,叫做 Return 啦,你在这个 RL 的文献上,常常会同时看到 Reward 跟 Return,这两个词会出现,那 Reward 跟 Return 其实有点不一样,Reward 指的是你採取某一个行为的时候,立即得到的好处,这个是 Reward,把整场游戏裡面所有的 Reward 通通加起来,这个叫做 Return</p><p>但是我知道说,很快你就会忘记 Reward 跟 Return 的差别了,所以我们等一下就不要再用 Return 这个词彙,我们直接告诉你说,整场游戏的 Reward 的总和,就是 Total 的 Reward,而这个 Total 的 Reward 啊,就是我们想要去最大化的东西,就是我们训练的目标</p><p>那你可能会说,欸 这个跟 Loss 不一样啊,Loss 是要越小越好啊,这个 Total Reward 是要越大越好啊,所以有点不一样吧,但是我们可以说在 RL 的这个情境下,我们把那个 Total Reward 的负号,<strong>负的 Total Reward,就当做我们的 Loss,Total Reward 是要越大越好,那负的 Total Reward,当然就是要它越小越好</strong>吧,就我们完全可以说负的 Total Reward,就是我们的 Loss,就是 RL 裡面的 Loss</p><a href=#step-3-optimization><h4 id=step-3-optimization><span class=hanchor arialabel=Anchor># </span>Step 3: Optimization</h4></a><p>那我们再把这个环境跟,Agent 互动的这一件事情啊,再用不一样的图示,再显示一次</p><p><img src=https://jieye-ericx.github.io//image-20210912220011837.png width=auto alt=image-20210912220011837></p><p>这个是你的环境,你的环境呢,输出一个 Observation,叫做 s1</p><ul><li>这个 s1 呢,会变成你的 Actor 的输入</li><li>你的 Actor 呢,接下来就是输出 a1</li><li>然后这个 a1 呢,又变成环境的输入</li><li>你的环境呢,看到 a1 以后,又输出 s2</li></ul><p>然后这个互动的过程啊,就会继续下去,s2 又输入给 Actor,它就输出 a2,a2 又输入给 Environment,它就输出给,它就产生 s3</p><p>它这个互动呢,一直下去,直到满足游戏中止的条件,好 那这个 s 跟 a 所形成的这个 Sequence,就是 s1 a1 s2 a2 s3 a3 <strong>这个 Sequence,又叫做 Trajectory</strong>,那我们用 τ来表示 Trajectory</p><p><img src=https://jieye-ericx.github.io//image-20210912220319849.png width=auto alt=image-20210912220319849></p><p>那根据这个互动的过程,Machine 会得到 Reward,你其实可以把 Reward 也想成是一个 Function,我们这边用一个绿色的方块来代表,这个 Reward 所构成的 Function</p><p>那这个 Reward 这个 Function,有不同的表示方法啦,在有的游戏裡面,也许你的 Reward,只需要看你採取哪一个 Action 就可以决定,不过通常我们在决定 Reward 的时候,光看 Action 是不够的,你还要看现在的 Observation 才可以,因为并不是每一次开火你都一定会得到分数,开火要正好有击到外星人,外星人正好在你前面,你开火才有分数</p><p>所以通常 <strong>Reward Function 在定义的时候,不是只看 Action,它还需要看 Observation</strong>,同时看 Action 跟 Observation,才能够知道现在有没有得到分数,所以 Reward 是一个 Function,</p><p>这个 Reward 的 Function,它拿 a1 跟 s1 当作输入,然后它产生 r1 作为输出,它拿 a2 跟 s2 当作输入,产生 r2 作为输出,把所有的 r 通通结合起来,把 r1 加 r2 加 r3 一直加到 T,全部结合起来就得到 R,这个就是 Total Reward,也就是 Return,这个是我们要最大化要去 Maximize 的对象</p><p>这个 Optimization 的问题是这个样子,你要去找一个 Network,其实是 Network 裡面的参数,你要去 Learn 出一组参数,这一组参数放在 Actor 的裡面,它可以让这个 R 的数值越大越好,就这样,结束了,整个 Optimization 的过程就是这样,你要去找一个 Network 的参数,让这边产生出来的 R 越大越好</p><p><img src=https://jieye-ericx.github.io//image-20210912220319849.png width=auto alt=image-20210912220319849></p><p>那乍看之下,如果这边的,这个 Environment Actor 跟 Reward,它们<strong>都是 Network 的话,这个问题其实也没有什么难的</strong>,这个搞不好你现在都可以解,它看起来就有点像是一个 Recurrent Network,这是一个 Recurrent Network,然后你的 Loss 就是这个样子,那只是这边是 Reward 不是 Loss,所以你是要让它越大越好,你就去 Learn 这个参数,用 Gradient Descent 你就可以让它越大越好</p><p>但是 RL 困难的地方是,<strong>这不是一个一般的 Optimization 的问题</strong>,因为你的 Environment,这边有很多问题导致说,它跟一般的 Network Training 不太一样</p><p>第一个问题是,<strong>你的 Actor 的输出是有随机性的</strong></p><p><img src=https://jieye-ericx.github.io//image-20210912221135459.png width=auto alt=image-20210912221135459></p><p>这个 <strong>a1 它是用 Sample 产生的</strong>,你定同样的 s1 每次产生的 a1 不一定会一样,所以假设你把 Environment Actor 跟 Reward,合起来当做是一个巨大的 Network 来看待,这个 Network 可不是一般的 Network,<strong>这个 Network 裡面是有随机性的</strong>,这个 Network 裡面的某一个 Layer 是,每次产生出来结果是不一样的,<strong>这个 Network 裡面某一个 Layer 是,它的输出每次都是不一样的</strong></p><p>另外还有一个更大的问题就是,<strong>你的 Environment 跟 Reward,它根本就不是 Network 啊,它只是一个黑盒子而已</strong>,你根本不知道裡面发生了什么事情,Environment 就是游戏机,那这个游戏机它裡面发生什么事情你不知道,你只知道说你输入一个东西会输出一个东西,你採取一个行为它会有对应的回应,但是到底是怎麽产生这个对应的回应,我们不知道,它只是一个黑盒子,</p><p><strong>Reward 可能比较明确,但它也不是一个 Network,它就是一条规则</strong>嘛,它就是一个规则说,看到这样子的 Optimization 跟这样的 Action,会得到多少的分数,它就只是一个规则而已,所以它也不是 Network</p><p>而且更麻烦的地方是,<strong>往往 Reward 跟 Environment,它也是有随机性的</strong>,如果是在电玩裡面,通常 Reward 可能比较不会有随机性,因为规则是定好的,对有一些 RL 的问题裡面,Reward 是有可能有随机性的</p><p>但是在 Environment 裡面,就算是在电玩的这个应用中,它也是有随机性的,你给定同样的行为,到底游戏机会怎麽样回应,它裡面可能也是有乱数的,它可能每次的回应也都是不一样,如果是下围棋,你落同一个子,你落在,你落子在同一个位置,你的对手会怎麽样回应,每次可能也是不一样</p><p><img src=https://jieye-ericx.github.io//image-20210912230423421.png width=auto alt=image-20210912230423421></p><p>所以环境很有可能也是有随机性的,所以这不是一个一般的 Optimization 的问题,你可能不能够用我们这门课已经学过的,训练 Network 的方法来找出这个 Actor,来最大化 Reward</p><p><strong>所以 RL 真正的难点就是,我们怎麽解这一个 Optimization 的问题</strong>,怎麽找到一组 Network 参数,可以让 R 越大越好</p><p>其实你再仔细想一想啊,这整个问题<strong>跟 GAN 其实有异曲同工之妙</strong>,它们有一样的地方,也有不一样的地方</p><ul><li>先说它们<strong>一样</strong>的地方在哪裡,你记不记得在训练 GAN 的时候,在训练 Generator 的时候,你会把 Generator 跟 Discriminator 接在一起,然后你希望去调整 Generator 的参数,让 Discriminator 的输出越大越好,今天在 RL 裡面,我们也可以说这个 Actor 就像是 Generator,Environment 跟 Reward 就像是 Discriminator,我们要去<strong>调整 Generator 的参数,让 Discriminator 的输出越大越好</strong>,所以它跟 GAN 有异曲同工之妙</li><li>但什么地方<strong>不一样</strong>呢,在 GAN 裡面你的 Discriminator,也是一个 Neural Network,你了解 Discriminator 裡面的每一件事情,它也是一个 Network,你可以用 Gradient Descent,来 train 你的 Generator,让 Discriminator 得到最大的输出,但是在 RL 的问题裡面,你的 <strong>Reward 跟 Environment,你可以把它们当 Discriminator 来看,但它们不是 Network,它们是一个黑盒子,所以你没有办法用,一般 Gradient Descent 的方法来调整你的参数,来得到最大的输出</strong>,所以这是 RL,跟一般 Machine Learning不一样的地方</li></ul><p>但是我们还是可以把 RL 就看成三个阶段,只是在 Optimization 的时候,在你怎麽 Minimize Loss,也就怎麽 Maximize Reward 的时候,跟之前我们学到的方法是不太一样的</p><a href=#2-policy-gradient><h2 id=2-policy-gradient><span class=hanchor arialabel=Anchor># </span>2 Policy Gradient</h2></a><p>接下来啊,我们就要讲一个拿来解 RL,拿来做 Optimization 那一段常用的一个演算法,叫做 Policy Gradient</p><p><img src=https://jieye-ericx.github.io//image-20210912231113114.png width=auto alt=image-20210912231113114></p><p>那如果你真的想知道,Policy Gradient 是哪裡来的,你可以参见过去上课的
<a href=https://youtu.be/W8XF3ME8G2I rel=noopener>录影</a>,对 Policy Gradient 有比较详细的推导,那今天我们是从另外一个角度,来讲 Policy Gradient 这件事情</p><a href=#21-how-to-control-your-actor><h3 id=21-how-to-control-your-actor><span class=hanchor arialabel=Anchor># </span>2.1 How to control your actor</h3></a><p>那在讲 Policy Gradient 之前,我们先来想想看,我们要怎麽操控一个 Actor 的输出,我们要怎麽让一个 Actor,在看到某一个特定的 Observation 的时候,採取某一个特定的行为呢,我们怎麽让一个 Actor,它的输入是 s 的时候,它就要输出 Action a^ 呢</p><p><img src=https://jieye-ericx.github.io//image-20210912231731634.png width=auto alt=image-20210912231731634></p><p>那你其实完全可以把它<strong>想成一个分类的问题</strong>,也就是说假设你要让 Actor 输入 s,输出就是 a^,假设 a^ 就是向左好了,假设你要让,假设你已经知道,假设你就是要教你的 Actor 说,看到这个游戏画面向左就是对的,你就是给我向左,那你要怎麽让你的 Actor 学到这件事呢</p><p><img src=https://jieye-ericx.github.io//image-20210912231832863.png width=auto alt=image-20210912231832863></p><p>那也就说 s 是 Actor 的输入,a^ 就是我们的 Label,就是我们的 Ground Truth,就是我们的正确答案,而接下来呢,你就可以计算你的 Actor,它的输出跟 Ground Truth 之间的 Cross-entropy,那接下来你就可以定义一个 Loss</p><p>假设你希望你的 Actor,它<strong>採取 a^ 这个行为的话</strong>,你就定一个 Loss,这个 Loss 等于 Cross-entropy</p><p><img src=https://jieye-ericx.github.io//image-20210912232003035.png width=auto alt=image-20210912232003035></p><p>然后呢,你再去 Learn 一个 θ,你再去 Learn 一个 θ,然后这个 θ 可以让 Loss 最小,那你就可以让这个 Actor 的输出,跟你的 Ground Truth 越接近越好</p><p>你就可以让你的 Actor 学到说,看到这个游戏画面的时候,它就是要向左,这个是要让你的 Actor,採取某一个行为的时候的做法</p><p>但是假设你想要让你的 Actor,<strong>不要採取某一个行为</strong>的话,那要怎麽做呢,假设你希望做到的事情是,你的 Actor 看到某一个 Observation s 的时候,我就千万不要向左的话怎麽做呢,其实很容易,你<strong>只需要把 Loss 的定义反过来就好</strong></p><p><img src=https://jieye-ericx.github.io//image-20210912232223834.png width=auto alt=image-20210912232223834></p><p>你希望你的 Actor 採取 a^ 这个行为,你就定义你的大 L 等于 Cross-entropy,然后你要 Minimize Cross-entropy,假设你要让你的 Actor,不要採取 a^ 这个行为的话,那你就把你就定一个 Loss,叫做负的 Cross-entropy,Cross-entropy 乘一个负号,那你去 Minimize 这个 L,你去 Minimize 这个 L,就是让 Cross-entropy 越大越好,那也就是让 a 跟 a^ 的距离越远越好,那你就可以避免你的 Actor 在看到 s 的时候,去採取 a^ 这个行为,所以我们有办法控制我们的 Actor,做我们想要做的事,只要我们给它适当的 Label 跟适当的 Loss,</p><p>所以假设我们要让我们的 Actor,<strong>看到 s 的时候採取 a^,看到 s&rsquo; 的时候不要採取 a^&rsquo;</strong> 的话,要怎麽做呢</p><p>这个时候你就会说,Given s 这个 Observation,我们的 Ground Truth 叫做 a^,Given s&rsquo; 这个 Observation 的时候,我们有个 Ground Truth 叫做 a^&rsquo;,那对这两个 Ground Truth, 我们都可以去计算 Cross-entropy,e1 跟 e2</p><p><img src=https://jieye-ericx.github.io//image-20210912232542292.png width=auto alt=image-20210912232542292></p><p>然后接下来呢,我们就定义说我们的 Loss,就是 e1 减 e2,也就是说我们要让这个 Case,它的 Cross-entropy 越小越好,这个 Case 它的 Cross-entropy 越大越好</p><p>然后呢,我们去找一个 θ 去 Minimize Loss,得到 θ⋆,那就是一个可以在 s,可以在看到 s 的时候採取 a^,看到 s&rsquo; 的时候採取 a^&rsquo; 的 Actor,所以藉由很像是在,Train 一个 Classifier 的这种行为,藉由很像是现在 Train 一个 Classifier,的这种 Data,我们可以去控制一个 Actor 的行为,</p><p>有一个同学问了一个非常好的问题</p><ul><li><p>**Q:**就是如果以 Alien 的游戏来说的话,因为只有射中 Alien 才会有 Reward,这样 Model 不是就会一直倾向于射击吗</p><p>**A:**对 这个问题我们等一下会来解决它,之后的投影片就会来解决它</p></li></ul><p>然后又有另外一个同学,问了一个非常好的问题就是</p><ul><li>**Q:**哇 这样不就回到 Supervised Learning 了嘛,这个投影片上看起来,就是在训练一个 Classifier 而已啊,我们就是在训练 Classifier,你只是告诉它说,看到 s 的时候就要输出 a^,看到 s&rsquo; 的时候就不要输出 a^,a^&rsquo;,这不就是 Supervised Learning 吗</li><li>**A:**这就是 Supervised Learning,这个就是跟 Supervised Learning Train 的,Image Classifier 是一模一样的,但等下我们会看到它跟,一般的 Supervised Learning 不一样在哪裡,</li></ul><p>那所以呢,如果我们要训练一个 Actor,我们其实就需要收集一些训练资料,就收集训练资料说,我希望在 s1 的时候採取 a^1,我希望在 s2 的时候不要採取 a^2</p><p><img src=https://jieye-ericx.github.io//image-20210912233019856.png width=auto alt=image-20210912233019856></p><p>但可能会问说,欸 这个训练资料哪来的,这个我们等一下再讲训练资料哪来的</p><p>所以你就<strong>收集一大堆的资料,这个跟 Train 一个 Image 的 Classifier 很像的</strong>,这个 s 你就想成是 Image,这个 a^ 你就想成是 Label,只是现在有的行为是想要被採取的,有的行为是不想要被採取的,你就收集一堆这种资料,你就可以去定义一个 Loss Function,有了这个 Loss Function 以后,你就可以去训练你的 Actor,去 <strong>Minimize 这个 Loss Function,就结束了</strong>,你就可以训练一个 Actor,期待它执行我们的行为,期待它执行的行为是我们想要的</p><p>而你甚至还可以更进一步,你可以说<strong>每一个行为并不是只有好或不好</strong>,并不是有想要执行跟不想要执行而已,<strong>它是有程度的差别的</strong>,有执行的非常好的,有 Nice to have 的,有有点不好的,有非常差的</p><p>所以刚才啊,我们是说每一个行为就是要执行 不要执行,这是一个 Binary 的问题,这是我们就用 ±1 来表示</p><p><img src=https://jieye-ericx.github.io//image-20210912233324323.png width=auto alt=image-20210912233324323></p><p>但是现在啊,我们改成<strong>每一个 s 跟 a 的 Pair,它有对应的一个分数</strong>,这个分数代表说,我们多希望机器在看到 s1 的时候,执行 a^1 这个行为</p><p>那比如说这边第一笔资料跟第三笔资料,我们分别是定 +1.5 跟 +0.5,就代表说我们期待机器看到 s1 的时候,它可以做 a^1,看到 s3 的时候它可以做 a^3,但是我们期待它看到 s1 的时候,做 a^1 的这个期待更强烈一点,比看到 s3 做 a^3 的期待更强烈一点</p><p>那我们希望它在看到 s2 的时候,不要做 a^2,我们期待它看到 sN 的时候,不要做 a^N,而且我们非常不希望,它在看到 sN 的时候做 a^N</p><p>有了这些资讯,你一样可以定义一个 Loss Function,你只是在你的原来的 Cross-entropy 前面,本来是 Cross-entropy 前面,要嘛是 +1 要嘛是 -1</p><p>现在改成乘上 An 这一项,改成乘上 An 这一项,告诉它说有一些行为,我们非常期待 Actor 去执行,有一些行为我们非常不期待 Actor 去执行,有一些行为如果执行是比较好的,有一些行为希望儘量不要执行比较好,但就算执行了也许伤害也没有那麽大</p><p>所以我们<strong>透过这个 An 来控制说,每一个行为我们多希望 Actor 去执行</strong>,然后接下来有这个 Loss 以后,一样 Train 一个 θ,Train 下去你就找一个 θ⋆,你就有个 Actor 它的行为是符合我们期待的</p><p><img src=https://jieye-ericx.github.io//image-20210912233513464.png width=auto alt=image-20210912233513464></p><p>那接下来的难点就是,要怎麽定出这一个 a 呢,这个就是我们接下来的难点,就是我们接下来要面对的问题,我们还有另外一个要面对的问题是,怎麽产生这个 s 跟 a 的 Pair 呢,怎麽知道在 s1 的时候要执行 a1,或在 s2 的时候不要执行 a2 呢,那这个也是等一下我们要处理的问题,</p><p>讲到目前为止,你可能觉得跟 Supervised LearNing,没有什么不同,那确实就是没有什么不同,接下来真正的重点是,在我们怎么定义 a 上面：</p><a href=#22-version-0><h3 id=22-version-0><span class=hanchor arialabel=Anchor># </span>2.2 Version 0</h3></a><p>那先讲一个最简单的,但是其实是不正确的版本,那这个其实也是,助教的 Sample Code 的版本,那这个不正确的版本是怎么做的呢</p><p>首先我们还是需要收集一些训练资料,就是需要<strong>收集 s 跟 a 的 Pair</strong></p><p>怎么收集这个 s 跟 a 的 Pair 呢?</p><p>你需要先有一个 Actor,这个 Actor 去跟环境做互动,它就可以收集到 s 跟 a 的 Pair</p><p>那这个 Actor 是哪裡来的呢,你可能觉得很奇怪,我们今天的目标,不就是要训练一个 Actor 吗,那你又说你需要拿一个 Actor,去跟环境做互动,然后把这个 Actor,它的 s 跟 a 记录下来,那这个 Actor 是哪裡来的呢?</p><p>你先把这个 Actor,想成就是一个<strong>随机的 Actor</strong> 好了,就它是一个,它就是一个随机的东西,那看到 s1,然后它执行的行为就是乱七八糟的,就是随机的,但是我们会把它在,每一个 s 执行的行为 a,通通都记录下来,好 那通常我们在这个收集资料的话,你不会只把 Actor 跟环境做一个 Episode,通常会做多个 Episode,然后期待你可以收集到足够的资料,比如说在助教 Sample Code裡面,可能就是跑了 5 个 Episode,然后才收集到足够的资料</p><p><img src=https://jieye-ericx.github.io//image-20211025092210834.png width=auto alt=image-20211025092210834></p><p>所以我们就是去观察,某一个 Actor 它跟环境互动的状况,那把这个 Actor,它在每一个 Observation,执行的 Action 都记录下来,然后接下来,我们就去<strong>评价每一个 Action,它到底是好还是不好</strong>,评价完以后,我们就可以拿我们评价的结果,来训练我们的 Actor</p><p>那怎么评价呢,我们刚才有说,我们会用 A 这一个东西,来评价在每一个 Step,我们希不希望我们的 Actor,採取某一个行为,那最简单的评价方式是,假设在某一个 Step s1,我们执行了 a1,然后得到 Reward r1</p><ul><li>那 Reward 如果如果是正的,那也许就代表这个 Action 是好的</li><li>那如果 Reward 是负的,那也许就代表这个 Action 是不好的</li></ul><p>那我们就把这个 Reward r1 r2,当做是 a,A1 就是 r1,A2 就是 r2,A3 就是 r3,AN 就是 rN,那这样等同于你就告诉 machine 说,如果我们执行完某一个 Action,a1 那得到的 <strong>Reward 是正的,那这就是一个好的 Action</strong>,你以后看到 s1 就要执行 a1,如果今天在 s2 执行 a2,得到 Reward 是负的,就代表 a2 是不好的 a2,就代表所以以后看到 s2 的时候,就不要执行 a2</p><p><img src=https://jieye-ericx.github.io//image-20211025092531818.png width=auto alt=image-20211025092531818></p><p>那这个,那这个 Version 0,它并不是一个好的版本,为什么它不是一个好的版本呢,因为你用这一个方法,你把 a1 设为 r1,A2 设为 r2,这个方法认出来的 Network,它是一个<strong>短视近利的 Actor</strong>,它就是一个只知道会一时爽的 Actor,它<strong>完全没有长程规划的概念</strong></p><p><img src=https://jieye-ericx.github.io//image-20211025092851736.png width=auto alt=image-20211025092851736></p><ul><li><p>我们知道说每一个行为,其实都会影响互动接下来的发展,也就是说 Actor 在 s1 执行 a1 得到 r1,这个并不是互动的全部,因为 a1 影响了我们接下来会看到 s2,s2 会影响到接下来会执行 a2,也影响到接下来会产生 r2,所以 a1 也会影响到,我们会不会得到 r2,所以<strong>每一个行为并不是独立的,每一个行为都会影响到接下来发生的事情</strong>,</p></li><li><p>而且我们今天在跟环境做互动的时候,有一个问题叫做,Reward Delay,就是有时候你需要<strong>牺牲短期的利益,以换取更长程的目标</strong>,如果在下围棋的时候,如果你有看天龙八部的时候你就知道说,这个虚竹在破解珍珑棋局的时候,堵死自己一块子,让自己被杀了很多子以后,最后反而赢了整局棋</p><p>那如果是在这个,Space Invaders 的游戏裡面,你可能需要先左右移动一下进行瞄准,然后射击才会得到分数,而左右移动这件事情是,没有任何 Reward 的,左右移动这件事情得到的 Reward 是零,只有射击才会得到 Reward,但是并不代表左右移动是不重要的,我们会先需要左右移动进行瞄准,那我们的射击才会有效果,所以有时候我们会,需要牺牲一些近期的 Reward,而换取更长程的 Reward</p></li><li><p>所以今天假设我们用 Version 0,会发生说今天 Machine,只要是採取向左跟向右,它得到的 Reward 会是 0,如果它採取开火,它得到的 Reward 就会,只有开火的时候,它得到的 Reward 才会是正的,才会是正的,那这样 Machine 就会学到,它只有疯狂狂开火才是对的,因为只有开火这件事才会得到 Reward,其它行为都不会得到 Reward,所以其它行为都是不被鼓励的,只有开火这件事是被鼓励的,那个 Version 0 就只会学到疯狂开火而已,那 Version 0 是助教的范例程式,那这个当然也是可以执行的,那只是它的结果不会太好而已,那助教范例十,程式之所以是 Version 0 是因为,我不知道为什么这个 Version 0,好像是大家,如果你自己在 Implement rl 的时候,你特别容易犯的错误,你特别容易拿自己 Implement 的时候,就直接使用 Version 0,但是得到一个很差的结果</p></li></ul><p>所以接下来怎么办呢,我们开始正式进入 rl 的领域,真正来看 Policy Gradient 是怎么做的,所以我们需要有 Version 1</p><a href=#23-version-1><h3 id=23-version-1><span class=hanchor arialabel=Anchor># </span>2.3 Version 1</h3></a><p>在 Version 1 裡面,a1 它有多好,不是在取决于 r1,而是取决于 <strong>a1 之后所有发生的事情</strong>,我们会把 a1,执行完 a1 以后,所有得到的 Reward,r1 r2 r3 到 rN,通通集合起来,通通加起来,得到一个数值叫做 G1,然后我们会说 a1 就等于 G1,我们拿这个 G1,来当作评估一个 Action 好不好的标准</p><p><img src=https://jieye-ericx.github.io//image-20211025095137691.png width=auto alt=image-20211025095137691></p><p>刚才是直接拿 r1 来评估,现在不是,拿 G1 来评估,那接下来所有发生的 r 通通加起来,拿来评估 a1 的好坏,因为我们执行完 a1 以后,就发生这么一连串的事情,那这么一连串的事情加起来,也许就可以评估 a1,到底是不是一个好的 Action</p><p>所以以此类推,a2 它有多好呢,就把执行完 a2 以后,所有的 r,r2 到 rN,通通加起来得到 G2,然后那 a3 它有多好呢,就把执行完 a3 以后,所有的 r 通通加起来,就得到 G3,所以把这些东西通通都加起来,就把那 这些这个 G,叫做 Cumulated Reward,叫做累积的 Reward,把未来所有的 Reward 加起来,来评估一个 Action 的好坏,那像这样子的方法听起来就合理多了</p><p>Gt 是什么呢,就是从 t 这个时间点开始,我们把 rt 一直加到 rN,全部合起来就是,Cumulated 的 Reward Gt,那当我们用,Cumulated 的 Reward 以后,我们就可以解决 Version 0 遇到的问题,因为你可能向右移动以后进行瞄准,接下来开火,就有打中外星人,那这样向右这件事情,它也有 Accumulate Reward,虽然向右这件事情没有立即的 Reward,假设 a1 是向右,那 r1 可能是 0,但接下来可能会因为向右这件事,导致有瞄准,导致有打到外星人,那 Cumulated 的 Reward 就会正的,那我们就会知道说,其实向右也是一个好的 Action,这个是 Version 1</p><p>但是你仔细想一想会发现,Version 1 好像也有点问题</p><p><strong>假设这个游戏非常地长</strong>,你<strong>把 rN 归功于 a1 好像不太合适</strong>吧,就是当我们採取 a1 这个行为的时候,立即有影响的是 r1,接下来有影响到 r2,接下来影响到 r3,那假设这个过程非常非常地长的话,那我们说因为有做 a1,导致我们可以得到 rN,这个可能性应该很低吧,也许得到 rN 的功劳,不应该归功于 a1,好 所以怎么办呢</p><a href=#24-version-2><h3 id=24-version-2><span class=hanchor arialabel=Anchor># </span>2.4 Version 2</h3></a><p>有第二个版本的 Cumulated 的 Reward,我们这边用 G&rsquo;,来表示 Cumulated 的 Reward,好 这个我们会在 r 前面,乘一个 Discount 的 Factor</p><p><img src=https://jieye-ericx.github.io//image-20211025095219890.png width=auto alt=image-20211025095219890></p><p>这个 Discount 的 Factor γ,也会设一个小于 1 的值,有可能会设,比如说 0.9 或 0.99 之类的,所以这个 G'1 相较于 G1 有什么不同呢,G1 是 r1 加 r2 加 r3,那 G'1 呢,是 r1 加 γr2 加 γ 平方 r3,就是距离採取这个 Action 越远,我们 γ 平方项就越多,所以 r2 距离 a1 一步,就乘个 γ,r3 距离 a1 两步,就乘 γ 平方,那这样一直加到 rN 的时候,rN 对 G'1 就几乎没有影响力了,因为你 γ 乘了非常非常多次了,γ 是一个小于 1 的值,就算你设 0.9,0.9 的比如说 10 次方,那其实也很小了</p><p>所以你今天用这个方法,就可以<strong>把离 a1 比较近的那些 Reward,给它比较大的权重</strong>,离我比较远的那些 Reward,给它比较小的权重,所以我们现在有一个新的 A,这个新的 A 这个评估,这个 Action 好坏的这个 A,我们现在用 G'1 来表示它,那它的式子可以写成这个样子,这个 G&rsquo;t 就是 Summention over,n 等于 t 到 N,然后我们把 rN 乘上 γ 的 n-t 次方,所以离我们现在,採取的 Action 越远的那些 Reward,它的 γ 就被乘越多次,它对我们的 G&rsquo; 的影响就越小,这是第二个版本,听到这边你是不是觉得合理多了呢</p><a href=#qa><h3 id=qa><span class=hanchor arialabel=Anchor># </span>Q&A</h3></a><p>Q1:一个大括号是一个 Episode,还是这样蓝色的框住的多个大括号,是一个 Episode</p><p>A1: 一个大括号不是一个 Episode,一个大括号是,我们在这一个 Observation,执行这一个 Action 的时候,这个是一笔资料,它不是一个 Episode,Episode 是很多的,很多次的 Observation,跟很多次的 Action 才合起来,才是一个 Episode</p><p>Q2: G1 需不需要做标准化之类的动作</p><p>A2: 这个问题太棒了,为什么呢,因为这个就是 Version 3</p><p>Q3: 越早的动作就会累积到越多的分数吗,越晚的动作累积的分数就越少</p><p>A3: 对 没错 是,在这个设计的情境裡面,是,越早的动作就会累积到越多的分数,但是这个其实也是一个合理的情境,因为你想想看,比较早的那些动作对接下来的影响比较大,到游戏的终局,没什么外星人可以杀了,你可能做什么事对结果影响都不大,所以比较早的那些 Observation,它们的动作是我们可能需要特别在意的,不过像这种 A 要怎么决定,有很多种不同的方法,如果你不想要比较早的动作 Action 比较大,你完全可以改变这个 A 的定义,事实上不同的 rl 的方法,其实就是在 A 上面下文章,有不同的定义 A 的方法</p><p>Q4: 看来仍然不适合用在围棋之类的游戏,毕竟围棋这种游戏只有结尾才有分数</p><p>A4: 这是一个好问题,这个我们现在讲的这些方法,到底能不能够处理,这种结尾才有分数的游戏呢,其实也是可以的,怎么说呢,假设今天只有 rN 有分数,其它通通都是 0,那会发生什么事,那会发生说,今天我们採取一连串的 Action,<strong>只要最后赢了,这一串的 Action 都是好的,如果输了,这一连串的 Action,通通都算是不好的</strong>,而你可能会觉得这样做,感觉 Train Network 应该会很难 Train,确实很难 Train,但是就我所知,最早的那个版本的 AlphaGo,它是这样 Train 的,很神奇,它就是这样做的,它裡面有用到这样子的技术,当然还有一些其它的方法,比如说 Value Network 等等,那这个等一下也会讲到,那最早的 AlphaGo,它有採取这样子的技术来做学习,它有试著採取这样的技术,看起来是学得起来的,拿预估的误差当 Reward,拿,有一个同学说那其实 AlphaGo,可以拿预估的误差当 Reward,那你要有一个办法先预估误差,那你才拿它来当 Reward,那有没有办法事先预估,我们接下来会得到多少的 Reward 呢,有 那这个在之后的版本裡面,会有这样的技术,但我目前还没有讲到那一块,好 那我们接下来就讲 Version 3,</p><a href=#25-version-3><h3 id=25-version-3><span class=hanchor arialabel=Anchor># </span>2.5 Version 3</h3></a><p>Version 3 就是像刚才同学问的,要不要做标准化呢?</p><p>要,因为好或坏是相对的,<strong>好或坏是相对的</strong>,怎么说好或坏是相对的呢,假设所有,假设今天在这个游戏裡面,你每次採取一个行动的时候,最低分就预设是 10 分,那你其实得到 10 分的 Reward,根本算是差的,就好像说今天你说你得,在某一门课得到 60 分,这个叫做好或不好,还是不好呢,没有人知道</p><p>因为那要看别人得到多少分数,如果别人都是 40 分,你是全班最高分,那你很厉害,如果别人都是 80 分,你是全班最低分,那就很不厉害,所以 Reward 这个东西是相对的</p><p><img src=https://jieye-ericx.github.io//image-20211025102800545.png width=auto alt=image-20211025102800545></p><p>所以如果我们只是单纯的把 G 算出来,你可能会遇到一个问题,假设这个游戏裡面,可能永远都是拿到正的分数,每一个行为都会给我们正的分数,只是有大有小的不同,那你这边 G 算出来通通都会是正的,有些行为其实是不好的,但是你 仍然会鼓励你的 Model,去採取这些行为</p><p>所以怎么办,我们需要做一下标准化,那这边先讲一个最简单的方法就是,<strong>把所有的 G&rsquo; 都减掉一个 b</strong>,这个 b 在这边叫做,在 rl 的文献上通常叫做 Baseline,那这个跟我们作业的 Baseline 有点不像,但是反正在 rl 的文献上,就叫做 Baseline 就对了,我们把所有的 G&rsquo; 都减掉一个 b,目标就是让 G&rsquo; 有正有负,特别高的 G&rsquo; 让它是正的,特别低的 G&rsquo; 让它是负的</p><p>但是这边会有一个问题就是,那要<strong>怎么样设定这个 Baseline</strong> 呢,我们怎么设定一个好的 Baseline,让 G&rsquo; 有正有负呢,那这个我们在接下来的版本裡面还会再提到,但目前为止我们先讲到这个地方</p><p>Q: 需要个比较好的,Heuristic Function</p><p>A: 对 需要个,就是说在下围棋的时候,假设今天你的 Reward 非常地 Sparse,那你可能会需要一个好的,Heuristic Function,如果你有看过那个最原始的,那个深蓝的那篇 Paper,就是在这个机器下围棋打爆人类之前,其实已经在西洋棋上打爆人类了,那个就叫深蓝,深蓝就有蛮多 Heuristic 的 Function,它就不是只有下到游戏的中盘,才知道 才得到 Reward,中间会有蛮多的状况它都会得到 Reward,好</p><a href=#26-policy-gradient><h3 id=26-policy-gradient><span class=hanchor arialabel=Anchor># </span>2.6 Policy Gradient</h3></a><p>接下来就会实际告诉你说,Policy Gradient 是怎么操作的,那你可以仔细读一下助教的程式,助教就是这么操作的</p><p><img src=https://jieye-ericx.github.io//image-20211025103507578.png width=auto alt=image-20211025103507578></p><p>首先你要先 Random 初始化,随机初始化你的 Actor,你就给你的 Actor 一个随机初始化的参数,叫做 θ0,然后接下来你进入你的 Training Iteration,假设你要跑 T 个 Training Iteration,好 那你就拿你的这个,现在手上有的 Actor,一开始是这个 θ0</p><p>一开始很笨 它什么都不会,它採取的行为都是随机的,但它会越来越好,你拿你的 Actor 去跟环境做互动,那你就得到一大堆的 s 跟 a,你就得到一大堆的 s 跟 a,就把它互动的过程记录下来,得到这些 s 跟 a,那接下来你就要进行评价,你用 A1 到 AN 来决定说,这些 Action 到底是好还是不好</p><p>你先拿你的 Actor 去跟环境做互动,收集到这些观察,接下来你要进行评价,看这些 Action 是好的还是不好的,那你真正需要这个在意的地方,你最需要把它改动的地方,就是在评价这个过程裡面,那助教程式这个 A 就直接设成,Immediate Reward,那你写的要改这一段,你才有可能得到好的结果</p><p>设完这个 A 以后,就结束了</p><p>你就把 Loss 定义出来,然后 Update 你的 Model,这个 Update 的过程,就跟 Gradient Descent 一模一样的,会去计算 L 的 Gradient,前面乘上 Learning Rate,然后拿这个 Gradient 去 Update 你的 Model,就把 θi−1 Update 成 θi,</p><p>但是这边有一个神奇的地方是,一般的 training,在我们到目前为止的 Training,Data Collection 都是在 For 循环之外,比如说我有一堆资料,然后把这堆资料拿来做 Training,拿来 Update,Model 很多次,然后最后得到一个收敛的参数,然后拿这个参数来做 Testing</p><p><img src=https://jieye-ericx.github.io//image-20211025104111127.png width=auto alt=image-20211025104111127></p><p>但在 RL 裡面不是这样,你发现<strong>收集资料这一段,居然是在 For 循环裡面</strong>,假设这个 For 循环,你打算跑 400 次,那你就得收集资料 400 次,或者是我们用一个图像化的方式来表示</p><p><img src=https://jieye-ericx.github.io//image-20211025104803588.png width=auto alt=image-20211025104803588></p><p>这个是你收集到的资料,就是你观察了某一个 Actor,它在每一个 State 执行的 Action,然后接下来你给予一个评价,但要用什么评价 要用哪一个版本,这个是你自己决定的,你给予一个评价,说每一个 Action 是好或不好,你有了这些资料 这些评价以后,拿去训练你的 Actor,你拿这些评价可以定义出一个 Loss,然后你可以更新你的参数一次</p><p>但是有趣的地方是,你<strong>只能更新一次而已,一旦更新完一次参数以后,接下来你就要重新去收集资料了</strong>,登记一次参数以后,你就要重新收集资料,才能更新下一次参数,所以这就是为什么 RL,往往它的训练过程非常花时间</p><p>收集资料这件事情,居然是在 For 循环裡面的,你每次 Update 完一次参数以后,你的资料就要重新再收集一次,再去 Update 参数,然后 Update 完一次以后,又要再重新收集资料,如果你参数要 Update 400 次,那你资料就要收集 400 次,那这个过程显然非常地花时间,那你接下来就会问说,那为什么会这样呢</p><p><strong>为什么我们不能够一组资料,就拿来 Update 模型 Update 400 次,然后就结束了呢,为什么每次 Update 完我们的模型参数以后,Update Network 参数以后,就要重新再收集资料呢</strong></p><p>那我们,那这边一个比较简单的比喻是,你知道<strong>一个人的食物,可能是另外一个人的毒药</strong></p><p><img src=https://jieye-ericx.github.io//image-20211025105544243.png width=auto alt=image-20211025105544243></p><p>这些资料是由 θi−1 所收集出来的,这是 θi−1 跟环境互动的结果,这个是 θi−1 的经验,这些经验可以拿来更新 θi−1,可以拿来 Update θi−1 的参数,但它不一定适合拿来 Update θi 的参数</p><p>或者是我们举一个具体的例子,这个例子来自棋魂的第八集,大家看过棋魂吧,我应该就不需要解释棋魂的剧情了吧</p><p><img src=https://jieye-ericx.github.io//image-20211025105804642.png width=auto alt=image-20211025105804642></p><p>这个是进藤光,然后他在跟佐为下棋,然后进藤光就下一步,在大马 现在在小马步飞,这小马步飞具体是什么,我其实也没有非常地确定,但这边有解释一下,就是棋子斜放一格叫做小马步飞,斜放好几格叫做大马步飞,好 阿光下完棋以后,佐为就说这个时候不要下小马步飞,而是要下大马步飞,然后阿光说为什么要下大马步飞呢,我觉得小马步飞也不错</p><p><img src=https://jieye-ericx.github.io//image-20211025105831934.png width=auto alt=image-20211025105831934></p><p>这个时候佐为就解释了,如果大马步飞有 100 手的话,小马步飞只有 99 手,接下来是重点,之前走小马步飞是对的,因为小马步飞的后续比较容易预测,也比较不容易出错,但是大马步飞的下法会比较複杂,但是阿光假设想要变强的话,他应该要学习下大马步飞,或者是阿光变得比较强以后,他应该要下大马步飞,所以你知道说同样的一个行为,同样是做下小马步飞这件事,对不同棋力的棋士来说,也许它的好是不一样的,对于比较弱的阿光来说,下小马步飞是对的,因为他比较不容易出错,但对于已经变强的阿光来说,应该要下大马步飞比较好,下小马步飞反而是比较不好的</p><p>所以<strong>同一个 Action 同一个行为,对于不同的 Actor 而言,它的好是不一样的</strong></p><p><img src=https://jieye-ericx.github.io//image-20211025110056680.png width=auto alt=image-20211025110056680></p><p>所以今天假设我们用 θi−1,收集了一堆的资料,这个是 θi−1 的 Trajectory,这些资料只能拿来训练 θi−1,你不能拿这些资料来训练 θi,为什么不能拿这些资料来训练 θi 呢</p><p>因为假设 假设就算是从 θi−1 跟 θi,它们在 s1 都会採取 a1 好了,但之后到了 s2 以后,它们<strong>可能採取的行为就不一样了</strong>,所以假设对 θ,假设今天 θi,它是看 θi−1 的这个 Trajectory,那 θi−1 会执行的这个 Trajectory,跟 θi 它会採取的行为根本就不一样,所以你拿著 θi−1 接下来会得到的 Reward,来评估 θi 接下来会得到的 Reward,其实是不合适的</p><p>所以如果再回到刚才棋魂的那个例子,同样是假设这个 a1 就是下小马步飞,那对于变强以前的阿光,这是一个合适的走法,但是对于变强以后的阿光,它可能就不是一个合适的走法</p><p>所以今天我们在收集资料,来训练你的 Actor 的时候,你要注意就是<strong>收集资料的那个 Actor,要跟被训练的那个 Actor,最好就是同一个</strong>,那当你的 <strong>Actor 更新以后,你就最好要重新去收集资料</strong>,这就是为什么 RL它非常花时间的原因</p><a href=#on-policy-vs-off-policy><h4 id=on-policy-vs-off-policy><span class=hanchor arialabel=Anchor># </span>On-policy v.s. Off-policy</h4></a><p>刚才我们说,这个要被训练的 Actor,跟要拿来跟环境互动的 Actor,最好是同一个,当我们训练的 Actor,跟互动的 Actor 是同一个的时候,这种叫做 On-policy Learning,那我们刚才示范的那个,Policy Gradient 的整个 Algorithm,它就是 On-policy 的 Learning,那但是还有另外一种状况叫做,Off-policy Learning,</p><p><img src=https://jieye-ericx.github.io//image-20211025110753839.png width=auto alt=image-20211025110753839></p><p>Off-policy 的 Learning 我们今天就不会细讲,Off-policy 的 Learning,期待能够做到的事情是,我们能不能够<strong>让要训练的那个 Actor,还有跟环境互动的那个 Actor,是分开的两个 Actor 呢</strong>,我们要训练的 Actor,能不能够根据其他 Actor 跟环境互动的经验,来进行学习呢</p><p>Off-policy 有一个非常显而易见的好处,你就<strong>不用一直收集资料了</strong>,刚才说 Reinforcement Learning,一个很卡的地方就是,每次更新一次参数就要收集一次资料,你看助教的示范历程是更新 400 次参数,400 次参数相较于你之前 Train 的 Network,没有很多,但我们要收集 400 次资料,跑起来也已经是很卡了,那如果我们可以收一次资料,就 Update 参数很多次,这样不是很好吗,所以 Off-policy 它有不错的优势</p><a href=#off-policy--proximal-policy-optimizationppo><h4 id=off-policy--proximal-policy-optimizationppo><span class=hanchor arialabel=Anchor># </span>Off-policy → Proximal Policy Optimization(PPO)</h4></a><p>但是 Off-policy 要怎么做呢,我们这边就不细讲,有一个非常经典的 Off-policy 的方法,叫做 <strong>Proximal Policy Optimization,缩写是 PPO</strong>,那这个是今天蛮常使用的一个方法,它也是一个蛮强的方法,蛮常使用的方法</p><p>Off-policy 的重点就是,你<strong>在训练的那个 Network,要知道自己跟别人之间的差距,它要有意识的知道说,它跟环境互动的那个 Actor 是不一样的</strong>,那至于细节我们就不细讲,那我有留那个上课的录影的
<a href=https://disp.cc/b/115-bLHe rel=noopener>连结</a>,在投影片的下方,等一下大家如果有兴趣的话,再自己去研究 PPO</p><p><img src=https://jieye-ericx.github.io//image-20211025111501212.png width=auto alt=image-20211025111501212></p><p>那如果要举个比喻的话,就好像是你去问克里斯伊凡 就是美国队长,怎么追一个女生,然后克里斯伊凡就告诉你说,他就示范给你看,他就是 Actor To Interact,他就是负责去示范的那个 Actor,他说他只要去告白,从来没有失败过,但是你要知道说,你跟克里斯伊凡其实还是不一样,人帅真好 人丑吃草,你跟克里斯伊凡是不一样的,所以克里斯伊凡可以採取的招数,你不一定能够採取,你可能要打一个折扣,那这个就是 Off-policy 的精神</p><p>你的 Actor To Train,要知道 Actor To Interact,跟它是不一样的,<strong>所以 Actor To Interact 示范的那些经验,有些可以採纳,有些不一定可以採纳</strong>,至于细节怎么做,那过去的上课录影留在这边,给大家参考</p><a href=#collection-training-data-exploration><h4 id=collection-training-data-exploration><span class=hanchor arialabel=Anchor># </span>Collection Training Data: Exploration</h4></a><p>那还有另外一个很重要的概念,叫做 Exploration,Exploration 指的是什么呢,我们刚才有讲过说,我们今天的,我们今天的这个 <strong>Actor,它在採取行为的时候,它是有一些随机性的</strong></p><p>而这个随机性其实非常地重要,很多时候你随机性不够,你会 Train 不起来,为什么呢,举一个最简单的例子,假设你一开始初始的 Actor,它永远都只会向右移动,它从来都不会知道要开火,如果它从来没有採取开火这个行为,你就永远不知道开火这件事情,到底是好还是不好,唯有今天某一个 Actor,去试图做开火这件事得到 Reward,你才有办法去评估这个行为好或不好,假设有一些 Action 从来没被执行过,那你根本就无从知道,这个 Action 好或不好</p><p><img src=https://jieye-ericx.github.io//image-20211025111831335.png width=auto alt=image-20211025111831335></p><p>所以你今天在训练的过程中,这个拿去跟环境的互动的这个 Actor,它本身的随机性是非常重要的,你其实会期待说跟环境互动的这个 Actor,它的<strong>随机性可以大一点</strong>,这样我们才能够收集到,比较多的 比较丰富的资料,才不会有一些状况,它的 Reward 是从来不知道,那为了要让这个 Actor 的随机性大一点,甚至你在 Training 的时候,你会刻意加大它的随机性</p><p>比如说 Actor 的 Output,不是一个 Distribution 吗,有人会刻意加大,那个 Distribution 的 Entropy,那让它在训练的时候,比较容易 Sample 到那些机率比较低的行为,或者是有人会直接在这个 Actor,它的那个参数上面加 Noise,直接在 Actor 参数上加 Noise,让它每一次採取的行为都不一样,好 那这个就是 Exploration,那 Exploration,其实也是 RL Training 的过程中,一个非常重要的技巧,如果你在训练过程中,你没有让 Network 尽量去试不同的 Action,你很有可能你会 Train 不出好的结果</p><p>那我们来看一下,其实这个 PPO 这个方法,DeepMind 跟 Open AI,都同时提出了 PPO 的想法</p><p>那我们来看一下,DeepMind 的 PPO 的 Demo 的影片https://youtu.be/gn4nRCC9TwQ,它看起来是这样子的,好 那这个是 DeepMind 的 PPO,那就是可以用 PPO 这个方法,用这个 Reinforcement Learning 的方法,去 Learn什么,蜘蛛型的机器人或人形的机器人,做一些动作,比如说跑起来 或者是蹦跳,或者是跨过围牆等等</p><p>那接下来是 OpenAI 的 PPOhttps://blog.openai.com/openai-baselines-ppo/,它这个影片就没有刚才那个潮,它没有那个配音,不过我帮它配个音好了,这个影片我叫它,修机器学习的你,好 我修了一门课叫做机器学习,但在这门课裡面,有非常多的障碍 我一直遇到挫折,那个红色的球是 Baseline,而这个 Baseline 一个接一个,永远都不会停止,然后我 Train 一个 Network 很久,我 collate 它就掉线啦,Train 了三个小时的 Model 不见,但我仍然是爬起来继续地向前,我想开一个比较大的模型,看看可不可以 Train 得比较好一点,但是结果发生什么事情呢,Out Of Memory,那个圈圈一直在转,它就是不跑,怎么办,但我还是爬起来,继续向前,结果 Private Set 跟 Public Set,结果不一样,真的是让人觉得非常地生气,这个影片到这边就结束了吗</p><p>没关係 我们最后还是要给它一个正面的结尾,就算是遭遇到这么多挫折,我仍然努力向前好好地学习,这个就是 PPO,好 那讲到这边正好告一个段落</p><a href=#3-critic><h2 id=3-critic><span class=hanchor arialabel=Anchor># </span>3 Critic</h2></a><p>那上一次 RL 的部分,我们讲说我们要 Learn 一个 Actor,那这一次,我们要 Learn 另外一个东西,这个东西叫做 Critic</p><p>我会先解释 Critic 是什麽,然后我们再来讲说,这个 Critic 对 Learn Actor 这个东西,有什麽样的帮助</p><p><img src=https://jieye-ericx.github.io//image-20211027150436777.png width=auto alt=image-20211027150436777></p><p><strong>Critic 它的工作是要来评估一个 Actor 的好坏</strong>,就你现在已经有一个 Actor,它的参数叫 θ,那 Critic 的工作就是,它要评估说如果这个 Actor,它看到某个样子的 Observation,看到某一个游戏画面,接下来它<strong>可能会得到多少的 Reward</strong></p><p>那 Critic 有好多种不同的变形,有的 Critic 是只看游戏画面来判断,有的 Critic 是说采取某,看到某一个游戏画面,接下来又发现 Actor 採取某一个 Action,在这两者都具备的前提下,那接下来会得到多少 Reward</p><p><img src=https://jieye-ericx.github.io//image-20211027150753604.png width=auto alt=image-20211027150753604></p><p>那这样讲,还是有点抽象,所以我们讲的更具体一点,我们直接介绍一个,我们等一下会真的被用上,你在作业裡面真的派得上用场的,这个 Critic 叫做 Value Function,那这个 Value Function,我们这边用大写的 Vθ(S) 来表示</p><p>它的<strong>输入是 s</strong>,也就是现在游戏的状况,比如说游戏的画面,那这边要特别注意一下 V,它是有一个<strong>上标 θ</strong> 的</p><p><img src=https://jieye-ericx.github.io//image-20211027151809732.png width=auto alt=image-20211027151809732></p><p><strong>这个上标 θ 代表这个 V ,它观察的对象是 θ 这个 Actor</strong>,它观察的这个 Actor 它的参数是 θ,那这个 V ,Vθ就是一个 Function,它的输入是 S,那输出是一个 Scalar,这边用 Vθ(S) 来表示这一个 Scalar</p><p>那 Scalar这个数值的含义是,这一个 Actor θ,放在上标的这个 Actor θ,它如果看到 Observation S,如果看到输入的这个 S 的游戏画面,接下来它得到的,Discounted Cumulated Reward 是多少</p><p><img src=https://jieye-ericx.github.io//image-20211027152150748.png width=auto alt=image-20211027152150748></p><p>这个的 Value Function 它的工作,就是要去估测说,对某一个 Actor 来说,如果现在它已经看到某一个游戏画面,那接下来会得到的,Discounted Cumulated Reward 应该是多少</p><p>当然 Discounted Cumulated Reward,你可以直接透过<strong>把游戏玩到底</strong>,就你看到你已经有了 Actor θ,那假设它看到这个 State s,那最后它到底会得到多少的这个 G&rsquo; ,你就把这个游戏玩完你就知道了</p><p>但是这些这个 Value Function,它的能力就是它要<strong>未卜先知,未看先猜</strong>,游戏还没有玩完,只光看到 S 就要预测这个 Actor,它可以得到什麽样的表现,那这个就是 Value Function 要做的事情</p><p>举例来说,假设你给 Value Function 这一个游戏画面,它就要直接预测说,看到这个游戏画面,接下来应该会得到很高的 Cumulated Reward,为什麽,因为游戏,这个游戏画面裡面还有很多的外星人</p><p><img src=https://jieye-ericx.github.io//image-20211027152406211.png width=auto alt=image-20211027152406211></p><p>假设你的这个 Actor 它很厉害,它是一个好的 Actor,它是能杀得了外星人的 Actor,那接下来它就会得到很多的 Reward</p><p>那像这个画面,这已经是游戏的中盘</p><p><img src=https://jieye-ericx.github.io//image-20211027152442705.png width=auto alt=image-20211027152442705></p><p>游戏的残局,游戏快结束了,剩下的外星人没几隻了,那可以得到的 Reward 就比较少,那这些数值,你把整场游戏玩完你也会知道,但是 Value Function 想要做的事情,就是未卜先知,在游戏没玩完之前,就先猜应该会得到多少的,Discounted Cumulated Reward</p><p>那这边有一件要跟大家特别强调的事情是,这个 Value Function 是有一个上标 θ 的,这个 <strong>Value Function,跟我们观察的 Actor 是有关係的</strong>,同样的 Observation,同样的游戏画面,不同的 Actor,它应该要得到不同的,Discounted Cumulated Reward</p><p>我刚才在举例子的时候我说,假设我们有一个好的 Actor,看到这个游戏画面会有高的 Value,看到这个游戏画面会有低的 Value,但是假设你的 Actor 其实很烂,它很容易被外星人杀死,那也许看到这个画面,它的 Value 也是低的,因为有一堆外星人,它随便动两下它就被杀死了,它根本得不到 Reward,这个烂的 Actor 在这个画面,它可能拿到的 V 也是低的,所以 Value Function 的数值,是跟观察的对象有关係的,好 这个是 Critic</p><a href=#31-how-to-estimate-vθs><h3 id=31-how-to-estimate-vθs><span class=hanchor arialabel=Anchor># </span>3.1 How to estimate Vθ(s)</h3></a><a href=#monte-carlo-mc-based-approach><h4 id=monte-carlo-mc-based-approach><span class=hanchor arialabel=Anchor># </span>Monte-Carlo (MC) based approach</h4></a><p>那在讲 Critic 要怎麽被使用,在 Reinforcement Learning 之前,我们来讲一下 Critic 是怎麽被训练出来的,那有两种常用的训练方法,第一种方法,是 Monte Carlo Based 的方法,这边缩写成 MC</p><p><img src=https://jieye-ericx.github.io//image-20211027153610879.png width=auto alt=image-20211027153610879></p><p>如果是用 MC 的方法的话,你就<strong>把 Actor 拿去跟环境互动</strong>,互动很多轮,那 Actor 跟环境互动以后,Actor 去玩这个游戏以后,你就会得到一些游戏的记录</p><p>你就会发现说,那这个时候,你的 Value Function 就得到一笔训练资料,这笔训练资料告诉它说,如果看到 sa 作为输入,它的输出,这个 Vθ(sa) 应该要跟 G&rsquo;a 越接近越好</p><p>那假设你今天 sample 到另外一个 Observation,看到另外一个游戏画面,把游戏玩完之后发现,得到的 Cumulated Reward 是 G&rsquo;b,那这个时候,你的这个 Value Function ,输入 sb 它就应该得到 Vθ(sb),那这个 Vθ(sb) 就应该跟 G&rsquo;b 越接近越好</p><p>那这个非常直觉,你就去观察 Actor,会得到的 Cumulated Reward,那观察完你就有训练资料,直接拿这些训练资料来训练 Value Function,好 这个 MC ,是一个很直觉的作法</p><a href=#temporal-difference-td-approach><h4 id=temporal-difference-td-approach><span class=hanchor arialabel=Anchor># </span>Temporal-difference (TD) approach</h4></a><p>接下来我们来看另外一个,没有那麽直觉的作法,这个作法叫做 Temporal-Difference Approach,缩写是 TD</p><p>那 Temporal-Difference Approach,它希望做到的事情是,<strong>不用玩完整场游戏</strong>,才能得到训练 Value 的资料,你**只要在某一个 Observation st 的,看到 st 的时候,你的 Actor 执行了 At 得到 Reward rt,**然后接下来再看到 St+1 这样的游戏画面,光看到这样一笔资料,就能够训练 Vπ(S) 了,光看到这样子的资料,就可以拿来更新 Vπ(S) 的参数了</p><p>那如果光看这样一笔资料,就可以更新 Vπ(S) 的参数有什麽样的好处,它的好处是你想在 MC 裡面,你要玩完整场游戏,你才能得到一笔训练资料,那<strong>有的游戏其实很长,甚至有的游戏也许,它根本就没有不会结束</strong>,它永远它都一直继续下去,它永远都不会结束,那像这样子的游戏,你用 MC 就非常地不适合。那这个时候,你可能就希望採用 TD 的方法,好 那怎麽只看到这样子的资料,就拿来训练 Vπ(S)</p><p>这边举一个例子,我们先来看一下,Vθ(st) 跟 Vθ(st+1) 它们之间的关係</p><p><img src=https://jieye-ericx.github.io//../../../../../../../c%5CUsers%5C10131%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20211027155628077.png width=auto alt=image-20211027155628077></p><p>我们说Vθ(st),就是看到 st之后的 Cumulated Reward,所以Vθ(st) 就是 ²rt+γrt+1+γ²rt+2 以此类推</p><p>然后 Vθ(st+1) 就是 rt+1+γrt+2 以此类推</p><p>那你发现说这两个 Vθ,Vθ(st) 跟 Vθ(st+1),它们之间是有关係的</p><p>你可以把它写成这样一个式子,把 Vθ(st+1) 乘上 γ 再加 rt,把 Vθ(st+1) 每一项都乘 γ 再加上 rt,就会变成Vθ(st),所以Vθ(st) 跟 Vθ(st+1) 中间,有这样子的关係</p><p>我们现在,有这样一笔资料以后,我们就可以拿来训练我们的 Value Function,希望 Value Function 可以满足,这边我们所写的这个式子</p><p><img src=https://jieye-ericx.github.io//image-20211027160335508.png width=auto alt=image-20211027160335508></p><p>那什麽意思,就假设我们现在有这样一笔资料,我们就把 St,代到 Value Function 裡面得到Vθ(st),我们有 st+1 代到 Value Function 裡面,得到 Vθ(st+1),虽然我们不知道Vθ(st) 是多少,我们也不知道 Vθ(st+1) 应该是多少,我们没有这两个东西的标准答案,但我们<strong>知道它们相减应该是多少</strong></p><p>根据上面这一个式子,我们把 Vθ(st+1) 乘上 γ,然后再去减Vθ(st),把Vθ(st) 减掉 γ 乘 Vθ(st+1),应该要跟 rt 越接近越好,rt 在这边,我们是有蒐集到 rt 这一笔资料的</p><p>我们又知道Vθ(st),跟这个 Vθ(st+1) 之间的关係,所以我们知道Vθ(st) 减掉 γ 乘上 Vθ(st+1),应该跟 rt 越接近越好,所以你就有了这样子训练资料,输入st,输入 st+1,它们都通过 Vθ,然后把它们相减,然后要跟 rt 越接近越好</p><p>那这个就是 TD 的方法</p><a href=#mc-vs-td><h4 id=mc-vs-td><span class=hanchor arialabel=Anchor># </span>MC v.s. TD</h4></a><p>这两个方法,其实你拿来计算同样的,观察到的结果,同样的资料,同样的 θ,你用 MC 跟 TD 来观察,你算出来的 Value Function,很有可能会是不一样的</p><p>那这边,就举一个例子,这个例子是这样子的,我们观察某一个 Actor,这个 Actor ,跟环境互动玩了某一个游戏八次,当然这边为了简化计算,我们假设这些游戏都非常简单,都一个回合,就到两个回合就结束了</p><p><img src=https://jieye-ericx.github.io//image-20211029140442476.png width=auto alt=image-20211029140442476></p><ul><li>所以那个 Actor 第一次玩游戏的时候,它先看到 sa 这个画面,得到 Reware 0</li><li>接下来看到 sb 这个画面,得到 Reware 0 游戏结束</li><li>接下来,这个有连续六场游戏,都是看到 sb 这个画面,得到 Reward 1 就结束了</li><li>最后一场游戏,看到 sb 这个画面,得到 Reward 0 就结束了</li></ul><p>那我们这边,先无视 Actor,为了简化起见无视 Actor,我们也假设,γ 就等于 1,也就是没有做 Discount,好 那这个 sb 应该是多少,Vθ(sb) 应该是多少</p><p>我们知道这个 Vθ(sb),它的意思就是这个<strong>看到 sb 这一个画面,你会得到的 Reward 的期望值</strong>,那 sb 这个画面,我们在这八次游戏中,总共看到了八次,每次游戏都有看到 sb 这个画面,看到 sb 这个画面之后会得到多少 Reward</p><p><strong>八次游戏裡面,有六次得到 1 分</strong>,两次得到 0 分,所以平均是 3/4 分没有问题,所以 Vθ(sb) 就是 3/4,妥妥的没有争议</p><p>那 Vθ(sa) 应该是多少,你觉得看到 sa,接下来应该要得到多少 Reward ,根据这八笔训练资料,看到 sa 接下来该得到多少 Reward</p><p>几乎没有其他答案,所有人都说是 0,好 多数同学都说是 0,<strong>0 是不是一个正确的答案,它既对也不对</strong></p><p>其实还有另外一个可能的答案是 3/4,我看没有人写 3/4,等一下来解释,为什麽有可能算出 3/4,但 0 也是一个合理的答案,为什麽你会觉得是应该是 0 ,<strong>0 是用 Monte-Carlo 的想法得到的</strong></p><p>为什麽是 0,因为我们看到 sa 只有一次,看到 sa 以后会得到多少 Reward,这是 0,看到 sa 以后得到 Reward 0,再看到 sb 得到 Reward 还是 0,所以 Cumulated Reward 就是 0,所以如果从 Monte-Carlo 的角度来看,我们看到 sa,接下来算出来的 G 应该是多少,就是 0 ,所以 Vθ(sa) 应该就是 0,妥妥的没问题,几乎所有同学都得到了正确答案</p><p>但如果你<strong>用 TD,你算出来的,可会是不一样的结果</strong></p><p><img src=https://jieye-ericx.github.io//image-20211029141053625.png width=auto alt=image-20211029141053625></p><p>因为 Vθ(sa) 跟 Vθ(sb) 中间,有这样子的一个关係,这个 Vθ(sa) 应该要等于 Vθ(sb) 加上 Reward,就是你在看到 sa 之后得到 Reward,接下来进入 sb,那这个 Vθ(sa),应该等于 Vθ(sb) 加上这一个 Reward</p><p>所以按照这个想法,Vθ(sb) 是3/4,这个 r 是 0,但 Vθ(sa) 应该是 3/4 对不对,按照 TD 的想法,Vθ(sa) 应该是 3/4</p><p><img src=https://jieye-ericx.github.io//image-20211029141247164.png width=auto alt=image-20211029141247164></p><p>你可能会问说,那到底 Monte-Carlo 跟 TD,谁算出来是对的,<strong>都可以说是对的,它们只是背后的假设是不同的</strong>,对 Monte-Carlo 而言,它就是直接看我们观察到的资料,sa 之后接 sb 得到的,Cumulated Reward 就是 0,所以 Vθ(sa) 当然是 0</p><p>但对于 TD 而言,它背后的假设是这个 <strong>sa 跟 sb 是没有关係</strong>的,看到 sa 之后再看到 sb,并不会影响看到 sb 的 Reward,你现在看这八笔训练资料,给你一种错觉,觉得说 Vθ(sa) 应该是 0,那只是因为<strong>你 sample 到的资料太少了</strong>,看到 sb,应该可以期望的 Reward 是 3/4,只是因为今天正好运气不好,看完 sa 以后再看 sb,正好 r 是 0,但是期望值应该是 3/4,你只是正好运气不好看到 r 是 0,你才会觉得 sa 是 0</p><p>但是 sb,看到 sb 以后得到的期望 Reward 应该是 3/4,所以看到 sa 以后你会看到 sb,那你得到的这个期望的 Reward 也应该是 3/4,所以从 TD 的角度来看,sb 会得到多少 Reward,跟 sa 是没有关係的</p><p>所以你应该,所以 sa 的这个 Cumulated Reward 应该是 3/4</p><p>所以总之用 MC 来计算,用 TD 来计算,会有微妙的差异</p><a href=#32-version-35><h3 id=32-version-35><span class=hanchor arialabel=Anchor># </span>3.2 Version 3.5</h3></a><p>Critic 怎麽被用在训练 Actor 上面,还记不记得我们上一次,最后我们讲到这个 Actor 的方法的时候,我们说怎麽训练一个 Actor,你就先把 Actor 跟环境互动,得到一些 Reward,然后你得到一堆这个 Observation,跟这个 Action 的 Pair</p><p>这个在 s1 执行 A1 的时候多好,得到一个分数 A1,那我们说这个 A1 ,它是 Cumulative 的 Reward,那上週也有同学问到说,难道 Cumulative 的 Reward,不需要做 Normalization 吗,需要做 Normalization,所以我们说,这个减掉一个 b 当做 Normalization</p><p><img src=https://jieye-ericx.github.io//image-20211029144710982.png width=auto alt=image-20211029144710982></p><p>但这个 b 的值应该设多少,就不好说,那我这边 告诉大家说,一个 V 合理的设法,是把它设成 Vθ(S)</p><p><img src=https://jieye-ericx.github.io//image-20211029144900910.png width=auto alt=image-20211029144900910></p><p>你现在 Learn 出这个 Critic 以后,这个 Critic 给它一个 Step,它就会产生一个分数,那你把这个分数 当做 B,,所以 G1&rsquo; 就是要减掉 Vθ(s1),G2&rsquo; 就是减掉 Vθ(s2),以此类推</p><p>那再来的问题就是,<strong>为什麽减掉 V 是一个合理的选择</strong>,那我们在下一页投影片,来跟大家解释一下</p><p>我们已经知道说这个 At 代表s，a 这个 Pair 有多好,我们是用 G&rsquo; 减掉Vθ(st),来定义这个 A,好 那我们先来看一下这个Vθ(st),到底代表什麽意思</p><p><img src=https://jieye-ericx.github.io//image-20211030105849219.png width=auto alt=image-20211030105849219></p><p><strong>Vθ(st)是看到某一个画面 St 以后,接下来会得到的 Reward</strong></p><p>它其实是一个<strong>期望值</strong>,因为假设你今天看到同一个画面,接下来再继续玩游戏,<strong>游戏有随机性</strong>,你每次得到的 Reward 都不太一样的话,那Vθ(st) 其实是一个期望值</p><p>那在这个时候,在<strong>看到 St 的时候,你的 Actor 不一定会执行At 这一个 Action</strong></p><p>因为 Actor 本身是有随机性的,在训练的过程中,我们甚至鼓励 Actor 是有随机性的,所以同样的 S,你的 Actor ,它会输出的这个 Action 不一定是一样的</p><p>我们说 <strong>Actor 的输出其实是一个 Probability Distribution</strong>,是一个在这个 Action 的 space 上面的,Probability Distribution,它还给每一个 Action 一个分数,你按照这个分数去做sample,有些 Action 被sample 到的机率高,有些 Action 被sample 到的机率低,但每一次sample 出来的 Action,并不保证一定要是一样的,</p><p>所以看到 St 之后,接下来有很多的可能 很多的可能,所以你会算出不同的 Cumulative 的 Reward</p><p><img src=https://jieye-ericx.github.io//image-20211030110507941.png width=auto alt=image-20211030110507941></p><p>那当然如果你有 Discount 的话,就是 Discounted 的 Cumulative Reward,那我们这边,是把 Discount 这件事情暂时省略掉，那<strong>把这些可能的结果平均起来,就是Vθ(st)</strong>,这是Vθ(st) 这一项的含义</p><p>那 Gt&rsquo; 这一项的含义是什麽？</p><p>Gt&rsquo; 这一项的含义是,在 St 这个位置 <strong>在 St 这个画面下,执行 At 以后,接下来会得到的 Cumulative Reward</strong></p><p><img src=https://jieye-ericx.github.io//image-20211030132243867.png width=auto alt=image-20211030132243867></p><p>所以你<strong>执行 At 以后,接下来 再一路玩下去,你会得到一个结果 得到一个 Reward,就是 Gt&rsquo;</strong></p><ul><li><p>如果 At 大于 0 代表说,Gt&rsquo; 大于Vθ(st),这个时候代表说,这个 Action 是比,我们 Random sample 到的 Action 还要好的,在这边得到 Gt&rsquo; 的时候,我们确定是执行了 At,那在 St 在算这个Vθ(st) 的时候,我们不确定我们会执行哪一个 Action</p><p>所以我们执行 Action At 的时候,得到的 Reward大于随便执行一个 Action 得到的 Reward,所以当 At 大于 0 的时候代表说,<strong>At 大于随便执行的一个 Action,那这个时候这个 Action At 它就是好的</strong>,所以我们给它一个大于 0 的 At</p></li><li><p>如果 At 小于 0 代表说,这个平均的 Reward,大过执行 At 得到的 Reward,你随机採取的 Action,按照某一个 Distribution,sample 出来的 Action,得到的这个 Cumulative Reward 的期望值,大过採取 At 这个 Action 所得到的 Reward,那这个时候 At 就是坏的,所要给它负的大 At</p><p>所以这样就非常地直觉,为什麽我们应该把 Gt&rsquo; 减掉Vθ(st),但讲到这边,你有没有觉得有一些地方有点违和,什麽地方有点违和,这个 Gt&rsquo; 它是一个sample 的结果,它是执行 At 以后,一直玩玩玩 玩到游戏结束,某一个sample 出来的结果,而Vθ(st) 是很多条路径 很多个可能性,平均以后的结果,我们把一个sample 去减掉平均,这样会准吗,<strong>也许这个sample 特别好或特别坏,我们为什麽不是拿平均去减掉平均</strong></p></li></ul><a href=#33-version-4><h3 id=33-version-4><span class=hanchor arialabel=Anchor># </span>3.3 Version 4</h3></a><p>所以我们这一门课要讲的最后一个版本,就是拿平均去减掉平均</p><p>我们执行完 At 以后 得到 Reward rt,然后跑到下一个画面 St+1,把这个 St+1 接下来一直玩下去,有很多不同的可能,每个可能通通会得到一个 Reward,把这些 Reward 平均起来</p><p><img src=https://jieye-ericx.github.io//image-20211030132330419.png width=auto alt=image-20211030132330419></p><p>把这些 Cumulative 的 Reward 平均起来,其实就是 Vθ(st+1),本来你会需要玩很多场游戏,才能够得到这个平均值,</p><p>但没关係,假设你训练出一个好的 Critic,那你直接代 Vθ(st+1),你就知道说,在 St+1 这个画面下,接下来会得到的,Cumulative Reward 的期望值应该多少</p><p>而接下来 你再加上 rt,接下来再加上 rt,代表说在 St 这个位置採取 at</p><p><img src=https://jieye-ericx.github.io//image-20211030132914309.png width=auto alt=image-20211030132914309></p><p>跳到 St+1以后,会得到的 Reward 的期望值,因为我们已经知道说,在 St 这边採取 at 会得到 Reward rt,再跳到 St+1,然后 St+1 会得到期望值,期望的 Reward 是 Vθ(st+1)</p><p>所以我们这边,再给它加上 rt,代表说在 St 这边执行 At 以后,会得到的 Reward 的期望值,接下来再把这两个东西相减,再把 rt+Vθ(st+1) 减掉Vθ(st)</p><p><img src=https://jieye-ericx.github.io//image-20211030133441524.png width=auto alt=image-20211030133441524></p><p>也就是我们把 G&rsquo; 换成 rt+Vθ(st+1),再减掉Vθ(st)</p><p>我们就知道说,採取 at 这个 Action得到的期望 Reward,减掉根据某个 Distribution sample 一个 Action得到的 Reward,两者的期望值差距有多大</p><p>那<strong>如果 rt+Vθ(st+1) 比较大,就代表 at 比较好</strong>,它比随便 sample Reward 好</p><p>rt+Vθ(st+1) 小于Vθ(st),就代表 at 它是 Lower Than Average,它比从一个 Distribution,sample 到的 Action 还要差</p><p>所以今天,这个就是大名鼎鼎的一个常用的方法,叫做 Advantage Actor-Critic,在 Advantage Actor-Critic 裡面,你是怎麽定义 at 的,也就是 rt+Vθ(st+1) 减掉, rt+Vθ(st+1) -Vθ(st),就是我们的 At 了</p><a href=#34-tip-of-actor-critic><h3 id=34-tip-of-actor-critic><span class=hanchor arialabel=Anchor># </span>3.4 Tip of Actor-Critic</h3></a><p>这边有一个训练 Actor-Critic 的小技巧,那你在作业裡面也不妨使用这个技巧</p><p><img src=https://jieye-ericx.github.io//image-20211030134011817.png width=auto alt=image-20211030134011817></p><p>Actor 是一个 Network,Critic 也是一个 Network,Actor 这个 Network,是一个游戏画面当做输入,它的输出是每一个 Action 的分数,Critic 是一个游戏画面当做输入,输出是一个数值,代表接下来会得到的 Cumulative 的 Reward</p><p>这边有<strong>两个 Network,它们的输入是一样的东西</strong>,所以这两个 Network,它们应该<strong>有部分的参数可以共用</strong>吧,尤其假设你的输入又是一个非常複杂的东西,比如说游戏画面的时候,前面几层应该都需要是 CNN 吧,要了解这个游戏画面需要用的 CNN,也许是差不多的吧</p><p>所以 Actor 跟 Critic,它们可以共用前面几个 Layer,所以你今天在实作的时候往往,你会把你的 Actor-Critic 设计成这个样子,Actor-Critic ,它们有共用大部分的 Network,然后只是最后,输出不同的 Action,就是 Actor,输出一个 Scalar,就是 Critic,好 那这是一个训练 Actor-Critic 的小技巧</p><a href=#35-outlook-deep-q-network-dqn><h3 id=35-outlook-deep-q-network-dqn><span class=hanchor arialabel=Anchor># </span>3.5 Outlook: Deep Q Network (DQN)</h3></a><p>那其实今天讲的,并不是 Reinforcement Learning 的全部,那其实在 Reinforcement Learning 裡面,还有一个犀利的做法,是直接採取 Critic,也就是<strong>直接用 Critic,就可以决定要用什麽样的 Action</strong></p><p><img src=https://jieye-ericx.github.io//image-20211030134218607.png width=auto alt=image-20211030134218607></p><p>那其中最知名的就是,Deep Q Network (DQN),那不过 这边我们就不细讲 DQN 了,如果你真的想知道 DQN 的话,可以参考过去上课的录影,那 DQN 哇 有非常非常多的变形</p><p>这边 就是找一个非常,有一篇非常知名的 Paper 叫做 Rainbow,裡面 就是试著去尝试了各种 DQN 的变形,试了七种 然后再把这七种变形集合起来,因为有七种变形集合起来,所以他说它是一个彩虹,所以他把它的方法叫做 Rainbow,那我也把这个 Paper 留在这边给你参考,那如果你想知道 Rainbow 裡面的,每一个小技巧是怎麽做的话,你就参见上课录影,过去的课程,有把 Rainbow 裡面的每一个小技巧,都讲过一遍</p><a href=#qa-1><h3 id=qa-1><span class=hanchor arialabel=Anchor># </span>Q&A</h3></a><p>Q1: sa 后面接的不一定是 sb 吧,这样怎麽办</p><p>A1: 这是一个很好的问题,sa 后面不一定接 sb,那这个问题,在刚才我们看到的那个例子裡面,就没有办法处理,因为在刚才那个,我们看到那个只有 8 个 Episode 的例子裡面,sa 后面就只会接 sb,所以我们观察 没有观察到其它的可能性,所以我们没办法处理这个问题,所以这就告诉我们说,在做 Reinforcement Learning 的时候,sample 这件事情是非常重要的,你 Reinforcement Learning,最后 Learn 得好不好,跟你在 sample 的时候 sample 得好不好,关係非常大,喔 所以这个 Reinforcement Learning,是一个非常吃人品的方法啦,所以你在作业裡面你可以体验一下,就你 sample 到的结果,对你最后 Training 的结果,有非常大的影响</p><p>Q2: 每一个 V,都需对应到固定的环境发生顺序吗</p><p>A2: 我没有很确定你的问题,但是我试著回答一下,就是每一个 V 它不会固定,它不会对应到固定的环境发生顺序,如果你的游戏有随机性的话,那 V 其实是代表了一个期望值,它想要算的就是,给某一个 Observation,看到某一个游戏画面以后,接下来你会得到的 Cumulative Reward 的平均值,它的期望值,如果你的游戏有随机性的话,V 代表的是期望值,你看到某一个游戏画面以后,然后接下来会发生什麽事情,不见得是一样的,但把所有的可能性都平均起来,取它的期望值,这个就是 V 所代表的意思</p><p>Q3: 后面出现的 S 应该是不固定的,这样怎麽代公式</p><p>A3: 好 那个我想我刚才应该算是有回答到了,后面出现的 看到某一个这个 Observation,后面出现的 Observation 确实是不固定的,那如果有些状况,某些 Observation 你没观察到的话,哇 那你真的就没办法训练</p><p>Q4: 就是拿 V 当一般人的实力,超过它就是猛,没超过就是烂吗</p><p>A4: 对 就是这样,V 就是平均的实力,超过 V 就是好</p><p>Q5: 想请问这个 Distribution 要从哪裡知道</p><p>A5: 我想你这个 Distribution 问的是那个,Actor 的 Distribution 啦对不对,我们说,Distribution Action 的 Distribution,Action 是从某一个 Distribution,sample 出来的,那个 Distribution 是谁,那个 Distribution 是这样,就是你的 Actor 不是像是一个 Classifier 吗,你的 Actor 像是一个 Classifier,然后你把 S 丢进去,每一个 Action 都会有一个分数,那你把这个分数,通过 Soft Mess,就做一个 Normalize,它就变得像机率一样,然后按照那个机率去做 sample,那这个就是 Actor,从一个 Distribution sample 出来的,这句话的意思,</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/Python/ data-ctx=强化学习课程-李宏毅 data-src=/Python class=internal-link>Python目录</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://jieye-ericx.github.io/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Jieye ericx using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://jieye-ericx.github.io/>Home</a></li><li><a href=https://github.com/jieye-ericx>Github</a></li></ul></footer></div></div></body></html>