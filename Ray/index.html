<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="my 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  'checkpoint_at_end' = {bool} True 'checkpoint_freq' = {int} 4 'max_failures' = {int} 1000 'resume' = {bool} False 'export_formats' = {list: 2} ['model', 'checkpoint'] 'stop' = {dict: 1} {'time_total_s': 14400} 'config' = {dict: 8} { 'log_level': 'ERROR', 'num_workers': 1, 'num_gpus': 0, 'horizon': 1000, 'env': <class 'baselines."><meta property="og:title" content="my"><meta property="og:description" content="my 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  'checkpoint_at_end' = {bool} True 'checkpoint_freq' = {int} 4 'max_failures' = {int} 1000 'resume' = {bool} False 'export_formats' = {list: 2} ['model', 'checkpoint'] 'stop' = {dict: 1} {'time_total_s': 14400} 'config' = {dict: 8} { 'log_level': 'ERROR', 'num_workers': 1, 'num_gpus': 0, 'horizon': 1000, 'env': <class 'baselines."><meta property="og:type" content="website"><meta property="og:image" content="https://jieye-ericx.github.io/icon.png"><meta property="og:url" content="https://jieye-ericx.github.io/Ray/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="my"><meta name=twitter:description content="my 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  'checkpoint_at_end' = {bool} True 'checkpoint_freq' = {int} 4 'max_failures' = {int} 1000 'resume' = {bool} False 'export_formats' = {list: 2} ['model', 'checkpoint'] 'stop' = {dict: 1} {'time_total_s': 14400} 'config' = {dict: 8} { 'log_level': 'ERROR', 'num_workers': 1, 'num_gpus': 0, 'horizon': 1000, 'env': <class 'baselines."><meta name=twitter:image content="https://jieye-ericx.github.io/icon.png"><title>my</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://jieye-ericx.github.io//icon.png><link href=https://jieye-ericx.github.io/styles.80333fa2099c0bee674efa435fde378c.min.css rel=stylesheet><link href=https://jieye-ericx.github.io/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://jieye-ericx.github.io/js/darkmode.48459b7116d092b4e98d2cab704cad80.min.js></script>
<script src=https://jieye-ericx.github.io/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://jieye-ericx.github.io/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://jieye-ericx.github.io/",fetchData=Promise.all([fetch("https://jieye-ericx.github.io/indices/linkIndex.762b6dcff4c3893cdd65608c69e9b98c.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://jieye-ericx.github.io/indices/contentIndex.196678a44c633bf69ab26e63de73de99.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://jieye-ericx.github.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://jieye-ericx.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/jieye-ericx.github.io\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=jieye-ericx.github.io src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://jieye-ericx.github.io/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://jieye-ericx.github.io/>jieye の 数字花园</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>my</h1><p class=meta>Last updated
Jul 19, 2022</p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#tunerunconfig>tune.run(config)</a><ol><li><a href=#config>config</a></li></ol></li></ol><ol><li><ol><li><a href=#算法超参数列表>算法超参数列表</a></li></ol></li></ol></nav></details></aside><a href=#my><h1 id=my><span class=hanchor arialabel=Anchor># </span>my</h1></a><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s1>&#39;checkpoint_at_end&#39;</span> <span class=o>=</span> <span class=p>{</span><span class=nb>bool</span><span class=p>}</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;checkpoint_freq&#39;</span> <span class=o>=</span> <span class=p>{</span><span class=nb>int</span><span class=p>}</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;max_failures&#39;</span> <span class=o>=</span> <span class=p>{</span><span class=nb>int</span><span class=p>}</span> <span class=mi>1000</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;resume&#39;</span> <span class=o>=</span> <span class=p>{</span><span class=nb>bool</span><span class=p>}</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;export_formats&#39;</span> <span class=o>=</span> <span class=p>{</span><span class=nb>list</span><span class=p>:</span> <span class=mi>2</span><span class=p>}</span> <span class=p>[</span><span class=s1>&#39;model&#39;</span><span class=p>,</span> <span class=s1>&#39;checkpoint&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;stop&#39;</span> <span class=o>=</span> <span class=p>{</span><span class=nb>dict</span><span class=p>:</span> <span class=mi>1</span><span class=p>}</span> <span class=p>{</span><span class=s1>&#39;time_total_s&#39;</span><span class=p>:</span> <span class=mi>14400</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;config&#39;</span> <span class=o>=</span> <span class=p>{</span><span class=nb>dict</span><span class=p>:</span> <span class=mi>8</span><span class=p>}</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;log_level&#39;</span><span class=p>:</span> <span class=s1>&#39;ERROR&#39;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>  <span class=s1>&#39;num_workers&#39;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>  <span class=s1>&#39;num_gpus&#39;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>  <span class=s1>&#39;horizon&#39;</span><span class=p>:</span> <span class=mi>1000</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>  <span class=s1>&#39;env&#39;</span><span class=p>:</span> <span class=o>&lt;</span><span class=k>class</span> <span class=err>&#39;</span><span class=nc>baselines</span><span class=o>.</span><span class=n>marl_benchmark</span><span class=o>.</span><span class=n>wrappers</span><span class=o>.</span><span class=n>rllib</span><span class=o>.</span><span class=n>frame_stack</span><span class=o>.</span><span class=n>FrameStack</span><span class=s1>&#39;&gt;,</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;multiagent&#39;</span><span class=p>:</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;env_config&#39;</span> <span class=o>=</span> <span class=p>{</span><span class=nb>dict</span><span class=p>:</span> <span class=mi>5</span><span class=p>}</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  	<span class=s1>&#39;custom_config&#39;</span><span class=p>:{</span>
</span></span><span class=line><span class=cl>      <span class=s1>&#39;name&#39;</span><span class=p>:</span> <span class=s1>&#39;FrameStack&#39;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    	<span class=s1>&#39;num_stack&#39;</span><span class=p>:</span> <span class=mi>3</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    	<span class=s1>&#39;reward_adapter&#39;</span><span class=p>:</span> <span class=o>&lt;</span><span class=n>function</span> <span class=n>FrameStack</span><span class=o>.</span><span class=n>get_reward_adapter</span><span class=o>.&lt;</span><span class=nb>locals</span><span class=o>&gt;.</span><span class=n>func</span> <span class=n>at</span> <span class=mh>0x7fab767d09e0</span><span class=o>&gt;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    	<span class=s1>&#39;observation_adapter&#39;</span><span class=p>:</span> <span class=o>&lt;</span><span class=n>function</span> <span class=n>FrameStack</span><span class=o>.</span><span class=n>get_observation_adapter</span><span class=o>.&lt;</span><span class=nb>locals</span><span class=o>&gt;.</span><span class=n>func</span> <span class=n>at</span> <span class=mh>0x7fab767d0b00</span><span class=o>&gt;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    	<span class=s1>&#39;action_adapter&#39;</span><span class=p>:</span> <span class=o>&lt;</span><span class=n>function</span> <span class=n>ActionAdapter</span><span class=o>.</span><span class=n>discrete_action_adapter</span> <span class=n>at</span> <span class=mh>0x7fab7939ab90</span><span class=o>&gt;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    	<span class=s1>&#39;info_adapter&#39;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    	<span class=s1>&#39;observation_space&#39;</span><span class=p>:</span> <span class=s1>&#39;&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    	<span class=s1>&#39;action_space&#39;</span><span class=p>:</span> <span class=n>Discrete</span><span class=p>(</span><span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;seed&#39;</span><span class=p>:</span><span class=mi>42</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;headless&#39;</span><span class=p>:</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;scenarios&#39;</span><span class=p>:[</span><span class=s1>&#39;/home/LiKuiHao/pycharm/s6/baselines/marl_benchmark/scenarios/two_ways/bid&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;agent_specs&#39;</span><span class=p>:{</span><span class=s1>&#39;default_policy&#39;</span><span class=p>:</span> 
</span></span><span class=line><span class=cl>                   <span class=n>AgentSpec</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                     <span class=n>interface</span><span class=o>=</span><span class=n>AgentInterface</span><span class=p>(</span><span class=n>debug</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>event_configuration</span><span class=o>=</span><span class=n>EventConfiguration</span><span class=p>(</span><span class=n>not_moving_time</span><span class=o>=</span><span class=mi>60</span><span class=p>,</span> <span class=n>not_moving_distance</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span> <span class=n>done_criteria</span><span class=o>=</span><span class=n>DoneCriteria</span><span class=p>(</span><span class=n>collision</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>off_road</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>off_route</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>on_shoulder</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>wrong_way</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>not_moving</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>agents_alive</span><span class=o>=</span><span class=kc>None</span><span class=p>),</span> <span class=n>max_episode_steps</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span> <span class=n>neighborhood_vehicles</span><span class=o>=</span><span class=n>NeighborhoodVehicles</span><span class=p>(</span><span class=n>radius</span><span class=o>=</span><span class=mi>50</span><span class=p>),</span> <span class=n>waypoints</span><span class=o>=</span><span class=n>Waypoints</span><span class=p>(</span><span class=n>lookahead</span><span class=o>=</span><span class=mi>50</span><span class=p>),</span> <span class=n>road_waypoints</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>drivable_area_grid_map</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>ogm</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>rgb</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>lidar</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>action</span><span class=o>=&lt;</span><span class=n>ActionSpaceType</span><span class=o>.</span><span class=n>Lane</span><span class=p>:</span> <span class=mi>1</span><span class=o>&gt;</span><span class=p>,</span> <span class=n>vehicle_type</span><span class=o>=</span><span class=s1>&#39;sedan&#39;</span><span class=p>,</span> <span class=n>accelerometer</span><span class=o>=</span><span class=n>Accelerometer</span><span class=p>()),</span> 
</span></span><span class=line><span class=cl>                     <span class=n>agent_builder</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                     <span class=n>agent_params</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                     <span class=n>observation_adapter</span><span class=o>=&lt;</span><span class=n>function</span> <span class=n>AgentSpec</span><span class=o>.&lt;</span><span class=k>lambda</span><span class=o>&gt;</span> <span class=n>at</span> <span class=mh>0x7fab786d8a70</span><span class=o>&gt;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                     <span class=n>action_adapter</span><span class=o>=&lt;</span><span class=n>function</span> <span class=n>AgentSpec</span><span class=o>.&lt;</span><span class=k>lambda</span><span class=o>&gt;</span> <span class=n>at</span> <span class=mh>0x7fab786d8c20</span><span class=o>&gt;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                     <span class=n>reward_adapter</span><span class=o>=&lt;</span><span class=n>function</span> <span class=n>AgentSpec</span><span class=o>.&lt;</span><span class=k>lambda</span><span class=o>&gt;</span> <span class=n>at</span> <span class=mh>0x7fab786d8b00</span><span class=o>&gt;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                     <span class=n>info_adapter</span><span class=o>=&lt;</span><span class=n>function</span> <span class=n>AgentSpec</span><span class=o>.&lt;</span><span class=k>lambda</span><span class=o>&gt;</span> <span class=n>at</span> <span class=mh>0x7fab786d8dd0</span><span class=o>&gt;</span><span class=p>)}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>	<span class=s1>&#39;callbacks&#39;</span> <span class=o>=</span> <span class=p>{</span><span class=nb>type</span><span class=p>}</span> <span class=o>&lt;</span><span class=k>class</span> <span class=err>&#39;</span><span class=nc>baselines</span><span class=o>.</span><span class=n>my</span><span class=o>.</span><span class=n>MyCallback</span><span class=o>.</span><span class=n>SimpleCallbacks</span><span class=s1>&#39;&gt;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;run_or_experiment&#39;</span> <span class=o>=</span> <span class=p>{</span><span class=nb>type</span><span class=p>}</span> <span class=o>&lt;</span><span class=k>class</span> <span class=err>&#39;</span><span class=nc>ray</span><span class=o>.</span><span class=n>rllib</span><span class=o>.</span><span class=n>agents</span><span class=o>.</span><span class=n>trainer_template</span><span class=o>.</span><span class=n>DQN</span><span class=s1>&#39;&gt;</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;name&#39;</span> <span class=o>=</span> <span class=p>{</span><span class=nb>str</span><span class=p>}</span> <span class=s1>&#39;bid-4&#39;</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;local_dir&#39;</span> <span class=o>=</span> <span class=p>{</span><span class=nb>str</span><span class=p>}</span> <span class=s1>&#39;/home/LiKuiHao/pycharm/s6/baselines/marl_benchmark/log/results/myrun&#39;</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;restore&#39;</span> <span class=o>=</span> <span class=p>{</span><span class=n>NoneType</span><span class=p>}</span> <span class=kc>None</span>
</span></span></code></pre></td></tr></table></div></div><a href=#tune><h1 id=tune><span class=hanchor arialabel=Anchor># </span>Tune</h1></a><a href=#tunerunconfig><h2 id=tunerunconfig><span class=hanchor arialabel=Anchor># </span>tune.run(config)</h2></a><a href=#config><h3 id=config><span class=hanchor arialabel=Anchor># </span>config</h3></a><p>用于 Tune 变体生成的<strong>特定于算法的配置</strong>（例如 env、hyperparams）。默认为空字典。自定义搜索算法可能会忽略这一点。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span><span class=lnt>105
</span><span class=lnt>106
</span><span class=lnt>107
</span><span class=lnt>108
</span><span class=lnt>109
</span><span class=lnt>110
</span><span class=lnt>111
</span><span class=lnt>112
</span><span class=lnt>113
</span><span class=lnt>114
</span><span class=lnt>115
</span><span class=lnt>116
</span><span class=lnt>117
</span><span class=lnt>118
</span><span class=lnt>119
</span><span class=lnt>120
</span><span class=lnt>121
</span><span class=lnt>122
</span><span class=lnt>123
</span><span class=lnt>124
</span><span class=lnt>125
</span><span class=lnt>126
</span><span class=lnt>127
</span><span class=lnt>128
</span><span class=lnt>129
</span><span class=lnt>130
</span><span class=lnt>131
</span><span class=lnt>132
</span><span class=lnt>133
</span><span class=lnt>134
</span><span class=lnt>135
</span><span class=lnt>136
</span><span class=lnt>137
</span><span class=lnt>138
</span><span class=lnt>139
</span><span class=lnt>140
</span><span class=lnt>141
</span><span class=lnt>142
</span><span class=lnt>143
</span><span class=lnt>144
</span><span class=lnt>145
</span><span class=lnt>146
</span><span class=lnt>147
</span><span class=lnt>148
</span><span class=lnt>149
</span><span class=lnt>150
</span><span class=lnt>151
</span><span class=lnt>152
</span><span class=lnt>153
</span><span class=lnt>154
</span><span class=lnt>155
</span><span class=lnt>156
</span><span class=lnt>157
</span><span class=lnt>158
</span><span class=lnt>159
</span><span class=lnt>160
</span><span class=lnt>161
</span><span class=lnt>162
</span><span class=lnt>163
</span><span class=lnt>164
</span><span class=lnt>165
</span><span class=lnt>166
</span><span class=lnt>167
</span><span class=lnt>168
</span><span class=lnt>169
</span><span class=lnt>170
</span><span class=lnt>171
</span><span class=lnt>172
</span><span class=lnt>173
</span><span class=lnt>174
</span><span class=lnt>175
</span><span class=lnt>176
</span><span class=lnt>177
</span><span class=lnt>178
</span><span class=lnt>179
</span><span class=lnt>180
</span><span class=lnt>181
</span><span class=lnt>182
</span><span class=lnt>183
</span><span class=lnt>184
</span><span class=lnt>185
</span><span class=lnt>186
</span><span class=lnt>187
</span><span class=lnt>188
</span><span class=lnt>189
</span><span class=lnt>190
</span><span class=lnt>191
</span><span class=lnt>192
</span><span class=lnt>193
</span><span class=lnt>194
</span><span class=lnt>195
</span><span class=lnt>196
</span><span class=lnt>197
</span><span class=lnt>198
</span><span class=lnt>199
</span><span class=lnt>200
</span><span class=lnt>201
</span><span class=lnt>202
</span><span class=lnt>203
</span><span class=lnt>204
</span><span class=lnt>205
</span><span class=lnt>206
</span><span class=lnt>207
</span><span class=lnt>208
</span><span class=lnt>209
</span><span class=lnt>210
</span><span class=lnt>211
</span><span class=lnt>212
</span><span class=lnt>213
</span><span class=lnt>214
</span><span class=lnt>215
</span><span class=lnt>216
</span><span class=lnt>217
</span><span class=lnt>218
</span><span class=lnt>219
</span><span class=lnt>220
</span><span class=lnt>221
</span><span class=lnt>222
</span><span class=lnt>223
</span><span class=lnt>224
</span><span class=lnt>225
</span><span class=lnt>226
</span><span class=lnt>227
</span><span class=lnt>228
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>config</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1># =<mark> Debugging </mark>=</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to write episode stats and videos to the agent log dir</span>
</span></span><span class=line><span class=cl>    <span class=c1># 是否把每次迭代的状态和videos 写入智能体日志文件中</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;monitor&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Set the ray.rllib.* log level for the agent process and its workers.</span>
</span></span><span class=line><span class=cl>    <span class=c1># Should be one of DEBUG, INFO, WARN, or ERROR. The DEBUG level will also</span>
</span></span><span class=line><span class=cl>    <span class=c1># periodically print out summaries of relevant internal dataflow (this is</span>
</span></span><span class=line><span class=cl>    <span class=c1># also printed out once at startup at the INFO level).</span>
</span></span><span class=line><span class=cl>    <span class=c1># 设置ray.rllib.*代理（智能体）进程及其worker的日志级别。应该是调试（ DEBUG）、信息（INFO）、</span>
</span></span><span class=line><span class=cl>    <span class=c1># 警告（WARN）或错误（ERROR）之一。 调试级别还将定期打印出相关内部数据流的摘要(在INFO</span>
</span></span><span class=line><span class=cl>    <span class=c1># 级别启动时也会打印一次)。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;log_level&#34;</span><span class=p>:</span> <span class=s2>&#34;INFO&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Callbacks that will be run during various phases of training. These all</span>
</span></span><span class=line><span class=cl>    <span class=c1># take a single &#34;info&#34; dict as an argument. For episode callbacks, custom</span>
</span></span><span class=line><span class=cl>    <span class=c1># metrics can be attached to the episode by updating the episode object&#39;s</span>
</span></span><span class=line><span class=cl>    <span class=c1># custom metrics dict (see examples/custom_metrics_and_callbacks.py). You</span>
</span></span><span class=line><span class=cl>    <span class=c1># may also mutate the passed in batch data in your callback.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 将在不同训练阶段运行的回调。这些都以一个“info”dict作为参数。对于迭代回调，可以通过更新迭代</span>
</span></span><span class=line><span class=cl>    <span class=c1># 对象的自定义度量dict将自定义度量附加到迭代中(参见示例/custom_metrics_and_callbacks.py)。</span>
</span></span><span class=line><span class=cl>    <span class=c1># 还可以在回调中修改传入的批处理数据。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;callbacks&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;on_episode_start&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>     <span class=c1># arg: {&#34;env&#34;: .., &#34;episode&#34;: ...}</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;on_episode_step&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>      <span class=c1># arg: {&#34;env&#34;: .., &#34;episode&#34;: ...}</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;on_episode_end&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>       <span class=c1># arg: {&#34;env&#34;: .., &#34;episode&#34;: ...}</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;on_sample_end&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>        <span class=c1># arg: {&#34;samples&#34;: .., &#34;worker&#34;: ...}</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;on_train_result&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>      <span class=c1># arg: {&#34;trainer&#34;: ..., &#34;result&#34;: ...}</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;on_postprocess_traj&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>  <span class=c1># arg: {</span>
</span></span><span class=line><span class=cl>                                      <span class=c1>#   &#34;agent_id&#34;: ..., &#34;episode&#34;: ...,</span>
</span></span><span class=line><span class=cl>                                      <span class=c1>#   &#34;pre_batch&#34;: (before processing),</span>
</span></span><span class=line><span class=cl>                                      <span class=c1>#   &#34;post_batch&#34;: (after processing),</span>
</span></span><span class=line><span class=cl>                                      <span class=c1>#   &#34;all_pre_batches&#34;: (other agent ids),</span>
</span></span><span class=line><span class=cl>                                      <span class=c1># }</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to attempt to continue training if a worker crashes.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 是否忽略失败worker继续运行训练</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;ignore_worker_failures&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Execute TF loss functions in eager mode. This is currently experimental</span>
</span></span><span class=line><span class=cl>    <span class=c1># and only really works with the basic PG algorithm.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 是否在紧急（eager）模式下执行TF的损失函数。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;use_eager&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># =<mark> Policy </mark>=</span>
</span></span><span class=line><span class=cl>    <span class=c1># Arguments to pass to model. See models/catalog.py for a full list of the</span>
</span></span><span class=line><span class=cl>    <span class=c1># available model options.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 传递给模型的参数。有关可用模型选项的完整列表，请参见models/catalog.py。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;model&#34;</span><span class=p>:</span> <span class=n>MODEL_DEFAULTS</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Arguments to pass to the policy optimizer. These vary by optimizer.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 传递给策略优化器的参数。这些参数因优化器而异。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;optimizer&#34;</span><span class=p>:</span> <span class=p>{},</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># =<mark> Environment </mark>=</span>
</span></span><span class=line><span class=cl>    <span class=c1># Discount factor of the MDP</span>
</span></span><span class=line><span class=cl>    <span class=c1># MDP的折扣系数</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;gamma&#34;</span><span class=p>:</span> <span class=mf>0.99</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Number of steps after which the episode is forced to terminate. Defaults</span>
</span></span><span class=line><span class=cl>    <span class=c1># to `env.spec.max_episode_steps` (if present) for Gym envs.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 事件被迫终止的步骤数。默认为“env.spec.max_episode_steps &#39;(如果有的话)用于env.spec.max_episode_steps的envs。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;horizon&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Calculate rewards but don&#39;t reset the environment when the horizon is</span>
</span></span><span class=line><span class=cl>    <span class=c1># hit. This allows value estimation and RNN state to span across logical</span>
</span></span><span class=line><span class=cl>    <span class=c1># episodes denoted by horizon. This only has an effect if horizon != inf.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 计算奖励，但不要在horizon 被击中时重置环境。这使得值估计和RNN状态可以跨由horizon表示的逻辑事件。这只有在horizon != inf时才有效。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;soft_horizon&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Arguments to pass to the env creator</span>
</span></span><span class=line><span class=cl>    <span class=c1># 传递给env创建者的参数</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;env_config&#34;</span><span class=p>:</span> <span class=p>{},</span>
</span></span><span class=line><span class=cl>    <span class=c1># Environment name can also be passed via config</span>
</span></span><span class=line><span class=cl>    <span class=c1># 环境名称，也可以通过配置传递</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;env&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to clip rewards prior to experience postprocessing. Setting to</span>
</span></span><span class=line><span class=cl>    <span class=c1># None means clip for Atari only.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 是否在实验后处理之前剪辑奖励。设置为None只表示剪辑Atari 。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;clip_rewards&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to np.clip() actions to the action space low/high range spec.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 是否通过np.clip() 剪辑动作 到动作空间的低/高范围规范。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;clip_actions&#34;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to use rllib or deepmind preprocessors by default</span>
</span></span><span class=line><span class=cl>    <span class=c1># 默认情况下是否使用rllib或deepmind预处理器</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;preprocessor_pref&#34;</span><span class=p>:</span> <span class=s2>&#34;deepmind&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># The default learning rate</span>
</span></span><span class=line><span class=cl>    <span class=c1># 学习率</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;lr&#34;</span><span class=p>:</span> <span class=mf>0.0001</span><span class=p>,</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># =<mark> Evaluation </mark>=</span>
</span></span><span class=line><span class=cl>    <span class=c1># Evaluate with every `evaluation_interval` training iterations.</span>
</span></span><span class=line><span class=cl>    <span class=c1># The evaluation stats will be reported under the &#34;evaluation&#34; metric key.</span>
</span></span><span class=line><span class=cl>    <span class=c1># Note that evaluation is currently not parallelized, and that for Ape-X</span>
</span></span><span class=line><span class=cl>    <span class=c1># metrics are already only reported for the lowest epsilon workers.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 使用每个“evaluation_interval”训练迭代进行评估。评估统计数据将</span>
</span></span><span class=line><span class=cl>    <span class=c1># 在“评估”度量键下报告。注意，评估目前没有并行化，而且对于Ape-X指标，</span>
</span></span><span class=line><span class=cl>    <span class=c1># 只报告了最低的epsilon worker。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;evaluation_interval&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Number of episodes to run per evaluation period.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 每个评估期要运行的迭代数。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;evaluation_num_episodes&#34;</span><span class=p>:</span> <span class=mi>10</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Extra arguments to pass to evaluation workers.</span>
</span></span><span class=line><span class=cl>    <span class=c1># Typical usage is to pass extra args to evaluation env creator</span>
</span></span><span class=line><span class=cl>    <span class=c1># and to disable exploration by computing deterministic actions</span>
</span></span><span class=line><span class=cl>    <span class=c1># TODO(kismuz): implement determ. actions and include relevant keys hints</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;evaluation_config&#34;</span><span class=p>:</span> <span class=p>{},</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># =<mark> Resources </mark>=</span>
</span></span><span class=line><span class=cl>    <span class=c1># Number of actors used for parallelism</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;num_workers&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Number of GPUs to allocate to the driver. Note that not all algorithms</span>
</span></span><span class=line><span class=cl>    <span class=c1># can take advantage of driver GPUs. This can be fraction (e.g., 0.3 GPUs).</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;num_gpus&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Number of CPUs to allocate per worker.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;num_cpus_per_worker&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Number of GPUs to allocate per worker. This can be fractional.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;num_gpus_per_worker&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Any custom resources to allocate per worker.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;custom_resources_per_worker&#34;</span><span class=p>:</span> <span class=p>{},</span>
</span></span><span class=line><span class=cl>    <span class=c1># Number of CPUs to allocate for the driver. Note: this only takes effect</span>
</span></span><span class=line><span class=cl>    <span class=c1># when running in Tune.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;num_cpus_for_driver&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># =<mark> Execution </mark>=</span>
</span></span><span class=line><span class=cl>    <span class=c1># Number of environments to evaluate vectorwise per worker.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;num_envs_per_worker&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Default sample batch size (unroll length). Batches of this size are</span>
</span></span><span class=line><span class=cl>    <span class=c1># collected from workers until train_batch_size is met. When using</span>
</span></span><span class=line><span class=cl>    <span class=c1># multiple envs per worker, this is multiplied by num_envs_per_worker.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;sample_batch_size&#34;</span><span class=p>:</span> <span class=mi>200</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Training batch size, if applicable. Should be &gt;= sample_batch_size.</span>
</span></span><span class=line><span class=cl>    <span class=c1># Samples batches will be concatenated together to this size for training.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;train_batch_size&#34;</span><span class=p>:</span> <span class=mi>200</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to rollout &#34;complete_episodes&#34; or &#34;truncate_episodes&#34;</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;batch_mode&#34;</span><span class=p>:</span> <span class=s2>&#34;truncate_episodes&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># (Deprecated) Use a background thread for sampling (slightly off-policy)</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;sample_async&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Element-wise observation filter, either &#34;NoFilter&#34; or &#34;MeanStdFilter&#34;</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;observation_filter&#34;</span><span class=p>:</span> <span class=s2>&#34;NoFilter&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to synchronize the statistics of remote filters.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;synchronize_filters&#34;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Configure TF for single-process operation by default</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;tf_session_args&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1># note: overriden by `local_tf_session_args`</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;intra_op_parallelism_threads&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;inter_op_parallelism_threads&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;gpu_options&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;allow_growth&#34;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;log_device_placement&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;device_count&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;CPU&#34;</span><span class=p>:</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;allow_soft_placement&#34;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span>  <span class=c1># required by PPO multi-gpu</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=c1># Override the following tf session args on the local worker</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;local_tf_session_args&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1># Allow a higher level of parallelism by default, but not unlimited</span>
</span></span><span class=line><span class=cl>        <span class=c1># since that can cause crashes with many concurrent drivers.</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;intra_op_parallelism_threads&#34;</span><span class=p>:</span> <span class=mi>8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;inter_op_parallelism_threads&#34;</span><span class=p>:</span> <span class=mi>8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to LZ4 compress individual observations</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;compress_observations&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Drop metric batches from unresponsive workers after this many seconds</span>
</span></span><span class=line><span class=cl>    <span class=c1># 在经过这么多秒后，把反应迟钝的worker剔除</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;collect_metrics_timeout&#34;</span><span class=p>:</span> <span class=mi>180</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Smooth metrics over this many episodes.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;metrics_smoothing_episodes&#34;</span><span class=p>:</span> <span class=mi>100</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># If using num_envs_per_worker &gt; 1, whether to create those new envs in</span>
</span></span><span class=line><span class=cl>    <span class=c1># remote processes instead of in the same worker. This adds overheads, but</span>
</span></span><span class=line><span class=cl>    <span class=c1># can make sense if your envs can take much time to step / reset</span>
</span></span><span class=line><span class=cl>    <span class=c1># (e.g., for StarCraft). Use this cautiously; overheads are significant.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;remote_worker_envs&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Timeout that remote workers are waiting when polling environments.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 0 (continue when at least one env is ready) is a reasonable default,</span>
</span></span><span class=line><span class=cl>    <span class=c1># but optimal value could be obtained by measuring your environment</span>
</span></span><span class=line><span class=cl>    <span class=c1># step / reset and model inference perf.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;remote_env_batch_wait_ms&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Minimum time per iteration</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;min_iter_time_s&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Minimum env steps to optimize for per train call. This value does</span>
</span></span><span class=line><span class=cl>    <span class=c1># not affect learning, only the length of iterations.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;timesteps_per_iteration&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># =<mark> Offline Datasets </mark>=</span>
</span></span><span class=line><span class=cl>    <span class=c1># Specify how to generate experiences:</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - &#34;sampler&#34;: generate experiences via online simulation (default)</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - a local directory or file glob expression (e.g., &#34;/tmp/*.json&#34;)</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - a list of individual file paths/URIs (e.g., [&#34;/tmp/1.json&#34;,</span>
</span></span><span class=line><span class=cl>    <span class=c1>#    &#34;s3://bucket/2.json&#34;])</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - a dict with string keys and sampling probabilities as values (e.g.,</span>
</span></span><span class=line><span class=cl>    <span class=c1>#    {&#34;sampler&#34;: 0.4, &#34;/tmp/*.json&#34;: 0.4, &#34;s3://bucket/expert.json&#34;: 0.2}).</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - a function that returns a rllib.offline.InputReader</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;input&#34;</span><span class=p>:</span> <span class=s2>&#34;sampler&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Specify how to evaluate the current policy. This only has an effect when</span>
</span></span><span class=line><span class=cl>    <span class=c1># reading offline experiences. Available options:</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - &#34;wis&#34;: the weighted step-wise importance sampling estimator.</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - &#34;is&#34;: the step-wise importance sampling estimator.</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - &#34;simulation&#34;: run the environment in the background, but use</span>
</span></span><span class=line><span class=cl>    <span class=c1>#    this data for evaluation only and not for learning.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;input_evaluation&#34;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&#34;is&#34;</span><span class=p>,</span> <span class=s2>&#34;wis&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to run postprocess_trajectory() on the trajectory fragments from</span>
</span></span><span class=line><span class=cl>    <span class=c1># offline inputs. Note that postprocessing will be done using the *current*</span>
</span></span><span class=line><span class=cl>    <span class=c1># policy, not the *behaviour* policy, which is typically undesirable for</span>
</span></span><span class=line><span class=cl>    <span class=c1># on-policy algorithms.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;postprocess_inputs&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># If positive, input batches will be shuffled via a sliding window buffer</span>
</span></span><span class=line><span class=cl>    <span class=c1># of this number of batches. Use this if the input data is not in random</span>
</span></span><span class=line><span class=cl>    <span class=c1># enough order. Input is delayed until the shuffle buffer is filled.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;shuffle_buffer_size&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Specify where experiences should be saved:</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - None: don&#39;t save any experiences</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - &#34;logdir&#34; to save to the agent log dir</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - a path/URI to save to a custom output directory (e.g., &#34;s3://bucket/&#34;)</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - a function that returns a rllib.offline.OutputWriter</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;output&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># What sample batch columns to LZ4 compress in the output data.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;output_compress_columns&#34;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&#34;obs&#34;</span><span class=p>,</span> <span class=s2>&#34;new_obs&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=c1># Max output file size before rolling over to a new file.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;output_max_file_size&#34;</span><span class=p>:</span> <span class=mi>64</span> <span class=o>*</span> <span class=mi>1024</span> <span class=o>*</span> <span class=mi>1024</span><span class=p>,</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># =<mark> Multiagent </mark>=</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;multiagent&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1># Map from policy ids to tuples of (policy_cls, obs_space,</span>
</span></span><span class=line><span class=cl>        <span class=c1># act_space, config). See rollout_worker.py for more info.</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;policies&#34;</span><span class=p>:</span> <span class=p>{},</span>
</span></span><span class=line><span class=cl>        <span class=c1># Function mapping agent ids to policy ids.</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;policy_mapping_fn&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=c1># Optional whitelist of policies to train, or None for all policies.</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;policies_to_train&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><a href=#rllib><h1 id=rllib><span class=hanchor arialabel=Anchor># </span>Rllib</h1></a><a href=#算法超参数列表><h3 id=算法超参数列表><span class=hanchor arialabel=Anchor># </span>算法超参数列表</h3></a><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span><span class=lnt>105
</span><span class=lnt>106
</span><span class=lnt>107
</span><span class=lnt>108
</span><span class=lnt>109
</span><span class=lnt>110
</span><span class=lnt>111
</span><span class=lnt>112
</span><span class=lnt>113
</span><span class=lnt>114
</span><span class=lnt>115
</span><span class=lnt>116
</span><span class=lnt>117
</span><span class=lnt>118
</span><span class=lnt>119
</span><span class=lnt>120
</span><span class=lnt>121
</span><span class=lnt>122
</span><span class=lnt>123
</span><span class=lnt>124
</span><span class=lnt>125
</span><span class=lnt>126
</span><span class=lnt>127
</span><span class=lnt>128
</span><span class=lnt>129
</span><span class=lnt>130
</span><span class=lnt>131
</span><span class=lnt>132
</span><span class=lnt>133
</span><span class=lnt>134
</span><span class=lnt>135
</span><span class=lnt>136
</span><span class=lnt>137
</span><span class=lnt>138
</span><span class=lnt>139
</span><span class=lnt>140
</span><span class=lnt>141
</span><span class=lnt>142
</span><span class=lnt>143
</span><span class=lnt>144
</span><span class=lnt>145
</span><span class=lnt>146
</span><span class=lnt>147
</span><span class=lnt>148
</span><span class=lnt>149
</span><span class=lnt>150
</span><span class=lnt>151
</span><span class=lnt>152
</span><span class=lnt>153
</span><span class=lnt>154
</span><span class=lnt>155
</span><span class=lnt>156
</span><span class=lnt>157
</span><span class=lnt>158
</span><span class=lnt>159
</span><span class=lnt>160
</span><span class=lnt>161
</span><span class=lnt>162
</span><span class=lnt>163
</span><span class=lnt>164
</span><span class=lnt>165
</span><span class=lnt>166
</span><span class=lnt>167
</span><span class=lnt>168
</span><span class=lnt>169
</span><span class=lnt>170
</span><span class=lnt>171
</span><span class=lnt>172
</span><span class=lnt>173
</span><span class=lnt>174
</span><span class=lnt>175
</span><span class=lnt>176
</span><span class=lnt>177
</span><span class=lnt>178
</span><span class=lnt>179
</span><span class=lnt>180
</span><span class=lnt>181
</span><span class=lnt>182
</span><span class=lnt>183
</span><span class=lnt>184
</span><span class=lnt>185
</span><span class=lnt>186
</span><span class=lnt>187
</span><span class=lnt>188
</span><span class=lnt>189
</span><span class=lnt>190
</span><span class=lnt>191
</span><span class=lnt>192
</span><span class=lnt>193
</span><span class=lnt>194
</span><span class=lnt>195
</span><span class=lnt>196
</span><span class=lnt>197
</span><span class=lnt>198
</span><span class=lnt>199
</span><span class=lnt>200
</span><span class=lnt>201
</span><span class=lnt>202
</span><span class=lnt>203
</span><span class=lnt>204
</span><span class=lnt>205
</span><span class=lnt>206
</span><span class=lnt>207
</span><span class=lnt>208
</span><span class=lnt>209
</span><span class=lnt>210
</span><span class=lnt>211
</span><span class=lnt>212
</span><span class=lnt>213
</span><span class=lnt>214
</span><span class=lnt>215
</span><span class=lnt>216
</span><span class=lnt>217
</span><span class=lnt>218
</span><span class=lnt>219
</span><span class=lnt>220
</span><span class=lnt>221
</span><span class=lnt>222
</span><span class=lnt>223
</span><span class=lnt>224
</span><span class=lnt>225
</span><span class=lnt>226
</span><span class=lnt>227
</span><span class=lnt>228
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>COMMON_CONFIG</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1># =<mark> Debugging </mark>=</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to write episode stats and videos to the agent log dir</span>
</span></span><span class=line><span class=cl>    <span class=c1># 是否把每次迭代的状态和videos 写入智能体日志文件中</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;monitor&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Set the ray.rllib.* log level for the agent process and its workers.</span>
</span></span><span class=line><span class=cl>    <span class=c1># Should be one of DEBUG, INFO, WARN, or ERROR. The DEBUG level will also</span>
</span></span><span class=line><span class=cl>    <span class=c1># periodically print out summaries of relevant internal dataflow (this is</span>
</span></span><span class=line><span class=cl>    <span class=c1># also printed out once at startup at the INFO level).</span>
</span></span><span class=line><span class=cl>    <span class=c1># 设置ray.rllib.*代理（智能体）进程及其worker的日志级别。应该是调试（ DEBUG）、信息（INFO）、</span>
</span></span><span class=line><span class=cl>    <span class=c1># 警告（WARN）或错误（ERROR）之一。 调试级别还将定期打印出相关内部数据流的摘要(在INFO</span>
</span></span><span class=line><span class=cl>    <span class=c1># 级别启动时也会打印一次)。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;log_level&#34;</span><span class=p>:</span> <span class=s2>&#34;INFO&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Callbacks that will be run during various phases of training. These all</span>
</span></span><span class=line><span class=cl>    <span class=c1># take a single &#34;info&#34; dict as an argument. For episode callbacks, custom</span>
</span></span><span class=line><span class=cl>    <span class=c1># metrics can be attached to the episode by updating the episode object&#39;s</span>
</span></span><span class=line><span class=cl>    <span class=c1># custom metrics dict (see examples/custom_metrics_and_callbacks.py). You</span>
</span></span><span class=line><span class=cl>    <span class=c1># may also mutate the passed in batch data in your callback.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 将在不同训练阶段运行的回调。这些都以一个“info”dict作为参数。对于迭代回调，可以通过更新迭代</span>
</span></span><span class=line><span class=cl>    <span class=c1># 对象的自定义度量dict将自定义度量附加到迭代中(参见示例/custom_metrics_and_callbacks.py)。</span>
</span></span><span class=line><span class=cl>    <span class=c1># 还可以在回调中修改传入的批处理数据。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;callbacks&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;on_episode_start&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>     <span class=c1># arg: {&#34;env&#34;: .., &#34;episode&#34;: ...}</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;on_episode_step&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>      <span class=c1># arg: {&#34;env&#34;: .., &#34;episode&#34;: ...}</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;on_episode_end&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>       <span class=c1># arg: {&#34;env&#34;: .., &#34;episode&#34;: ...}</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;on_sample_end&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>        <span class=c1># arg: {&#34;samples&#34;: .., &#34;worker&#34;: ...}</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;on_train_result&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>      <span class=c1># arg: {&#34;trainer&#34;: ..., &#34;result&#34;: ...}</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;on_postprocess_traj&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>  <span class=c1># arg: {</span>
</span></span><span class=line><span class=cl>                                      <span class=c1>#   &#34;agent_id&#34;: ..., &#34;episode&#34;: ...,</span>
</span></span><span class=line><span class=cl>                                      <span class=c1>#   &#34;pre_batch&#34;: (before processing),</span>
</span></span><span class=line><span class=cl>                                      <span class=c1>#   &#34;post_batch&#34;: (after processing),</span>
</span></span><span class=line><span class=cl>                                      <span class=c1>#   &#34;all_pre_batches&#34;: (other agent ids),</span>
</span></span><span class=line><span class=cl>                                      <span class=c1># }</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to attempt to continue training if a worker crashes.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 是否忽略失败worker继续运行训练</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;ignore_worker_failures&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Execute TF loss functions in eager mode. This is currently experimental</span>
</span></span><span class=line><span class=cl>    <span class=c1># and only really works with the basic PG algorithm.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 是否在紧急（eager）模式下执行TF的损失函数。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;use_eager&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># =<mark> Policy </mark>=</span>
</span></span><span class=line><span class=cl>    <span class=c1># Arguments to pass to model. See models/catalog.py for a full list of the</span>
</span></span><span class=line><span class=cl>    <span class=c1># available model options.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 传递给模型的参数。有关可用模型选项的完整列表，请参见models/catalog.py。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;model&#34;</span><span class=p>:</span> <span class=n>MODEL_DEFAULTS</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Arguments to pass to the policy optimizer. These vary by optimizer.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 传递给策略优化器的参数。这些参数因优化器而异。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;optimizer&#34;</span><span class=p>:</span> <span class=p>{},</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># =<mark> Environment </mark>=</span>
</span></span><span class=line><span class=cl>    <span class=c1># Discount factor of the MDP</span>
</span></span><span class=line><span class=cl>    <span class=c1># MDP的折扣系数</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;gamma&#34;</span><span class=p>:</span> <span class=mf>0.99</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Number of steps after which the episode is forced to terminate. Defaults</span>
</span></span><span class=line><span class=cl>    <span class=c1># to `env.spec.max_episode_steps` (if present) for Gym envs.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 事件被迫终止的步骤数。默认为“env.spec.max_episode_steps &#39;(如果有的话)用于env.spec.max_episode_steps的envs。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;horizon&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Calculate rewards but don&#39;t reset the environment when the horizon is</span>
</span></span><span class=line><span class=cl>    <span class=c1># hit. This allows value estimation and RNN state to span across logical</span>
</span></span><span class=line><span class=cl>    <span class=c1># episodes denoted by horizon. This only has an effect if horizon != inf.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 计算奖励，但不要在horizon 被击中时重置环境。这使得值估计和RNN状态可以跨由horizon表示的逻辑事件。这只有在horizon != inf时才有效。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;soft_horizon&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Arguments to pass to the env creator</span>
</span></span><span class=line><span class=cl>    <span class=c1># 传递给env创建者的参数</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;env_config&#34;</span><span class=p>:</span> <span class=p>{},</span>
</span></span><span class=line><span class=cl>    <span class=c1># Environment name can also be passed via config</span>
</span></span><span class=line><span class=cl>    <span class=c1># 环境名称，也可以通过配置传递</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;env&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to clip rewards prior to experience postprocessing. Setting to</span>
</span></span><span class=line><span class=cl>    <span class=c1># None means clip for Atari only.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 是否在实验后处理之前剪辑奖励。设置为None只表示剪辑Atari 。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;clip_rewards&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to np.clip() actions to the action space low/high range spec.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 是否通过np.clip() 剪辑动作 到动作空间的低/高范围规范。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;clip_actions&#34;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to use rllib or deepmind preprocessors by default</span>
</span></span><span class=line><span class=cl>    <span class=c1># 默认情况下是否使用rllib或deepmind预处理器</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;preprocessor_pref&#34;</span><span class=p>:</span> <span class=s2>&#34;deepmind&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># The default learning rate</span>
</span></span><span class=line><span class=cl>    <span class=c1># 学习率</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;lr&#34;</span><span class=p>:</span> <span class=mf>0.0001</span><span class=p>,</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># =<mark> Evaluation </mark>=</span>
</span></span><span class=line><span class=cl>    <span class=c1># Evaluate with every `evaluation_interval` training iterations.</span>
</span></span><span class=line><span class=cl>    <span class=c1># The evaluation stats will be reported under the &#34;evaluation&#34; metric key.</span>
</span></span><span class=line><span class=cl>    <span class=c1># Note that evaluation is currently not parallelized, and that for Ape-X</span>
</span></span><span class=line><span class=cl>    <span class=c1># metrics are already only reported for the lowest epsilon workers.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 使用每个“evaluation_interval”训练迭代进行评估。评估统计数据将</span>
</span></span><span class=line><span class=cl>    <span class=c1># 在“评估”度量键下报告。注意，评估目前没有并行化，而且对于Ape-X指标，</span>
</span></span><span class=line><span class=cl>    <span class=c1># 只报告了最低的epsilon worker。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;evaluation_interval&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Number of episodes to run per evaluation period.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 每个评估期要运行的迭代数。</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;evaluation_num_episodes&#34;</span><span class=p>:</span> <span class=mi>10</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Extra arguments to pass to evaluation workers.</span>
</span></span><span class=line><span class=cl>    <span class=c1># Typical usage is to pass extra args to evaluation env creator</span>
</span></span><span class=line><span class=cl>    <span class=c1># and to disable exploration by computing deterministic actions</span>
</span></span><span class=line><span class=cl>    <span class=c1># TODO(kismuz): implement determ. actions and include relevant keys hints</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;evaluation_config&#34;</span><span class=p>:</span> <span class=p>{},</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># =<mark> Resources </mark>=</span>
</span></span><span class=line><span class=cl>    <span class=c1># Number of actors used for parallelism</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;num_workers&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Number of GPUs to allocate to the driver. Note that not all algorithms</span>
</span></span><span class=line><span class=cl>    <span class=c1># can take advantage of driver GPUs. This can be fraction (e.g., 0.3 GPUs).</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;num_gpus&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Number of CPUs to allocate per worker.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;num_cpus_per_worker&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Number of GPUs to allocate per worker. This can be fractional.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;num_gpus_per_worker&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Any custom resources to allocate per worker.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;custom_resources_per_worker&#34;</span><span class=p>:</span> <span class=p>{},</span>
</span></span><span class=line><span class=cl>    <span class=c1># Number of CPUs to allocate for the driver. Note: this only takes effect</span>
</span></span><span class=line><span class=cl>    <span class=c1># when running in Tune.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;num_cpus_for_driver&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># =<mark> Execution </mark>=</span>
</span></span><span class=line><span class=cl>    <span class=c1># Number of environments to evaluate vectorwise per worker.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;num_envs_per_worker&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Default sample batch size (unroll length). Batches of this size are</span>
</span></span><span class=line><span class=cl>    <span class=c1># collected from workers until train_batch_size is met. When using</span>
</span></span><span class=line><span class=cl>    <span class=c1># multiple envs per worker, this is multiplied by num_envs_per_worker.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;sample_batch_size&#34;</span><span class=p>:</span> <span class=mi>200</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Training batch size, if applicable. Should be &gt;= sample_batch_size.</span>
</span></span><span class=line><span class=cl>    <span class=c1># Samples batches will be concatenated together to this size for training.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;train_batch_size&#34;</span><span class=p>:</span> <span class=mi>200</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to rollout &#34;complete_episodes&#34; or &#34;truncate_episodes&#34;</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;batch_mode&#34;</span><span class=p>:</span> <span class=s2>&#34;truncate_episodes&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># (Deprecated) Use a background thread for sampling (slightly off-policy)</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;sample_async&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Element-wise observation filter, either &#34;NoFilter&#34; or &#34;MeanStdFilter&#34;</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;observation_filter&#34;</span><span class=p>:</span> <span class=s2>&#34;NoFilter&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to synchronize the statistics of remote filters.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;synchronize_filters&#34;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Configure TF for single-process operation by default</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;tf_session_args&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1># note: overriden by `local_tf_session_args`</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;intra_op_parallelism_threads&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;inter_op_parallelism_threads&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;gpu_options&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;allow_growth&#34;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;log_device_placement&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;device_count&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;CPU&#34;</span><span class=p>:</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;allow_soft_placement&#34;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span>  <span class=c1># required by PPO multi-gpu</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=c1># Override the following tf session args on the local worker</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;local_tf_session_args&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1># Allow a higher level of parallelism by default, but not unlimited</span>
</span></span><span class=line><span class=cl>        <span class=c1># since that can cause crashes with many concurrent drivers.</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;intra_op_parallelism_threads&#34;</span><span class=p>:</span> <span class=mi>8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;inter_op_parallelism_threads&#34;</span><span class=p>:</span> <span class=mi>8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to LZ4 compress individual observations</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;compress_observations&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Drop metric batches from unresponsive workers after this many seconds</span>
</span></span><span class=line><span class=cl>    <span class=c1># 在经过这么多秒后，把反应迟钝的worker剔除</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;collect_metrics_timeout&#34;</span><span class=p>:</span> <span class=mi>180</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Smooth metrics over this many episodes.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;metrics_smoothing_episodes&#34;</span><span class=p>:</span> <span class=mi>100</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># If using num_envs_per_worker &gt; 1, whether to create those new envs in</span>
</span></span><span class=line><span class=cl>    <span class=c1># remote processes instead of in the same worker. This adds overheads, but</span>
</span></span><span class=line><span class=cl>    <span class=c1># can make sense if your envs can take much time to step / reset</span>
</span></span><span class=line><span class=cl>    <span class=c1># (e.g., for StarCraft). Use this cautiously; overheads are significant.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;remote_worker_envs&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Timeout that remote workers are waiting when polling environments.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 0 (continue when at least one env is ready) is a reasonable default,</span>
</span></span><span class=line><span class=cl>    <span class=c1># but optimal value could be obtained by measuring your environment</span>
</span></span><span class=line><span class=cl>    <span class=c1># step / reset and model inference perf.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;remote_env_batch_wait_ms&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Minimum time per iteration</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;min_iter_time_s&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Minimum env steps to optimize for per train call. This value does</span>
</span></span><span class=line><span class=cl>    <span class=c1># not affect learning, only the length of iterations.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;timesteps_per_iteration&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># =<mark> Offline Datasets </mark>=</span>
</span></span><span class=line><span class=cl>    <span class=c1># Specify how to generate experiences:</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - &#34;sampler&#34;: generate experiences via online simulation (default)</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - a local directory or file glob expression (e.g., &#34;/tmp/*.json&#34;)</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - a list of individual file paths/URIs (e.g., [&#34;/tmp/1.json&#34;,</span>
</span></span><span class=line><span class=cl>    <span class=c1>#    &#34;s3://bucket/2.json&#34;])</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - a dict with string keys and sampling probabilities as values (e.g.,</span>
</span></span><span class=line><span class=cl>    <span class=c1>#    {&#34;sampler&#34;: 0.4, &#34;/tmp/*.json&#34;: 0.4, &#34;s3://bucket/expert.json&#34;: 0.2}).</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - a function that returns a rllib.offline.InputReader</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;input&#34;</span><span class=p>:</span> <span class=s2>&#34;sampler&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Specify how to evaluate the current policy. This only has an effect when</span>
</span></span><span class=line><span class=cl>    <span class=c1># reading offline experiences. Available options:</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - &#34;wis&#34;: the weighted step-wise importance sampling estimator.</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - &#34;is&#34;: the step-wise importance sampling estimator.</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - &#34;simulation&#34;: run the environment in the background, but use</span>
</span></span><span class=line><span class=cl>    <span class=c1>#    this data for evaluation only and not for learning.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;input_evaluation&#34;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&#34;is&#34;</span><span class=p>,</span> <span class=s2>&#34;wis&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=c1># Whether to run postprocess_trajectory() on the trajectory fragments from</span>
</span></span><span class=line><span class=cl>    <span class=c1># offline inputs. Note that postprocessing will be done using the *current*</span>
</span></span><span class=line><span class=cl>    <span class=c1># policy, not the *behaviour* policy, which is typically undesirable for</span>
</span></span><span class=line><span class=cl>    <span class=c1># on-policy algorithms.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;postprocess_inputs&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># If positive, input batches will be shuffled via a sliding window buffer</span>
</span></span><span class=line><span class=cl>    <span class=c1># of this number of batches. Use this if the input data is not in random</span>
</span></span><span class=line><span class=cl>    <span class=c1># enough order. Input is delayed until the shuffle buffer is filled.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;shuffle_buffer_size&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># Specify where experiences should be saved:</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - None: don&#39;t save any experiences</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - &#34;logdir&#34; to save to the agent log dir</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - a path/URI to save to a custom output directory (e.g., &#34;s3://bucket/&#34;)</span>
</span></span><span class=line><span class=cl>    <span class=c1>#  - a function that returns a rllib.offline.OutputWriter</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;output&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># What sample batch columns to LZ4 compress in the output data.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;output_compress_columns&#34;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&#34;obs&#34;</span><span class=p>,</span> <span class=s2>&#34;new_obs&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=c1># Max output file size before rolling over to a new file.</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;output_max_file_size&#34;</span><span class=p>:</span> <span class=mi>64</span> <span class=o>*</span> <span class=mi>1024</span> <span class=o>*</span> <span class=mi>1024</span><span class=p>,</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># =<mark> Multiagent </mark>=</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;multiagent&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1># Map from policy ids to tuples of (policy_cls, obs_space,</span>
</span></span><span class=line><span class=cl>        <span class=c1># act_space, config). See rollout_worker.py for more info.</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;policies&#34;</span><span class=p>:</span> <span class=p>{},</span>
</span></span><span class=line><span class=cl>        <span class=c1># Function mapping agent ids to policy ids.</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;policy_mapping_fn&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=c1># Optional whitelist of policies to train, or None for all policies.</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;policies_to_train&#34;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/Python/ data-ctx=Ray data-src=/Python class=internal-link>Python目录</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://jieye-ericx.github.io/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Jieye ericx using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://jieye-ericx.github.io/>Home</a></li><li><a href=https://github.com/jieye-ericx>Github</a></li></ul></footer></div></div></body></html>