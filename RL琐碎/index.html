<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="基础 episode：从一个游戏开始到结束，叫做一个episode
loss:可以作为负的reward
Monte-Carlo(MC):
分类  是否理解环境？
不理解环境：不尝试去理解环境，环境给什么就是什么 Model-free
理解环境：为真实世界建模 Model-based
Model-based 就是在model free的基础上多一个虚拟环境 基于Policy-Based与基于Value-Based
基于概率：直接输出下一步要采取动作的概率，根据概率选取行动，可以支持连续动作 Policy Gradients
基于价值（连续动作无能为力）：而基于价值的方法输出则是所有动作的价值, 根据最高价值来选着动作 Q-Learning Sarsa"><meta property="og:title" content="基础"><meta property="og:description" content="基础 episode：从一个游戏开始到结束，叫做一个episode
loss:可以作为负的reward
Monte-Carlo(MC):
分类  是否理解环境？
不理解环境：不尝试去理解环境，环境给什么就是什么 Model-free
理解环境：为真实世界建模 Model-based
Model-based 就是在model free的基础上多一个虚拟环境 基于Policy-Based与基于Value-Based
基于概率：直接输出下一步要采取动作的概率，根据概率选取行动，可以支持连续动作 Policy Gradients
基于价值（连续动作无能为力）：而基于价值的方法输出则是所有动作的价值, 根据最高价值来选着动作 Q-Learning Sarsa"><meta property="og:type" content="website"><meta property="og:image" content="https://jieye-ericx.github.io/icon.png"><meta property="og:url" content="https://jieye-ericx.github.io/RL%E7%90%90%E7%A2%8E/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="基础"><meta name=twitter:description content="基础 episode：从一个游戏开始到结束，叫做一个episode
loss:可以作为负的reward
Monte-Carlo(MC):
分类  是否理解环境？
不理解环境：不尝试去理解环境，环境给什么就是什么 Model-free
理解环境：为真实世界建模 Model-based
Model-based 就是在model free的基础上多一个虚拟环境 基于Policy-Based与基于Value-Based
基于概率：直接输出下一步要采取动作的概率，根据概率选取行动，可以支持连续动作 Policy Gradients
基于价值（连续动作无能为力）：而基于价值的方法输出则是所有动作的价值, 根据最高价值来选着动作 Q-Learning Sarsa"><meta name=twitter:image content="https://jieye-ericx.github.io/icon.png"><title>基础</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://jieye-ericx.github.io//icon.png><link href=https://jieye-ericx.github.io/styles.80333fa2099c0bee674efa435fde378c.min.css rel=stylesheet><link href=https://jieye-ericx.github.io/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://jieye-ericx.github.io/js/darkmode.48459b7116d092b4e98d2cab704cad80.min.js></script>
<script src=https://jieye-ericx.github.io/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://jieye-ericx.github.io/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://jieye-ericx.github.io/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://jieye-ericx.github.io/",fetchData=Promise.all([fetch("https://jieye-ericx.github.io/indices/linkIndex.a91535da5bc472c24479fcf5acf89c4d.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://jieye-ericx.github.io/indices/contentIndex.b8152005f0966b730b62085a4100f699.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://jieye-ericx.github.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://jieye-ericx.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/jieye-ericx.github.io\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=jieye-ericx.github.io src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://jieye-ericx.github.io/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://jieye-ericx.github.io/>jieye の 数字花园</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>基础</h1><p class=meta>Last updated
Dec 12, 2021</p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><ol><li><a href=#model-based-rl>Model-based RL</a></li><li><a href=#model-free-rl>Model free RL</a></li><li><a href=#policy-basedvalue-based>Policy-Based&Value-Based</a></li><li><a href=#on-policyoff-policy>On-policy&Off-policy</a></li></ol></li></ol><ol><li><ol><li><a href=#q-learning>Q-learning</a></li><li><a href=#dqn>DQN</a></li><li><a href=#policy-gradient>Policy Gradient</a></li><li><a href=#dpg>DPG</a></li><li><a href=#ddpg>DDPG</a></li><li><a href=#maddpg>MADDPG</a></li></ol></li></ol></nav></details></aside><a href=#基础><h1 id=基础><span class=hanchor arialabel=Anchor># </span>基础</h1></a><p><strong>episode</strong>：从一个游戏开始到结束，叫做一个episode</p><p><strong>loss</strong>:可以作为负的reward</p><p><strong>Monte-Carlo(MC)</strong>:</p><a href=#分类><h1 id=分类><span class=hanchor arialabel=Anchor># </span>分类</h1></a><ol><li>是否理解环境？<br>不理解环境：不尝试去理解环境，环境给什么就是什么 Model-free<br>理解环境：为真实世界建模 Model-based<br>Model-based 就是在model free的基础上多一个虚拟环境</li><li>基于Policy-Based与基于Value-Based<br>基于概率：直接输出下一步要采取动作的概率，根据概率选取行动，可以支持连续动作 Policy Gradients<br>基于价值（连续动作无能为力）：而基于价值的方法输出则是所有动作的价值, 根据最高价值来选着动作 Q-Learning Sarsa<br>基于价值的谁价值高选谁，基于概率根据概率执行动作</li><li>更新<br>回合更新：回合结束后更新行为准则 基础Policy Gradients Monte-carlo Learning<br>单步更新：每一步都更新准则 Q-Learning Sarsa 升级版Policy Gradients</li><li>在线与否<br>在线学习 边玩边学，sarsa、sarsa(lambda)<br>离线学习 学完再玩，Q Learning、Deep Q Network</li></ol><p><img src=https://jieye-ericx.github.io//watermarktype_ZmFuZ3poZW5naGVpdGkshadow_10text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RheWRheWp1bXAsize_16color_FFFFFFt_70.png width=auto alt=img></p><p>现在，如果我们知道MDP中的所有东西，那么我们可以不用在环境中做出动作便可直接求解，我们通常称在执行动作前作出的决策为规划(planning)，那么一些经典的规划算法能够直接求解MDP问题，包括值迭代和策略迭代等。</p><p>那么，当agent不知道转移概率函数TT和奖励函数RR，它是如何找到一个好的策略的呢，当然会有很多方法：</p><a href=#model-based-rl><h3 id=model-based-rl><span class=hanchor arialabel=Anchor># </span>Model-based RL</h3></a><p>一种方法就是Model-based方法，让agent学习一种模型，这种模型能够从它的观察角度描述环境是如何工作的，然后利用这个模型做出动作规划，具体来说，当agent处于s1s1状态，执行了a1a1动作，然后观察到了环境从s1s1转化到了s2s2以及收到的奖励rr, 那么这些信息能够用来提高它对T(s2|s1,a1)T(s2|s1,a1)和R(s1,a1)R(s1,a1)的估计的准确性，当agent学习的模型能够非常贴近于环境时，它就可以直接通过一些规划算法来找到最优策略，具体来说：当agent已知任何状态下执行任何动作获得的回报，即R(st,at)R(st,at)已知，而且下一个状态也能通过T(st+1|st,at)T(st+1|st,at)被计算，那么这个问题很容易就通过动态规划算法求解，尤其是当T(st+1|st,at)＝1T(st+1|st,at)＝1时，直接利用贪心算法，每次执行只需选择当前状态stst下回报函数取最大值的动作(maxaR(s,a|s=st)maxaR(s,a|s=st))即可，这种采取对环境进行建模的强化学习方法就是Model-based方法</p><a href=#model-free-rl><h3 id=model-free-rl><span class=hanchor arialabel=Anchor># </span>Model free RL</h3></a><p>但是，事实证明，我们有时候并不需要对环境进行建模也能找到最优的策略，一种经典的例子就是Q-learning，Q-learning直接对未来的回报Q(s,a)Q(s,a)进行估计，Q(sk,ak)Q(sk,ak)表示对sksk状态下执行动作atat后获得的未来收益总和E(∑nt=kγkRk)E(∑t=knγkRk)的估计，若对这个Q值估计的越准确，那么我们就越能确定如何选择当前stst状态下的动作：选择让Q(st,at)Q(st,at)最大的atat即可，而Q值的更新目标由Bellman方程定义，更新的方式可以有TD（Temporal Difference）等，这种是基于值迭代的方法，类似的还有基于策略迭代的方法以及结合值迭代和策略迭代的actor-critic方法，基础的策略迭代方法一般回合制更新（Monte Carlo Update），这些方法由于没有去对环境进行建模，因此他们都是Model-free的方法</p><p>所以，如果你想查看这个强化学习算法是model-based还是model-free的，你就问你自己这个问题：在agent执行它的动作之前，它是否能对下一步的状态和回报做出预测，如果可以，那么就是model-based方法，如果不能，即为model-free方法。</p><a href=#policy-basedvalue-based><h3 id=policy-basedvalue-based><span class=hanchor arialabel=Anchor># </span>Policy-Based&Value-Based</h3></a><p>Policy-Based的方法直接输出下一步动作的概率，根据概率来选取动作。但不一定概率最高就会选择该动作，还是会从整体进行考虑。适用于非连续和连续的动作。常见的方法有<strong>policy gradients</strong>。</p><p>Value-Based的方法输出的是动作的价值，选择价值最高的动作。适用于非连续的动作。常见的方法有<strong>Q-learning</strong>和Sarsa。</p><img src=../../pics/image-20211113200432205.png alt=image-20211113200432205 style=zoom:50%><p>更为厉害的方法是二者的结合：Actor-Critic，Actor根据概率做出动作，Critic根据动作给出价值，从而加速学习过程。</p><img src=../../pics/image-20211113200459813.png alt=image-20211113200459813 style=zoom:20%>
<a href=#on-policyoff-policy><h3 id=on-policyoff-policy><span class=hanchor arialabel=Anchor># </span>On-policy&Off-policy</h3></a><p>更新值函数时是否<strong>只使用</strong>当前策略所产生的样本</p><p><strong>Q-learning, Deterministic policy gradient是Off-police算法</strong>,这是因为他们更新值函数时,不一定使用当前策略$\pi_t$产生的样本. 可以回想DQN算法,其包含一个replay memory.这个经验池中存储的是很多历史样本(包含$\pi_1 ,\pi_2,…,\pi_t$的样本 ),而更新Q函数时的target用的样本是从这些样本中采样而来,因此,其并不一定使用当前策略的样本.</p><p>Reinforce, trpo, sarsa都是On-policy,这是因为他们更新值函数时,只能使用当前策略产生的样本.具体的,reinforce的梯度更新公式中
<img src=https://jieye-ericx.github.io//image-20211113205001938.png width=auto alt=image-20211113205001938> ,这里的R就是整个episode的累积奖赏,它用到的样本必然只是来自于$\pi_t$。</p><a href=#算法><h1 id=算法><span class=hanchor arialabel=Anchor># </span>算法</h1></a><a href=#q-learning><h3 id=q-learning><span class=hanchor arialabel=Anchor># </span>Q-learning</h3></a><p>Q-learning算法最主要的就是Q表格,里面存着每个状态的动作价值。然后用Q表格用来指导每一步的动作。并且每走一步,就更新一次Q表格,也就是说用下一个状态的Q值去更新当前状态的Q值。</p><a href=#dqn><h3 id=dqn><span class=hanchor arialabel=Anchor># </span>DQN</h3></a><p>Deep Q Network(DQN)的本质其实是Q-learning算法,改进就是把Q表格换成了神经网络,向神经网络输入状态state,就能输出所有状态对应的动作action。</p><a href=#policy-gradient><h3 id=policy-gradient><span class=hanchor arialabel=Anchor># </span>Policy Gradient</h3></a><p>在讲PG算法前,我们需要知道的是,在强化学习中,有两大类方法,一种基于值（Value-based）,一种基于策略（Policy-based）:</p><p><img src=https://jieye-ericx.github.io//watermarktype_ZmFuZ3poZW5naGVpdGkshadow_10text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3picF8xMjEzOAsize_16color_FFFFFFt_70.png width=auto alt=在这里插入图片描述></p><p>Value-based的算法的典型代表为Q-learning和SARSA,将Q函数优化到最优,再根据Q函数取最优策略;Policy-based的算法的典型代表为Policy Gradient,直接优化策略函数。</p><blockquote><p>可以举一个例子区分这两种方法:如果用DQN玩剪刀石头布这种随机性很大的游戏,很可能训练到最后,一直输出同一个动作;但是用Policy Gradient的话,优化到最后就会发现三个动作的概率都是一样的。</p></blockquote><p>可以通过类比监督学习的方式来理解Policy Gradient。向神经网络输入状态state,输出的是每个动作的概率,然后选择概率最高的动作作为输出。训练时,要不断地优化概率,尽可能地使输出值的概率逼近1。</p><a href=#dpg><h3 id=dpg><span class=hanchor arialabel=Anchor># </span>DPG</h3></a><p>Deterministic policy gradient(DPG)算法可以理解为PG+DQN,它是首次能处理确定性的连续动作空间问题的算法。要学习DPG算法,就要知道<strong>Actor-Critic结构</strong>,Actor的前生是Policy Gradient,可以在连续动作空间内选择合适的动作action;Critic的前生是DQN或者其他的以值为基础的算法，可以进行单步更新，效率更高。Actor基于概率分布选择行为,Critic基于Actor生成的行为评判得分,Actor再根据Critic的评分修改选行为的概率。DPG就是在Actor-Critic结构上做的改进,让Actor输出的action是确定值而不是概率分布。</p><a href=#ddpg><h3 id=ddpg><span class=hanchor arialabel=Anchor># </span>DDPG</h3></a><p>Deep Deterministic Policy Gradient(DDPG)算法可以理解为DPG+DQN。因为Q网络的参数在频繁更新梯度的同时，又用于计算Q网络和策略网络的梯度,所以Q网络是不稳定的,所以为了稳定Q网络,DDPG分别给策略网络和Q网络都搭建了一个目标网络,专门用来稳定Q网络:</p><p><img src=https://jieye-ericx.github.io//20200719091044431.png width=auto alt=在这里插入图片描述></p><a href=#maddpg><h3 id=maddpg><span class=hanchor arialabel=Anchor># </span>MADDPG</h3></a><p>Multi-Agent Deep Deterministic Policy Gradient</p><p><img src=https://jieye-ericx.github.io//image-20211113191625880.png width=auto alt=image-20211113191625880></p><p>简单来看,MADDPG其实就是在DDPG的基础上,解决一个环境里存在多个智能体的问题。</p><p>像Q-Learning或者policy gradient都不适用于多智能体环境。主要的问题是,在训练过程中,每个智能体的策略都在变化,因此从每个智能体的角度来看,环境变得十分不稳定,其他智能体的行动带来环境变化:</p><ul><li>对DQN算法来说,经验回放的方法变的不再适用,因为如果不知道其他智能体的状态,那么不同情况下自身的状态转移会不同。</li><li>对PG算法来说,环境的不断变化导致了学习的方差进一步增大。</li></ul><p><strong>在单智能体强化学习中,智能体所在的环境是稳定不变的,但是在多智能体强化学习中,环境是复杂的、动态的</strong>,因此给学习过程带来很大的困难。我理解的多智能体环境是一个环境下存在多个智能体,并且每个智能体都要互相学习,合作或者竞争。</p><blockquote><p>比较有意思的环境是OpenAI的捉迷藏环境，主要讲的是两队开心的小朋友agents在玩捉迷藏游戏中经过训练逐渐学到的各种策略:</p><p><img src=https://jieye-ericx.github.io//watermarktype_ZmFuZ3poZW5naGVpdGkshadow_10text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3picF8xMjEzOAsize_16color_FFFFFFt_70-20211113192651170.png width=auto alt=在这里插入图片描述></p></blockquote></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/Python/ data-ctx=RL琐碎 data-src=/Python class=internal-link>Python目录</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://jieye-ericx.github.io/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Jieye ericx using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2024</p><ul><li><a href=https://jieye-ericx.github.io/>Home</a></li><li><a href=https://github.com/jieye-ericx>Github</a></li></ul></footer></div></div></body></html>